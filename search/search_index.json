{
    "docs": [
        {
            "location": "/",
            "text": "Introduction \n\u00b6\n\n\n\n\nWhat is BOSH?\n\n\nWhat problems does BOSH solve?\n\n\nStemcell\n\n\nRelease\n\n\nDeployment\n\n\n\n\n\n\nComparison to other tools\n\n\n\n\n\n\nGeneral architecture\n\n\nPersonas\n\n\nTerminology\n\n\n\n\n\n\nInstall BOSH \n\u00b6\n\n\n\n\nCreate an environment\n\n\nOn Local machine (BOSH Lite)\n\n\nOn AWS\n\n\nExpose Director on a Public IP\n\n\nOn Azure\n\n\nOn OpenStack\n\n\nOn vSphere\n\n\nOn vCloud\n\n\nOn SoftLayer\n\n\nOn Google Compute Platform\n\n\nOn RackHD\n\n\n\n\n\n\n\n\n\n\nDeploy software with BOSH \n\u00b6\n\n\n\n\nDeploy workflow\n\n\nUpdate cloud config\n\n\nBuild deployment manifest\n\n\nUpload stemcells\n\n\nUpload releases\n\n\nDeploy\n\n\nRun one-off tasks\n\n\nUpdate deployment\n\n\n\n\n\n\n\n\nCLI v2+ \n\u00b6\n\n\n\n\nCommand reference\n\n\ncreate-env\n Dependencies\n\n\nDifferences between CLI v2+ vs v1\n\n\nGlobal Flags\n\n\nEnvironments\n\n\nOperations files\n\n\nVariable Interpolation\n\n\nTunneling\n\n\n\n\n\n\n\n\nRunning Director \n\u00b6\n\n\n\n\nTroubleshooting\n\n\nEvents\n\n\nDirector tasks\n\n\nManaging releases\n\n\nManaging stemcells\n\n\nUser management\n\n\nUAA Integration\n\n\nUAA Permissions\n\n\n\n\n\n\nCredHub Integration\n\n\nVariable Types\n\n\n\n\n\n\nBackup and restore\n\n\n\n\nMisc\n\u00b6\n\n\n\n\nDeploying step-by-step\n\n\nSSL certificate configuration\n\n\nRemoval of compilers\n\n\nAccess event logging\n\n\nExplicit ARP Flushing\n\n\nConfiguring external database\n\n\nConfiguring external blobstore\n\n\n\n\nDetailed deployment configuration \n\u00b6\n\n\n\n\nManifest v2 schema\n\n\nLinks\n\n\nLink properties\n\n\nManual linking\n\n\nRenaming/migrating instance groups\n\n\n\n\n\n\nPersistent and orphaned disks\n\n\nCustomizing persistent disk FS\n\n\n\n\n\n\nConfigs\n\n\nCloud config\n\n\nAZs\n\n\nNetworks\n\n\nVM anti-affinity\n\n\n\n\n\n\nRuntime config\n\n\nAddons\n\n\nCommon addons\n\n\n\n\n\n\nCPI config\n\n\nTrusted certificates\n\n\nNative DNS Support\n\n\n\n\nDetailed CPI configuration & troubleshooting \n\u00b6\n\n\n\n\nAWS\n\n\nUsing IAM instance profiles\n\n\nUsing instance storage\n\n\nCreating IAM users\n\n\n\n\n\n\nAzure\n\n\nCreating resources\n\n\n\n\n\n\nOpenStack\n\n\nUsing Auto-anti-affinity\n\n\nUsing Keystone v2 API\n\n\nUsing nova-networking\n\n\nExtended Registry configuration\n\n\nHuman-readable VM names\n\n\nValidating self-signed OpenStack endpoints\n\n\nMulti-homed VMs\n\n\nUsing Light Stemcells\n\n\n\n\n\n\nvSphere\n\n\nvSphere HA\n\n\nMigrating from one datastore to another\n\n\nStorage DRS and vMotion Support\n\n\n\n\n\n\nvCloud\n\n\nSoftLayer\n\n\nGoogle Compute Engine\n\n\nRequired service account permissions\n\n\n\n\n\n\nRackHD\n\n\nWarden/Garden\n\n\nVirtualBox\n\n\n\n\nHealth management of VMs and processes \n\u00b6\n\n\n\n\nMonitoring\n\n\nConfiguring Health Monitor\n\n\n\n\n\n\nProcess monitoring with Monit\n\n\nManual repair with Cloud Check\n\n\nAutomatic repair with Resurrector\n\n\nPersistent disk snapshotting\n\n\n\n\nVM configuration (looking inside a deployment) \n\u00b6\n\n\n\n\nStructure of a managed VM\n\n\nVM configuration locations\n\n\n\n\n\n\nLocation and use of logs\n\n\nInstance metadata\n\n\nDebugging issues with jobs\n\n\n\n\n\n\nGuides \n\u00b6\n\n\n\n\nIPv6 on vSphere\n\n\nMulti-CPI\n\n\nOn AWS\n\n\n\n\n\n\nPackage software with BOSH \n\u00b6\n\n\n\n\nWhat is a release?\n\n\nCreating a release\n\n\nTesting with dev releases\n\n\nCutting final releases\n\n\nCompiled releases\n\n\n\n\n\n\nWhat is a job?\n\n\nCreating a job\n\n\nJob Templates\n\n\n\n\n\n\nErrands\n\n\nProperties: Suggested configurations\n\n\nLinks: Common types\n\n\nJob lifecycle\n\n\nPre-start script\n\n\nPost-start script\n\n\nPost-deploy script\n\n\nDrain script\n\n\n\n\n\n\nScheduled processes\n\n\n\n\n\n\nReleases and Jobs in Windows\n\n\nReleases\n\n\nJobs\n\n\nSample BOSH Windows Release\n\n\n\n\n\n\nWhat is a package?\n\n\nCreating a package\n\n\nRelationship to release blobs\n\n\nVendoring packages\n\n\nRelease blobs\n\n\n\n\n\n\nHow do releases, jobs, and packages interact?\n\n\nManaging release repository\n\n\nRelease blobstore\n\n\nConfiguring S3 release blobstore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtend BOSH \n\u00b6\n\n\n\n\nDirector API v1\n\n\nWhat is a CPI?\n\n\nBuilding a CPI\n\n\nCPI API v1\n\n\nAgent-CPI interactions\n\n\n\n\n\n\nBuilding a stemcell\n\n\nRepacking Stemcells\n\n\n\n\n\n\n\n\n\n\nOther \n\u00b6\n\n\nCLI v1 (superseded by \nCLI v2\n) \n\u00b6\n\n\n\n\nInstall BOSH CLI v1\n\n\nCLI v1\n\n\n\n\nManifest v1 \n\u00b6\n\n\n\n\nManifest v1 schema",
            "title": "Welcome"
        },
        {
            "location": "/#introduction",
            "text": "What is BOSH?  What problems does BOSH solve?  Stemcell  Release  Deployment    Comparison to other tools    General architecture  Personas  Terminology",
            "title": "Introduction "
        },
        {
            "location": "/#install-bosh",
            "text": "Create an environment  On Local machine (BOSH Lite)  On AWS  Expose Director on a Public IP  On Azure  On OpenStack  On vSphere  On vCloud  On SoftLayer  On Google Compute Platform  On RackHD",
            "title": "Install BOSH "
        },
        {
            "location": "/#deploy-software-with-bosh",
            "text": "Deploy workflow  Update cloud config  Build deployment manifest  Upload stemcells  Upload releases  Deploy  Run one-off tasks  Update deployment",
            "title": "Deploy software with BOSH "
        },
        {
            "location": "/#cli-v2",
            "text": "Command reference  create-env  Dependencies  Differences between CLI v2+ vs v1  Global Flags  Environments  Operations files  Variable Interpolation  Tunneling",
            "title": "CLI v2+ "
        },
        {
            "location": "/#running-director",
            "text": "Troubleshooting  Events  Director tasks  Managing releases  Managing stemcells  User management  UAA Integration  UAA Permissions    CredHub Integration  Variable Types    Backup and restore",
            "title": "Running Director "
        },
        {
            "location": "/#misc",
            "text": "Deploying step-by-step  SSL certificate configuration  Removal of compilers  Access event logging  Explicit ARP Flushing  Configuring external database  Configuring external blobstore",
            "title": "Misc"
        },
        {
            "location": "/#detailed-deployment-configuration",
            "text": "Manifest v2 schema  Links  Link properties  Manual linking  Renaming/migrating instance groups    Persistent and orphaned disks  Customizing persistent disk FS    Configs  Cloud config  AZs  Networks  VM anti-affinity    Runtime config  Addons  Common addons    CPI config  Trusted certificates  Native DNS Support",
            "title": "Detailed deployment configuration "
        },
        {
            "location": "/#detailed-cpi-configuration-troubleshooting",
            "text": "AWS  Using IAM instance profiles  Using instance storage  Creating IAM users    Azure  Creating resources    OpenStack  Using Auto-anti-affinity  Using Keystone v2 API  Using nova-networking  Extended Registry configuration  Human-readable VM names  Validating self-signed OpenStack endpoints  Multi-homed VMs  Using Light Stemcells    vSphere  vSphere HA  Migrating from one datastore to another  Storage DRS and vMotion Support    vCloud  SoftLayer  Google Compute Engine  Required service account permissions    RackHD  Warden/Garden  VirtualBox",
            "title": "Detailed CPI configuration &amp; troubleshooting "
        },
        {
            "location": "/#health-management-of-vms-and-processes",
            "text": "Monitoring  Configuring Health Monitor    Process monitoring with Monit  Manual repair with Cloud Check  Automatic repair with Resurrector  Persistent disk snapshotting",
            "title": "Health management of VMs and processes "
        },
        {
            "location": "/#vm-configuration-looking-inside-a-deployment",
            "text": "Structure of a managed VM  VM configuration locations    Location and use of logs  Instance metadata  Debugging issues with jobs",
            "title": "VM configuration (looking inside a deployment) "
        },
        {
            "location": "/#guides",
            "text": "IPv6 on vSphere  Multi-CPI  On AWS",
            "title": "Guides "
        },
        {
            "location": "/#package-software-with-bosh",
            "text": "What is a release?  Creating a release  Testing with dev releases  Cutting final releases  Compiled releases    What is a job?  Creating a job  Job Templates    Errands  Properties: Suggested configurations  Links: Common types  Job lifecycle  Pre-start script  Post-start script  Post-deploy script  Drain script    Scheduled processes    Releases and Jobs in Windows  Releases  Jobs  Sample BOSH Windows Release    What is a package?  Creating a package  Relationship to release blobs  Vendoring packages  Release blobs    How do releases, jobs, and packages interact?  Managing release repository  Release blobstore  Configuring S3 release blobstore",
            "title": "Package software with BOSH "
        },
        {
            "location": "/#extend-bosh",
            "text": "Director API v1  What is a CPI?  Building a CPI  CPI API v1  Agent-CPI interactions    Building a stemcell  Repacking Stemcells",
            "title": "Extend BOSH "
        },
        {
            "location": "/#other",
            "text": "",
            "title": "Other "
        },
        {
            "location": "/#cli-v1-superseded-by-cli-v2",
            "text": "Install BOSH CLI v1  CLI v1",
            "title": "CLI v1 (superseded by CLI v2) "
        },
        {
            "location": "/#manifest-v1",
            "text": "Manifest v1 schema",
            "title": "Manifest v1 "
        },
        {
            "location": "/problems/",
            "text": "BOSH allows individual developers and teams to easily version, package and deploy software in a reproducible manner.\n\n\nAny software, whether it is a simple static site or a complex multi-component service, will need to be updated and repackaged at some point. This updated software might need to be deployed to a cluster, or it might need to be packaged for end-users to deploy to their own servers. In a lot of cases, the developers who produced the software will be deploying it to their own production environment. Usually, a team will use a staging, development, or demo environment that is similarly configured to their production environment to verify that updates run as expected. These staging environments are often taxing to build and administer. Maintaining consistency between multiple environments is often painful to manage.\n\n\nDeveloper/operator communities have come far in solving similar situations with tools like Chef, Puppet, and Docker. However, each organization solves these problems in a different way, which usually involves a variety of different, and not necessarily well-integrated, tools. While these tools exist to solve the individual parts of versioning, packaging, and deploying software reproducibly, BOSH was designed to do each of these as a whole.\n\n\nBOSH was purposefully constructed to address the four principles of modern \nRelease Engineering\n in the following ways:\n\n\n\n\nIdentifiability\n: Being able to identify all of the source, tools, environment, and other components that make up a particular release.\n\n\n\n\nBOSH has a concept of a software release which packages up all related source code, binary assets, configuration etc. This allows users to easily track contents of a particular release. In addition to releases BOSH provides a way to capture all Operating System dependencies as one image.\n\n\n\n\nReproducibility\n: The ability to integrate source, third party components, data, and deployment externals of a software system in order to guarantee operational stability.\n\n\n\n\nBOSH tool chain provides a centralized server that manages software releases, Operating System images, persistent data, and system configuration. It provides a clear and simple way of operating a deployed system.\n\n\n\n\nConsistency\n: The mission to provide a stable framework for development, deployment, audit, and accountability for software components.\n\n\n\n\nBOSH software releases workflows are used throughout the development of the software and when the system needs to be deployed. BOSH centralized server allows users to see and track changes made to the deployed system.\n\n\n\n\nAgility\n: The ongoing research into what are the repercussions of modern software engineering practices on the productivity in the software cycle, i.e. continuous integration.\n\n\n\n\nBOSH tool chain integrates well with current best practices of software engineering (including Continuous Delivery) by providing ways to easily create software releases in an automated way and to update complex deployed systems with simple commands.\n\n\n\n\nNext: \nWhat is a Stemcell?\n\n\nPrevious: \nWhat is BOSH?",
            "title": "Project Goals"
        },
        {
            "location": "/community/",
            "text": "Community\n\u00b6\n\n\nBOSH is part of the open-source community, so you can easily find us in a few places...\n\n\nSlack\n\u00b6\n\n\nThe \nCloud Foundry Slack\n is a great place to ask questions or discuss issues - especially if you are still trying to figure out what might actually be wrong. Contributors, operators, and developers of BOSH are typically hanging out in the \n#bosh\n channel and happy to help answer questions.\n\n\n\n\nSlack Invitation\n\n\nSlack requires members to be invited, so please \nrequest an invite\n from our bot first if you are new to the community.\n\n\n\n\nGitHub\n\u00b6\n\n\nBOSH is open source, so you can find most of the code in either the \ncloudfoundry\n or \ncloudfoundry-incubator\n GitHub organizations. If you are looking for a repository to start with, \ncloudfoundry/bosh\n is a great place to start.\n\n\nSince BOSH is a larger project, there are quite a few repositories for the various components. If you are not sure which repository is best to discuss an issue or make a change, please feel free to ask! \nSlack\n usually works best for that, but you can also just \ncreate an issue\n if that's easier.\n\n\nMailing List\n\u00b6\n\n\nIf you prefer mailing lists, you can find us through the \ncf-bosh\n mailing list\n. Although it is quieter than \nSlack\n, the community is usually able to help answer questions raised on the list. We also occasionally use this for announcements or product discussion.\n\n\nAdditional Resources\n\u00b6\n\n\n\n\nUltimate Guide to BOSH\n - a great resource for learning more about BOSH concepts",
            "title": "Community"
        },
        {
            "location": "/community/#community",
            "text": "BOSH is part of the open-source community, so you can easily find us in a few places...",
            "title": "Community"
        },
        {
            "location": "/community/#slack",
            "text": "The  Cloud Foundry Slack  is a great place to ask questions or discuss issues - especially if you are still trying to figure out what might actually be wrong. Contributors, operators, and developers of BOSH are typically hanging out in the  #bosh  channel and happy to help answer questions.   Slack Invitation  Slack requires members to be invited, so please  request an invite  from our bot first if you are new to the community.",
            "title": "Slack"
        },
        {
            "location": "/community/#github",
            "text": "BOSH is open source, so you can find most of the code in either the  cloudfoundry  or  cloudfoundry-incubator  GitHub organizations. If you are looking for a repository to start with,  cloudfoundry/bosh  is a great place to start.  Since BOSH is a larger project, there are quite a few repositories for the various components. If you are not sure which repository is best to discuss an issue or make a change, please feel free to ask!  Slack  usually works best for that, but you can also just  create an issue  if that's easier.",
            "title": "GitHub"
        },
        {
            "location": "/community/#mailing-list",
            "text": "If you prefer mailing lists, you can find us through the  cf-bosh  mailing list . Although it is quieter than  Slack , the community is usually able to help answer questions raised on the list. We also occasionally use this for announcements or product discussion.",
            "title": "Mailing List"
        },
        {
            "location": "/community/#additional-resources",
            "text": "Ultimate Guide to BOSH  - a great resource for learning more about BOSH concepts",
            "title": "Additional Resources"
        },
        {
            "location": "/cli-v2-install/",
            "text": "Installing the CLI\n\u00b6\n\n\nThe \nbosh\n CLI is the command line tool used for interacting with all things BOSH - whether it is deployment operations or software release management.\n\n\nInstall\n\u00b6\n\n\nChoose your preferred installation method below to get the latest version of \nbosh\n...\n\n\nUsing curl\n\u00b6\n\n\nTo install the \nbosh\n binary directly, choose the correct download for your system:\n\n\n\n\n\n\n\n\nSystem\n\n\nDownload\n\n\nChecksum (SHA1)\n\n\n\n\n\n\n\n\n\n\nDarwin / macOS\n\n\nbosh-cli-3.0.1-darwin-amd64\n\n\nd2fea20210a47b8c8f1f7dbb27ffb5808d47ce87\n\n\n\n\n\n\nLinux\n\n\nbosh-cli-3.0.1-linux-amd64\n\n\nccc893bab8b219e9e4a628ed044ebca6c6de9ca0\n\n\n\n\n\n\nWindows\n\n\nbosh-cli-3.0.1-windows-amd64.exe\n\n\n41c23c90cab9dc62fa0a1275dcaf64670579ed33\n\n\n\n\n\n\n\n\n\n\n\n\nDownload the binary (this example is using Linux):\n\n\ncurl -LOo ./bosh https://s3.amazonaws.com/bosh-cli-artifacts/bosh-cli-3.0.1-linux-amd64\n\n\n\n\n\n\n\n\nMake the \nbosh\n binary executable:\n\n\nchmod +x ./bosh\n\n\n\n\n\n\n\n\nMove the binary to your \nPATH\n:\n\n\nsudo mv ./bosh /usr/local/bin/bosh\n\n\n\n\n\n\n\n\nVerify that \nbosh -v\n shows the installed version and a \nSucceeded\n message:\n\n\n$ bosh -v\nversion \n3\n.0.1-712bfd7-2018-03-13T23:26:42Z\n\nSucceeded\n\n\n\n\n\n\n\n\nUsing Homebrew on macOS\n\u00b6\n\n\nIf you are on macOS with \nHomebrew\n, you can install using the \nCloud Foundry tap\n.\n\n\n\n\n\n\nUse \nbrew\n to install \nbosh-cli\n:\n\n\nbrew install cloudfoundry/tap/bosh-cli\n\n\n\n\n\n\n\n\nVerify that \nbosh -v\n shows the installed version and a \nSucceeded\n message:\n\n\n$ bosh -v\nversion \n3\n.0.1-712bfd7-2018-03-13T23:26:42Z\n\nSucceeded\n\n\n\n\n\n\n\n\nAdditional Dependencies\n\u00b6\n\n\nWhen you are using \nbosh\n to bootstrap BOSH or other standalone VMs, you will need a few extra dependencies installed on your local system.\n\n\n\n\nTip\n\n\nIf you will not be using \ncreate-env\n and \ndelete-env\n commands, you can skip this section.\n\n\n\n\nUbuntu Trusty\n\u00b6\n\n\nIf you are running on Ubuntu Trusty, ensure the following packages are installed on your system:\n\n\nsudo apt-get install -y build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt-dev libxml2-dev libssl-dev libreadline6 libreadline6-dev libyaml-dev libsqlite3-dev sqlite3\n\n\n\n\nmacOS\n\u00b6\n\n\n\n\n\n\nInstall the \nApple Command Line Tools\n:\n\n\nxcode-select --install\n\n\n\n\n\n\n\n\nUse \nHomebrew\n to additionally install OpenSSL:\n\n\nbrew install openssl\n\n\n\n\n\n\n\n\nCentOS\n\u00b6\n\n\nIf you are running on CentOS, ensure the following packages are installed on your system:\n\n\n$ sudo yum install gcc gcc-c++ ruby ruby-devel mysql-devel postgresql-devel postgresql-libs sqlite-devel libxslt-devel libxml2-devel patch openssl\n$ gem install yajl-ruby\n\n\n\n\nWindows\n\u00b6\n\n\nThe \ncreate-env\n and \ndestroy-env\n commands are not yet supported on Windows. Feel free to give it a try (and let us know if you have feedback), but we would recommend using a Linux-based virtual machine to run those commands instead.\n\n\nOther\n\u00b6\n\n\nYou should be able to use \nbosh\n on other systems... we just don't know the exact packages to recommend. In general, use these recommendations (and send us a pull request to update this page once you figure it out!):\n\n\n\n\ncompilation tools (often a \nbuild-essential\n-like package or \"Development Tools\"-like group)\n\n\nRuby v2.4+",
            "title": "Installing the CLI"
        },
        {
            "location": "/cli-v2-install/#installing-the-cli",
            "text": "The  bosh  CLI is the command line tool used for interacting with all things BOSH - whether it is deployment operations or software release management.",
            "title": "Installing the CLI"
        },
        {
            "location": "/cli-v2-install/#install",
            "text": "Choose your preferred installation method below to get the latest version of  bosh ...",
            "title": "Install"
        },
        {
            "location": "/cli-v2-install/#using-curl",
            "text": "To install the  bosh  binary directly, choose the correct download for your system:     System  Download  Checksum (SHA1)      Darwin / macOS  bosh-cli-3.0.1-darwin-amd64  d2fea20210a47b8c8f1f7dbb27ffb5808d47ce87    Linux  bosh-cli-3.0.1-linux-amd64  ccc893bab8b219e9e4a628ed044ebca6c6de9ca0    Windows  bosh-cli-3.0.1-windows-amd64.exe  41c23c90cab9dc62fa0a1275dcaf64670579ed33       Download the binary (this example is using Linux):  curl -LOo ./bosh https://s3.amazonaws.com/bosh-cli-artifacts/bosh-cli-3.0.1-linux-amd64    Make the  bosh  binary executable:  chmod +x ./bosh    Move the binary to your  PATH :  sudo mv ./bosh /usr/local/bin/bosh    Verify that  bosh -v  shows the installed version and a  Succeeded  message:  $ bosh -v\nversion  3 .0.1-712bfd7-2018-03-13T23:26:42Z\n\nSucceeded",
            "title": "Using curl"
        },
        {
            "location": "/cli-v2-install/#using-homebrew-on-macos",
            "text": "If you are on macOS with  Homebrew , you can install using the  Cloud Foundry tap .    Use  brew  to install  bosh-cli :  brew install cloudfoundry/tap/bosh-cli    Verify that  bosh -v  shows the installed version and a  Succeeded  message:  $ bosh -v\nversion  3 .0.1-712bfd7-2018-03-13T23:26:42Z\n\nSucceeded",
            "title": "Using Homebrew on macOS"
        },
        {
            "location": "/cli-v2-install/#additional-dependencies",
            "text": "When you are using  bosh  to bootstrap BOSH or other standalone VMs, you will need a few extra dependencies installed on your local system.   Tip  If you will not be using  create-env  and  delete-env  commands, you can skip this section.",
            "title": "Additional Dependencies"
        },
        {
            "location": "/cli-v2-install/#ubuntu-trusty",
            "text": "If you are running on Ubuntu Trusty, ensure the following packages are installed on your system:  sudo apt-get install -y build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt-dev libxml2-dev libssl-dev libreadline6 libreadline6-dev libyaml-dev libsqlite3-dev sqlite3",
            "title": "Ubuntu Trusty"
        },
        {
            "location": "/cli-v2-install/#macos",
            "text": "Install the  Apple Command Line Tools :  xcode-select --install    Use  Homebrew  to additionally install OpenSSL:  brew install openssl",
            "title": "macOS"
        },
        {
            "location": "/cli-v2-install/#centos",
            "text": "If you are running on CentOS, ensure the following packages are installed on your system:  $ sudo yum install gcc gcc-c++ ruby ruby-devel mysql-devel postgresql-devel postgresql-libs sqlite-devel libxslt-devel libxml2-devel patch openssl\n$ gem install yajl-ruby",
            "title": "CentOS"
        },
        {
            "location": "/cli-v2-install/#windows",
            "text": "The  create-env  and  destroy-env  commands are not yet supported on Windows. Feel free to give it a try (and let us know if you have feedback), but we would recommend using a Linux-based virtual machine to run those commands instead.",
            "title": "Windows"
        },
        {
            "location": "/cli-v2-install/#other",
            "text": "You should be able to use  bosh  on other systems... we just don't know the exact packages to recommend. In general, use these recommendations (and send us a pull request to update this page once you figure it out!):   compilation tools (often a  build-essential -like package or \"Development Tools\"-like group)  Ruby v2.4+",
            "title": "Other"
        },
        {
            "location": "/quick-start/",
            "text": "The easiest ways to get started with BOSH is by running on your local workstation with \nVirtualBox\n. If you are interested in bringing up a director in another environment, like \nGoogle Cloud Platform\n, choose your IaaS from the navigation for more detailed instructions.\n\n\nPrerequisites\n\u00b6\n\n\nBefore trying to deploy the Director, make sure you have satisfied the following requirements:\n\n\n\n\n\n\nFor best performance, ensure you have at least 8GB RAM and 50GB of free disk space.\n\n\n\n\n\n\nInstall the \nbosh\n CLI\n and its \nadditional dependencies\n.\n\n\n\n\n\n\nInstall \nVirtualBox\n.\n\n\n\n\n\n\nInstall\n\u00b6\n\n\nFirst, create a workspace for our \nvirtualbox\n environment. This directory will keep some state and configuration files that we will need.\n\n\n$ mkdir -p ~/bosh-env/virtualbox\n$ \ncd\n ~/bosh-env/virtualbox\n\n\n\n\nNext, we'll use \nbosh-deployment\n, the recommended installation method, to bootstrap our director.\n\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment.git\n\n\n\n\nNow, we can run the \nvirtualbox/create-env.sh\n script to create our test director and configure the environment with some defaults.\n\n\n$ ./bosh-deployment/virtualbox/create-env.sh\n\n\n\n\nDuring the bootstrap process, you will see a few stages:\n\n\n\n\nCreating BOSH Director - dependencies are downloaded, the VM is created, and BOSH is installed, configured, and started.\n\n\nAdding Network Routes - a route to the virtual network is added to ensure you will be able to connect to BOSH-managed VMs.\n\n\nGenerating \n.envrc\n - a settings file is generated so you can easily connect to the environment later.\n\n\nConfiguring Environment Alias - an alias is added for the \nbosh\n command so you can reference the environment as \nvbox\n.\n\n\nUpdating Cloud Config - default settings are applied to the Director so you easily deploy software later.\n\n\n\n\nAfter a few moments, BOSH should be started. To verify, first load your connection settings, and then run your first \nbosh\n command where you should see similar output.\n\n\n$ \nsource\n .envrc\n$ bosh -e vbox env\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nName      bosh-lite\nUUID      7ce65259-471a-424b-88cb-9d3cee85db2c\nVersion   \n265\n.2.0 \n(\n00000000\n)\n\nCPI       warden_cpi\nUser      admin\n\n\n\n\nCongratulations - BOSH is running! Now you're ready to \ndeploy\n\n\n\n\nTroubleshooting\n\n\nIf you run into any trouble, please continue to the \nVirtualBox Troubleshooting\n section.\n\n\n\n\nDeploy\n\u00b6",
            "title": "Quick Start"
        },
        {
            "location": "/quick-start/#prerequisites",
            "text": "Before trying to deploy the Director, make sure you have satisfied the following requirements:    For best performance, ensure you have at least 8GB RAM and 50GB of free disk space.    Install the  bosh  CLI  and its  additional dependencies .    Install  VirtualBox .",
            "title": "Prerequisites"
        },
        {
            "location": "/quick-start/#install",
            "text": "First, create a workspace for our  virtualbox  environment. This directory will keep some state and configuration files that we will need.  $ mkdir -p ~/bosh-env/virtualbox\n$  cd  ~/bosh-env/virtualbox  Next, we'll use  bosh-deployment , the recommended installation method, to bootstrap our director.  $ git clone https://github.com/cloudfoundry/bosh-deployment.git  Now, we can run the  virtualbox/create-env.sh  script to create our test director and configure the environment with some defaults.  $ ./bosh-deployment/virtualbox/create-env.sh  During the bootstrap process, you will see a few stages:   Creating BOSH Director - dependencies are downloaded, the VM is created, and BOSH is installed, configured, and started.  Adding Network Routes - a route to the virtual network is added to ensure you will be able to connect to BOSH-managed VMs.  Generating  .envrc  - a settings file is generated so you can easily connect to the environment later.  Configuring Environment Alias - an alias is added for the  bosh  command so you can reference the environment as  vbox .  Updating Cloud Config - default settings are applied to the Director so you easily deploy software later.   After a few moments, BOSH should be started. To verify, first load your connection settings, and then run your first  bosh  command where you should see similar output.  $  source  .envrc\n$ bosh -e vbox env\nUsing environment  '192.168.50.6'  as client  'admin' \n\nName      bosh-lite\nUUID      7ce65259-471a-424b-88cb-9d3cee85db2c\nVersion    265 .2.0  ( 00000000 ) \nCPI       warden_cpi\nUser      admin  Congratulations - BOSH is running! Now you're ready to  deploy   Troubleshooting  If you run into any trouble, please continue to the  VirtualBox Troubleshooting  section.",
            "title": "Install"
        },
        {
            "location": "/quick-start/#deploy",
            "text": "",
            "title": "Deploy"
        },
        {
            "location": "/init-aws/",
            "text": "This document shows how to set up new \nenvironment\n on Amazon Web Services (AWS).\n\n\nStep 1: Prepare an AWS Account\n\u00b6\n\n\nIf you do not have an AWS account, \ncreate one\n.\n\n\nTo configure your AWS account:\n\n\n\n\nObtain AWS credentials\n\n\nCreate a Virtual Private Cloud (VPC)\n\n\nCreate an Elastic IP\n\n\nCreate a Key Pair\n\n\nCreate and Configure Security Group\n\n\n\n\n\n\nObtain AWS Credentials \n\u00b6\n\n\nYour AWS credentials consist of an Access Key ID and a Secret Access Key. Follow \nCreating IAM Users\n to create a new IAM user.\n\n\n\n\nCreate a Virtual Private Cloud (VPC) \n\u00b6\n\n\n\n\n\n\nIn the upper-right corner of the AWS Console, select a Region.\n\n\n\n\n\n\n\n\nOn the AWS Console, select \nVPC\n to get to the VPC Dashboard.\n\n\n\n\n\n\n\n\nClick \nStart VPC Wizard\n.\n\n\n\n\n\n\n\n\nSelect \nVPC with a Single Public Subnet\n and click \nSelect\n.\n\n\n\n\n\n\nComplete the VPC form with the following information:\n\n\n\n\nIP CIDR block\n: 10.0.0.0/16\n\n\nVPC name\n: bosh\n\n\nPublic subnet\n: 10.0.0.0/24\n\n\nAvailability Zone\n: us-east-1a\n\n\nSubnet name\n: public\n\n\nEnable DNS hostnames\n: Yes\n\n\nHardware tenancy\n: Default\n\n\n\n\n\n\n\n\n\n\nClick \nCreate VPC\n and click \nOK\n once VPC is successfully created.\n\n\n\n\n\n\nClick \nSubnets\n and locate the \"public\" subnet in the VPC. Replace \nSUBNET-ID\n and \nAVAILABILITY-ZONE\n in your deployment manifest with the \"public\" subnet \nSubnet ID\n, \nAvailability Zone\n and \nRegion\n (AZ without the trailing character).\n\n\n\n\n\n\n\n\n\n\nCreate an Elastic IP \n\u00b6\n\n\n\n\n\n\nOn the VPC Dashboard, click \nElastic IPs\n and click \nAllocate New Address\n.\n\n\n\n\n\n\n\n\nIn the Allocate Address dialog box, click \nYes, Allocate\n.\n\n\n\n\n\n\nReplace \nELASTIC-IP\n in your deployment manifest with the allocated Elastic IP Address.\n\n\n\n\n\n\n\n\n\n\nCreate a Key Pair \n\u00b6\n\n\n\n\n\n\nIn the AWS Console, select \nEC2\n to get to the EC2 Dashboard.\n\n\n\n\n\n\nClick \nKey Pairs\n and click \nCreate Key Pair\n.\n\n\n\n\n\n\n\n\nIn the Create Key Pair dialog box, enter \"bosh\" as the Key Pair name and click \nCreate\n.\n\n\n\n\n\n\n\n\nSave private key to \n~/Downloads/bosh.pem\n.\n\n\n\n\n\n\n\n\nCreate and Configure Security Group \n\u00b6\n\n\n\n\n\n\nOn the EC2 Dashboard, click \nSecurity Groups\n and then click \nCreate Security Group\n.\n\n\n\n\n\n\n\n\nComplete the Create Security Group form with the following information:\n\n\n\n\nSecurity group name\n: bosh\n\n\nDescription\n: BOSH deployed VMs\n\n\nVPC\n: Select the \"bosh\" VPC that you created in \nCreate a Virtual Private Cloud\n.\n\n\n\n\n\n\n\n\nClick \nCreate\n\n\n\n\n\n\n\n\nSelect the created security group with group name \"bosh\", click the \nInbound\n tab and click \nEdit\n.\n\n\n\n\n\n\n\n\nFill out the Edit inbound rules form and click \nSave\n.\n\n\nNote\n: It highly discouraged to run any production environment with \n0.0.0.0/0\n source or to make any BOSH management ports publicly accessible.\n\n\n\n  \n\n    \nType\n\n    \nPort Range\n\n    \nSource\n\n    \nPurpose\n\n  \n\n\nCustom TCP Rule\n22\n(My IP)\nSSH access from CLI\n\n  \nCustom TCP Rule\n6868\n(My IP)\nBOSH Agent access from CLI\n\n  \nCustom TCP Rule\n25555\n(My IP)\nBOSH Director access from CLI\n\n\nAll TCP\n0 - 65535\nID of this security group\nManagement and data access\n\n  \nAll UDP\n0 - 65535\nID of this security group\nManagement and data access\n\n\n\n\nNote\n:  To enter your security group as a \nSource\n, select \nCustom IP\n, and enter \"bosh\". Note: The AWS Console should autocomplete the security group ID (e.g. \"sg-12ab34cd\").\n\n\n\n\n\n\n\n\n\n\nStep 2: Deploy \n\u00b6\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n\n\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/aws/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \naccess_key_id\n=\nAKI... \n\\\n\n    -v \nsecret_access_key\n=\nwfh28... \n\\\n\n    -v \nregion\n=\nus-east-1 \n\\\n\n    -v \naz\n=\nus-east-1a \n\\\n\n    -v \ndefault_key_name\n=\nbosh \n\\\n\n    -v \ndefault_security_groups\n=[\nbosh\n]\n \n\\\n\n    --var-file \nprivate_key\n=\n~/Downloads/bosh.pem \n\\\n\n    -v \nsubnet_id\n=\nsubnet-ait8g34t\n\n\n\n\nIf running above commands outside of an AWS VPC, refer to [Exposing environment on a public IP](init-external-ip.md) for additional CLI flags.\n\nSee [AWS CPI errors](aws-cpi.md#errors) for list of common errors and resolutions.\n\n\n\n\n\n\n\nConnect to the Director.\n\n\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "Amazon Web Services"
        },
        {
            "location": "/init-aws/#obtain-aws-credentials",
            "text": "Your AWS credentials consist of an Access Key ID and a Secret Access Key. Follow  Creating IAM Users  to create a new IAM user.",
            "title": "Obtain AWS Credentials "
        },
        {
            "location": "/init-aws/#create-a-virtual-private-cloud-vpc",
            "text": "In the upper-right corner of the AWS Console, select a Region.     On the AWS Console, select  VPC  to get to the VPC Dashboard.     Click  Start VPC Wizard .     Select  VPC with a Single Public Subnet  and click  Select .    Complete the VPC form with the following information:   IP CIDR block : 10.0.0.0/16  VPC name : bosh  Public subnet : 10.0.0.0/24  Availability Zone : us-east-1a  Subnet name : public  Enable DNS hostnames : Yes  Hardware tenancy : Default      Click  Create VPC  and click  OK  once VPC is successfully created.    Click  Subnets  and locate the \"public\" subnet in the VPC. Replace  SUBNET-ID  and  AVAILABILITY-ZONE  in your deployment manifest with the \"public\" subnet  Subnet ID ,  Availability Zone  and  Region  (AZ without the trailing character).",
            "title": "Create a Virtual Private Cloud (VPC) "
        },
        {
            "location": "/init-aws/#create-an-elastic-ip",
            "text": "On the VPC Dashboard, click  Elastic IPs  and click  Allocate New Address .     In the Allocate Address dialog box, click  Yes, Allocate .    Replace  ELASTIC-IP  in your deployment manifest with the allocated Elastic IP Address.",
            "title": "Create an Elastic IP "
        },
        {
            "location": "/init-aws/#create-a-key-pair",
            "text": "In the AWS Console, select  EC2  to get to the EC2 Dashboard.    Click  Key Pairs  and click  Create Key Pair .     In the Create Key Pair dialog box, enter \"bosh\" as the Key Pair name and click  Create .     Save private key to  ~/Downloads/bosh.pem .",
            "title": "Create a Key Pair "
        },
        {
            "location": "/init-aws/#create-and-configure-security-group",
            "text": "On the EC2 Dashboard, click  Security Groups  and then click  Create Security Group .     Complete the Create Security Group form with the following information:   Security group name : bosh  Description : BOSH deployed VMs  VPC : Select the \"bosh\" VPC that you created in  Create a Virtual Private Cloud .     Click  Create     Select the created security group with group name \"bosh\", click the  Inbound  tab and click  Edit .     Fill out the Edit inbound rules form and click  Save .  Note : It highly discouraged to run any production environment with  0.0.0.0/0  source or to make any BOSH management ports publicly accessible.  \n   \n     Type \n     Port Range \n     Source \n     Purpose \n    Custom TCP Rule 22 (My IP) SSH access from CLI \n   Custom TCP Rule 6868 (My IP) BOSH Agent access from CLI \n   Custom TCP Rule 25555 (My IP) BOSH Director access from CLI  All TCP 0 - 65535 ID of this security group Management and data access \n   All UDP 0 - 65535 ID of this security group Management and data access   Note :  To enter your security group as a  Source , select  Custom IP , and enter \"bosh\". Note: The AWS Console should autocomplete the security group ID (e.g. \"sg-12ab34cd\").",
            "title": "Create and Configure Security Group "
        },
        {
            "location": "/init-aws/#step-2-deploy",
            "text": "Install  CLI v2 .    Use  bosh create-env  command to deploy the Director.    # Create directory to keep state \n$ mkdir bosh-1  &&   cd  bosh-1 # Clone Director templates \n$ git clone https://github.com/cloudfoundry/bosh-deployment # Fill below variables (replace example values) and deploy the Director \n$ bosh create-env bosh-deployment/bosh.yml  \\ \n    --state = state.json  \\ \n    --vars-store = creds.yml  \\ \n    -o bosh-deployment/aws/cpi.yml  \\ \n    -v  director_name = bosh-1  \\ \n    -v  internal_cidr = 10 .0.0.0/24  \\ \n    -v  internal_gw = 10 .0.0.1  \\ \n    -v  internal_ip = 10 .0.0.6  \\ \n    -v  access_key_id = AKI...  \\ \n    -v  secret_access_key = wfh28...  \\ \n    -v  region = us-east-1  \\ \n    -v  az = us-east-1a  \\ \n    -v  default_key_name = bosh  \\ \n    -v  default_security_groups =[ bosh ]   \\ \n    --var-file  private_key = ~/Downloads/bosh.pem  \\ \n    -v  subnet_id = subnet-ait8g34t  If running above commands outside of an AWS VPC, refer to [Exposing environment on a public IP](init-external-ip.md) for additional CLI flags.\n\nSee [AWS CPI errors](aws-cpi.md#errors) for list of common errors and resolutions.   Connect to the Director.   # Configure local alias \n$ bosh alias-env bosh-1 -e  10 .0.0.6 --ca-cert < ( bosh int ./creds.yml --path /director_ssl/ca )  # Log in to the Director \n$  export   BOSH_CLIENT = admin\n$  export   BOSH_CLIENT_SECRET = ` bosh int ./creds.yml --path /admin_password `  # Query the Director for more info \n$ bosh -e bosh-1 env   Save the deployment state files left in your deployment directory  bosh-1  so you can later update/delete your Director. See  Deployment state  for details.    Back to Table of Contents  Previous:  Create an environment",
            "title": "Step 2: Deploy "
        },
        {
            "location": "/init-google/",
            "text": "This document shows how to initialize new \nenvironment\n on Google Cloud Platform.\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/gcp/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    --var-file \ngcp_credentials_json\n=\n~/Downloads/gcp-23r82r3y2.json \n\\\n\n    -v \nproject_id\n=\nmoonlight-2389ry3 \n\\\n\n    -v \nzone\n=\nus-east1-c \n\\\n\n    -v \ntags\n=[\ninternal\n]\n \n\\\n\n    -v \nnetwork\n=\ndefault \n\\\n\n    -v \nsubnetwork\n=\ndefault\n\n\n\n\nIf running above commands outside of a connected Google network, refer to \nExposing environment on a public IP\n for additional CLI flags.\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "Google Cloud Platform"
        },
        {
            "location": "/init-azure/",
            "text": "This document shows how to initialize new \nenvironment\n on Microsoft Azure.\n\n\nStep 1: Prepare an Azure Environment \n\u00b6\n\n\nIf you do not have an Azure account, \ncreate one\n.\n\n\nThen follow this \nguide\n to create your Azure service principal.\n\n\nWe strongly recommend you to use Azure template \nbosh-setup\n to initialize the new environment on Microsoft Azure.\n\n\nTo prepare your Azure environment find out and/or create any missing resources in Azure. If you are not familiar with Azure take a look at \nCreating Azure resources\n page for more details on how to create and configure necessary resources:\n\n\n\n\nStep 2: Deploy \n\u00b6\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/azure/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \nvnet_name\n=\nboshnet \n\\\n\n    -v \nsubnet_name\n=\nbosh \n\\\n\n    -v \nsubscription_id\n=\n3c39a033-c306-4615-a4cb-260418d63879 \n\\\n\n    -v \ntenant_id\n=\n0412d4fa-43d2-414b-b392-25d5ca46561da \n\\\n\n    -v \nclient_id\n=\n33e56099-0bde-8z93-a005-89c0f6df7465 \n\\\n\n    -v \nclient_secret\n=\nclient-secret \n\\\n\n    -v \nresource_group_name\n=\nbosh-res-group \n\\\n\n    -v \nstorage_account_name\n=\nboshstore \n\\\n\n    -v \ndefault_security_group\n=\nnsg-bosh\n\n\n\n\nIf running above commands outside of a connected Azure network, refer to \nExposing environment on a public IP\n for additional CLI flags.\n\n\nSee \nAzure CPI errors\n for list of common errors and resolutions.\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "Microsoft Azure"
        },
        {
            "location": "/init-azure/#step-1-prepare-an-azure-environment",
            "text": "If you do not have an Azure account,  create one .  Then follow this  guide  to create your Azure service principal.  We strongly recommend you to use Azure template  bosh-setup  to initialize the new environment on Microsoft Azure.  To prepare your Azure environment find out and/or create any missing resources in Azure. If you are not familiar with Azure take a look at  Creating Azure resources  page for more details on how to create and configure necessary resources:",
            "title": "Step 1: Prepare an Azure Environment "
        },
        {
            "location": "/init-azure/#step-2-deploy",
            "text": "Install  CLI v2 .    Use  bosh create-env  command to deploy the Director.  # Create directory to keep state \n$ mkdir bosh-1  &&   cd  bosh-1 # Clone Director templates \n$ git clone https://github.com/cloudfoundry/bosh-deployment # Fill below variables (replace example values) and deploy the Director \n$ bosh create-env bosh-deployment/bosh.yml  \\ \n    --state = state.json  \\ \n    --vars-store = creds.yml  \\ \n    -o bosh-deployment/azure/cpi.yml  \\ \n    -v  director_name = bosh-1  \\ \n    -v  internal_cidr = 10 .0.0.0/24  \\ \n    -v  internal_gw = 10 .0.0.1  \\ \n    -v  internal_ip = 10 .0.0.6  \\ \n    -v  vnet_name = boshnet  \\ \n    -v  subnet_name = bosh  \\ \n    -v  subscription_id = 3c39a033-c306-4615-a4cb-260418d63879  \\ \n    -v  tenant_id = 0412d4fa-43d2-414b-b392-25d5ca46561da  \\ \n    -v  client_id = 33e56099-0bde-8z93-a005-89c0f6df7465  \\ \n    -v  client_secret = client-secret  \\ \n    -v  resource_group_name = bosh-res-group  \\ \n    -v  storage_account_name = boshstore  \\ \n    -v  default_security_group = nsg-bosh  If running above commands outside of a connected Azure network, refer to  Exposing environment on a public IP  for additional CLI flags.  See  Azure CPI errors  for list of common errors and resolutions.    Connect to the Director.  # Configure local alias \n$ bosh alias-env bosh-1 -e  10 .0.0.6 --ca-cert < ( bosh int ./creds.yml --path /director_ssl/ca )  # Log in to the Director \n$  export   BOSH_CLIENT = admin\n$  export   BOSH_CLIENT_SECRET = ` bosh int ./creds.yml --path /admin_password `  # Query the Director for more info \n$ bosh -e bosh-1 env    Save the deployment state files left in your deployment directory  bosh-1  so you can later update/delete your Director. See  Deployment state  for details.     Back to Table of Contents  Previous:  Create an environment",
            "title": "Step 2: Deploy "
        },
        {
            "location": "/init-openstack/",
            "text": "This document shows how to initialize new \nenvironment\n on OpenStack.\n\n\nStep 1: Prepare an OpenStack environment\n\u00b6\n\n\nPrerequisites \n\u00b6\n\n\n\n\n\n\nAn OpenStack environment running one of the following supported releases:\n\n\n\n\nLiberty\n (actively tested)\n\n\nMitaka\n (actively tested)\n\n\nNewton\n (actively tested)\n\n\n\n\nNote: Juno has a \nbug\n that prevents BOSH to assign specific IPs to VMs. You have to apply a Nova patch to avoid this problem.\n\n\n\n\n\n\nThe following OpenStack services:\n\n\n\n\nIdentity\n:\n    BOSH authenticates credentials and retrieves the endpoint URLs for other OpenStack services.\n\n\nCompute\n:\n    BOSH boots new VMs, assigns floating IPs to VMs, and creates and attaches volumes to VMs.\n\n\nImage\n:\n    BOSH stores stemcells using the Image service.\n\n\n(Optional)\n \nOpenStack Networking\n:\n    Provides network scaling and automated management functions that are useful when deploying complex distributed systems. \nNote:\n OpenStack networking is used as default as of v28 of the OpenStack CPI. To disable the use of the OpenStack Networking project, see \nusing nova-networking\n.\n\n\n\n\n\n\n\n\nThe following OpenStack networks:\n\n\n\n\nAn external network with a subnet.\n\n\nAn private network with a subnet. The subnet must have an IP address allocation pool.\n\n\n\n\n\n\n\n\nConfiguration of a new OpenStack Project\n\n\n\n\n\n\nAutomated configuration\n\n\nYou can use a \nTerraform enviroment template\n to configure your OpenStack project.\n\n\n\n\n\n\nManual configuration\n\n\nNote\n: See the \nOpenStack documentation\n for help finding more information.\n\n\nAlternatively, you can do the following things manually as described below:\n* Create a \nKeypair\n.\n* Create and configure \nSecurity Groups\n.\n* Allocate a \nfloating IP address\n.\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Keypair\n\u00b6\n\n\n\n\n\n\nSelect \nAccess & Security\n from the left navigation panel.\n\n\n\n\n\n\nSelect the \nKeypairs\n tab.\n\n\n\n\n\n\n\n\nClick \nCreate Keypair\n.\n\n\n\n\n\n\nName the Keypair \"bosh\" and click \nCreate Keypair\n.\n\n\n\n\n\n\n\n\nSave the \nbosh.pem\n file to \n~/Downloads/bosh.pem\n.\n\n\n\n\n\n\n\n\n\n\nCreate and Configure Security Groups\n\u00b6\n\n\nYou must create and configure two Security Groups to restrict incoming network traffic to the BOSH VMs.\n\n\nBOSH Security Group\n\u00b6\n\n\n\n\n\n\nSelect \nAccess & Security\n from the left navigation panel.\n\n\n\n\n\n\nSelect the \nSecurity Groups\n tab.\n\n\n\n\n\n\n\n\nClick \nCreate Security Group\n.\n\n\n\n\n\n\nName the security group \"bosh\" and add the description \"BOSH Security Group\"\n\n\n\n\n\n\n\n\nClick \nCreate Security Group\n.\n\n\n\n\n\n\nSelect the BOSH Security Group and click \nEdit Rules\n.\n\n\n\n\n\n\nClick \nAdd Rule\n.\n\n\n\n\n\n\n\n\nAdd the following rules to the BOSH Security Group:\n\n\nNote\n: It highly discouraged to run any production environment with \n0.0.0.0/0\n source or to make any BOSH management ports publicly accessible.\n\n\n\n  \n\n    \nDirection\n\n    \nEther Type\n\n    \nIP Protocol\n\n    \nPort Range\n\n    \nRemote\n\n    \nPurpose\n\n  \n\n\nIngress\nIPv4\nTCP\n22\n0.0.0.0/0 (CIDR)\nSSH access from CLI\n\n  \nIngress\nIPv4\nTCP\n6868\n0.0.0.0/0 (CIDR)\nBOSH Agent access from CLI\n\n  \nIngress\nIPv4\nTCP\n25555\n0.0.0.0/0 (CIDR)\nBOSH Director access from CLI\n\n\nEgress\nIPv4\nAny\n-\n0.0.0.0/0 (CIDR)\n\n  \nEgress\nIPv6\nAny\n-\n::/0 (CIDR)\n\n\nIngress\nIPv4\nTCP\n1-65535\nbosh\nManagement and data access\n\n\n\n\n\n\n\n\n\n\nAllocate a floating IP address\n\u00b6\n\n\n\n\n\n\nSelect \nAccess & Security\n from the left navigation panel.\n\n\n\n\n\n\nSelect the \nFloating IPs\n tab.\n\n\n\n\n\n\n\n\nClick \nAllocate IP to Project\n.\n\n\n\n\n\n\nSelect \nExternal\n from the \nPool\n dropdown menu.\n\n\n\n\n\n\nClick \nAllocate IP\n.\n\n\n\n\n\n\n\n\nReplace \nFLOATING-IP\n in your deployment manifest with the allocated Floating IP Address.\n\n\n\n\n\n\n\n\n\n\nStep 2: Deploy \n\u00b6\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/openstack/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \nauth_url\n=\ntest\n \n\\\n\n    -v \naz\n=\ntest\n \n\\\n\n    -v \ndefault_key_name\n=\ntest\n \n\\\n\n    -v \ndefault_security_groups\n=[\ntest\n]\n \n\\\n\n    -v \nnet_id\n=\ntest\n \n\\\n\n    -v \nopenstack_password\n=\ntest\n \n\\\n\n    -v \nopenstack_username\n=\ntest\n \n\\\n\n    -v \nopenstack_domain\n=\ntest\n \n\\\n\n    -v \nopenstack_project\n=\ntest\n \n\\\n\n    -v \nprivate_key\n=\ntest\n \n\\\n\n    -v \nregion\n=\ntest\n\n\n\n\n\nIf running above commands outside of an OpenStack network, refer to \nExposing environment on a public IP\n for additional CLI flags.\n\n\nSee \nOpenStack CPI errors\n for list of common errors and resolutions.\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "OpenStack"
        },
        {
            "location": "/init-openstack/#prerequisites",
            "text": "An OpenStack environment running one of the following supported releases:   Liberty  (actively tested)  Mitaka  (actively tested)  Newton  (actively tested)   Note: Juno has a  bug  that prevents BOSH to assign specific IPs to VMs. You have to apply a Nova patch to avoid this problem.    The following OpenStack services:   Identity :\n    BOSH authenticates credentials and retrieves the endpoint URLs for other OpenStack services.  Compute :\n    BOSH boots new VMs, assigns floating IPs to VMs, and creates and attaches volumes to VMs.  Image :\n    BOSH stores stemcells using the Image service.  (Optional)   OpenStack Networking :\n    Provides network scaling and automated management functions that are useful when deploying complex distributed systems.  Note:  OpenStack networking is used as default as of v28 of the OpenStack CPI. To disable the use of the OpenStack Networking project, see  using nova-networking .     The following OpenStack networks:   An external network with a subnet.  An private network with a subnet. The subnet must have an IP address allocation pool.     Configuration of a new OpenStack Project    Automated configuration  You can use a  Terraform enviroment template  to configure your OpenStack project.    Manual configuration  Note : See the  OpenStack documentation  for help finding more information.  Alternatively, you can do the following things manually as described below:\n* Create a  Keypair .\n* Create and configure  Security Groups .\n* Allocate a  floating IP address .",
            "title": "Prerequisites "
        },
        {
            "location": "/init-openstack/#bosh-security-group",
            "text": "Select  Access & Security  from the left navigation panel.    Select the  Security Groups  tab.     Click  Create Security Group .    Name the security group \"bosh\" and add the description \"BOSH Security Group\"     Click  Create Security Group .    Select the BOSH Security Group and click  Edit Rules .    Click  Add Rule .     Add the following rules to the BOSH Security Group:  Note : It highly discouraged to run any production environment with  0.0.0.0/0  source or to make any BOSH management ports publicly accessible.  \n   \n     Direction \n     Ether Type \n     IP Protocol \n     Port Range \n     Remote \n     Purpose \n    Ingress IPv4 TCP 22 0.0.0.0/0 (CIDR) SSH access from CLI \n   Ingress IPv4 TCP 6868 0.0.0.0/0 (CIDR) BOSH Agent access from CLI \n   Ingress IPv4 TCP 25555 0.0.0.0/0 (CIDR) BOSH Director access from CLI  Egress IPv4 Any - 0.0.0.0/0 (CIDR) \n   Egress IPv6 Any - ::/0 (CIDR)  Ingress IPv4 TCP 1-65535 bosh Management and data access",
            "title": "BOSH Security Group"
        },
        {
            "location": "/init-openstack/#step-2-deploy",
            "text": "Install  CLI v2 .    Use  bosh create-env  command to deploy the Director.  # Create directory to keep state \n$ mkdir bosh-1  &&   cd  bosh-1 # Clone Director templates \n$ git clone https://github.com/cloudfoundry/bosh-deployment # Fill below variables (replace example values) and deploy the Director \n$ bosh create-env bosh-deployment/bosh.yml  \\ \n    --state = state.json  \\ \n    --vars-store = creds.yml  \\ \n    -o bosh-deployment/openstack/cpi.yml  \\ \n    -v  director_name = bosh-1  \\ \n    -v  internal_cidr = 10 .0.0.0/24  \\ \n    -v  internal_gw = 10 .0.0.1  \\ \n    -v  internal_ip = 10 .0.0.6  \\ \n    -v  auth_url = test   \\ \n    -v  az = test   \\ \n    -v  default_key_name = test   \\ \n    -v  default_security_groups =[ test ]   \\ \n    -v  net_id = test   \\ \n    -v  openstack_password = test   \\ \n    -v  openstack_username = test   \\ \n    -v  openstack_domain = test   \\ \n    -v  openstack_project = test   \\ \n    -v  private_key = test   \\ \n    -v  region = test   If running above commands outside of an OpenStack network, refer to  Exposing environment on a public IP  for additional CLI flags.  See  OpenStack CPI errors  for list of common errors and resolutions.    Connect to the Director.  # Configure local alias \n$ bosh alias-env bosh-1 -e  10 .0.0.6 --ca-cert < ( bosh int ./creds.yml --path /director_ssl/ca )  # Log in to the Director \n$  export   BOSH_CLIENT = admin\n$  export   BOSH_CLIENT_SECRET = ` bosh int ./creds.yml --path /admin_password `  # Query the Director for more info \n$ bosh -e bosh-1 env    Save the deployment state files left in your deployment directory  bosh-1  so you can later update/delete your Director. See  Deployment state  for details.     Back to Table of Contents  Previous:  Create an environment",
            "title": "Step 2: Deploy "
        },
        {
            "location": "/init-softlayer/",
            "text": "This document shows how to create new \nenvironment\n on SoftLayer.\n\n\nStep 1: Prepare a SoftLayer Environment \n\u00b6\n\n\nTo prepare your SoftLayer environment:\n\n\n\n\nCreate a SoftLayer account\n\n\nGenerate an API Key\n\n\nAccess SoftLayer VPN\n\n\nOrder VLANs\n\n\n\n\n\n\nCreate a SoftLayer account \n\u00b6\n\n\nIf you do not have an SoftLayer account, \ncreate one for one month free\n.\n\n\nUse the login credentials received in your provided email to login to SoftLayer \nCustomer Portal\n.\n\n\n\n\nGenerate an API Key \n\u00b6\n\n\nAPI keys are used to securely access the SoftLayer API. Follow \nGenerate an API Key\n to generate your API key.\n\n\n\n\nAccess SoftLayer VPN \n\u00b6\n\n\nTo access SoftLayer Private network, you need to access SoftLayer VPN. Follow \nVPN Access\n to access the VPN. You can get your VPN password from your \nuser profile\n. Follow \nVPN Access\n to access the VPN.\n\n\n\n\nOrder VLANs \n\u00b6\n\n\nVLANs provide the ability to partition devices and subnets on the network. To order VLANs, login to SoftLayer \nCustomer Portal\n and navigate to Network > IP Management > VLANs. Once on the page, click the \"Order VLAN\" link in the top-right corner. Fill in the pop-up window to order the VLANs as you need. The VLAN IDs are needed in the deployment manifest.\n\n\n\n\nStep 2: Deploy \n\u00b6\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nEstablish VPN connection between your host and SoftLayer. The machine where to run CLI needs to communicate with the target Director VM over the SoftLayer private network.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ sudo bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/softlayer/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \nsl_datacenter\n=\n \n\\\n\n    -v \nsl_vm_domain\n=\n \n\\\n\n    -v \nsl_vm_name_prefix\n=\n \n\\\n\n    -v \nsl_vlan_public\n=\n \n\\\n\n    -v \nsl_vlan_private\n=\n \n\\\n\n    -v \nsl_username\n=\n \n\\\n\n    -v \nsl_api_key\n=\n\n\n\n\n\nNote: The reason why need to run \nbosh create-env\n command with sudo is that it needs to update \n/etc/hosts\n file which needs suffient permission.\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "SoftLayer"
        },
        {
            "location": "/init-softlayer/#step-1-prepare-a-softlayer-environment",
            "text": "To prepare your SoftLayer environment:   Create a SoftLayer account  Generate an API Key  Access SoftLayer VPN  Order VLANs",
            "title": "Step 1: Prepare a SoftLayer Environment "
        },
        {
            "location": "/init-softlayer/#create-a-softlayer-account",
            "text": "If you do not have an SoftLayer account,  create one for one month free .  Use the login credentials received in your provided email to login to SoftLayer  Customer Portal .",
            "title": "Create a SoftLayer account "
        },
        {
            "location": "/init-softlayer/#generate-an-api-key",
            "text": "API keys are used to securely access the SoftLayer API. Follow  Generate an API Key  to generate your API key.",
            "title": "Generate an API Key "
        },
        {
            "location": "/init-softlayer/#access-softlayer-vpn",
            "text": "To access SoftLayer Private network, you need to access SoftLayer VPN. Follow  VPN Access  to access the VPN. You can get your VPN password from your  user profile . Follow  VPN Access  to access the VPN.",
            "title": "Access SoftLayer VPN "
        },
        {
            "location": "/init-softlayer/#order-vlans",
            "text": "VLANs provide the ability to partition devices and subnets on the network. To order VLANs, login to SoftLayer  Customer Portal  and navigate to Network > IP Management > VLANs. Once on the page, click the \"Order VLAN\" link in the top-right corner. Fill in the pop-up window to order the VLANs as you need. The VLAN IDs are needed in the deployment manifest.",
            "title": "Order VLANs "
        },
        {
            "location": "/init-softlayer/#step-2-deploy",
            "text": "Install  CLI v2 .    Establish VPN connection between your host and SoftLayer. The machine where to run CLI needs to communicate with the target Director VM over the SoftLayer private network.    Use  bosh create-env  command to deploy the Director.  # Create directory to keep state \n$ mkdir bosh-1  &&   cd  bosh-1 # Clone Director templates \n$ git clone https://github.com/cloudfoundry/bosh-deployment # Fill below variables (replace example values) and deploy the Director \n$ sudo bosh create-env bosh-deployment/bosh.yml  \\ \n    --state = state.json  \\ \n    --vars-store = creds.yml  \\ \n    -o bosh-deployment/softlayer/cpi.yml  \\ \n    -v  director_name = bosh-1  \\ \n    -v  internal_cidr = 10 .0.0.0/24  \\ \n    -v  internal_gw = 10 .0.0.1  \\ \n    -v  internal_ip = 10 .0.0.6  \\ \n    -v  sl_datacenter =   \\ \n    -v  sl_vm_domain =   \\ \n    -v  sl_vm_name_prefix =   \\ \n    -v  sl_vlan_public =   \\ \n    -v  sl_vlan_private =   \\ \n    -v  sl_username =   \\ \n    -v  sl_api_key =   Note: The reason why need to run  bosh create-env  command with sudo is that it needs to update  /etc/hosts  file which needs suffient permission.    Connect to the Director.  # Configure local alias \n$ bosh alias-env bosh-1 -e  10 .0.0.6 --ca-cert < ( bosh int ./creds.yml --path /director_ssl/ca )  # Log in to the Director \n$  export   BOSH_CLIENT = admin\n$  export   BOSH_CLIENT_SECRET = ` bosh int ./creds.yml --path /admin_password `  # Query the Director for more info \n$ bosh -e bosh-1 env    Save the deployment state files left in your deployment directory  bosh-1  so you can later update/delete your Director. See  Deployment state  for details.     Back to Table of Contents  Previous:  Create an environment",
            "title": "Step 2: Deploy "
        },
        {
            "location": "/bosh-lite/",
            "text": "BOSH Lite v2 is a Director VM running in VirtualBox (typically locally). It is managed via \nCLI v2\n. Internally CPI uses containers to emulate VMs which makes it an excellent choice for:\n\n\n\n\nGeneral BOSH exploration without investing time and resources to configure an IaaS\n\n\nDevelopment of releases (including BOSH itself)\n\n\nTesting releases locally or in CI\n\n\n\n\n\n\nInstall \n\u00b6\n\n\nFollow below steps to get it running on locally on VirtualBox:\n\n\n\n\n\n\nCheck that your machine has at least 8GB RAM, and 100GB free disk space. Smaller configurations may work.\n\n\n\n\n\n\nInstall \nCLI v2\n\n\n\n\n\n\nInstall \nVirtualBox\n\n\nKnown working version:\n\n\n```shell\n$ VBoxManage --version\n5.1...\n````\n\n\nNote: If you encounter problems with VirtualBox networking try installing \nOracle VM VirtualBox Extension Pack\n as suggested by \nIssue 202\n. Alternatively make sure you are on VirtualBox 5.1+ since previous versions had a \nnetwork connectivity bug\n.\n\n\n\n\n\n\nInstall Director VM\n\n\n```shell\n$ git clone \nhttps://github.com/cloudfoundry/bosh-deployment\n ~/workspace/bosh-deployment\n\n\n$ mkdir -p ~/deployments/vbox\n\n\n$ cd ~/deployments/vbox\n````\n\n\nBelow command will try automatically create/enable Host-only network 192.168.50.0/24 (\ndetails\n) and NAT network 'NatNetwork' with DHCP enabled (\ndetails\n).\n\n\n```shell\n$ bosh create-env ~/workspace/bosh-deployment/bosh.yml \\\n  --state ./state.json \\\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml \\\n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml \\\n  -o ~/workspace/bosh-deployment/bosh-lite.yml \\\n  -o ~/workspace/bosh-deployment/bosh-lite-runc.yml \\\n  -o ~/workspace/bosh-deployment/jumpbox-user.yml \\\n  --vars-store ./creds.yml \\\n  -v director_name=\"bosh-lite\" \\\n  -v internal_ip=192.168.50.6 \\\n  -v internal_gw=192.168.50.1 \\\n  -v internal_cidr=192.168.50.0/24 \\\n  -v outbound_network_name=NatNetwork\n````\n\n\n\n\n\n\nAlias and log into the Director\n\n\n``shell$ bosh alias-env vbox -e 192.168.50.6 --ca-cert <(bosh int ./creds.yml --path /director_ssl/ca)$ export BOSH_CLIENT=admin$ export BOSH_CLIENT_SECRET=\nbosh int ./creds.yml --path /admin_password`\n````\n\n\n\n\n\n\nConfirm that it works\n\n\n```shell\n$ bosh -e vbox env\nUsing environment '192.168.50.6' as '?'\n\n\nName: ...\nUser: admin\n\n\nSucceeded\n````\n\n\n\n\n\n\nOptionally, set up a local route for \nbosh ssh\n commands or accessing VMs directly\n\n\n```shell\n$ sudo route add -net 10.244.0.0/16     192.168.50.6 # Mac OS X\n$ sudo ip route add   10.244.0.0/16 via 192.168.50.6 # Linux (using iproute2 suite)\n$ sudo route add -net 10.244.0.0/16 gw  192.168.50.6 # Linux (using DEPRECATED route command)\n$ route add           10.244.0.0/16     192.168.50.6 # Windows\n````\n\n\n\n\n\n\n\n\nDeploy example Zookeeper deployment \n\u00b6\n\n\nRun through quick steps below or follow \ndeploy workflow\n that goes through the same steps but with more explanation.\n\n\n\n\n\n\nUpdate cloud config\n\n\n```shell\n$ bosh -e vbox update-cloud-config ~/workspace/bosh-deployment/warden/cloud-config.yml\n````\n\n\n\n\n\n\nUpload stemcell\n\n\n```shell\n$ bosh -e vbox upload-stemcell \nhttps://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent?v=3468.17\n \\\n  --sha1 1dad6d85d6e132810439daba7ca05694cec208ab\n````\n\n\n\n\n\n\nDeploy example deployment\n\n\n```shell\n$ bosh -e vbox -d zookeeper deploy <(wget -O- \nhttps://raw.githubusercontent.com/cppforlife/zookeeper-release/master/manifests/zookeeper.yml\n)\n````\n\n\n\n\n\n\nRun Zookeeper smoke tests\n\n\n```shell\n$ bosh -e vbox -d zookeeper run-errand smoke-tests\n````\n\n\n\n\n\n\nTips \n\u00b6\n\n\n\n\nIn case you need to SSH into the Director VM, see \nJumpbox\n.\n\n\nIn case VirtualBox VM shuts down or reboots, you will have to re-run \ncreate-env\n command from above with \n--recreate\n flag. The containers will be lost after a VM restart, but you can restore your deployment with \nbosh cck\n command. Alternatively \nPause\n the VM from the VirtualBox UI before shutting down VirtualBox host, or making your computer sleep.",
            "title": "VirtualBox"
        },
        {
            "location": "/bosh-lite/#install",
            "text": "Follow below steps to get it running on locally on VirtualBox:    Check that your machine has at least 8GB RAM, and 100GB free disk space. Smaller configurations may work.    Install  CLI v2    Install  VirtualBox  Known working version:  ```shell\n$ VBoxManage --version\n5.1...\n````  Note: If you encounter problems with VirtualBox networking try installing  Oracle VM VirtualBox Extension Pack  as suggested by  Issue 202 . Alternatively make sure you are on VirtualBox 5.1+ since previous versions had a  network connectivity bug .    Install Director VM  ```shell\n$ git clone  https://github.com/cloudfoundry/bosh-deployment  ~/workspace/bosh-deployment  $ mkdir -p ~/deployments/vbox  $ cd ~/deployments/vbox\n````  Below command will try automatically create/enable Host-only network 192.168.50.0/24 ( details ) and NAT network 'NatNetwork' with DHCP enabled ( details ).  ```shell\n$ bosh create-env ~/workspace/bosh-deployment/bosh.yml \\\n  --state ./state.json \\\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml \\\n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml \\\n  -o ~/workspace/bosh-deployment/bosh-lite.yml \\\n  -o ~/workspace/bosh-deployment/bosh-lite-runc.yml \\\n  -o ~/workspace/bosh-deployment/jumpbox-user.yml \\\n  --vars-store ./creds.yml \\\n  -v director_name=\"bosh-lite\" \\\n  -v internal_ip=192.168.50.6 \\\n  -v internal_gw=192.168.50.1 \\\n  -v internal_cidr=192.168.50.0/24 \\\n  -v outbound_network_name=NatNetwork\n````    Alias and log into the Director  ``shell$ bosh alias-env vbox -e 192.168.50.6 --ca-cert <(bosh int ./creds.yml --path /director_ssl/ca)$ export BOSH_CLIENT=admin$ export BOSH_CLIENT_SECRET= bosh int ./creds.yml --path /admin_password`\n````    Confirm that it works  ```shell\n$ bosh -e vbox env\nUsing environment '192.168.50.6' as '?'  Name: ...\nUser: admin  Succeeded\n````    Optionally, set up a local route for  bosh ssh  commands or accessing VMs directly  ```shell\n$ sudo route add -net 10.244.0.0/16     192.168.50.6 # Mac OS X\n$ sudo ip route add   10.244.0.0/16 via 192.168.50.6 # Linux (using iproute2 suite)\n$ sudo route add -net 10.244.0.0/16 gw  192.168.50.6 # Linux (using DEPRECATED route command)\n$ route add           10.244.0.0/16     192.168.50.6 # Windows\n````",
            "title": "Install "
        },
        {
            "location": "/bosh-lite/#deploy-example-zookeeper-deployment",
            "text": "Run through quick steps below or follow  deploy workflow  that goes through the same steps but with more explanation.    Update cloud config  ```shell\n$ bosh -e vbox update-cloud-config ~/workspace/bosh-deployment/warden/cloud-config.yml\n````    Upload stemcell  ```shell\n$ bosh -e vbox upload-stemcell  https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent?v=3468.17  \\\n  --sha1 1dad6d85d6e132810439daba7ca05694cec208ab\n````    Deploy example deployment  ```shell\n$ bosh -e vbox -d zookeeper deploy <(wget -O-  https://raw.githubusercontent.com/cppforlife/zookeeper-release/master/manifests/zookeeper.yml )\n````    Run Zookeeper smoke tests  ```shell\n$ bosh -e vbox -d zookeeper run-errand smoke-tests\n````",
            "title": "Deploy example Zookeeper deployment "
        },
        {
            "location": "/bosh-lite/#tips",
            "text": "In case you need to SSH into the Director VM, see  Jumpbox .  In case VirtualBox VM shuts down or reboots, you will have to re-run  create-env  command from above with  --recreate  flag. The containers will be lost after a VM restart, but you can restore your deployment with  bosh cck  command. Alternatively  Pause  the VM from the VirtualBox UI before shutting down VirtualBox host, or making your computer sleep.",
            "title": "Tips "
        },
        {
            "location": "/init-vcloud/",
            "text": "This document shows how to initialize new \nenvironment\n on vCloud.\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/vcloud/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \nnetwork_name\n=\n\"VM Network\"\n \n\\\n\n    -v \nvcloud_url\n=\nhttps://jf629-vcd.vchs.vmware.com \n\\\n\n    -v \nvcloud_user\n=\nroot \n\\\n\n    -v \nvcloud_password\n=\nvmware \n\\\n\n    -v \nvcd_org\n=\nVDC-M127910816-4610-275 \n\\\n\n    -v \nvcd_name\n=\nVDC-M127910816-4610-275\n\n\n\n\nTo prepare your vCloud environment find out and/or create any missing resources listed below:\n- Configure \nvcloud_url\n (e.g. '\nhttps://jf629-vcd.vchs.vmware.com\n') with the URL of the vCloud Director.\n- Configure \nvcloud_user\n (e.g. 'root') and \nvcloud_password\n (e.g. 'vmware') in your deployment manifest with vCloud user name and password. BOSH does not require user to be an admin; however, it does need certain user privileges.\n- Configure \nnetwork_name\n (e.g. 'VM Network') with the name of the vCloud network. Above example uses \n10.0.0.0/24\n network and Director VM will be placed at \n10.0.0.6\n.\n- Configure \nvcd_org\n (e.g. 'VDC-M127910816-4610-275')\n- Configure \nvcd_name\n (e.g. 'VDC-M127910816-4610-275')\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "VMware vCloud"
        },
        {
            "location": "/init-vsphere/",
            "text": "This document shows how to set up new \nenvironment\n on vSphere.\n\n\n\n\n\n\nInstall \nCLI v2\n.\n\n\n\n\n\n\nUse \nbosh create-env\n command to deploy the Director.\n\n\n# Create directory to keep state\n\n$ mkdir bosh-1 \n&&\n \ncd\n bosh-1\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n\n# Fill below variables (replace example values) and deploy the Director\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/vsphere/cpi.yml \n\\\n\n    -v \ndirector_name\n=\nbosh-1 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.0.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.0.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.0.6 \n\\\n\n    -v \nnetwork_name\n=\n\"VM Network\"\n \n\\\n\n    -v \nvcenter_dc\n=\nmy-dc \n\\\n\n    -v \nvcenter_ds\n=\ndatastore0 \n\\\n\n    -v \nvcenter_ip\n=\n192\n.168.0.10 \n\\\n\n    -v \nvcenter_user\n=\nroot \n\\\n\n    -v \nvcenter_password\n=\nvmware \n\\\n\n    -v \nvcenter_templates\n=\nbosh-1-templates \n\\\n\n    -v \nvcenter_vms\n=\nbosh-1-vms \n\\\n\n    -v \nvcenter_disks\n=\nbosh-1-disks \n\\\n\n    -v \nvcenter_cluster\n=\ncluster1\n\n\n\n\nIf Resource Pools want to be utilized, refer to \nDeploying BOSH into Resource Pools\n for additional CLI flags.\n\n\nUse the vSphere Web Client to find out and/or create any missing resources listed below:\n- Configure \nvcenter_ip\n (e.g. '192.168.0.10') with the IP of the vCenter.\n- Configure \nvcenter_user\n (e.g. 'root') and \nvcenter_password\n (e.g. 'vmware') with vCenter user name and password.\n  BOSH does not require user to be an admin, but it does require the following \nprivileges\n.\n- Configure \nvcenter_dc\n (e.g. 'my-dc') with the name of the datacenter the Director will use for VM creation.\n- Configure \nvcenter_vms\n (e.g. 'my-bosh-vms') and \nTEMPLATES-FOLDER-NAME\n (e.g. 'my-bosh-templates') with the name of the folder created to hold VMs and the name of the folder created to hold stemcells. Folders will be automatically created under the chosen datacenter.\n- Configure \nvcenter_ds\n (e.g. 'datastore[1-9]') with a regex matching the names of potential datastores the Director will use for storing VMs and associated persistent disks.\n- Configure \nvcenter_disks\n (e.g. 'my-bosh-disks') with the name of the VMs folder. Disk folder will be automatically created in the chosen datastore.\n- Configure \nvcenter_cluster\n (e.g. 'cluster1') with the name of the vSphere cluster. Create cluster under the chosen datacenter in the Clusters tab.\n- Configure \nnetwork_name\n (e.g. 'VM Network') with the name of the vSphere network. Create network under the chosen datacenter in the Networks tab. Above example uses \n10.0.0.0/24\n network and Director VM will be placed at \n10.0.0.6\n.\n- [Optional] Configure \nvcenter_rp\n (eg. 'my-bosh-rp') with the name of the vSphere resource pool. Create resource pool under the choosen datacenter in the Clusters tab.\n\n\nSee \nvSphere CPI errors\n for list of common errors and resolutions.\n\n\n\n\n\n\nConnect to the Director.\n\n\n# Configure local alias\n\n$ bosh alias-env bosh-1 -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n\n\n# Log in to the Director\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n# Query the Director for more info\n\n$ bosh -e bosh-1 env\n\n\n\n\n\n\n\n\nSave the deployment state files left in your deployment directory \nbosh-1\n so you can later update/delete your Director. See \nDeployment state\n for details.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nCreate an environment",
            "title": "VMware vSphere"
        },
        {
            "location": "/update-cloud-config/",
            "text": "Note: Document uses CLI v2.\n\n\n\nThe \ncloud config\n is a YAML file that defines IaaS specific configuration used by all deployments. It allows to separate IaaS specific configuration into its own file and keep deployment manifests IaaS agnostic.\n\n\nHere is an example cloud config used with \nBOSH Lite\n:\n\n\n---\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n-\n \nname\n:\n \nz2\n\n\n-\n \nname\n:\n \nz3\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n1024\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nazs\n:\n \n[\nz1\n,\n \nz2\n,\n \nz3\n]\n\n    \ndns\n:\n \n[\n8.8.8.8\n]\n\n    \nrange\n:\n \n10.244.0.0/24\n\n    \ngateway\n:\n \n10.244.0.1\n\n    \nstatic\n:\n \n[\n10.244.0.34\n]\n\n    \nreserved\n:\n \n[]\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \naz\n:\n \nz1\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n(Taken from \nhttps://github.com/cloudfoundry/bosh-deployment/blob/master/warden/cloud-config.yml\n)\n\n\nWithout going into much detail, above cloud config defines three \navailability zones\n, one \ndefault\n \nVM type\n and one \ndefault\n \ndisk types\n and a \ndefault\n \nnetwork\n. All of these definitions will be referenced by the deployment manifest.\n\n\nSee \ncloud config schema\n for detailed breakdown.\n\n\nTo configure Director with above cloud config use \nbosh update-cloud-config\n command\n:\n\n\n$ bosh -e vbox update-cloud-config cloud-config.yml\n\n\n\n\n\n\nNext: \nBuild deployment manifest\n\n\nPrevious: \nDeploy Workflow",
            "title": "Updating Cloud Config"
        },
        {
            "location": "/deployment-basics/",
            "text": "(See \nWhat is a Deployment?\n for an introduction to deployments.)\n\n\nA deployment is a collection of VMs, persistent disks and other resources. To create a deployment in the Director, it has to be described with a \ndeployment manifest\n. Most deployment manifests look something like this:\n\n\n---\n\n\nname\n:\n \nzookeeper\n\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nversion\n:\n \n0.0.5\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/cppforlife/zookeeper-release?v=0.0.5\n\n  \nsha1\n:\n \n65a07b7526f108b0863d76aada7fc29e2c9e2095\n\n\n\nstemcells\n:\n\n\n-\n \nalias\n:\n \ndefault\n\n  \nos\n:\n \nubuntu-trusty\n\n  \nversion\n:\n \nlatest\n\n\n\nupdate\n:\n\n  \ncanaries\n:\n \n2\n\n  \nmax_in_flight\n:\n \n1\n\n  \ncanary_watch_time\n:\n \n5000-60000\n\n  \nupdate_watch_time\n:\n \n5000-60000\n\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n,\n \nz3\n]\n\n  \ninstances\n:\n \n5\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nzookeeper\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n-\n \nname\n:\n \nsmoke-tests\n\n  \nazs\n:\n \n[\nz1\n]\n\n  \nlifecycle\n:\n \nerrand\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nsmoke-tests\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n(Taken from \nhttps://github.com/cppforlife/zookeeper-release/blob/master/manifests/zookeeper.yml\n)\n\n\nHere is how deployment manifest describes a reasonably complex Zookeeper cluster:\n\n\n\n\nZookepeer source code, configuration file, startup scripts\n\n\ninclude \nzookeeper\n release version \n0.0.5\n to the \nreleases\n section\n\n\nOperating system image onto which install software\n\n\ninclude latest version of \nubuntu-trusty\n stemcell\n\n\nCreate 5 Zookeeper VMs spread\n\n\nadd \nzookeeper\n \ninstance group\n with \ninstances\n:\n \n5\n\n\nSpread VMs over multiple availability zones\n\n\nadd \nazs: [z1, z2, z3]\n\n\nInstall Zookeeper software onto VMs\n\n\nadd \nzookeeper\n job to this instance group\n\n\nSize VMs in the same way\n\n\nadd \nvm_type\n:\n \ndefault\n which references VM type from cloud config\n\n\nAttach a 10GB \npersistent disk\n to each Zookeeper VM\n\n\nadd \npersistent_disk\n:\n \n10240\n to \nzookeeper\n instance group\n\n\nPlace VMs onto some \nnetwork\n\n\nadd \nnetworks: [{name: default}]\n to \nzookeeper\n instance group\n\n\nProvide a way to smoke test Zookeeper cluster\n\n\nadd \nsmoke-tests\n instance group with \nsmoke-tests\n job from Zookeeper release\n\n\n\n\nRefer to \nmanifest v2 schema\n for detailed breakdown.\n\n\nOnce manifest is complete referenced stemcells and releases must be uploaded.\n\n\n\n\nNext: \nUpload Stemcells\n\n\nPrevious: \nUpdate Cloud Config",
            "title": "Building a Manifest"
        },
        {
            "location": "/uploading-stemcells/",
            "text": "Note: Document uses CLI v2.\n\n\n\n(See \nWhat is a Stemcell?\n for an introduction to stemcells.)\n\n\nAs described earlier, each deployment can reference one or more stemcells. For a deploy to succeed, necessary stemcells must be uploaded to the Director.\n\n\nFinding Stemcells \n\u00b6\n\n\nThe \nstemcells section of bosh.io\n lists official stemcells.\n\n\n\n\nUploading to the Director \n\u00b6\n\n\nCLI provides \nbosh upload-stemcell\n command\n.\n\n\n\n\n\n\nIf you have a URL to a stemcell tarball (for example URL provided by bosh.io):\n\n\n$ bosh -e vbox upload-stemcell https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent?v\n=\n3468\n.17 --sha1 1dad6d85d6e132810439daba7ca05694cec208ab\n\n\n\n\n\n\n\n\nIf you have a stemcell tarball on your local machine:\n\n\n$ bosh upload-stemcell ~/Downloads/bosh-stemcell-3468.17-warden-boshlite-ubuntu-trusty-go_agent.tgz\n\n\n\n\n\n\n\n\nOnce the command succeeds you can view all uploaded stemcells in the Director:\n\n\n$ bosh -e vbox stemcells\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nName                                         Version  OS             CPI  CID\nbosh-warden-boshlite-ubuntu-trusty-go_agent  \n3468\n.17* ubuntu-trusty  -    6c9c002e-bb46-4838-4b73-ff1afaa0aa21\n\n\n(\n*\n)\n Currently deployed\n\n\n1\n stemcells\n\nSucceeded\n\n\n\n\n\n\nDeployment Manifest Usage \n\u00b6\n\n\nTo use uploaded stemcell in your deployment, add stemcells:\n\n\nstemcells\n:\n\n\n-\n \nalias\n:\n \ndefault\n\n  \nos\n:\n \nubuntu-trusty\n\n  \nversion\n:\n \n3468.17\n\n\n\n\n\n\n\nNext: \nUpload Releases\n\n\nPrevious: \nBuild Deployment Manifest",
            "title": "Uploading Stemcells"
        },
        {
            "location": "/uploading-stemcells/#finding-stemcells",
            "text": "The  stemcells section of bosh.io  lists official stemcells.",
            "title": "Finding Stemcells "
        },
        {
            "location": "/uploading-stemcells/#uploading-to-the-director",
            "text": "CLI provides  bosh upload-stemcell  command .    If you have a URL to a stemcell tarball (for example URL provided by bosh.io):  $ bosh -e vbox upload-stemcell https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent?v = 3468 .17 --sha1 1dad6d85d6e132810439daba7ca05694cec208ab    If you have a stemcell tarball on your local machine:  $ bosh upload-stemcell ~/Downloads/bosh-stemcell-3468.17-warden-boshlite-ubuntu-trusty-go_agent.tgz    Once the command succeeds you can view all uploaded stemcells in the Director:  $ bosh -e vbox stemcells\nUsing environment  '192.168.50.6'  as client  'admin' \n\nName                                         Version  OS             CPI  CID\nbosh-warden-boshlite-ubuntu-trusty-go_agent   3468 .17* ubuntu-trusty  -    6c9c002e-bb46-4838-4b73-ff1afaa0aa21 ( * )  Currently deployed 1  stemcells\n\nSucceeded",
            "title": "Uploading to the Director "
        },
        {
            "location": "/uploading-stemcells/#deployment-manifest-usage",
            "text": "To use uploaded stemcell in your deployment, add stemcells:  stemcells :  -   alias :   default \n   os :   ubuntu-trusty \n   version :   3468.17    Next:  Upload Releases  Previous:  Build Deployment Manifest",
            "title": "Deployment Manifest Usage "
        },
        {
            "location": "/uploading-releases/",
            "text": "Note: Document uses CLI v2.\n\n\n\n(See \nWhat is a Release?\n for an introduction to releases.)\n\n\nEach deployment can reference one or many releases. For a deploy to succeed, all necessary releases must be uploaded to the Director.\n\n\nFinding Releases \n\u00b6\n\n\nReleases are distributed in two ways: as a release tarball or through a source code repository. The \nreleases section of bosh.io\n provides a good list of available releases and their tarballs.\n\n\nHere are a few popular releases:\n\n\n\n\ncf-release\n provides CloudFoundry\n\n\nconcourse\n provides a Continious Integration system called Concourse CI\n\n\ncf-rabbitmq-release\n provides RabbitMQ\n\n\n\n\n\n\nUploading to the Director \n\u00b6\n\n\nCLI provides \nbosh upload-release\n command\n.\n\n\n\n\n\n\nIf you have a URL to a release tarball (for example a URL provided by bosh.io):\n\n\n$ bosh -e vbox upload-release https://bosh.io/d/github.com/cppforlife/zookeeper-release?v\n=\n0\n.0.5 --sha1 65a07b7526f108b0863d76aada7fc29e2c9e2095\n\n\n\n\nAlternatively, if you have a release tarball on your local machine:\n\n\n$ bosh -e vbox upload-release ~/Downloads/zookeeper-0.0.5.tgz\n\n\n\n\n\n\n\n\nIf you cloned release Git repository:\n\n\nNote that all release repositories have a \nreleases/\n folder that contains release YAML files. These files have all the required information about how to assemble a specific version of a release (provided that the release maintainers produce and commit that version to the repository). You can use the YAML files to either directly upload a release, or to create a release tarball locally and then upload it.\n\n\n```shell\n  $ git clone \nhttps://github.com/cppforlife/zookeeper-release\n\n$ cd zookeeper-release/\n$ bosh -e vbox upload-release\n\nAlternatively, to build a release tarball locally from a release YAML file:\n\n```shell\n$ cd zookeeper-release/\n$ bosh create-release releases/zookeeper/zookeeper-0.0.5.yml --tarball x.tgz\n$ bosh -e vbox upload-release x.tgz\n\n\n\n\n\n\n\nOnce the command succeeds, you can view all uploaded releases in the Director:\n\n\n$ bosh -e vbox releases\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nName       Version            Commit Hash\ndns        \n0\n+dev.1496791266*  65f3b30+\nzookeeper  \n0\n.0.5*             b434447\n\n\n(\n*\n)\n Currently deployed\n\n(\n+\n)\n Uncommitted changes\n\n\n3\n releases\n\nSucceeded\n\n\n\n\n\n\nDeployment Manifest Usage \n\u00b6\n\n\nTo use an uploaded release in your deployment, update the \nreleases\n section in your deployment manifest:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nversion\n:\n \n0.0.5\n\n\n\n\n\n\n\nNext: \nDeploy\n\n\nPrevious: \nUploading Stemcells",
            "title": "Uploading Releases"
        },
        {
            "location": "/uploading-releases/#finding-releases",
            "text": "Releases are distributed in two ways: as a release tarball or through a source code repository. The  releases section of bosh.io  provides a good list of available releases and their tarballs.  Here are a few popular releases:   cf-release  provides CloudFoundry  concourse  provides a Continious Integration system called Concourse CI  cf-rabbitmq-release  provides RabbitMQ",
            "title": "Finding Releases "
        },
        {
            "location": "/uploading-releases/#uploading-to-the-director",
            "text": "CLI provides  bosh upload-release  command .    If you have a URL to a release tarball (for example a URL provided by bosh.io):  $ bosh -e vbox upload-release https://bosh.io/d/github.com/cppforlife/zookeeper-release?v = 0 .0.5 --sha1 65a07b7526f108b0863d76aada7fc29e2c9e2095  Alternatively, if you have a release tarball on your local machine:  $ bosh -e vbox upload-release ~/Downloads/zookeeper-0.0.5.tgz    If you cloned release Git repository:  Note that all release repositories have a  releases/  folder that contains release YAML files. These files have all the required information about how to assemble a specific version of a release (provided that the release maintainers produce and commit that version to the repository). You can use the YAML files to either directly upload a release, or to create a release tarball locally and then upload it.  ```shell\n  $ git clone  https://github.com/cppforlife/zookeeper-release \n$ cd zookeeper-release/\n$ bosh -e vbox upload-release Alternatively, to build a release tarball locally from a release YAML file:\n\n```shell\n$ cd zookeeper-release/\n$ bosh create-release releases/zookeeper/zookeeper-0.0.5.yml --tarball x.tgz\n$ bosh -e vbox upload-release x.tgz    Once the command succeeds, you can view all uploaded releases in the Director:  $ bosh -e vbox releases\nUsing environment  '192.168.50.6'  as client  'admin' \n\nName       Version            Commit Hash\ndns         0 +dev.1496791266*  65f3b30+\nzookeeper   0 .0.5*             b434447 ( * )  Currently deployed ( + )  Uncommitted changes 3  releases\n\nSucceeded",
            "title": "Uploading to the Director "
        },
        {
            "location": "/uploading-releases/#deployment-manifest-usage",
            "text": "To use an uploaded release in your deployment, update the  releases  section in your deployment manifest:  releases :  -   name :   zookeeper \n   version :   0.0.5    Next:  Deploy  Previous:  Uploading Stemcells",
            "title": "Deployment Manifest Usage "
        },
        {
            "location": "/deploying/",
            "text": "Once referenced stemcells and releases are uploaded to the Director and the deployment manifest is complete, Director can successfull make a deployment. The CLI has a single command to create and update a deployment: \nbosh deploy\n command\n. From the perspective of the Director same steps are taken to create or update a deployment.\n\n\nTo create a Zookeeper deployment from \nzookeeper.yml\n deployment manifest run the deploy command:\n\n\n$ bosh -e vbox -d zookeeper deploy zookeeper.yml\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nTask \n1133\n\n\n\n08\n:41:15 \n|\n Preparing deployment: Preparing deployment \n(\n00\n:00:00\n)\n\n\n08\n:41:15 \n|\n Preparing package compilation: Finding packages to compile \n(\n00\n:00:00\n)\n\n\n08\n:41:15 \n|\n Creating missing vms: zookeeper/6b7a51c4-1aeb-4cea-a2da-fdac3044bdee \n(\n1\n)\n \n(\n00\n:00:10\n)\n\n\n08\n:41:25 \n|\n Updating instance zookeeper: zookeeper/3f9980b4-d02f-4754-bb53-0d1458e447ac \n(\n0\n)\n \n(\ncanary\n)\n \n(\n00\n:00:27\n)\n\n\n08\n:41:52 \n|\n Updating instance zookeeper: zookeeper/b8b577e7-d745-4d06-b2b7-c7cdeb46c78f \n(\n4\n)\n \n(\ncanary\n)\n \n(\n00\n:00:25\n)\n\n\n08\n:42:17 \n|\n Updating instance zookeeper: zookeeper/5a901538-be10-4d53-a3e9-3e23d3e3a07a \n(\n3\n)\n \n(\n00\n:00:25\n)\n\n\n08\n:42:42 \n|\n Updating instance zookeeper: zookeeper/c5a3f7e6-4311-43ac-8500-a2337ca3e8a7 \n(\n2\n)\n \n(\n00\n:00:26\n)\n\n\n08\n:43:08 \n|\n Updating instance zookeeper: zookeeper/6b7a51c4-1aeb-4cea-a2da-fdac3044bdee \n(\n1\n)\n \n(\n00\n:00:39\n)\n\n\nStarted  Mon Jul \n24\n \n08\n:41:15 UTC \n2017\n\nFinished Mon Jul \n24\n \n08\n:43:47 UTC \n2017\n\nDuration \n00\n:02:32\n\nTask \n1133\n \ndone\n\n\nSucceeded\n\n\n\n\nAfter the deploy command completes with either success or failure you can run a command to list VMs created for this deployment:\n\n\n$ bosh -e vbox -d zookeeper instances\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nDeployment \n'zookeeper'\n\n\nInstance                                          Process State  AZ  IPs\nsmoke-tests/42e003c1-1c05-453e-a946-c2e77935cff0  -              z1  -\nzookeeper/3f9980b4-d02f-4754-bb53-0d1458e447ac    running        z2  \n10\n.244.0.2\nzookeeper/5a901538-be10-4d53-a3e9-3e23d3e3a07a    -              z1  \n10\n.244.0.3\nzookeeper/6b7a51c4-1aeb-4cea-a2da-fdac3044bdee    running        z3  \n10\n.244.0.6\nzookeeper/b8b577e7-d745-4d06-b2b7-c7cdeb46c78f    running        z2  \n10\n.244.0.4\nzookeeper/c5a3f7e6-4311-43ac-8500-a2337ca3e8a7    -              z1  \n10\n.244.0.5\n\n\n6\n instances\n\nSucceeded\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nUpload Releases",
            "title": "Deploying"
        },
        {
            "location": "/persistent-disks/",
            "text": "Note: This document was updated to mention orphaned disks introduced in bosh-release v241+ (1.3163.0).\n\n\n\nInstance groups may need to store persistent data.\n\n\nIf you attach a persistent disk to a virtual machine and then stop, terminate, or delete the VM, your persistent disk data remains intact. Attaching the persistent disk to another VM allows you to access your data.\n\n\nPersistent disks are kept for each instance under the following circumstances:\n\n\n\n\nupdating deployment to use new releases or stemcells\n\n\nusing cloud check to recover deleted VMs\n\n\ninstances are hard stopped and later started again\n\n\n\n\nAs of bosh-release v241+ (1.3163.0), the Director no longer deletes persistent disks that are no longer needed. Unnecessary persistent disks will be marked as orphaned so that they can be garbage collected after 5 days.\n\n\nThe following conditions result in persistent disks to be marked as orphaned:\n\n\n\n\ninstance group no longer specifies a persistent disk size or a disk pool\n\n\ninstance group changes the size or cloud properties of a disk\n\n\ninstance group is renamed without \nmigrated_from\n configuration\n\n\ninstance group is scaled down\n\n\ninstance group is deleted or AZ assignment is removed\n\n\ndeployment is deleted\n\n\n\n\nYou can specify that an instance group needs an attached persistent disk in one of two ways:\n\n\n\n\nPersistent Disk declaration\n\n\nPersistent Disk Pool declaration\n\n\n\n\n\n\nPersistent Disk Declaration \n\u00b6\n\n\nTo specify that an instance group needs an attached persistent disk, add a \npersistent_disk\n key-value pair to the instance group in the \nJobs\n block of your deployment manifest.\n\n\nThe \npersistent_disk\n key-value pair specifies the persistent disk size, and defaults to 0 (no persistent disk). If the \npersistent_disk\n value is a positive integer, BOSH creates a persistent disk of that size in megabytes and attaches it to each instance VM for the job.\n\n\nExample:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nredis\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \nredis\n,\n \nrelease\n:\n \nredis\n}\n\n  \ninstances\n:\n \n1\n\n  \nresource_pool\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n1024\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\nNote: If you use persistent disk declaration, you cannot specify the persistent disk type or size that the CPI attaches to your job VMs. Instead, the CPI uses its default disk configuration when deploying the VMs.\n\n\n\n\n\nPersistent Disk Pool Declaration \n\u00b6\n\n\nTo specify that an instance group needs an attached persistent disk, add a \nDisk Pool\n block to your deployment manifest.\n\n\nThe persistent disk pool declaration allows you to specify the precise type and size of the persistent disks attached to your instance group VMs.\n\n\n\n\n\n\npersistent_disk_pool\n [String, optional]: Associated with an instance group; specifies a particular disk_pool.\n\n\n\n\n\n\ndisk_pools\n [Array, optional]: Specifies the \ndisk_pools\n a deployment uses. A deployment manifest can describe multiple disk pools and uses unique names to identify and reference them.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the disk pool.\n\n\ndisk_size\n [Integer, required]: Size of the disk in megabytes.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create disk. Examples: \ntype\n, \nops\n\n\n\n\n\n\n\n\nExample:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \nmy-fast-disk\n\n  \ndisk_size\n:\n \n1_024\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n\n-\n \nname\n:\n \nmy-standard-disk\n\n  \ndisk_size\n:\n \n1_024\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \nstandard\n}\n\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nredis\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \nredis\n,\n \nrelease\n:\n \nredis\n}\n\n  \ninstances\n:\n \n1\n\n  \nresource_pool\n:\n \ndefault\n\n  \npersistent_disk_pool\n:\n \nmy-fast-disk\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nChecking Stats \n\u00b6\n\n\nAfter your deployment completes, run \nbosh vms --vitals\n from a terminal window to view persistent disk usage percentage values under \nPersistent Disk Usage\n.\n\n\n\n\nAccessing Persistent Disks \n\u00b6\n\n\nThe CPI mounts persistent disks \n/var/vcap/store\n on deployed VMs, and persists any files stored in \n/var/vcap/store\n.\n\n\nYou specify jobs using the \njobs\n key when defining a instance group. By convention, each job creates a self-named directory in \n/var/vcap/store\n and sets the correct permissions on this directory.\n\n\nFor example, a \nredis\n job creates the following directory: \n/var/vcap/store/redis\n\n\n\n\nChanging Disk Properties \n\u00b6\n\n\nBOSH allows you to change disk types and sizes by modifying the deployment manifest. As long as the instance group name stays the same, data on existing persistent disks will be migrated onto new persistent disks. Old persistent disks will be marked as orphaned.\n\n\nDuring the disk migration from one disk type and size to another, the Director communicates with the Agent to attach both existing and newly created disk to the same VM and copy over any existing data. After the transfer successfully completes, the Director deletes the original disk and keeps the new disk attached to the VM instance.\n\n\nNote: An IaaS might disallow attaching particular disk types and sizes to certain VM types. Consult your IaaS documentation for more information.\n\n\n\n\n\nOrphaned Disks \n\u00b6\n\n\nOrphaned persistent disks are not attached to any VM and are not associated with any deployment. You can list orphaned disks known to the Director via \nbosh disks --orphaned\n command\n. If deployment changes were done erroneously and you would like to reattach specific orphaned persistent disk to an instance follow these steps:\n\n\n\n\nrun \nbosh stop name/id\n command to stop instance (or multiple instances) for repair\n\n\nrun \nbosh attach-disk name/id disk-cid\n command\n to attach disk to given instance\n\n\nrun \nbosh start name/id\n command to resume running instance workload\n\n\n\n\nFor example, to re-attach the disk:\n\n\nbosh attach-disk redis/a4ecc903-e342-4a40-8a59-4c9e4aeba28d 1c13b266-6e14-4124-51f6-24ec3bc05344\n\n\nNote: `attach-disk` command can also attach available disks found in the IaaS. They don't have to be listed in the orphaned disks list.\n\n\n\nOrphaned disks are deleted after \n5 days by default\n. You can decide to clean up orphaned disks manually with \nbosh clean-up --all\n or one-by-one with \nbosh delete-disk\n.\n\n\n\n\nBack to Table of Contents",
            "title": "Manually Attaching Disks"
        },
        {
            "location": "/persistent-disks/#persistent-disk-declaration",
            "text": "To specify that an instance group needs an attached persistent disk, add a  persistent_disk  key-value pair to the instance group in the  Jobs  block of your deployment manifest.  The  persistent_disk  key-value pair specifies the persistent disk size, and defaults to 0 (no persistent disk). If the  persistent_disk  value is a positive integer, BOSH creates a persistent disk of that size in megabytes and attaches it to each instance VM for the job.  Example:  instance_groups :  -   name :   redis \n   jobs : \n   -   { name :   redis ,   release :   redis } \n   instances :   1 \n   resource_pool :   default \n   persistent_disk :   1024 \n   networks : \n   -   name :   default   Note: If you use persistent disk declaration, you cannot specify the persistent disk type or size that the CPI attaches to your job VMs. Instead, the CPI uses its default disk configuration when deploying the VMs.",
            "title": "Persistent Disk Declaration "
        },
        {
            "location": "/persistent-disks/#persistent-disk-pool-declaration",
            "text": "To specify that an instance group needs an attached persistent disk, add a  Disk Pool  block to your deployment manifest.  The persistent disk pool declaration allows you to specify the precise type and size of the persistent disks attached to your instance group VMs.    persistent_disk_pool  [String, optional]: Associated with an instance group; specifies a particular disk_pool.    disk_pools  [Array, optional]: Specifies the  disk_pools  a deployment uses. A deployment manifest can describe multiple disk pools and uses unique names to identify and reference them.   name  [String, required]: A unique name used to identify and reference the disk pool.  disk_size  [Integer, required]: Size of the disk in megabytes.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to create disk. Examples:  type ,  ops     Example:  disk_pools :  -   name :   my-fast-disk \n   disk_size :   1_024 \n   cloud_properties :   { type :   gp2 }  -   name :   my-standard-disk \n   disk_size :   1_024 \n   cloud_properties :   { type :   standard }  instance_groups :  -   name :   redis \n   jobs : \n   -   { name :   redis ,   release :   redis } \n   instances :   1 \n   resource_pool :   default \n   persistent_disk_pool :   my-fast-disk \n   networks : \n   -   name :   default",
            "title": "Persistent Disk Pool Declaration "
        },
        {
            "location": "/persistent-disks/#checking-stats",
            "text": "After your deployment completes, run  bosh vms --vitals  from a terminal window to view persistent disk usage percentage values under  Persistent Disk Usage .",
            "title": "Checking Stats "
        },
        {
            "location": "/persistent-disks/#accessing-persistent-disks",
            "text": "The CPI mounts persistent disks  /var/vcap/store  on deployed VMs, and persists any files stored in  /var/vcap/store .  You specify jobs using the  jobs  key when defining a instance group. By convention, each job creates a self-named directory in  /var/vcap/store  and sets the correct permissions on this directory.  For example, a  redis  job creates the following directory:  /var/vcap/store/redis",
            "title": "Accessing Persistent Disks "
        },
        {
            "location": "/persistent-disks/#changing-disk-properties",
            "text": "BOSH allows you to change disk types and sizes by modifying the deployment manifest. As long as the instance group name stays the same, data on existing persistent disks will be migrated onto new persistent disks. Old persistent disks will be marked as orphaned.  During the disk migration from one disk type and size to another, the Director communicates with the Agent to attach both existing and newly created disk to the same VM and copy over any existing data. After the transfer successfully completes, the Director deletes the original disk and keeps the new disk attached to the VM instance.  Note: An IaaS might disallow attaching particular disk types and sizes to certain VM types. Consult your IaaS documentation for more information.",
            "title": "Changing Disk Properties "
        },
        {
            "location": "/persistent-disks/#orphaned-disks",
            "text": "Orphaned persistent disks are not attached to any VM and are not associated with any deployment. You can list orphaned disks known to the Director via  bosh disks --orphaned  command . If deployment changes were done erroneously and you would like to reattach specific orphaned persistent disk to an instance follow these steps:   run  bosh stop name/id  command to stop instance (or multiple instances) for repair  run  bosh attach-disk name/id disk-cid  command  to attach disk to given instance  run  bosh start name/id  command to resume running instance workload   For example, to re-attach the disk:  bosh attach-disk redis/a4ecc903-e342-4a40-8a59-4c9e4aeba28d 1c13b266-6e14-4124-51f6-24ec3bc05344  Note: `attach-disk` command can also attach available disks found in the IaaS. They don't have to be listed in the orphaned disks list.  Orphaned disks are deleted after  5 days by default . You can decide to clean up orphaned disks manually with  bosh clean-up --all  or one-by-one with  bosh delete-disk .   Back to Table of Contents",
            "title": "Orphaned Disks "
        },
        {
            "location": "/persistent-disks/",
            "text": "Note: This document was updated to mention orphaned disks introduced in bosh-release v241+ (1.3163.0).\n\n\n\nInstance groups may need to store persistent data.\n\n\nIf you attach a persistent disk to a virtual machine and then stop, terminate, or delete the VM, your persistent disk data remains intact. Attaching the persistent disk to another VM allows you to access your data.\n\n\nPersistent disks are kept for each instance under the following circumstances:\n\n\n\n\nupdating deployment to use new releases or stemcells\n\n\nusing cloud check to recover deleted VMs\n\n\ninstances are hard stopped and later started again\n\n\n\n\nAs of bosh-release v241+ (1.3163.0), the Director no longer deletes persistent disks that are no longer needed. Unnecessary persistent disks will be marked as orphaned so that they can be garbage collected after 5 days.\n\n\nThe following conditions result in persistent disks to be marked as orphaned:\n\n\n\n\ninstance group no longer specifies a persistent disk size or a disk pool\n\n\ninstance group changes the size or cloud properties of a disk\n\n\ninstance group is renamed without \nmigrated_from\n configuration\n\n\ninstance group is scaled down\n\n\ninstance group is deleted or AZ assignment is removed\n\n\ndeployment is deleted\n\n\n\n\nYou can specify that an instance group needs an attached persistent disk in one of two ways:\n\n\n\n\nPersistent Disk declaration\n\n\nPersistent Disk Pool declaration\n\n\n\n\n\n\nPersistent Disk Declaration \n\u00b6\n\n\nTo specify that an instance group needs an attached persistent disk, add a \npersistent_disk\n key-value pair to the instance group in the \nJobs\n block of your deployment manifest.\n\n\nThe \npersistent_disk\n key-value pair specifies the persistent disk size, and defaults to 0 (no persistent disk). If the \npersistent_disk\n value is a positive integer, BOSH creates a persistent disk of that size in megabytes and attaches it to each instance VM for the job.\n\n\nExample:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nredis\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \nredis\n,\n \nrelease\n:\n \nredis\n}\n\n  \ninstances\n:\n \n1\n\n  \nresource_pool\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n1024\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\nNote: If you use persistent disk declaration, you cannot specify the persistent disk type or size that the CPI attaches to your job VMs. Instead, the CPI uses its default disk configuration when deploying the VMs.\n\n\n\n\n\nPersistent Disk Pool Declaration \n\u00b6\n\n\nTo specify that an instance group needs an attached persistent disk, add a \nDisk Pool\n block to your deployment manifest.\n\n\nThe persistent disk pool declaration allows you to specify the precise type and size of the persistent disks attached to your instance group VMs.\n\n\n\n\n\n\npersistent_disk_pool\n [String, optional]: Associated with an instance group; specifies a particular disk_pool.\n\n\n\n\n\n\ndisk_pools\n [Array, optional]: Specifies the \ndisk_pools\n a deployment uses. A deployment manifest can describe multiple disk pools and uses unique names to identify and reference them.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the disk pool.\n\n\ndisk_size\n [Integer, required]: Size of the disk in megabytes.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create disk. Examples: \ntype\n, \nops\n\n\n\n\n\n\n\n\nExample:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \nmy-fast-disk\n\n  \ndisk_size\n:\n \n1_024\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n\n-\n \nname\n:\n \nmy-standard-disk\n\n  \ndisk_size\n:\n \n1_024\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \nstandard\n}\n\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nredis\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \nredis\n,\n \nrelease\n:\n \nredis\n}\n\n  \ninstances\n:\n \n1\n\n  \nresource_pool\n:\n \ndefault\n\n  \npersistent_disk_pool\n:\n \nmy-fast-disk\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nChecking Stats \n\u00b6\n\n\nAfter your deployment completes, run \nbosh vms --vitals\n from a terminal window to view persistent disk usage percentage values under \nPersistent Disk Usage\n.\n\n\n\n\nAccessing Persistent Disks \n\u00b6\n\n\nThe CPI mounts persistent disks \n/var/vcap/store\n on deployed VMs, and persists any files stored in \n/var/vcap/store\n.\n\n\nYou specify jobs using the \njobs\n key when defining a instance group. By convention, each job creates a self-named directory in \n/var/vcap/store\n and sets the correct permissions on this directory.\n\n\nFor example, a \nredis\n job creates the following directory: \n/var/vcap/store/redis\n\n\n\n\nChanging Disk Properties \n\u00b6\n\n\nBOSH allows you to change disk types and sizes by modifying the deployment manifest. As long as the instance group name stays the same, data on existing persistent disks will be migrated onto new persistent disks. Old persistent disks will be marked as orphaned.\n\n\nDuring the disk migration from one disk type and size to another, the Director communicates with the Agent to attach both existing and newly created disk to the same VM and copy over any existing data. After the transfer successfully completes, the Director deletes the original disk and keeps the new disk attached to the VM instance.\n\n\nNote: An IaaS might disallow attaching particular disk types and sizes to certain VM types. Consult your IaaS documentation for more information.\n\n\n\n\n\nOrphaned Disks \n\u00b6\n\n\nOrphaned persistent disks are not attached to any VM and are not associated with any deployment. You can list orphaned disks known to the Director via \nbosh disks --orphaned\n command\n. If deployment changes were done erroneously and you would like to reattach specific orphaned persistent disk to an instance follow these steps:\n\n\n\n\nrun \nbosh stop name/id\n command to stop instance (or multiple instances) for repair\n\n\nrun \nbosh attach-disk name/id disk-cid\n command\n to attach disk to given instance\n\n\nrun \nbosh start name/id\n command to resume running instance workload\n\n\n\n\nFor example, to re-attach the disk:\n\n\nbosh attach-disk redis/a4ecc903-e342-4a40-8a59-4c9e4aeba28d 1c13b266-6e14-4124-51f6-24ec3bc05344\n\n\nNote: `attach-disk` command can also attach available disks found in the IaaS. They don't have to be listed in the orphaned disks list.\n\n\n\nOrphaned disks are deleted after \n5 days by default\n. You can decide to clean up orphaned disks manually with \nbosh clean-up --all\n or one-by-one with \nbosh delete-disk\n.\n\n\n\n\nBack to Table of Contents",
            "title": "Orphan Disks"
        },
        {
            "location": "/persistent-disks/#persistent-disk-declaration",
            "text": "To specify that an instance group needs an attached persistent disk, add a  persistent_disk  key-value pair to the instance group in the  Jobs  block of your deployment manifest.  The  persistent_disk  key-value pair specifies the persistent disk size, and defaults to 0 (no persistent disk). If the  persistent_disk  value is a positive integer, BOSH creates a persistent disk of that size in megabytes and attaches it to each instance VM for the job.  Example:  instance_groups :  -   name :   redis \n   jobs : \n   -   { name :   redis ,   release :   redis } \n   instances :   1 \n   resource_pool :   default \n   persistent_disk :   1024 \n   networks : \n   -   name :   default   Note: If you use persistent disk declaration, you cannot specify the persistent disk type or size that the CPI attaches to your job VMs. Instead, the CPI uses its default disk configuration when deploying the VMs.",
            "title": "Persistent Disk Declaration "
        },
        {
            "location": "/persistent-disks/#persistent-disk-pool-declaration",
            "text": "To specify that an instance group needs an attached persistent disk, add a  Disk Pool  block to your deployment manifest.  The persistent disk pool declaration allows you to specify the precise type and size of the persistent disks attached to your instance group VMs.    persistent_disk_pool  [String, optional]: Associated with an instance group; specifies a particular disk_pool.    disk_pools  [Array, optional]: Specifies the  disk_pools  a deployment uses. A deployment manifest can describe multiple disk pools and uses unique names to identify and reference them.   name  [String, required]: A unique name used to identify and reference the disk pool.  disk_size  [Integer, required]: Size of the disk in megabytes.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to create disk. Examples:  type ,  ops     Example:  disk_pools :  -   name :   my-fast-disk \n   disk_size :   1_024 \n   cloud_properties :   { type :   gp2 }  -   name :   my-standard-disk \n   disk_size :   1_024 \n   cloud_properties :   { type :   standard }  instance_groups :  -   name :   redis \n   jobs : \n   -   { name :   redis ,   release :   redis } \n   instances :   1 \n   resource_pool :   default \n   persistent_disk_pool :   my-fast-disk \n   networks : \n   -   name :   default",
            "title": "Persistent Disk Pool Declaration "
        },
        {
            "location": "/persistent-disks/#checking-stats",
            "text": "After your deployment completes, run  bosh vms --vitals  from a terminal window to view persistent disk usage percentage values under  Persistent Disk Usage .",
            "title": "Checking Stats "
        },
        {
            "location": "/persistent-disks/#accessing-persistent-disks",
            "text": "The CPI mounts persistent disks  /var/vcap/store  on deployed VMs, and persists any files stored in  /var/vcap/store .  You specify jobs using the  jobs  key when defining a instance group. By convention, each job creates a self-named directory in  /var/vcap/store  and sets the correct permissions on this directory.  For example, a  redis  job creates the following directory:  /var/vcap/store/redis",
            "title": "Accessing Persistent Disks "
        },
        {
            "location": "/persistent-disks/#changing-disk-properties",
            "text": "BOSH allows you to change disk types and sizes by modifying the deployment manifest. As long as the instance group name stays the same, data on existing persistent disks will be migrated onto new persistent disks. Old persistent disks will be marked as orphaned.  During the disk migration from one disk type and size to another, the Director communicates with the Agent to attach both existing and newly created disk to the same VM and copy over any existing data. After the transfer successfully completes, the Director deletes the original disk and keeps the new disk attached to the VM instance.  Note: An IaaS might disallow attaching particular disk types and sizes to certain VM types. Consult your IaaS documentation for more information.",
            "title": "Changing Disk Properties "
        },
        {
            "location": "/persistent-disks/#orphaned-disks",
            "text": "Orphaned persistent disks are not attached to any VM and are not associated with any deployment. You can list orphaned disks known to the Director via  bosh disks --orphaned  command . If deployment changes were done erroneously and you would like to reattach specific orphaned persistent disk to an instance follow these steps:   run  bosh stop name/id  command to stop instance (or multiple instances) for repair  run  bosh attach-disk name/id disk-cid  command  to attach disk to given instance  run  bosh start name/id  command to resume running instance workload   For example, to re-attach the disk:  bosh attach-disk redis/a4ecc903-e342-4a40-8a59-4c9e4aeba28d 1c13b266-6e14-4124-51f6-24ec3bc05344  Note: `attach-disk` command can also attach available disks found in the IaaS. They don't have to be listed in the orphaned disks list.  Orphaned disks are deleted after  5 days by default . You can decide to clean up orphaned disks manually with  bosh clean-up --all  or one-by-one with  bosh delete-disk .   Back to Table of Contents",
            "title": "Orphaned Disks "
        },
        {
            "location": "/snapshots/",
            "text": "Note\n: This feature is experimental.\n\n\n\nA disk snapshot is a shallow or full copy of a persistent disk at the time of the snapshot creation. Disk snapshotting is implemented with the help of CPIs which use the IaaS snapshot functionality to efficiently make copies of disks.\n\n\nTake a disk snapshot of a persistent disk before deploying major updates or for other important events. If the changes corrupt persistent disk, promote a disk snapshot to be a persistent disk and attach it to the VM to restore data prior to your changes. Currently BOSH does not provide a CLI command to recover from a snapshot so you must use the recovery features of your IaaS with the \nsnapshot Content IDs (CIDs)\n to recover the snapshots.\n\n\nNote\n: While snapshots allow you to recover disk to a prior state, snapshots are not backups. Taking a snapshot does not necessarily create a complete copy of the original disk. If the original disk is deleted, your IaaS may invalidate any snapshot files.\n\n\n\nEnabling Snapshots \n\u00b6\n\n\nSince the IaaS might or might not provide snapshotting functionality, disk snapshots are disabled by default. If your IaaS supports snapshots, you must enable snapshots in your IaaS and in the Director to use disk snapshots.\n\n\nTo enable disk snapshots:\n\n\n\n\n\n\nChange the deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nenable_snapshots\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nRun \nbosh deploy\n to update your deployment.\n\n\n\n\n\n\nManual Snapshots \n\u00b6\n\n\nOnce you enable snapshots in your deployment, you can use following CLI commands to take snapshots on demand.\n\n\nNote\n: When you manually take a snapshot, the Director does not pause any processes or flush buffered data to disk. Depending on your IaaS, a snapshot taken manually might not fully capture all the data on your VM at the point you take the snapshot.\n\n\n\n$ bosh snapshots\n\n\n\n\nDisplays the job, Content ID (CID), and created date of all snapshots. Run \nbosh snapshots\n to display a list of CIDs if you need to find specific snapshots to recover.\n\n\n$ bosh take snapshot \n[\nJOB\n]\n \n[\nINDEX\n]\n\n\n\n\n\nTakes a snapshot of the job VM that you specify. If you do not specify a \nJOB\n, takes a snapshot of every VM in the current deployment.\n\n\n$ bosh delete snapshot SNAPSHOT-CID\n\n\n\n\nDeletes the snapshot that SNAPSHOT-CID specifies.\n\n\n$ bosh delete snapshots\n\n\n\n\nDeletes all snapshots.\n\n\nJob Update Snapshots \n\u00b6\n\n\nOnce you enable snapshots in the Director, the Director automatically takes a snapshot of the persistent disk whenever an event triggers a deployment job update. Before taking the snapshot, the Director waits for release job processes to stop (and/or drain).\n\n\nScheduled Snapshots \n\u00b6\n\n\nThe Director can take snapshot of a persistent disk at regular intervals for all VMs in all deployments and the VM the Director is running on.\n\n\nNote\n: When the Director starts a scheduled snapshot, it does not pause any processes or flush buffered data to disk. Depending on your IaaS, a scheduled snapshot might not fully capture all the data on your VM at the point you take the snapshot.\n\n\n\nTo schedule snapshots for all VMs in all deployments:\n\n\n\n\n\n\nAdd a \nsnapshot_schedule\n key to the \ndirector\n block of your deployment manifest.\n\n\n\n\n\n\nAdd a \ncron-formatted\n schedule as a value for the \nsnapshot_schedule\n key.\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nenable_snapshots\n:\n \ntrue\n\n    \nsnapshot_schedule\n:\n \n0 0 7 * * * UTC\n\n\n\n\n\n\n\n\n\nRun \nbosh deploy\n to update your deployment.\n\n\n\n\n\n\nTo schedule snapshots for the Director VM:\n\n\n\n\n\n\nAdd a \nself_snapshot_schedule\n key to the \ndirector\n block of your deployment manifest.\n\n\n\n\n\n\nAdd a cron-formatted schedule as a value for the \nself_snapshot_schedule\n key.\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nenable_snapshots\n:\n \ntrue\n\n    \nself_snapshot_schedule\n:\n \n0 0 6 * * * UTC\n\n\n\n\n\n\n\n\n\nRun \nbosh deploy\n to update your deployment.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nAutomatic repair with Resurrector",
            "title": "Taking Snapshots"
        },
        {
            "location": "/snapshots/#enabling-snapshots",
            "text": "Since the IaaS might or might not provide snapshotting functionality, disk snapshots are disabled by default. If your IaaS supports snapshots, you must enable snapshots in your IaaS and in the Director to use disk snapshots.  To enable disk snapshots:    Change the deployment manifest for the Director:  properties : \n   director : \n     enable_snapshots :   true     Run  bosh deploy  to update your deployment.",
            "title": "Enabling Snapshots "
        },
        {
            "location": "/snapshots/#manual-snapshots",
            "text": "Once you enable snapshots in your deployment, you can use following CLI commands to take snapshots on demand.  Note : When you manually take a snapshot, the Director does not pause any processes or flush buffered data to disk. Depending on your IaaS, a snapshot taken manually might not fully capture all the data on your VM at the point you take the snapshot.  $ bosh snapshots  Displays the job, Content ID (CID), and created date of all snapshots. Run  bosh snapshots  to display a list of CIDs if you need to find specific snapshots to recover.  $ bosh take snapshot  [ JOB ]   [ INDEX ]   Takes a snapshot of the job VM that you specify. If you do not specify a  JOB , takes a snapshot of every VM in the current deployment.  $ bosh delete snapshot SNAPSHOT-CID  Deletes the snapshot that SNAPSHOT-CID specifies.  $ bosh delete snapshots  Deletes all snapshots.",
            "title": "Manual Snapshots "
        },
        {
            "location": "/snapshots/#job-update-snapshots",
            "text": "Once you enable snapshots in the Director, the Director automatically takes a snapshot of the persistent disk whenever an event triggers a deployment job update. Before taking the snapshot, the Director waits for release job processes to stop (and/or drain).",
            "title": "Job Update Snapshots "
        },
        {
            "location": "/snapshots/#scheduled-snapshots",
            "text": "The Director can take snapshot of a persistent disk at regular intervals for all VMs in all deployments and the VM the Director is running on.  Note : When the Director starts a scheduled snapshot, it does not pause any processes or flush buffered data to disk. Depending on your IaaS, a scheduled snapshot might not fully capture all the data on your VM at the point you take the snapshot.  To schedule snapshots for all VMs in all deployments:    Add a  snapshot_schedule  key to the  director  block of your deployment manifest.    Add a  cron-formatted  schedule as a value for the  snapshot_schedule  key.  properties : \n   director : \n     enable_snapshots :   true \n     snapshot_schedule :   0 0 7 * * * UTC     Run  bosh deploy  to update your deployment.    To schedule snapshots for the Director VM:    Add a  self_snapshot_schedule  key to the  director  block of your deployment manifest.    Add a cron-formatted schedule as a value for the  self_snapshot_schedule  key.  properties : \n   director : \n     enable_snapshots :   true \n     self_snapshot_schedule :   0 0 6 * * * UTC     Run  bosh deploy  to update your deployment.     Back to Table of Contents  Previous:  Automatic repair with Resurrector",
            "title": "Scheduled Snapshots "
        },
        {
            "location": "/persistent-disk-fs/",
            "text": "Note: This feature is available with 3215+ stemcell series.\n\n\n\nCertain releases operate more reliably when persistent data is stored using particular filesystem. The Agent currently supports two different persistent disk filesystem types: \next4\n (default) and \nxfs\n.\n\n\nHere is an example:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nmongo\n\n  \ninstances\n:\n \n3\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nmongo\n\n    \nrelease\n:\n \nmongo\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n10_000\n\n  \nenv\n:\n\n    \npersistent_disk_fs\n:\n \nxfs\n\n\n\n\n\nCurrently this configuration lives in the instance group \nenv\n configuration. (Eventually we will move this configuration onto the disk type where it belongs.) There are few gotchas:\n\n\n\n\nchanging \npersistent_disk_fs\n in any way (even if just explicitly setting the default of \next4\n) results in a VM recreation (but reuses \nsame\n disk)\n\n\nchanging \npersistent_disk_fs\n for an instance group that previously had a persistent disk will not simply reformat existing disk\n\n\n\n\nTo move persistent data to a new persistent disk formatted with a new filesystem you have to set \npersistent_disk_fs\n configuration \nand\n change the disk size. If there was no existing persistent disk (for example, for a new deployment), the Agent will format it as requested.\n\n\n\n\nBack to Table of Contents",
            "title": "Using XFS"
        },
        {
            "location": "/links/",
            "text": "Note: This feature is available with bosh-release v255.5+.\n\n\n\nPreviously, if network communication was required between jobs, release authors had to add job properties to accept other job's network addresses (e.g. a \ndb_ips\n property). Operators then had to explicitly assign static IPs or DNS names for each instance group and fill out network address properties. Such configuration typically relied on some helper tool like spiff or careful manual configuration. It also led to inconsistent network configuration as different jobs named their properties differently. All of that did not make it easy to automate and operate multiple environments.\n\n\nLinks provide a solution to the above problem by making the Director responsible for the IP management. Release authors get a consistent way of retrieving networking (and topology) configuration, and operators have a way to consistently connect components.\n\n\n\n\nOverview \n\u00b6\n\n\nFirst, we provide an overview of the capabilities and logistics of links through a simple example. Here, we have two jobs: an application job and a database job. The database provides its connection information through a link, which the application consumes.\n\n\nIn the database job's spec file, it declares that it provides the connection information:\n\n\n# Database job spec file.\n\n\nname\n:\n \ndatabase_job\n\n\n# ...\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \ndatabase_conn\n\n  \ntype\n:\n \nconn\n\n  \n# Links always carry certain information, like its address and AZ.\n\n  \n# Optionally, the provider can specify other properties in the link.\n\n  \nproperties\n:\n\n  \n-\n \nport\n\n  \n-\n \nadapter\n\n  \n-\n \nusername\n\n  \n-\n \npassword\n\n  \n-\n \nname\n\n\n\nproperties\n:\n\n  \nport\n:\n\n    \ndefault\n:\n \n8080\n\n\n# ...\n\n\n\n\n\nLikewise, the application job's spec file declares that it consumes connection information:\n\n\n# Application job spec file.\n\n\nname\n:\n \napplication_job\n\n\n# ...\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \ndatabase_conn\n\n  \ntype\n:\n \nconn\n\n\n# ...\n\n\n\n\n\nThen, in the application job's templates, it can use the connection information from the link:\n\n\n#!/bin/bash\n\n\n# Application's templated control script.\n\n\n# ...\n\n\nexport\n \nDATABASE_HOST\n=\n\"<%= link('database_conn').instances[0].address %>\"\n\n\nexport\n \nDATABASE_PORT\n=\n\"<%= link('database_conn').p('port') %>\"\n\n\n# ...\n\n\n\n\n\nIf the application uses two database connections, each provided by a separate database instance group, the link becomes ambiguous and we must resolve the ambiguity in the deployment manifest. In this case, the application job's spec file could look like:\n\n\n# Application job spec file.\n\n\nname\n:\n \napplication_job_two_db\n\n\n# ...\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nfrontend_database\n\n  \ntype\n:\n \nconn\n\n\n-\n \nname\n:\n \nbackend_database\n\n  \ntype\n:\n \nconn\n\n\n\n\n\nAnd then in the deployment manifest, we disambiguate the links with \nprovides\n and \nconsumes\n declarations on the jobs. The database instance groupd name their \ndatabase_conn\n links. The application job uses these names to specify which database it will use for each of its links.\n\n\n# ...\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \ndatabase_1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \ndatabase_job\n\n    \nrelease\n:\n \n...\n\n    \nprovides\n:\n\n      \ndatabase_conn\n:\n \n{\nas\n:\n \ndb1\n}\n\n\n-\n \nname\n:\n \ndatabase_2\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \ndatabase_job\n\n    \nrelease\n:\n \n...\n\n    \nprovides\n:\n\n      \ndatabase_conn\n:\n \n{\nas\n:\n \ndb2\n}\n\n\n-\n \nname\n:\n \napplication_ig\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \napplication_job_two_db\n\n    \nrelease\n:\n \n...\n\n    \nconsumes\n:\n\n      \nfrontend_database\n:\n \n{\nfrom\n:\n \ndb1\n}\n\n      \nbackend_database\n:\n \n{\nfrom\n:\n \ndb2\n}\n\n\n# ...\n\n\n\n\n\n\n\nRelease Definitions \n\u00b6\n\n\nInstead of defining properties for every instance group, a job can declare links. (The job either 'consumes' a link provided by another job, or it can 'provide' itself so that any jobs, \nincluding itself\n can 'consume' it).\n\n\nIn the below yaml snippet the \nname\n field is used to differentiate between two links of the same \ntype\n (\ndb\n). Both the \nname\n and \ntype\n that are provided can be arbitrarily defined by release authors. Other releases which consume these links must match the \ntype\n specified.\n\n\nFor example, here is how a \nweb\n job which receives HTTP traffic and talks to at least one database server may be defined. To connect to a database, it consumes \nprimary_db\n and \nsecondary_db\n links of type \ndb\n. It also exposes an \"incoming\" link of type \nhttp\n so that other services can connect to it.\n\n\nNote that when the \nweb\n job is 'consuming' db links, the name of the link does not have to match the name of the provided db link (i.e. postgres has a link called \nconn\n while the \nweb\n job consumes \nprimary_db\n and/or \nsecondary_db\n). The mapping between the provided link named \nconn\n and the consumed link named \nprimary_db\n is done in the \ndeployment manifest file\n.\n\n\nname\n:\n \nweb\n\n\n\ntemplates\n:\n \n{\n...\n}\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nprimary_db\n\n  \ntype\n:\n \ndb\n\n\n-\n \nname\n:\n \nsecondary_db\n\n  \ntype\n:\n \ndb\n\n  \noptional\n:\n \ntrue\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nincoming\n\n  \ntype\n:\n \nhttp\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nNote that the \nsecondary_db\n link has been marked as optional, to indicate that the \nweb\n job will work correctly, even if the operator does not provide a \nsecondary_db\n link. Providing the \nsecondary_db\n link may enable some additional functionality.\n\n\nHere is an example Postgres job that provides a \nconn\n link of type \ndb\n.\n\n\nname\n:\n \npostgres\n\n\n\ntemplates\n:\n \n{\n...\n}\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nconn\n\n  \ntype\n:\n \ndb\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nTemplate Accessors \n\u00b6\n\n\nOnce a release is configured to consume links, the \nlink\n template accessor allows access to link information such as instance names, AZs, IDs, network addresses, etc.\n\n\n\n\nlink(\"...\")\n allows access to linked instances and their properties\n\n\nif_link(\"...\")\n allows conditional access to a link (useful for optional links)\n\n\n\n\nBesides just collecting all network addresses, links include information that may be useful for determining which instances should be selectively communicating (e.g. based on AZ affinity).\n\n\n<\n%=\n\n\n\nresult =\n \n{}\n\n\n\nresult\n[\n\"primary\"\n]\n \n=\n \nlink\n(\n\"primary_db\"\n)\n.\ninstances\n.\nmap\n \ndo\n \n|\ninstance\n|\n\n  \n{\n\n    \n\"name\"\n \n=>\n \ninstance\n.\nname\n,\n\n    \n\"id\"\n \n=>\n \ninstance\n.\nid\n,\n\n    \n\"index\"\n \n=>\n \ninstance\n.\nindex\n,\n\n    \n\"az\"\n \n=>\n \ninstance\n.\naz\n,\n\n    \n\"address\"\n \n=>\n \ninstance\n.\naddress\n,\n\n  \n}\n\n\nend\n\n\n\nif_link\n(\n\"secondary_db\"\n)\n \ndo\n \n|\nsecondary\n|\n\n  \nresult\n[\n\"secondary\"\n]\n \n=\n \nsecondary\n.\ninstances\n.\nmap\n \ndo\n \n|\ninstance\n|\n\n    \n{\n\n      \n\"name\"\n \n=>\n \ninstance\n.\nname\n,\n\n      \n\"id\"\n \n=>\n \ninstance\n.\nid\n,\n\n      \n\"index\"\n \n=>\n \ninstance\n.\nindex\n,\n\n      \n\"az\"\n \n=>\n \ninstance\n.\naz\n,\n\n      \n\"address\"\n \n=>\n \ninstance\n.\naddress\n,\n\n    \n}\n\n  \nend\n\n\nend\n\n\n\nJSON\n.\ndump\n(\nresult\n)\n\n\n\n%>\n\n\n\n\n\nAvailable \nlink\n object methods:\n\n\n\n\naddress\n [String]: Returns single DNS address representing link provider. Using single address is typically a more common way to reference a link provider instead of accessing individual instance addresses (for example, when connecting to a database). Example: \nlink(\"...\").address\n.\n\n\nazs\n [Array of strings, optional]: Argument to filter instance addresses by AZ. Logical OR will be used between AZs when multiple AZs are specified. Example: \nlink(\"...\").address(azs: [spec.az])\n. Default: all instances are returned without AZ filtering.\n\n\np\n [Anything]: Returns property value specified in a link. Works in the same way as regular \np\n accessor.\n\n\ninstances\n [Array of instances]: Returns list of instances included by this provider. Could be an empty array. See methods available on each instance below.\n\n\n\n\nAvailable \ninstance\n object methods:\n\n\n\n\nname\n [String, non-empty]: Instance name as configured in the deployment manifest.\n\n\nid\n [String, non-empty]: Unique ID.\n\n\nindex\n [Integer, non-empty]: Unique numeric index. May have gaps.\n\n\naz\n [String or null, non-empty]: AZ associated with the instance.\n\n\naddress\n [String, non-empty]: IPv4, IPv6 or DNS address. See \nNative DNS Support\n for more details.\n\n\nbootstrap\n [Boolean]: Whether or not this instance is a bootstrap instance.\n\n\n\n\nProperties \n\u00b6\n\n\nSee \nlink properties\n for including additional link information.\n\n\n\n\nDeployment Configuration \n\u00b6\n\n\nGiven the \nweb\n and \npostgres\n job examples above, one can configure a deployment that connects a web app to the database. The following example demonstrates linking defined explicitly in the manifest by saying which jobs provide and consume a link \ndata_db\n.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndata_db\n}\n\n      \nsecondary_db\n:\n \nnil\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndata_db\n}\n\n\n\n\n\nImplicit linking \n\u00b6\n\n\nIf a link type is provided by only one job within a deployment, all release jobs in that deployment that consume links of that type will be implicitly connected to that provider.\n\n\nOptional links are also implicitly connected; however, if no provider can be found, they continue to be \nnil\n.\n\n\nImplicit linking does not happen across deployments.\n\n\nIn the following example, it's unnecessary to explicitly specify that \nweb\n job consumes the \nprimary_db\n link of type \ndb\n from the postgres release job, since the postgres job is the only one that provides a link of type \ndb\n.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n \n{\nsecondary_db\n:\n \nnil\n}\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n\n\n\n\nCommon use cases:\n\n\n\n\ndeployment contains multiple components that are expected to communicate between each other and there is no benefit for the operator to configure these connections explicitly\n\n\n\n\nSelf linking \n\u00b6\n\n\nA job can consume a link that it provides. This could be used to determine a job's own peers.\n\n\nImplicit linking also applies.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \ndiego-etcd\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \netcd\n\n    \nrelease\n:\n \netcd\n\n    \nconsumes\n:\n\n      \netcd\n:\n \n{\nfrom\n:\n \ndiego-etcd\n}\n\n    \nprovides\n:\n\n      \netcd\n:\n \n{\nas\n:\n \ndiego-etcd\n}\n\n\n\n\n\nExample of self linking in Zookeeper release\n.\n\n\nCommon use cases:\n\n\n\n\njob is deployed across multiple instances and each node needs to communicate with other nodes\n\n\n\n\nCustom network linking \n\u00b6\n\n\nBy default, links include network addresses on the producer's default link network. The default link network is a network marked with \ndefault: [gateway]\n. A release job can also consume a link over a different network.\n\n\nFor example, this \nweb\n job will receive \ndata_db\n's network addresses on its \nvip\n network, instead of receiving network addresses from the \nprivate\n network.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndata_db\n,\n \nnetwork\n:\n \nvip\n}\n\n      \nsecondary_db\n:\n \nnil\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nprivate\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndata_db\n}\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nprivate\n\n    \ndefault\n:\n \n[\ngateway\n,\n \ndns\n]\n\n  \n-\n \nname\n:\n \nvip\n\n\n\n\n\nCommon use cases:\n\n\n\n\njob is deployed on two networks, and each network can only route to other particular network; consuming job deployed on a particular network needs to receive specific addresses so that it can connect to providing job.\n\n\n\n\nCross-deployment linking \n\u00b6\n\n\nLinks can be formed between jobs from different deployments as long as the link is marked as \nshared\n.\n\n\nUnlike links within a deployment, updating a link producing job in one deployment does not affect a link consuming job in another deployment \nuntil\n that deployment is redeployed. To do so, run the \nbosh deploy\n command.\n\n\nImplicit linking does not happen across deployments.\n\n\nHere is a deployment that provides a database:\n\n\nname\n:\n \ndata-dep\n\n\njobs\n:\n\n\n-\n \nname\n:\n \ndb\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndb\n,\n \nshared\n:\n \ntrue\n}\n\n\n\n\n\nHere is an app deployment that expects to use the database from the deployment above:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndb\n,\n \ndeployment\n:\n \ndata-dep\n}\n\n      \nsecondary_db\n:\n \nnil\n\n\n\n\n\nCommon use cases:\n\n\n\n\none team is managing one deployment and wants to expose a link for other teams to consume in their deployments in a self service manner\n\n\n\n\n\n\nNext: \nLink properties\n or \nManual linking\n.\n\n\nBack to Table of Contents",
            "title": "Using Implicit Links"
        },
        {
            "location": "/links/#overview",
            "text": "First, we provide an overview of the capabilities and logistics of links through a simple example. Here, we have two jobs: an application job and a database job. The database provides its connection information through a link, which the application consumes.  In the database job's spec file, it declares that it provides the connection information:  # Database job spec file.  name :   database_job  # ...  provides :  -   name :   database_conn \n   type :   conn \n   # Links always carry certain information, like its address and AZ. \n   # Optionally, the provider can specify other properties in the link. \n   properties : \n   -   port \n   -   adapter \n   -   username \n   -   password \n   -   name  properties : \n   port : \n     default :   8080  # ...   Likewise, the application job's spec file declares that it consumes connection information:  # Application job spec file.  name :   application_job  # ...  consumes :  -   name :   database_conn \n   type :   conn  # ...   Then, in the application job's templates, it can use the connection information from the link:  #!/bin/bash  # Application's templated control script.  # ...  export   DATABASE_HOST = \"<%= link('database_conn').instances[0].address %>\"  export   DATABASE_PORT = \"<%= link('database_conn').p('port') %>\"  # ...   If the application uses two database connections, each provided by a separate database instance group, the link becomes ambiguous and we must resolve the ambiguity in the deployment manifest. In this case, the application job's spec file could look like:  # Application job spec file.  name :   application_job_two_db  # ...  consumes :  -   name :   frontend_database \n   type :   conn  -   name :   backend_database \n   type :   conn   And then in the deployment manifest, we disambiguate the links with  provides  and  consumes  declarations on the jobs. The database instance groupd name their  database_conn  links. The application job uses these names to specify which database it will use for each of its links.  # ...  instance_groups :  -   name :   database_1 \n   jobs : \n   -   name :   database_job \n     release :   ... \n     provides : \n       database_conn :   { as :   db1 }  -   name :   database_2 \n   jobs : \n   -   name :   database_job \n     release :   ... \n     provides : \n       database_conn :   { as :   db2 }  -   name :   application_ig \n   jobs : \n   -   name :   application_job_two_db \n     release :   ... \n     consumes : \n       frontend_database :   { from :   db1 } \n       backend_database :   { from :   db2 }  # ...",
            "title": "Overview "
        },
        {
            "location": "/links/#release-definitions",
            "text": "Instead of defining properties for every instance group, a job can declare links. (The job either 'consumes' a link provided by another job, or it can 'provide' itself so that any jobs,  including itself  can 'consume' it).  In the below yaml snippet the  name  field is used to differentiate between two links of the same  type  ( db ). Both the  name  and  type  that are provided can be arbitrarily defined by release authors. Other releases which consume these links must match the  type  specified.  For example, here is how a  web  job which receives HTTP traffic and talks to at least one database server may be defined. To connect to a database, it consumes  primary_db  and  secondary_db  links of type  db . It also exposes an \"incoming\" link of type  http  so that other services can connect to it.  Note that when the  web  job is 'consuming' db links, the name of the link does not have to match the name of the provided db link (i.e. postgres has a link called  conn  while the  web  job consumes  primary_db  and/or  secondary_db ). The mapping between the provided link named  conn  and the consumed link named  primary_db  is done in the  deployment manifest file .  name :   web  templates :   { ... }  consumes :  -   name :   primary_db \n   type :   db  -   name :   secondary_db \n   type :   db \n   optional :   true  provides :  -   name :   incoming \n   type :   http  properties :   { ... }   Note that the  secondary_db  link has been marked as optional, to indicate that the  web  job will work correctly, even if the operator does not provide a  secondary_db  link. Providing the  secondary_db  link may enable some additional functionality.  Here is an example Postgres job that provides a  conn  link of type  db .  name :   postgres  templates :   { ... }  provides :  -   name :   conn \n   type :   db  properties :   { ... }",
            "title": "Release Definitions "
        },
        {
            "location": "/links/#template-accessors",
            "text": "Once a release is configured to consume links, the  link  template accessor allows access to link information such as instance names, AZs, IDs, network addresses, etc.   link(\"...\")  allows access to linked instances and their properties  if_link(\"...\")  allows conditional access to a link (useful for optional links)   Besides just collecting all network addresses, links include information that may be useful for determining which instances should be selectively communicating (e.g. based on AZ affinity).  < %=  result =   {}  result [ \"primary\" ]   =   link ( \"primary_db\" ) . instances . map   do   | instance | \n   { \n     \"name\"   =>   instance . name , \n     \"id\"   =>   instance . id , \n     \"index\"   =>   instance . index , \n     \"az\"   =>   instance . az , \n     \"address\"   =>   instance . address , \n   }  end  if_link ( \"secondary_db\" )   do   | secondary | \n   result [ \"secondary\" ]   =   secondary . instances . map   do   | instance | \n     { \n       \"name\"   =>   instance . name , \n       \"id\"   =>   instance . id , \n       \"index\"   =>   instance . index , \n       \"az\"   =>   instance . az , \n       \"address\"   =>   instance . address , \n     } \n   end  end  JSON . dump ( result )  %>   Available  link  object methods:   address  [String]: Returns single DNS address representing link provider. Using single address is typically a more common way to reference a link provider instead of accessing individual instance addresses (for example, when connecting to a database). Example:  link(\"...\").address .  azs  [Array of strings, optional]: Argument to filter instance addresses by AZ. Logical OR will be used between AZs when multiple AZs are specified. Example:  link(\"...\").address(azs: [spec.az]) . Default: all instances are returned without AZ filtering.  p  [Anything]: Returns property value specified in a link. Works in the same way as regular  p  accessor.  instances  [Array of instances]: Returns list of instances included by this provider. Could be an empty array. See methods available on each instance below.   Available  instance  object methods:   name  [String, non-empty]: Instance name as configured in the deployment manifest.  id  [String, non-empty]: Unique ID.  index  [Integer, non-empty]: Unique numeric index. May have gaps.  az  [String or null, non-empty]: AZ associated with the instance.  address  [String, non-empty]: IPv4, IPv6 or DNS address. See  Native DNS Support  for more details.  bootstrap  [Boolean]: Whether or not this instance is a bootstrap instance.",
            "title": "Template Accessors "
        },
        {
            "location": "/links/#properties",
            "text": "See  link properties  for including additional link information.",
            "title": "Properties "
        },
        {
            "location": "/links/#deployment-configuration",
            "text": "Given the  web  and  postgres  job examples above, one can configure a deployment that connects a web app to the database. The following example demonstrates linking defined explicitly in the manifest by saying which jobs provide and consume a link  data_db .  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   data_db } \n       secondary_db :   nil  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   data_db }",
            "title": "Deployment Configuration "
        },
        {
            "location": "/links/#implicit-linking",
            "text": "If a link type is provided by only one job within a deployment, all release jobs in that deployment that consume links of that type will be implicitly connected to that provider.  Optional links are also implicitly connected; however, if no provider can be found, they continue to be  nil .  Implicit linking does not happen across deployments.  In the following example, it's unnecessary to explicitly specify that  web  job consumes the  primary_db  link of type  db  from the postgres release job, since the postgres job is the only one that provides a link of type  db .  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes :   { secondary_db :   nil }  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres   Common use cases:   deployment contains multiple components that are expected to communicate between each other and there is no benefit for the operator to configure these connections explicitly",
            "title": "Implicit linking "
        },
        {
            "location": "/links/#self-linking",
            "text": "A job can consume a link that it provides. This could be used to determine a job's own peers.  Implicit linking also applies.  instance_groups :  -   name :   diego-etcd \n   jobs : \n   -   name :   etcd \n     release :   etcd \n     consumes : \n       etcd :   { from :   diego-etcd } \n     provides : \n       etcd :   { as :   diego-etcd }   Example of self linking in Zookeeper release .  Common use cases:   job is deployed across multiple instances and each node needs to communicate with other nodes",
            "title": "Self linking "
        },
        {
            "location": "/links/#custom-network-linking",
            "text": "By default, links include network addresses on the producer's default link network. The default link network is a network marked with  default: [gateway] . A release job can also consume a link over a different network.  For example, this  web  job will receive  data_db 's network addresses on its  vip  network, instead of receiving network addresses from the  private  network.  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   data_db ,   network :   vip } \n       secondary_db :   nil \n   networks : \n   -   name :   private  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   data_db } \n   networks : \n   -   name :   private \n     default :   [ gateway ,   dns ] \n   -   name :   vip   Common use cases:   job is deployed on two networks, and each network can only route to other particular network; consuming job deployed on a particular network needs to receive specific addresses so that it can connect to providing job.",
            "title": "Custom network linking "
        },
        {
            "location": "/links/#cross-deployment-linking",
            "text": "Links can be formed between jobs from different deployments as long as the link is marked as  shared .  Unlike links within a deployment, updating a link producing job in one deployment does not affect a link consuming job in another deployment  until  that deployment is redeployed. To do so, run the  bosh deploy  command.  Implicit linking does not happen across deployments.  Here is a deployment that provides a database:  name :   data-dep  jobs :  -   name :   db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   db ,   shared :   true }   Here is an app deployment that expects to use the database from the deployment above:  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   db ,   deployment :   data-dep } \n       secondary_db :   nil   Common use cases:   one team is managing one deployment and wants to expose a link for other teams to consume in their deployments in a self service manner    Next:  Link properties  or  Manual linking .  Back to Table of Contents",
            "title": "Cross-deployment linking "
        },
        {
            "location": "/links/",
            "text": "Note: This feature is available with bosh-release v255.5+.\n\n\n\nPreviously, if network communication was required between jobs, release authors had to add job properties to accept other job's network addresses (e.g. a \ndb_ips\n property). Operators then had to explicitly assign static IPs or DNS names for each instance group and fill out network address properties. Such configuration typically relied on some helper tool like spiff or careful manual configuration. It also led to inconsistent network configuration as different jobs named their properties differently. All of that did not make it easy to automate and operate multiple environments.\n\n\nLinks provide a solution to the above problem by making the Director responsible for the IP management. Release authors get a consistent way of retrieving networking (and topology) configuration, and operators have a way to consistently connect components.\n\n\n\n\nOverview \n\u00b6\n\n\nFirst, we provide an overview of the capabilities and logistics of links through a simple example. Here, we have two jobs: an application job and a database job. The database provides its connection information through a link, which the application consumes.\n\n\nIn the database job's spec file, it declares that it provides the connection information:\n\n\n# Database job spec file.\n\n\nname\n:\n \ndatabase_job\n\n\n# ...\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \ndatabase_conn\n\n  \ntype\n:\n \nconn\n\n  \n# Links always carry certain information, like its address and AZ.\n\n  \n# Optionally, the provider can specify other properties in the link.\n\n  \nproperties\n:\n\n  \n-\n \nport\n\n  \n-\n \nadapter\n\n  \n-\n \nusername\n\n  \n-\n \npassword\n\n  \n-\n \nname\n\n\n\nproperties\n:\n\n  \nport\n:\n\n    \ndefault\n:\n \n8080\n\n\n# ...\n\n\n\n\n\nLikewise, the application job's spec file declares that it consumes connection information:\n\n\n# Application job spec file.\n\n\nname\n:\n \napplication_job\n\n\n# ...\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \ndatabase_conn\n\n  \ntype\n:\n \nconn\n\n\n# ...\n\n\n\n\n\nThen, in the application job's templates, it can use the connection information from the link:\n\n\n#!/bin/bash\n\n\n# Application's templated control script.\n\n\n# ...\n\n\nexport\n \nDATABASE_HOST\n=\n\"<%= link('database_conn').instances[0].address %>\"\n\n\nexport\n \nDATABASE_PORT\n=\n\"<%= link('database_conn').p('port') %>\"\n\n\n# ...\n\n\n\n\n\nIf the application uses two database connections, each provided by a separate database instance group, the link becomes ambiguous and we must resolve the ambiguity in the deployment manifest. In this case, the application job's spec file could look like:\n\n\n# Application job spec file.\n\n\nname\n:\n \napplication_job_two_db\n\n\n# ...\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nfrontend_database\n\n  \ntype\n:\n \nconn\n\n\n-\n \nname\n:\n \nbackend_database\n\n  \ntype\n:\n \nconn\n\n\n\n\n\nAnd then in the deployment manifest, we disambiguate the links with \nprovides\n and \nconsumes\n declarations on the jobs. The database instance groupd name their \ndatabase_conn\n links. The application job uses these names to specify which database it will use for each of its links.\n\n\n# ...\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \ndatabase_1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \ndatabase_job\n\n    \nrelease\n:\n \n...\n\n    \nprovides\n:\n\n      \ndatabase_conn\n:\n \n{\nas\n:\n \ndb1\n}\n\n\n-\n \nname\n:\n \ndatabase_2\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \ndatabase_job\n\n    \nrelease\n:\n \n...\n\n    \nprovides\n:\n\n      \ndatabase_conn\n:\n \n{\nas\n:\n \ndb2\n}\n\n\n-\n \nname\n:\n \napplication_ig\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \napplication_job_two_db\n\n    \nrelease\n:\n \n...\n\n    \nconsumes\n:\n\n      \nfrontend_database\n:\n \n{\nfrom\n:\n \ndb1\n}\n\n      \nbackend_database\n:\n \n{\nfrom\n:\n \ndb2\n}\n\n\n# ...\n\n\n\n\n\n\n\nRelease Definitions \n\u00b6\n\n\nInstead of defining properties for every instance group, a job can declare links. (The job either 'consumes' a link provided by another job, or it can 'provide' itself so that any jobs, \nincluding itself\n can 'consume' it).\n\n\nIn the below yaml snippet the \nname\n field is used to differentiate between two links of the same \ntype\n (\ndb\n). Both the \nname\n and \ntype\n that are provided can be arbitrarily defined by release authors. Other releases which consume these links must match the \ntype\n specified.\n\n\nFor example, here is how a \nweb\n job which receives HTTP traffic and talks to at least one database server may be defined. To connect to a database, it consumes \nprimary_db\n and \nsecondary_db\n links of type \ndb\n. It also exposes an \"incoming\" link of type \nhttp\n so that other services can connect to it.\n\n\nNote that when the \nweb\n job is 'consuming' db links, the name of the link does not have to match the name of the provided db link (i.e. postgres has a link called \nconn\n while the \nweb\n job consumes \nprimary_db\n and/or \nsecondary_db\n). The mapping between the provided link named \nconn\n and the consumed link named \nprimary_db\n is done in the \ndeployment manifest file\n.\n\n\nname\n:\n \nweb\n\n\n\ntemplates\n:\n \n{\n...\n}\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nprimary_db\n\n  \ntype\n:\n \ndb\n\n\n-\n \nname\n:\n \nsecondary_db\n\n  \ntype\n:\n \ndb\n\n  \noptional\n:\n \ntrue\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nincoming\n\n  \ntype\n:\n \nhttp\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nNote that the \nsecondary_db\n link has been marked as optional, to indicate that the \nweb\n job will work correctly, even if the operator does not provide a \nsecondary_db\n link. Providing the \nsecondary_db\n link may enable some additional functionality.\n\n\nHere is an example Postgres job that provides a \nconn\n link of type \ndb\n.\n\n\nname\n:\n \npostgres\n\n\n\ntemplates\n:\n \n{\n...\n}\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nconn\n\n  \ntype\n:\n \ndb\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nTemplate Accessors \n\u00b6\n\n\nOnce a release is configured to consume links, the \nlink\n template accessor allows access to link information such as instance names, AZs, IDs, network addresses, etc.\n\n\n\n\nlink(\"...\")\n allows access to linked instances and their properties\n\n\nif_link(\"...\")\n allows conditional access to a link (useful for optional links)\n\n\n\n\nBesides just collecting all network addresses, links include information that may be useful for determining which instances should be selectively communicating (e.g. based on AZ affinity).\n\n\n<\n%=\n\n\n\nresult =\n \n{}\n\n\n\nresult\n[\n\"primary\"\n]\n \n=\n \nlink\n(\n\"primary_db\"\n)\n.\ninstances\n.\nmap\n \ndo\n \n|\ninstance\n|\n\n  \n{\n\n    \n\"name\"\n \n=>\n \ninstance\n.\nname\n,\n\n    \n\"id\"\n \n=>\n \ninstance\n.\nid\n,\n\n    \n\"index\"\n \n=>\n \ninstance\n.\nindex\n,\n\n    \n\"az\"\n \n=>\n \ninstance\n.\naz\n,\n\n    \n\"address\"\n \n=>\n \ninstance\n.\naddress\n,\n\n  \n}\n\n\nend\n\n\n\nif_link\n(\n\"secondary_db\"\n)\n \ndo\n \n|\nsecondary\n|\n\n  \nresult\n[\n\"secondary\"\n]\n \n=\n \nsecondary\n.\ninstances\n.\nmap\n \ndo\n \n|\ninstance\n|\n\n    \n{\n\n      \n\"name\"\n \n=>\n \ninstance\n.\nname\n,\n\n      \n\"id\"\n \n=>\n \ninstance\n.\nid\n,\n\n      \n\"index\"\n \n=>\n \ninstance\n.\nindex\n,\n\n      \n\"az\"\n \n=>\n \ninstance\n.\naz\n,\n\n      \n\"address\"\n \n=>\n \ninstance\n.\naddress\n,\n\n    \n}\n\n  \nend\n\n\nend\n\n\n\nJSON\n.\ndump\n(\nresult\n)\n\n\n\n%>\n\n\n\n\n\nAvailable \nlink\n object methods:\n\n\n\n\naddress\n [String]: Returns single DNS address representing link provider. Using single address is typically a more common way to reference a link provider instead of accessing individual instance addresses (for example, when connecting to a database). Example: \nlink(\"...\").address\n.\n\n\nazs\n [Array of strings, optional]: Argument to filter instance addresses by AZ. Logical OR will be used between AZs when multiple AZs are specified. Example: \nlink(\"...\").address(azs: [spec.az])\n. Default: all instances are returned without AZ filtering.\n\n\np\n [Anything]: Returns property value specified in a link. Works in the same way as regular \np\n accessor.\n\n\ninstances\n [Array of instances]: Returns list of instances included by this provider. Could be an empty array. See methods available on each instance below.\n\n\n\n\nAvailable \ninstance\n object methods:\n\n\n\n\nname\n [String, non-empty]: Instance name as configured in the deployment manifest.\n\n\nid\n [String, non-empty]: Unique ID.\n\n\nindex\n [Integer, non-empty]: Unique numeric index. May have gaps.\n\n\naz\n [String or null, non-empty]: AZ associated with the instance.\n\n\naddress\n [String, non-empty]: IPv4, IPv6 or DNS address. See \nNative DNS Support\n for more details.\n\n\nbootstrap\n [Boolean]: Whether or not this instance is a bootstrap instance.\n\n\n\n\nProperties \n\u00b6\n\n\nSee \nlink properties\n for including additional link information.\n\n\n\n\nDeployment Configuration \n\u00b6\n\n\nGiven the \nweb\n and \npostgres\n job examples above, one can configure a deployment that connects a web app to the database. The following example demonstrates linking defined explicitly in the manifest by saying which jobs provide and consume a link \ndata_db\n.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndata_db\n}\n\n      \nsecondary_db\n:\n \nnil\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndata_db\n}\n\n\n\n\n\nImplicit linking \n\u00b6\n\n\nIf a link type is provided by only one job within a deployment, all release jobs in that deployment that consume links of that type will be implicitly connected to that provider.\n\n\nOptional links are also implicitly connected; however, if no provider can be found, they continue to be \nnil\n.\n\n\nImplicit linking does not happen across deployments.\n\n\nIn the following example, it's unnecessary to explicitly specify that \nweb\n job consumes the \nprimary_db\n link of type \ndb\n from the postgres release job, since the postgres job is the only one that provides a link of type \ndb\n.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n \n{\nsecondary_db\n:\n \nnil\n}\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n\n\n\n\nCommon use cases:\n\n\n\n\ndeployment contains multiple components that are expected to communicate between each other and there is no benefit for the operator to configure these connections explicitly\n\n\n\n\nSelf linking \n\u00b6\n\n\nA job can consume a link that it provides. This could be used to determine a job's own peers.\n\n\nImplicit linking also applies.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \ndiego-etcd\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \netcd\n\n    \nrelease\n:\n \netcd\n\n    \nconsumes\n:\n\n      \netcd\n:\n \n{\nfrom\n:\n \ndiego-etcd\n}\n\n    \nprovides\n:\n\n      \netcd\n:\n \n{\nas\n:\n \ndiego-etcd\n}\n\n\n\n\n\nExample of self linking in Zookeeper release\n.\n\n\nCommon use cases:\n\n\n\n\njob is deployed across multiple instances and each node needs to communicate with other nodes\n\n\n\n\nCustom network linking \n\u00b6\n\n\nBy default, links include network addresses on the producer's default link network. The default link network is a network marked with \ndefault: [gateway]\n. A release job can also consume a link over a different network.\n\n\nFor example, this \nweb\n job will receive \ndata_db\n's network addresses on its \nvip\n network, instead of receiving network addresses from the \nprivate\n network.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndata_db\n,\n \nnetwork\n:\n \nvip\n}\n\n      \nsecondary_db\n:\n \nnil\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nprivate\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndata_db\n}\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nprivate\n\n    \ndefault\n:\n \n[\ngateway\n,\n \ndns\n]\n\n  \n-\n \nname\n:\n \nvip\n\n\n\n\n\nCommon use cases:\n\n\n\n\njob is deployed on two networks, and each network can only route to other particular network; consuming job deployed on a particular network needs to receive specific addresses so that it can connect to providing job.\n\n\n\n\nCross-deployment linking \n\u00b6\n\n\nLinks can be formed between jobs from different deployments as long as the link is marked as \nshared\n.\n\n\nUnlike links within a deployment, updating a link producing job in one deployment does not affect a link consuming job in another deployment \nuntil\n that deployment is redeployed. To do so, run the \nbosh deploy\n command.\n\n\nImplicit linking does not happen across deployments.\n\n\nHere is a deployment that provides a database:\n\n\nname\n:\n \ndata-dep\n\n\njobs\n:\n\n\n-\n \nname\n:\n \ndb\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndb\n,\n \nshared\n:\n \ntrue\n}\n\n\n\n\n\nHere is an app deployment that expects to use the database from the deployment above:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndb\n,\n \ndeployment\n:\n \ndata-dep\n}\n\n      \nsecondary_db\n:\n \nnil\n\n\n\n\n\nCommon use cases:\n\n\n\n\none team is managing one deployment and wants to expose a link for other teams to consume in their deployments in a self service manner\n\n\n\n\n\n\nNext: \nLink properties\n or \nManual linking\n.\n\n\nBack to Table of Contents",
            "title": "Sharing Links"
        },
        {
            "location": "/links/#overview",
            "text": "First, we provide an overview of the capabilities and logistics of links through a simple example. Here, we have two jobs: an application job and a database job. The database provides its connection information through a link, which the application consumes.  In the database job's spec file, it declares that it provides the connection information:  # Database job spec file.  name :   database_job  # ...  provides :  -   name :   database_conn \n   type :   conn \n   # Links always carry certain information, like its address and AZ. \n   # Optionally, the provider can specify other properties in the link. \n   properties : \n   -   port \n   -   adapter \n   -   username \n   -   password \n   -   name  properties : \n   port : \n     default :   8080  # ...   Likewise, the application job's spec file declares that it consumes connection information:  # Application job spec file.  name :   application_job  # ...  consumes :  -   name :   database_conn \n   type :   conn  # ...   Then, in the application job's templates, it can use the connection information from the link:  #!/bin/bash  # Application's templated control script.  # ...  export   DATABASE_HOST = \"<%= link('database_conn').instances[0].address %>\"  export   DATABASE_PORT = \"<%= link('database_conn').p('port') %>\"  # ...   If the application uses two database connections, each provided by a separate database instance group, the link becomes ambiguous and we must resolve the ambiguity in the deployment manifest. In this case, the application job's spec file could look like:  # Application job spec file.  name :   application_job_two_db  # ...  consumes :  -   name :   frontend_database \n   type :   conn  -   name :   backend_database \n   type :   conn   And then in the deployment manifest, we disambiguate the links with  provides  and  consumes  declarations on the jobs. The database instance groupd name their  database_conn  links. The application job uses these names to specify which database it will use for each of its links.  # ...  instance_groups :  -   name :   database_1 \n   jobs : \n   -   name :   database_job \n     release :   ... \n     provides : \n       database_conn :   { as :   db1 }  -   name :   database_2 \n   jobs : \n   -   name :   database_job \n     release :   ... \n     provides : \n       database_conn :   { as :   db2 }  -   name :   application_ig \n   jobs : \n   -   name :   application_job_two_db \n     release :   ... \n     consumes : \n       frontend_database :   { from :   db1 } \n       backend_database :   { from :   db2 }  # ...",
            "title": "Overview "
        },
        {
            "location": "/links/#release-definitions",
            "text": "Instead of defining properties for every instance group, a job can declare links. (The job either 'consumes' a link provided by another job, or it can 'provide' itself so that any jobs,  including itself  can 'consume' it).  In the below yaml snippet the  name  field is used to differentiate between two links of the same  type  ( db ). Both the  name  and  type  that are provided can be arbitrarily defined by release authors. Other releases which consume these links must match the  type  specified.  For example, here is how a  web  job which receives HTTP traffic and talks to at least one database server may be defined. To connect to a database, it consumes  primary_db  and  secondary_db  links of type  db . It also exposes an \"incoming\" link of type  http  so that other services can connect to it.  Note that when the  web  job is 'consuming' db links, the name of the link does not have to match the name of the provided db link (i.e. postgres has a link called  conn  while the  web  job consumes  primary_db  and/or  secondary_db ). The mapping between the provided link named  conn  and the consumed link named  primary_db  is done in the  deployment manifest file .  name :   web  templates :   { ... }  consumes :  -   name :   primary_db \n   type :   db  -   name :   secondary_db \n   type :   db \n   optional :   true  provides :  -   name :   incoming \n   type :   http  properties :   { ... }   Note that the  secondary_db  link has been marked as optional, to indicate that the  web  job will work correctly, even if the operator does not provide a  secondary_db  link. Providing the  secondary_db  link may enable some additional functionality.  Here is an example Postgres job that provides a  conn  link of type  db .  name :   postgres  templates :   { ... }  provides :  -   name :   conn \n   type :   db  properties :   { ... }",
            "title": "Release Definitions "
        },
        {
            "location": "/links/#template-accessors",
            "text": "Once a release is configured to consume links, the  link  template accessor allows access to link information such as instance names, AZs, IDs, network addresses, etc.   link(\"...\")  allows access to linked instances and their properties  if_link(\"...\")  allows conditional access to a link (useful for optional links)   Besides just collecting all network addresses, links include information that may be useful for determining which instances should be selectively communicating (e.g. based on AZ affinity).  < %=  result =   {}  result [ \"primary\" ]   =   link ( \"primary_db\" ) . instances . map   do   | instance | \n   { \n     \"name\"   =>   instance . name , \n     \"id\"   =>   instance . id , \n     \"index\"   =>   instance . index , \n     \"az\"   =>   instance . az , \n     \"address\"   =>   instance . address , \n   }  end  if_link ( \"secondary_db\" )   do   | secondary | \n   result [ \"secondary\" ]   =   secondary . instances . map   do   | instance | \n     { \n       \"name\"   =>   instance . name , \n       \"id\"   =>   instance . id , \n       \"index\"   =>   instance . index , \n       \"az\"   =>   instance . az , \n       \"address\"   =>   instance . address , \n     } \n   end  end  JSON . dump ( result )  %>   Available  link  object methods:   address  [String]: Returns single DNS address representing link provider. Using single address is typically a more common way to reference a link provider instead of accessing individual instance addresses (for example, when connecting to a database). Example:  link(\"...\").address .  azs  [Array of strings, optional]: Argument to filter instance addresses by AZ. Logical OR will be used between AZs when multiple AZs are specified. Example:  link(\"...\").address(azs: [spec.az]) . Default: all instances are returned without AZ filtering.  p  [Anything]: Returns property value specified in a link. Works in the same way as regular  p  accessor.  instances  [Array of instances]: Returns list of instances included by this provider. Could be an empty array. See methods available on each instance below.   Available  instance  object methods:   name  [String, non-empty]: Instance name as configured in the deployment manifest.  id  [String, non-empty]: Unique ID.  index  [Integer, non-empty]: Unique numeric index. May have gaps.  az  [String or null, non-empty]: AZ associated with the instance.  address  [String, non-empty]: IPv4, IPv6 or DNS address. See  Native DNS Support  for more details.  bootstrap  [Boolean]: Whether or not this instance is a bootstrap instance.",
            "title": "Template Accessors "
        },
        {
            "location": "/links/#properties",
            "text": "See  link properties  for including additional link information.",
            "title": "Properties "
        },
        {
            "location": "/links/#deployment-configuration",
            "text": "Given the  web  and  postgres  job examples above, one can configure a deployment that connects a web app to the database. The following example demonstrates linking defined explicitly in the manifest by saying which jobs provide and consume a link  data_db .  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   data_db } \n       secondary_db :   nil  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   data_db }",
            "title": "Deployment Configuration "
        },
        {
            "location": "/links/#implicit-linking",
            "text": "If a link type is provided by only one job within a deployment, all release jobs in that deployment that consume links of that type will be implicitly connected to that provider.  Optional links are also implicitly connected; however, if no provider can be found, they continue to be  nil .  Implicit linking does not happen across deployments.  In the following example, it's unnecessary to explicitly specify that  web  job consumes the  primary_db  link of type  db  from the postgres release job, since the postgres job is the only one that provides a link of type  db .  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes :   { secondary_db :   nil }  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres   Common use cases:   deployment contains multiple components that are expected to communicate between each other and there is no benefit for the operator to configure these connections explicitly",
            "title": "Implicit linking "
        },
        {
            "location": "/links/#self-linking",
            "text": "A job can consume a link that it provides. This could be used to determine a job's own peers.  Implicit linking also applies.  instance_groups :  -   name :   diego-etcd \n   jobs : \n   -   name :   etcd \n     release :   etcd \n     consumes : \n       etcd :   { from :   diego-etcd } \n     provides : \n       etcd :   { as :   diego-etcd }   Example of self linking in Zookeeper release .  Common use cases:   job is deployed across multiple instances and each node needs to communicate with other nodes",
            "title": "Self linking "
        },
        {
            "location": "/links/#custom-network-linking",
            "text": "By default, links include network addresses on the producer's default link network. The default link network is a network marked with  default: [gateway] . A release job can also consume a link over a different network.  For example, this  web  job will receive  data_db 's network addresses on its  vip  network, instead of receiving network addresses from the  private  network.  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   data_db ,   network :   vip } \n       secondary_db :   nil \n   networks : \n   -   name :   private  -   name :   data_db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   data_db } \n   networks : \n   -   name :   private \n     default :   [ gateway ,   dns ] \n   -   name :   vip   Common use cases:   job is deployed on two networks, and each network can only route to other particular network; consuming job deployed on a particular network needs to receive specific addresses so that it can connect to providing job.",
            "title": "Custom network linking "
        },
        {
            "location": "/links/#cross-deployment-linking",
            "text": "Links can be formed between jobs from different deployments as long as the link is marked as  shared .  Unlike links within a deployment, updating a link producing job in one deployment does not affect a link consuming job in another deployment  until  that deployment is redeployed. To do so, run the  bosh deploy  command.  Implicit linking does not happen across deployments.  Here is a deployment that provides a database:  name :   data-dep  jobs :  -   name :   db \n   jobs : \n   -   name :   postgres \n     release :   postgres \n     provides : \n       conn :   { as :   db ,   shared :   true }   Here is an app deployment that expects to use the database from the deployment above:  instance_groups :  -   name :   app \n   jobs : \n   -   name :   web \n     release :   my-app \n     consumes : \n       primary_db :   { from :   db ,   deployment :   data-dep } \n       secondary_db :   nil   Common use cases:   one team is managing one deployment and wants to expose a link for other teams to consume in their deployments in a self service manner    Next:  Link properties  or  Manual linking .  Back to Table of Contents",
            "title": "Cross-deployment linking "
        },
        {
            "location": "/links-manual/",
            "text": "(See \nLinks\n and \nLink properties\n for an introduction.)\n\n\nNote: This feature is available with bosh-release v256+.\n\n\n\nFor components/endpoints that are not managed by the Director or cannot be linked, operator can explicitly specify full link details in the manifest. This allows release authors to continue exposing a single interface (link) for connecting configuration, instead of exposing adhoc job properties for use when link is not provided.\n\n\nFrom our previous example here is a \nweb\n job that communicates with a database:\n\n\nname\n:\n \nweb\n\n\n\ntemplates\n:\n\n  \nconfig.erb\n:\n \nconfig/conf\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nprimary_db\n\n  \ntype\n:\n \ndb\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nincoming\n\n  \ntype\n:\n \nhttp\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nAnd here is how operator can configure \nweb\n job to talk to an RDS instance instead of a Postgres server deployed with BOSH:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n\n        \ninstances\n:\n\n        \n-\n \naddress\n:\n \nteswfbquts.cabsfabuo7yr.us-east-1.rds.amazonaws.com\n\n        \nproperties\n:\n\n          \nport\n:\n \n3306\n\n          \nadapter\n:\n \nmysql2\n\n          \nusername\n:\n \nsome-username\n\n          \npassword\n:\n \nsome-password\n\n          \nname\n:\n \nmy-app\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Manually Configuring Links"
        },
        {
            "location": "/errands/",
            "text": "(See \nJobs\n for an introduction to jobs.)\n\n\nAny job that includes \nbin/run\n script in its spec file's templates section is considered to be an errand. Operator can trigger execution of an errand at any time after the deploy and receive back script's stdout, stderr and exit code upon its completion.\n\n\n\n\nRelease Definition \n\u00b6\n\n\nExample of an errand job \nsmoke-tests\n from Zookeeper release. \nbin/run\n script is specified in its templates section:\n\n\n---\n\n\nname\n:\n \nsmoke-tests\n\n\n\ntemplates\n:\n\n  \nrun.sh\n:\n \nbin/run\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nconn\n\n  \ntype\n:\n \nzookeeper\n\n  \nproperties\n:\n\n  \n-\n \nclient_port\n\n\n\npackages\n:\n\n\n-\n \nsmoke-tests\n\n\n\nproperties\n:\n \n{}\n\n\n\n\n\nwith a \nrun.sh\n template:\n\n\n#!/bin/bash\n\n\nset\n -e\n<% \nconn\n \n=\n link\n(\n'conn'\n)\n %>\n\nexport\n \nZOOKEEPER_SERVERS\n=\n<%\n=\n conn.instances.map \n{\n \n|\ni\n|\n \n\"#{i.address}:#{conn.p('client_port')}\"\n \n}\n.join\n(\n\",\"\n)\n %>\n/var/vcap/packages/smoke-tests/bin/tests\n\n\n\n\n\n\nInclude in a Deployment \n\u00b6\n\n\nThere are two ways to add an errand to a deployment:\n\n\n\n\nadd it to a dedicated instance group\n\n\nadd it to an existing instance group (colocated) (available in bosh-release v263+)\n\n\n\n\nIn some cases it makes sense to place an errand in a dedicated instance group. You can add an instance group that specifies only an errand job in its jobs section:\n\n\n-\n \nname\n:\n \nsmoke-tests\n\n  \nazs\n:\n \n[\nz1\n]\n\n  \nlifecycle\n:\n \nerrand\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nsmoke-tests\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\nNote that above example uses \nlifecycle\n:\n \nerrand\n configuration to specify that \nsmoke-tests\n instances should only be present when the \nsmoke-tests\n errand is running. Cloud compute resources will be allocated right before errand is running and released when errand is finished.\n\n\nAlternatively, it might make sense to colocate an errand job with other jobs in an existing instance group. This might be useful if an errand is meant to perform work local to an instance or simply to avoid adding additional resources to your deployment:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n,\n \nz3\n]\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nzookeeper\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \n-\n \nname\n:\n \nstatus\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10240\n\n  \nstemcell\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nExecution \n\u00b6\n\n\nUnlike regular jobs which run continiously and get automatically restarted on failure, errand jobs are executed upon operator's request some time after a deploy and if fail do not get restarted. There is no timeout on how long an errand can execute.\n\n\nNote that currently Director will acquire deployment lock for chosen deployment which will prevent execution of other commands that also require deployment lock (for example \nbosh deploy\n or another errand execution). This behaviour will be made more granular over time allowing more commands to run in parallel against a single deployment.\n\n\nAfter running \nbosh deploy\n command\n to update your deployment, you can inspect which errands are available within a deployment via \nbosh errands\n command\n:\n\n\n$ bosh -e vbox -d zookeeper errands\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nUsing deployment \n'zookeeper'\n\n\nName\nsmoke-tests\nstatus\n\n\n2\n errands\n\nSucceeded\n\n\n\n\nTo execute an errand, use \nbosh run-errand\n command\n.\n\n\n$ bosh -e vbox -d zookeeper run-errand status\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nUsing deployment \n'zookeeper'\n\n\nTask \n5609\n\n\nTask \n5609\n \n|\n \n01\n:31:57 \n|\n Preparing deployment: Preparing deployment \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n \n(\n00\n:00:02\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n \n(\n00\n:00:02\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:01 \n|\n Fetching logs \nfor\n zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:01 \n|\n Fetching logs \nfor\n zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\n\nTask \n5609\n Started  Mon Sep \n18\n \n01\n:31:57 UTC \n2017\n\nTask \n5609\n Finished Mon Sep \n18\n \n01\n:32:01 UTC \n2017\n\nTask \n5609\n Duration \n00\n:00:04\nTask \n5609\n \ndone\n\n\nInstance   zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d\nExit Code  \n0\n\nStdout     Mode: leader\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\n\n5\n errand\n(\ns\n)\n\n\nSucceeded\n\n\n\n\nIf an errand job is colocated on multiple instances (over one or more instance groups), by default \nbosh run-errand\n command will execute them all in parallel. You can limit number of instances used for execution via \n--instance\n flag:\n\n\n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\n\n\n\n\nSee \nbosh run-errand\n command\n description for additional ways to use \n--instance\n flag. One of those way is to use \n--instance group/first\n (where \nfirst\n is a literal value) so that errand only runs on one of the instances.\n\n\n\n\nPrevious: \nJobs",
            "title": "Running Errands"
        },
        {
            "location": "/errands/#release-definition",
            "text": "Example of an errand job  smoke-tests  from Zookeeper release.  bin/run  script is specified in its templates section:  ---  name :   smoke-tests  templates : \n   run.sh :   bin/run  consumes :  -   name :   conn \n   type :   zookeeper \n   properties : \n   -   client_port  packages :  -   smoke-tests  properties :   {}   with a  run.sh  template:  #!/bin/bash  set  -e\n<%  conn   =  link ( 'conn' )  %> export   ZOOKEEPER_SERVERS = <% =  conn.instances.map  {   | i |   \"#{i.address}:#{conn.p('client_port')}\"   } .join ( \",\" )  %>\n/var/vcap/packages/smoke-tests/bin/tests",
            "title": "Release Definition "
        },
        {
            "location": "/errands/#include-in-a-deployment",
            "text": "There are two ways to add an errand to a deployment:   add it to a dedicated instance group  add it to an existing instance group (colocated) (available in bosh-release v263+)   In some cases it makes sense to place an errand in a dedicated instance group. You can add an instance group that specifies only an errand job in its jobs section:  -   name :   smoke-tests \n   azs :   [ z1 ] \n   lifecycle :   errand \n   instances :   1 \n   jobs : \n   -   name :   smoke-tests \n     release :   zookeeper \n     properties :   {} \n   vm_type :   default \n   stemcell :   default \n   networks : \n   -   name :   default   Note that above example uses  lifecycle :   errand  configuration to specify that  smoke-tests  instances should only be present when the  smoke-tests  errand is running. Cloud compute resources will be allocated right before errand is running and released when errand is finished.  Alternatively, it might make sense to colocate an errand job with other jobs in an existing instance group. This might be useful if an errand is meant to perform work local to an instance or simply to avoid adding additional resources to your deployment:  -   name :   zookeeper \n   azs :   [ z1 ,   z2 ,   z3 ] \n   instances :   1 \n   jobs : \n   -   name :   zookeeper \n     release :   zookeeper \n     properties :   {} \n   -   name :   status \n     release :   zookeeper \n     properties :   {} \n   vm_type :   default \n   persistent_disk :   10240 \n   stemcell :   default \n   networks : \n   -   name :   default",
            "title": "Include in a Deployment "
        },
        {
            "location": "/errands/#execution",
            "text": "Unlike regular jobs which run continiously and get automatically restarted on failure, errand jobs are executed upon operator's request some time after a deploy and if fail do not get restarted. There is no timeout on how long an errand can execute.  Note that currently Director will acquire deployment lock for chosen deployment which will prevent execution of other commands that also require deployment lock (for example  bosh deploy  or another errand execution). This behaviour will be made more granular over time allowing more commands to run in parallel against a single deployment.  After running  bosh deploy  command  to update your deployment, you can inspect which errands are available within a deployment via  bosh errands  command :  $ bosh -e vbox -d zookeeper errands\nUsing environment  '192.168.56.6'  as client  'admin' \n\nUsing deployment  'zookeeper' \n\nName\nsmoke-tests\nstatus 2  errands\n\nSucceeded  To execute an errand, use  bosh run-errand  command .  $ bosh -e vbox -d zookeeper run-errand status\nUsing environment  '192.168.56.6'  as client  'admin' \n\nUsing deployment  'zookeeper' \n\nTask  5609 \n\nTask  5609   |   01 :31:57  |  Preparing deployment: Preparing deployment  ( 00 :00:01 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) \nTask  5609   |   01 :31:59  |  Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) : Finding and packing log files\nTask  5609   |   01 :31:59  |  Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) : Finding and packing log files\nTask  5609   |   01 :31:59  |  Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) : Finding and packing log files\nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 )   ( 00 :00:02 ) \nTask  5609   |   01 :32:00  |  Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 )   ( 00 :00:02 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) : Finding and packing log files\nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) : Finding and packing log files\nTask  5609   |   01 :32:01  |  Fetching logs  for  zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:01  |  Fetching logs  for  zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) : Finding and packing log files  ( 00 :00:01 ) \n\nTask  5609  Started  Mon Sep  18   01 :31:57 UTC  2017 \nTask  5609  Finished Mon Sep  18   01 :32:01 UTC  2017 \nTask  5609  Duration  00 :00:04\nTask  5609   done \n\nInstance   zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d\nExit Code   0 \nStdout     Mode: leader\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg 5  errand ( s ) \n\nSucceeded  If an errand job is colocated on multiple instances (over one or more instance groups), by default  bosh run-errand  command will execute them all in parallel. You can limit number of instances used for execution via  --instance  flag:  $ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  See  bosh run-errand  command  description for additional ways to use  --instance  flag. One of those way is to use  --instance group/first  (where  first  is a literal value) so that errand only runs on one of the instances.   Previous:  Jobs",
            "title": "Execution "
        },
        {
            "location": "/vm-anti-affinity/",
            "text": "For certain deployment jobs, you might want to distribute the instances across multiple physical resources of the IaaS. Even though an IaaS abstracts away the underlying hardware resources, most have specific APIs to configure VM affinity and anti-affinity rules.\n\n\nOne popular example of a deployment job that needs this type of configuration is Hadoop Datanode. If multiple Datanode instances are placed on the same physical machine, replicated data becomes unavailable if that machine fails. To make replication useful in this scenario, BOSH allows you to configure the resource pool for a deployment job. You configure VM anti-affinity rules for an IaaS using the \ncloud_properties\n sub-block of the \nresource_pools\n block in your \ndeployment manifest\n.\n\n\nCurrently only vSphere and OpenStack CPIs provide a way to do so.\n\n\n\n\nvSphere Configuration \n\u00b6\n\n\nThe vSphere \nVM-VM Affinity Rules\n feature allows you to specify whether VMs should run on the same host or be kept on separate hosts. As of BOSH version 101 (stemcell 2693), you can configure the vSphere CPI to include all VMs of a specified BOSH resource pool within a single DRS rule and separate the VMs among multiple hosts.\n\n\nThe following resource pool and job configuration manifest example instructs BOSH to:\n\n\n\n\nCreate seven hadoop-datanode VMs in the \nmy-vsphere-cluster\n vSphere cluster.\n\n\nCreate a \nseparate-hadoop-datanodes-rule\n DRS rule in the \nmy-vsphere-cluster\n vSphere cluster.\n\n\nConfigure the DRS rule with a \ntype\n that separates the associated VMs onto different hosts.\n\n\nAssociate the seven VMs with the DRS rule.\n\n\n\n\n# Assuming that a Hadoop release is used...\n\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nhadoop-datanodes\n\n  \ncloud_properties\n:\n\n    \ndatacenters\n:\n\n    \n-\n \nname\n:\n \nmy-dc\n\n      \nclusters\n:\n\n      \n-\n \nmy-vsphere-cluster\n:\n\n          \ndrs_rules\n:\n\n          \n-\n \nname\n:\n \nseparate-hadoop-datanodes-rule\n\n            \ntype\n:\n \nseparate_vms\n\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nhadoop-datanode\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nhadoop-datanode\n,\n \nrelease\n:\n \nhadoop\n}\n\n  \ninstances\n:\n \n7\n\n  \nresource_pool\n:\n \nhadoop-datanodes\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\nIf the vSphere CPI does not place the VMs on different hosts, check that you have done the following:\n\n\n\n\nAssociated multiple healthy hosts to the vSphere cluster.\n\n\nEnabled DRS for the vSphere cluster. You can modify the DRS automation level in the cluster settings.\n\n\nEnabled a DRS rule and associated it with the appropriate VMs.\n\n\nGiven the DRS enough time to move the VMs to different hosts.\n\n\n\n\n\n  Notes:\n\n    \n\n      \nThe vSphere CPI currently only supports\none DRS rule per BOSH resource pool.\n\n    \nIf a BOSH resource pool contains only one VM, the vSphere CPI does not create a DRS rule. After BOSH adds a second VM, the vSphere CPI will create and apply a DRS rule to all VMs in the BOSH resource pool.\n\n  \n\n\n\n\n\n\n\nOpenStack Configuration \n\u00b6\n\n\nOpenStack's \nFilter scheduler\n allows to customize compute node selection algorithm which determines placement of new VMs. To enforce anti-affinity among VMs, \nServerGroupAntiAffinityFilter\n is available:\n\n\n\n\nServerGroupAntiAffinityFilter - This filter implements anti-affinity for a server group. First you must create a server group with a policy of 'anti-affinity' via the server groups API. Then, when you boot a new server, provide a scheduler hint of 'group=\n' where \n is the UUID of the server group you created. This will result in the server getting added to the group. When the server gets scheduled, anti-affinity will be enforced among all servers in that group.\n\n\n\n\nThe following resource pool and job configuration manifest example instructs BOSH to:\n\n\n\n\nAssume that the server group was created and its UUID is \naf09abf2-2283-47d6-f2bd-2932a9ae949c\n\n\nAssume that the server group specifies 'anti-affinity' policy\n\n\nCreate seven hadoop-datanode VMs and add them to the server group \naf09abf2-2283-47d6-f2bd-2932a9ae949c\n\n\n\n\n# Assuming that a Hadoop release is used...\n\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nhadoop-datanodes\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm3.xlarge\n\n    \nscheduler_hints\n:\n\n      \ngroup\n:\n \naf09abf2-2283-47d6-f2bd-2932a9ae949c\n\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nhadoop-datanode\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nhadoop-datanode\n,\n \nrelease\n:\n \nhadoop\n}\n\n  \ninstances\n:\n \n7\n\n  \nresource_pool\n:\n \nhadoop-datanodes\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "VM Anti-Affinity"
        },
        {
            "location": "/vm-anti-affinity/#vsphere-configuration",
            "text": "The vSphere  VM-VM Affinity Rules  feature allows you to specify whether VMs should run on the same host or be kept on separate hosts. As of BOSH version 101 (stemcell 2693), you can configure the vSphere CPI to include all VMs of a specified BOSH resource pool within a single DRS rule and separate the VMs among multiple hosts.  The following resource pool and job configuration manifest example instructs BOSH to:   Create seven hadoop-datanode VMs in the  my-vsphere-cluster  vSphere cluster.  Create a  separate-hadoop-datanodes-rule  DRS rule in the  my-vsphere-cluster  vSphere cluster.  Configure the DRS rule with a  type  that separates the associated VMs onto different hosts.  Associate the seven VMs with the DRS rule.   # Assuming that a Hadoop release is used...  resource_pools :  -   name :   hadoop-datanodes \n   cloud_properties : \n     datacenters : \n     -   name :   my-dc \n       clusters : \n       -   my-vsphere-cluster : \n           drs_rules : \n           -   name :   separate-hadoop-datanodes-rule \n             type :   separate_vms  jobs :  -   name :   hadoop-datanode \n   templates : \n   -   { name :   hadoop-datanode ,   release :   hadoop } \n   instances :   7 \n   resource_pool :   hadoop-datanodes \n   persistent_disk :   10_240 \n   networks : \n   -   name :   default   If the vSphere CPI does not place the VMs on different hosts, check that you have done the following:   Associated multiple healthy hosts to the vSphere cluster.  Enabled DRS for the vSphere cluster. You can modify the DRS automation level in the cluster settings.  Enabled a DRS rule and associated it with the appropriate VMs.  Given the DRS enough time to move the VMs to different hosts.   \n  Notes:\n\n     \n       The vSphere CPI currently only supports\none DRS rule per BOSH resource pool. \n     If a BOSH resource pool contains only one VM, the vSphere CPI does not create a DRS rule. After BOSH adds a second VM, the vSphere CPI will create and apply a DRS rule to all VMs in the BOSH resource pool.",
            "title": "vSphere Configuration "
        },
        {
            "location": "/vm-anti-affinity/#openstack-configuration",
            "text": "OpenStack's  Filter scheduler  allows to customize compute node selection algorithm which determines placement of new VMs. To enforce anti-affinity among VMs,  ServerGroupAntiAffinityFilter  is available:   ServerGroupAntiAffinityFilter - This filter implements anti-affinity for a server group. First you must create a server group with a policy of 'anti-affinity' via the server groups API. Then, when you boot a new server, provide a scheduler hint of 'group= ' where   is the UUID of the server group you created. This will result in the server getting added to the group. When the server gets scheduled, anti-affinity will be enforced among all servers in that group.   The following resource pool and job configuration manifest example instructs BOSH to:   Assume that the server group was created and its UUID is  af09abf2-2283-47d6-f2bd-2932a9ae949c  Assume that the server group specifies 'anti-affinity' policy  Create seven hadoop-datanode VMs and add them to the server group  af09abf2-2283-47d6-f2bd-2932a9ae949c   # Assuming that a Hadoop release is used...  resource_pools :  -   name :   hadoop-datanodes \n   cloud_properties : \n     instance_type :   m3.xlarge \n     scheduler_hints : \n       group :   af09abf2-2283-47d6-f2bd-2932a9ae949c  jobs :  -   name :   hadoop-datanode \n   templates : \n   -   { name :   hadoop-datanode ,   release :   hadoop } \n   instances :   7 \n   resource_pool :   hadoop-datanodes \n   persistent_disk :   10_240 \n   networks : \n   -   name :   default    Back to Table of Contents",
            "title": "OpenStack Configuration "
        },
        {
            "location": "/monitoring/",
            "text": "BOSH monitors deployed VMs and release jobs' processes on those VMs via the Health Monitor and the help of the Agent, and Monit.\n\n\n\n\nVMs \n\u00b6\n\n\nThe Health Monitor\n continuously checks presence of the deployed VMs. The Agent on each VM produces a heartbeat every minute and sends it to the Health Monitor over \nNATS\n.\n\n\nThe Health Monitor is extended by a set of plugins. Each plugin is given an opportunity to act on each heartbeat, so in cases of failure it can notify external services or perform actions against the Director.\n\n\nHealth Monitor includes the following plugins:\n\n\n\n\nEvent Logger: Logs events to a file\n\n\nResurrector: Recreates VMs that have stopped heartbeating\n\n\nEmailer: Sends configurable e-mails on events receipt\n\n\nJSON: Sends events over stdin to any executable matching the glob /var/vcap/jobs/\n/bin/bosh-monitor/\n\n\nOpenTSDB: Sends events to \nOpenTSDB\n\n\nGraphite: Sends events to \nGraphite\n\n\nPagerDuty: Sends events to \nPagerDuty.com\n using their API\n\n\nDataDog: Sends events to \nDataDog.com\n using their API\n\n\nAWS CloudWatch: Sends events to \nAmazon's CloudWatch\n using their API\n\n\n\n\nSee \nConfiguring Health Monitor\n for detailed plugins' configuration.\n\n\nResurrector Plugin \n\u00b6\n\n\nResurrector plugin continuously cross-references VMs expected to be running against the VMs that are sending heartbeats. When resurrector does not receive heartbeats for a VM for a certain period of time, it will kick off a task on the Director to try to \"resurrect\" that VM.\n\n\nSee \nAutomatic repair with Resurrector\n for details.\n\n\n\n\nProcesses on VMs \n\u00b6\n\n\nRelease jobs' process monitoring on each VM is done with the help of the \nMonit\n. Monit continuously monitors presence of the configured release jobs' processes and restarts processes that are not found. Process restarts, failures, etc. are reported to the Agent which in turn reports them as alerts to the Health Monitor. Each Health Monitor plugin is given an opportunity to act on each alert.\n\n\n\n\nSSH Events \n\u00b6\n\n\nThe Agent on each VM sends an alert when someone/something tries to log into the system via SSH. Successful and failed attempts are recorded.\n\n\n\n\nDeploy Events \n\u00b6\n\n\nThe Director sends an alert when a deployment starts, successfully completes or errors.\n\n\n\n\nNext: \nProcess monitoring with Monit",
            "title": "Monitoring"
        },
        {
            "location": "/monitoring/#vms",
            "text": "The Health Monitor  continuously checks presence of the deployed VMs. The Agent on each VM produces a heartbeat every minute and sends it to the Health Monitor over  NATS .  The Health Monitor is extended by a set of plugins. Each plugin is given an opportunity to act on each heartbeat, so in cases of failure it can notify external services or perform actions against the Director.  Health Monitor includes the following plugins:   Event Logger: Logs events to a file  Resurrector: Recreates VMs that have stopped heartbeating  Emailer: Sends configurable e-mails on events receipt  JSON: Sends events over stdin to any executable matching the glob /var/vcap/jobs/ /bin/bosh-monitor/  OpenTSDB: Sends events to  OpenTSDB  Graphite: Sends events to  Graphite  PagerDuty: Sends events to  PagerDuty.com  using their API  DataDog: Sends events to  DataDog.com  using their API  AWS CloudWatch: Sends events to  Amazon's CloudWatch  using their API   See  Configuring Health Monitor  for detailed plugins' configuration.",
            "title": "VMs "
        },
        {
            "location": "/monitoring/#resurrector-plugin",
            "text": "Resurrector plugin continuously cross-references VMs expected to be running against the VMs that are sending heartbeats. When resurrector does not receive heartbeats for a VM for a certain period of time, it will kick off a task on the Director to try to \"resurrect\" that VM.  See  Automatic repair with Resurrector  for details.",
            "title": "Resurrector Plugin "
        },
        {
            "location": "/monitoring/#processes-on-vms",
            "text": "Release jobs' process monitoring on each VM is done with the help of the  Monit . Monit continuously monitors presence of the configured release jobs' processes and restarts processes that are not found. Process restarts, failures, etc. are reported to the Agent which in turn reports them as alerts to the Health Monitor. Each Health Monitor plugin is given an opportunity to act on each alert.",
            "title": "Processes on VMs "
        },
        {
            "location": "/monitoring/#ssh-events",
            "text": "The Agent on each VM sends an alert when someone/something tries to log into the system via SSH. Successful and failed attempts are recorded.",
            "title": "SSH Events "
        },
        {
            "location": "/monitoring/#deploy-events",
            "text": "The Director sends an alert when a deployment starts, successfully completes or errors.   Next:  Process monitoring with Monit",
            "title": "Deploy Events "
        },
        {
            "location": "/vm-monit/",
            "text": "The Agent on each deployment job VM is responsible for managing lifecycle of each enabled release job. It starts, monitors, restarts and stops release jobs' processes. These tasks are done with the help of the  \nMonit\n. The Agent communicates with the Monit daemon through Monit HTTP APIs to add, remove, start, stop, monitor and unmonitor release jobs' processes.\n\n\n\n\nCheck Status \n\u00b6\n\n\nAssuming you have a deployment, run \nbosh instances\n to see aggregate status for each deployment job VM:\n\n\n$ bosh instances\n\nDeployment \n`\nmy-deployment\n'\n\n\nDirector task \n10326\n\n\nTask \n10326\n \ndone\n\n\n+----------------+---------+---------------+-------------+\n\n|\n Instance       \n|\n State   \n|\n Resource Pool \n|\n IPs         \n|\n\n+----------------+---------+---------------+-------------+\n\n|\n redis-master/0 \n|\n running \n|\n redis-servers \n|\n \n10\n.10.30.71 \n|\n\n\n|\n redis-slave/0  \n|\n running \n|\n redis-servers \n|\n \n10\n.10.30.72 \n|\n\n\n|\n redis-slave/1  \n|\n failing \n|\n redis-servers \n|\n \n10\n.10.30.73 \n|\n\n+----------------+---------+---------------+-------------+\n\n\n\n\nThere are 3 possible state values:\n\n\n\n\n\n\nrunning\n: the Director received response from the Agent and the Agent reported its aggregate status as successful. Running state indicates that all release jobs' processes are successfully running at that moment.\n\n\n\n\n\n\nfailing\n: the Director received response from the Agent and the Agent reported its aggregate status as not successful. Failing state indicatates \none\n of the release jobs' processes is not successfully running (could be failing to start, or exiting after some time, etc.).\n\n\n\n\n\n\nunresponsive\n: the Director did not receive any response from the Agent\n\n\n\n\n\n\nTo determine what the problem is with a specific VM, you can ssh into the VM and look at the logs and/or Monit directly.\n\n\n\n\nUsing Monit on the VM \n\u00b6\n\n\nOn any BOSH managed VM, you can access Monit status for release jobs' processes via Monit CLI. Before you can run the command you have to switch to become a \nroot\n user (via \nsudo su\n) since Monit executable is only available to root users.\n\n\nEach enabled release job has its own directory in \n/var/vcap/jobs/\n directory. Each release job directory contains a monit file (e.g. \n/var/vcap/jobs/redis-server/monit\n) with final monit configuration for that release job. This is how you can tell which processes belong to which release job. Most release job only start a single process.\n\n\nNote: Monit configuration file in release job directory is just a copy of the actual Monit configuration. Changing it will not affect running Monit configuration.\n\n\n\nTo view status for all processes Monit is managing you can run \nmonit summary\n:\n\n\n$ monit summary\n\nThe Monit daemon \n5\n.2.4 uptime: 1d 22h 7m\n\nProcess \n'nats'\n                      running\nProcess \n'redis'\n                     running\nProcess \n'postgres'\n                  running\nProcess \n'powerdns'\n                  running\nProcess \n'blobstore_nginx'\n           running\nProcess \n'director'\n                  running\nProcess \n'worker_1'\n                  running\nProcess \n'worker_2'\n                  running\nProcess \n'worker_3'\n                  running\nProcess \n'director_scheduler'\n        running\nProcess \n'director_nginx'\n            running\nProcess \n'registry'\n                  running\nProcess \n'health_monitor'\n            running\nSystem \n'system_bm-24638eb6-55b9-4670-bb1a-23c9e3f77d91'\n running\n\n\n\n\nNote: You can use standard \nwatch\n utility with the summary command to track process status over time.\n\n\n\nYou can also get more detailed information about individual processes via \nmonit status\n:\n\n\n$ monit status\n\nThe Monit daemon \n5\n.2.4 uptime: 1d 22h 8m\n\nProcess \n'nats'\n\n  status                            running\n  monitoring status                 monitored\n  pid                               \n2951\n\n  parent pid                        \n1\n\n  uptime                            1d 22h 8m\n  children                          \n0\n\n  memory kilobytes                  \n24420\n\n  memory kilobytes total            \n24420\n\n  memory percent                    \n0\n.6%\n  memory percent total              \n0\n.6%\n  cpu percent                       \n0\n.0%\n  cpu percent total                 \n0\n.0%\n  data collected                    Thu Dec  \n4\n \n22\n:44:36 \n2014\n\n...\n\n\n\n\nWhile debugging why certain process is failing it is usually useful to tell Monit to stop restarting the failing process. You can do so via \nmonit stop <process-name>\n command. To start it back up use \nmonit start <process-name>\n command.\n\n\nSee \nMonit manual\n to learn more about Monit.\n\n\n\n\nNext: \nManual repair with Cloud Check\n\n\nPrevious: \nMonitoring",
            "title": "Process Monitoring with Monit"
        },
        {
            "location": "/vm-monit/#check-status",
            "text": "Assuming you have a deployment, run  bosh instances  to see aggregate status for each deployment job VM:  $ bosh instances\n\nDeployment  ` my-deployment ' \n\nDirector task  10326 \n\nTask  10326   done \n\n+----------------+---------+---------------+-------------+ |  Instance        |  State    |  Resource Pool  |  IPs          | \n+----------------+---------+---------------+-------------+ |  redis-master/0  |  running  |  redis-servers  |   10 .10.30.71  |  |  redis-slave/0   |  running  |  redis-servers  |   10 .10.30.72  |  |  redis-slave/1   |  failing  |  redis-servers  |   10 .10.30.73  | \n+----------------+---------+---------------+-------------+  There are 3 possible state values:    running : the Director received response from the Agent and the Agent reported its aggregate status as successful. Running state indicates that all release jobs' processes are successfully running at that moment.    failing : the Director received response from the Agent and the Agent reported its aggregate status as not successful. Failing state indicatates  one  of the release jobs' processes is not successfully running (could be failing to start, or exiting after some time, etc.).    unresponsive : the Director did not receive any response from the Agent    To determine what the problem is with a specific VM, you can ssh into the VM and look at the logs and/or Monit directly.",
            "title": "Check Status "
        },
        {
            "location": "/vm-monit/#using-monit-on-the-vm",
            "text": "On any BOSH managed VM, you can access Monit status for release jobs' processes via Monit CLI. Before you can run the command you have to switch to become a  root  user (via  sudo su ) since Monit executable is only available to root users.  Each enabled release job has its own directory in  /var/vcap/jobs/  directory. Each release job directory contains a monit file (e.g.  /var/vcap/jobs/redis-server/monit ) with final monit configuration for that release job. This is how you can tell which processes belong to which release job. Most release job only start a single process.  Note: Monit configuration file in release job directory is just a copy of the actual Monit configuration. Changing it will not affect running Monit configuration.  To view status for all processes Monit is managing you can run  monit summary :  $ monit summary\n\nThe Monit daemon  5 .2.4 uptime: 1d 22h 7m\n\nProcess  'nats'                       running\nProcess  'redis'                      running\nProcess  'postgres'                   running\nProcess  'powerdns'                   running\nProcess  'blobstore_nginx'            running\nProcess  'director'                   running\nProcess  'worker_1'                   running\nProcess  'worker_2'                   running\nProcess  'worker_3'                   running\nProcess  'director_scheduler'         running\nProcess  'director_nginx'             running\nProcess  'registry'                   running\nProcess  'health_monitor'             running\nSystem  'system_bm-24638eb6-55b9-4670-bb1a-23c9e3f77d91'  running  Note: You can use standard  watch  utility with the summary command to track process status over time.  You can also get more detailed information about individual processes via  monit status :  $ monit status\n\nThe Monit daemon  5 .2.4 uptime: 1d 22h 8m\n\nProcess  'nats' \n  status                            running\n  monitoring status                 monitored\n  pid                                2951 \n  parent pid                         1 \n  uptime                            1d 22h 8m\n  children                           0 \n  memory kilobytes                   24420 \n  memory kilobytes total             24420 \n  memory percent                     0 .6%\n  memory percent total               0 .6%\n  cpu percent                        0 .0%\n  cpu percent total                  0 .0%\n  data collected                    Thu Dec   4   22 :44:36  2014 \n...  While debugging why certain process is failing it is usually useful to tell Monit to stop restarting the failing process. You can do so via  monit stop <process-name>  command. To start it back up use  monit start <process-name>  command.  See  Monit manual  to learn more about Monit.   Next:  Manual repair with Cloud Check  Previous:  Monitoring",
            "title": "Using Monit on the VM "
        },
        {
            "location": "/cck/",
            "text": "Note: Updated for bosh-release v183 (1.3010.0).\n\n\n\nBOSH provides the Cloud Check CLI command (a.k.a cck) to repair IaaS resources used by a specific deployment. It is not commonly used during normal operations; however, it becomes essential when some IaaS operations fail and the Director cannot resolve problems without a human decision or when the Resurrector is not enabled.\n\n\nThe Resurrector will only try to recover any VMs that are missing from the IaaS or that have non-responsive agents. The cck tool is similar to the Resurrector in that it also looks for those two conditions; however, instead of automatically trying to resolve these problems, it provides several options to the operator.\n\n\nIn addition to looking for those two types of problems, cck also checks correct attachment and presence of persistent disks for each deployment job instance.\n\n\nOnce the deployment is set via the \nbosh deployment\n command you can simply run \nbosh cck\n. Here is an example output when no problems are detected:\n\n\n$ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task \n622\n\n  Started scanning \n1\n vms\n  Started scanning \n1\n vms > Checking VM states. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n1\n vms > \n1\n OK, \n0\n unresponsive, \n0\n missing, \n0\n unbound, \n0\n out of sync. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n1\n vms \n(\n00\n:00:00\n)\n\n\n  Started scanning \n0\n persistent disks\n  Started scanning \n0\n persistent disks > Looking \nfor\n inactive disks. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n0\n persistent disks > \n0\n OK, \n0\n missing, \n0\n inactive, \n0\n mount-info mismatch. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n0\n persistent disks \n(\n00\n:00:00\n)\n\n\nTask \n622\n \ndone\n\n\nStarted     \n2015\n-01-09 \n23\n:29:34 UTC\nFinished    \n2015\n-01-09 \n23\n:29:34 UTC\nDuration    \n00\n:00:00\n\nScan is complete, checking \nif\n any problems found...\nNo problems found\n\n\n\n\n\n\nProblems \n\u00b6\n\n\nVM is missing \n\u00b6\n\n\nAssuming there was a deployment with a VM, somehow that VM was deleted from the IaaS outside of BOSH, here is what cck would report:\n\n\n$ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task \n623\n\n  Started scanning \n1\n vms\n  Started scanning \n1\n vms > Checking VM states. Done \n(\n00\n:00:10\n)\n\n  Started scanning \n1\n vms > \n0\n OK, \n0\n unresponsive, \n1\n missing, \n0\n unbound, \n0\n out of sync. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n1\n vms \n(\n00\n:00:10\n)\n\n\n  Started scanning \n0\n persistent disks\n  Started scanning \n0\n persistent disks > Looking \nfor\n inactive disks. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n0\n persistent disks > \n0\n OK, \n0\n missing, \n0\n inactive, \n0\n mount-info mismatch. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n0\n persistent disks \n(\n00\n:00:00\n)\n\n\nTask \n623\n \ndone\n\n\nStarted     \n2015\n-01-09 \n23\n:32:45 UTC\nFinished    \n2015\n-01-09 \n23\n:32:56 UTC\nDuration    \n00\n:00:11\n\nScan is complete, checking \nif\n any problems found...\n\nFound \n1\n problem\n\nProblem \n1\n of \n1\n: VM with cloud ID \n`\ni-914c046a\n' missing.\n\n\n  1. Skip for now\n\n\n  2. Recreate VM\n\n\n  3. Delete VM reference\n\n\nPlease choose a resolution [1 - 3]: 3\n\n\n\nBelow is the list of resolutions you'\nve provided\nPlease make sure everything is fine and confirm your changes\n\n  \n1\n. VM with cloud ID \n`\ni-914c046a\n' missing.\n\n\n     Delete VM reference\n\n\n\nApply resolutions? (type '\nyes\n'\n to \ncontinue\n)\n: yes\nApplying resolutions...\n\nDirector task \n624\n\n  Started applying problem resolutions > missing_vm \n168\n: Delete VM reference. Done \n(\n00\n:00:00\n)\n\n\nTask \n624\n \ndone\n\n\nStarted     \n2015\n-01-09 \n23\n:33:20 UTC\nFinished    \n2015\n-01-09 \n23\n:33:20 UTC\nDuration    \n00\n:00:00\nCloudcheck is finished\n\n\n\n\ncck determined that \ni-914c046a\n VM was missing. Possible options are:\n\n\n\n\n\n\nSkip for now\n: the Director will not try to resolve this problem right now\n\n\n\n\n\n\nRecreate VM\n: the Director will just create new VM, deploy specified releases according to deployed manifest and finally start release jobs\n\n\nNote on current behaviour: cck will not wait for all release job processes to start.\n\n\n\n\n\n\nDelete VM reference\n: the Director will not create new VM in its place. If cck is run again, it will not report that VM is missing since it's not expected to exist. Running \nbosh deploy\n after cck will backfill missing VMs.\n\n\n\n\n\n\nIn the above example options 3 was picked and VM reference was deleted.\n\n\n\n\nVM is not responsive (unresponsive agent) \n\u00b6\n\n\nAssuming there was a deployment with a VM, somehow Agent is no longer responding to the Director. In this situation \nbosh vms\n will report VM's agent as \nunresponsive agent\n:\n\n\n$ bosh vms simple-deployment --details\n\nDeployment \n`\nsimple-deployment\n'\n\n\nDirector task \n630\n\n\nTask \n630\n \ndone\n\n\n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+\n\n|\n Job/index       \n|\n State              \n|\n Resource Pool \n|\n IPs \n|\n CID        \n|\n Agent ID                             \n|\n Resurrection \n|\n\n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+\n\n|\n unknown/unknown \n|\n unresponsive agent \n|\n               \n|\n     \n|\n i-1db9ede6 \n|\n 59a30081-d63d-4c1b-80be-01fa681d8787 \n|\n active       \n|\n\n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+\n\nVMs total: \n1\n\n\n\n\n\nAlso \nbosh deploy\n will stop at \nBinding existing deployment\n stage since it is not able to communicate with unresponsive agent:\n\n\n$ bosh deploy\n\n..snip...\n\nDeploying\n---------\nDeployment name: \n`\ntiny-dummy.yml\n'\n\n\nDirector name: `idora'\n\nAre you sure you want to deploy? \n(\ntype\n \n'yes'\n to \ncontinue\n)\n: yes\n\nDirector task \n631\n\n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done \n(\n00\n:00:00\n)\n\n  Started preparing deployment > Binding releases. Done \n(\n00\n:00:00\n)\n\n  Started preparing deployment > Binding existing deployment. Failed: Timed out sending \n`\nget_state\n' to 59a30081-d63d-4c1b-80be-01fa681d8787 after 45 seconds (00:02:15)\n\n\n\nError 450002: Timed out sending `get_state'\n to 59a30081-d63d-4c1b-80be-01fa681d8787 after \n45\n seconds\n\nTask \n631\n error\n\nFor a more detailed error report, run: bosh task \n631\n --debug\n\n\n\n\n$ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task \n640\n\n  Started scanning \n1\n vms\n  Started scanning \n1\n vms > Checking VM states. Done \n(\n00\n:00:10\n)\n\n  Started scanning \n1\n vms > \n0\n OK, \n1\n unresponsive, \n0\n missing, \n0\n unbound, \n0\n out of sync. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n1\n vms \n(\n00\n:00:10\n)\n\n\n  Started scanning \n0\n persistent disks\n  Started scanning \n0\n persistent disks > Looking \nfor\n inactive disks. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n0\n persistent disks > \n0\n OK, \n0\n missing, \n0\n inactive, \n0\n mount-info mismatch. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n0\n persistent disks \n(\n00\n:00:00\n)\n\n\nTask \n640\n \ndone\n\n\nStarted   \n2015\n-01-09 \n23\n:33:45 UTC\nFinished  \n2015\n-01-09 \n23\n:33:55 UTC\nDuration  \n00\n:00:10\n\nScan is complete, checking \nif\n any problems found...\n\nFound \n1\n problem\n\nProblem \n1\n of \n1\n: dummy/0 \n(\ni-914c046a\n)\n is not responding.\n  \n1\n. Skip \nfor\n now\n  \n2\n. Reboot VM\n  \n3\n. Recreate VM\n  \n4\n. Delete VM reference \n(\nforceful\n;\n may need to manually delete VM from the Cloud to avoid IP conflicts\n)\n\nPlease choose a resolution \n[\n1\n - \n4\n]\n: \n4\n\n\nBelow is the list of resolutions you\n've provided\n\n\nPlease make sure everything is fine and confirm your changes\n\n\n\n  1. dummy/0 (i-914c046a) is not responding.\n\n\n     Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\n\n\n\nApply resolutions? (type '\nyes\n'\n to \ncontinue\n)\n: yes\nApplying resolutions...\n\nDirector task \n641\n\n  Started applying problem resolutions > unresponsive_agent \n168\n: Delete VM reference \n(\n...\n)\n. Done \n(\n00\n:00:05\n)\n\n\nTask \n641\n \ndone\n\n\nStarted   \n2015\n-01-09 \n23\n:35:20 UTC\nFinished  \n2015\n-01-09 \n23\n:35:25 UTC\nDuration  \n00\n:00:05\nCloudcheck is finished\n\n\n\n\ncck determined that \ni-914c046a\n VM is unresponsive. Possible options are:\n\n\n\n\n\n\nSkip for now\n: the Director will not try to resolve this problem right now\n\n\n\n\n\n\nReboot VM\n: the Director will power off and then power on existing VM\n\n\nNote on current behaviour: cck will not wait for all release job processes to start.\n\n\n\n\n\n\nRecreate VM\n: the Director will delete existing VM, then create a new VM, deploy specified releases according to deployed manifest and finally start release jobs\n\n\nNote on current behaviour: cck will not wait for all release job processes to start.\n\n\n\n\n\n\nDelete VM reference\n: the Director will not create new VM in its place. If cck is run again, it will not report that VM is unresponsive since it does not exist. Running \nbosh deploy\n after cck will backfill missing VMs.\n\n\n\n\n\n\nIn the above example options 4 was picked and VM reference was deleted.\n\n\n\n\nPersistent Disk is not attached \n\u00b6\n\n\nAssuming there was a deployment with a VM, somehow persistent disk got detached.\n\n\n$ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task \n656\n\n  Started scanning \n1\n vms\n  Started scanning \n1\n vms > Checking VM states. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n1\n vms > \n1\n OK, \n0\n unresponsive, \n0\n missing, \n0\n unbound, \n0\n out of sync. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n1\n vms \n(\n00\n:00:00\n)\n\n\n  Started scanning \n1\n persistent disks\n  Started scanning \n1\n persistent disks > Looking \nfor\n inactive disks. Done \n(\n00\n:00:00\n)\n\n  Started scanning \n1\n persistent disks > \n0\n OK, \n0\n missing, \n0\n inactive, \n1\n mount-info mismatch. Done \n(\n00\n:00:00\n)\n\n     Done scanning \n1\n persistent disks \n(\n00\n:00:00\n)\n\n\nTask \n656\n \ndone\n\n\nStarted   \n2015\n-01-13 \n22\n:04:56 UTC\nFinished  \n2015\n-01-13 \n22\n:04:56 UTC\nDuration  \n00\n:00:00\n\nScan is complete, checking \nif\n any problems found...\n\nFound \n1\n problem\n\nProblem \n1\n of \n1\n: Inconsistent mount information:\nRecord shows that disk \n'vol-549f071f'\n should be mounted on i-4fcd99b4.\nHowever it is currently :\n  Not mounted in any VM.\n  \n1\n. Skip \nfor\n now\n  \n2\n. Reattach disk to instance\n  \n3\n. Reattach disk and reboot instance\nPlease choose a resolution \n[\n1\n - \n3\n]\n: \n2\n\n\nBelow is the list of resolutions you\n've provided\n\n\nPlease make sure everything is fine and confirm your changes\n\n\n\n  1. Inconsistent mount information:\n\n\nRecord shows that disk '\nvol-549f071f\n' should be mounted on i-4fcd99b4.\n\n\nHowever it is currently :\n\n\n  Not mounted in any VM\n\n\n     Reattach disk to instance\n\n\n\nApply resolutions? (type '\nyes\n'\n to \ncontinue\n)\n: yes\nApplying resolutions...\n\nDirector task \n657\n\n  Started applying problem resolutions > mount_info_mismatch \n23\n: Reattach disk to instance. Done \n(\n00\n:00:22\n)\n\n\nTask \n657\n \ndone\n\n\nStarted   \n2015\n-01-13 \n22\n:05:19 UTC\nFinished  \n2015\n-01-13 \n22\n:05:41 UTC\nDuration  \n00\n:00:22\nCloudcheck is finished\n\n\n\n\ncck determined that \nvol-549f071f\n persistent disk is not attached to \ni-4fcd99b4\n VM. Possible options are:\n\n\n\n\n\n\nSkip for now\n: the Director will not try to resolve this problem during\n\n\n\n\n\n\nReattach disk to instance\n: the Director will reattach persistent disk to the VM and mount it at its usual location \n/var/vcap/store\n.\n\n\nNote on current behaviour: Release job processes will not be restarted when persistent disk is remounted.\n\n\n\n\n\n\nReattach disk and reboot instance\n: the Director will reattach persistent disk to the VM and reboot it so that Agent can safely mount persistent disk before starting any release job processes.\n\n\nNote on current behaviour: cck will not wait until VM reboots and restarts all release job processes.\n\n\n\n\n\n\n\n\nPersistent Disk is missing \n\u00b6\n\n\nAssuming there was a deployment with a VM, somehow persistent disk got deleted.\n\n\nNote: Not all CPIs implement needed functionality to determine if disk is missing. Those CPIs will report missing disk as \nPersistent Disk is not attached\n problem; however, both reattaching resolutions will fail since persistent disk would not be found.\n\n\n\n\nNext: \nAutomatic repair with Resurrector\n\n\nPrevious: \nProcess monitoring with Monit",
            "title": "IaaS Reconciliation"
        },
        {
            "location": "/cck/#problems",
            "text": "",
            "title": "Problems "
        },
        {
            "location": "/cck/#vm-is-missing",
            "text": "Assuming there was a deployment with a VM, somehow that VM was deleted from the IaaS outside of BOSH, here is what cck would report:  $ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task  623 \n  Started scanning  1  vms\n  Started scanning  1  vms > Checking VM states. Done  ( 00 :00:10 ) \n  Started scanning  1  vms >  0  OK,  0  unresponsive,  1  missing,  0  unbound,  0  out of sync. Done  ( 00 :00:00 ) \n     Done scanning  1  vms  ( 00 :00:10 ) \n\n  Started scanning  0  persistent disks\n  Started scanning  0  persistent disks > Looking  for  inactive disks. Done  ( 00 :00:00 ) \n  Started scanning  0  persistent disks >  0  OK,  0  missing,  0  inactive,  0  mount-info mismatch. Done  ( 00 :00:00 ) \n     Done scanning  0  persistent disks  ( 00 :00:00 ) \n\nTask  623   done \n\nStarted      2015 -01-09  23 :32:45 UTC\nFinished     2015 -01-09  23 :32:56 UTC\nDuration     00 :00:11\n\nScan is complete, checking  if  any problems found...\n\nFound  1  problem\n\nProblem  1  of  1 : VM with cloud ID  ` i-914c046a ' missing.    1. Skip for now    2. Recreate VM    3. Delete VM reference  Please choose a resolution [1 - 3]: 3  Below is the list of resolutions you' ve provided\nPlease make sure everything is fine and confirm your changes\n\n   1 . VM with cloud ID  ` i-914c046a ' missing.       Delete VM reference  Apply resolutions? (type ' yes '  to  continue ) : yes\nApplying resolutions...\n\nDirector task  624 \n  Started applying problem resolutions > missing_vm  168 : Delete VM reference. Done  ( 00 :00:00 ) \n\nTask  624   done \n\nStarted      2015 -01-09  23 :33:20 UTC\nFinished     2015 -01-09  23 :33:20 UTC\nDuration     00 :00:00\nCloudcheck is finished  cck determined that  i-914c046a  VM was missing. Possible options are:    Skip for now : the Director will not try to resolve this problem right now    Recreate VM : the Director will just create new VM, deploy specified releases according to deployed manifest and finally start release jobs  Note on current behaviour: cck will not wait for all release job processes to start.    Delete VM reference : the Director will not create new VM in its place. If cck is run again, it will not report that VM is missing since it's not expected to exist. Running  bosh deploy  after cck will backfill missing VMs.    In the above example options 3 was picked and VM reference was deleted.",
            "title": "VM is missing "
        },
        {
            "location": "/cck/#vm-is-not-responsive-unresponsive-agent",
            "text": "Assuming there was a deployment with a VM, somehow Agent is no longer responding to the Director. In this situation  bosh vms  will report VM's agent as  unresponsive agent :  $ bosh vms simple-deployment --details\n\nDeployment  ` simple-deployment ' \n\nDirector task  630 \n\nTask  630   done \n\n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+ |  Job/index        |  State               |  Resource Pool  |  IPs  |  CID         |  Agent ID                              |  Resurrection  | \n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+ |  unknown/unknown  |  unresponsive agent  |                 |       |  i-1db9ede6  |  59a30081-d63d-4c1b-80be-01fa681d8787  |  active        | \n+-----------------+--------------------+---------------+-----+------------+--------------------------------------+--------------+\n\nVMs total:  1   Also  bosh deploy  will stop at  Binding existing deployment  stage since it is not able to communicate with unresponsive agent:  $ bosh deploy\n\n..snip...\n\nDeploying\n---------\nDeployment name:  ` tiny-dummy.yml '  Director name: `idora' \nAre you sure you want to deploy?  ( type   'yes'  to  continue ) : yes\n\nDirector task  631 \n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done  ( 00 :00:00 ) \n  Started preparing deployment > Binding releases. Done  ( 00 :00:00 ) \n  Started preparing deployment > Binding existing deployment. Failed: Timed out sending  ` get_state ' to 59a30081-d63d-4c1b-80be-01fa681d8787 after 45 seconds (00:02:15)  Error 450002: Timed out sending `get_state'  to 59a30081-d63d-4c1b-80be-01fa681d8787 after  45  seconds\n\nTask  631  error\n\nFor a more detailed error report, run: bosh task  631  --debug  $ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task  640 \n  Started scanning  1  vms\n  Started scanning  1  vms > Checking VM states. Done  ( 00 :00:10 ) \n  Started scanning  1  vms >  0  OK,  1  unresponsive,  0  missing,  0  unbound,  0  out of sync. Done  ( 00 :00:00 ) \n     Done scanning  1  vms  ( 00 :00:10 ) \n\n  Started scanning  0  persistent disks\n  Started scanning  0  persistent disks > Looking  for  inactive disks. Done  ( 00 :00:00 ) \n  Started scanning  0  persistent disks >  0  OK,  0  missing,  0  inactive,  0  mount-info mismatch. Done  ( 00 :00:00 ) \n     Done scanning  0  persistent disks  ( 00 :00:00 ) \n\nTask  640   done \n\nStarted    2015 -01-09  23 :33:45 UTC\nFinished   2015 -01-09  23 :33:55 UTC\nDuration   00 :00:10\n\nScan is complete, checking  if  any problems found...\n\nFound  1  problem\n\nProblem  1  of  1 : dummy/0  ( i-914c046a )  is not responding.\n   1 . Skip  for  now\n   2 . Reboot VM\n   3 . Recreate VM\n   4 . Delete VM reference  ( forceful ;  may need to manually delete VM from the Cloud to avoid IP conflicts ) \nPlease choose a resolution  [ 1  -  4 ] :  4 \n\nBelow is the list of resolutions you 've provided  Please make sure everything is fine and confirm your changes    1. dummy/0 (i-914c046a) is not responding.       Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)  Apply resolutions? (type ' yes '  to  continue ) : yes\nApplying resolutions...\n\nDirector task  641 \n  Started applying problem resolutions > unresponsive_agent  168 : Delete VM reference  ( ... ) . Done  ( 00 :00:05 ) \n\nTask  641   done \n\nStarted    2015 -01-09  23 :35:20 UTC\nFinished   2015 -01-09  23 :35:25 UTC\nDuration   00 :00:05\nCloudcheck is finished  cck determined that  i-914c046a  VM is unresponsive. Possible options are:    Skip for now : the Director will not try to resolve this problem right now    Reboot VM : the Director will power off and then power on existing VM  Note on current behaviour: cck will not wait for all release job processes to start.    Recreate VM : the Director will delete existing VM, then create a new VM, deploy specified releases according to deployed manifest and finally start release jobs  Note on current behaviour: cck will not wait for all release job processes to start.    Delete VM reference : the Director will not create new VM in its place. If cck is run again, it will not report that VM is unresponsive since it does not exist. Running  bosh deploy  after cck will backfill missing VMs.    In the above example options 4 was picked and VM reference was deleted.",
            "title": "VM is not responsive (unresponsive agent) "
        },
        {
            "location": "/cck/#persistent-disk-is-not-attached",
            "text": "Assuming there was a deployment with a VM, somehow persistent disk got detached.  $ bosh cck\n\nPerforming cloud check...\n\nProcessing deployment manifest\n------------------------------\n\nDirector task  656 \n  Started scanning  1  vms\n  Started scanning  1  vms > Checking VM states. Done  ( 00 :00:00 ) \n  Started scanning  1  vms >  1  OK,  0  unresponsive,  0  missing,  0  unbound,  0  out of sync. Done  ( 00 :00:00 ) \n     Done scanning  1  vms  ( 00 :00:00 ) \n\n  Started scanning  1  persistent disks\n  Started scanning  1  persistent disks > Looking  for  inactive disks. Done  ( 00 :00:00 ) \n  Started scanning  1  persistent disks >  0  OK,  0  missing,  0  inactive,  1  mount-info mismatch. Done  ( 00 :00:00 ) \n     Done scanning  1  persistent disks  ( 00 :00:00 ) \n\nTask  656   done \n\nStarted    2015 -01-13  22 :04:56 UTC\nFinished   2015 -01-13  22 :04:56 UTC\nDuration   00 :00:00\n\nScan is complete, checking  if  any problems found...\n\nFound  1  problem\n\nProblem  1  of  1 : Inconsistent mount information:\nRecord shows that disk  'vol-549f071f'  should be mounted on i-4fcd99b4.\nHowever it is currently :\n  Not mounted in any VM.\n   1 . Skip  for  now\n   2 . Reattach disk to instance\n   3 . Reattach disk and reboot instance\nPlease choose a resolution  [ 1  -  3 ] :  2 \n\nBelow is the list of resolutions you 've provided  Please make sure everything is fine and confirm your changes    1. Inconsistent mount information:  Record shows that disk ' vol-549f071f ' should be mounted on i-4fcd99b4.  However it is currently :    Not mounted in any VM       Reattach disk to instance  Apply resolutions? (type ' yes '  to  continue ) : yes\nApplying resolutions...\n\nDirector task  657 \n  Started applying problem resolutions > mount_info_mismatch  23 : Reattach disk to instance. Done  ( 00 :00:22 ) \n\nTask  657   done \n\nStarted    2015 -01-13  22 :05:19 UTC\nFinished   2015 -01-13  22 :05:41 UTC\nDuration   00 :00:22\nCloudcheck is finished  cck determined that  vol-549f071f  persistent disk is not attached to  i-4fcd99b4  VM. Possible options are:    Skip for now : the Director will not try to resolve this problem during    Reattach disk to instance : the Director will reattach persistent disk to the VM and mount it at its usual location  /var/vcap/store .  Note on current behaviour: Release job processes will not be restarted when persistent disk is remounted.    Reattach disk and reboot instance : the Director will reattach persistent disk to the VM and reboot it so that Agent can safely mount persistent disk before starting any release job processes.  Note on current behaviour: cck will not wait until VM reboots and restarts all release job processes.",
            "title": "Persistent Disk is not attached "
        },
        {
            "location": "/cck/#persistent-disk-is-missing",
            "text": "Assuming there was a deployment with a VM, somehow persistent disk got deleted.  Note: Not all CPIs implement needed functionality to determine if disk is missing. Those CPIs will report missing disk as  Persistent Disk is not attached  problem; however, both reattaching resolutions will fail since persistent disk would not be found.   Next:  Automatic repair with Resurrector  Previous:  Process monitoring with Monit",
            "title": "Persistent Disk is missing "
        },
        {
            "location": "/migrated-from/",
            "text": "Occasionally, it's convenient to rename one or more instance groups as their purpose changes or as better names are found. In most cases it's desirable to maintain existing persistent data by keeping existing persistent disks.\n\n\nPreviously, the CLI provided the \nrename job\n command to rename a specific instance group one at a time. That approach worked OK in non-automated, non-frequently updated environments, but it was inconvenient for automated, frequently updated environments. As a replacement, the \nmigrated_from\n directive was added to allow renames to happen in a more systematic way.\n\n\nAdditionally \nmigrated_from\n directive can be used to migrate instance groups to use first class AZs.\n\n\n\n\nSchema \n\u00b6\n\n\nmigrated_from\n [Array, required]: The name and AZ of each instance group that should be used to form new instance group.\n\n\n\n\nname\n [String, required]: Name of an instance group that used to exist in the manifest.\n\n\naz\n [String, optional]: Availability zone that was used for the named instance group. This key is optional for instance groups that used first class AZs (via \nazs\n key). If first class AZ was not used, then this key must specify first class AZ that matches actual IaaS AZ configuration.\n\n\n\n\n\n\nRenaming Instance Groups \n\u00b6\n\n\n\n\n\n\nGiven follow deployment instance group \netcd\n:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \netcd-primary\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \netcd\n,\n \nrelease\n:\n \netcd\n}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\n\n\nChange instance group's name to \netcd\n and add \nmigrated_from\n with a previous name.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \netcd\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \netcd\n,\n \nrelease\n:\n \netcd\n}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n  \nmigrated_from\n:\n\n  \n-\n \nname\n:\n \netcd-primary\n\n\n\n\n\n\n\n\n\nDeploy.\n\n\n\n\n\n\n\n\nMigrating Instance Groups (to first class AZs) \n\u00b6\n\n\nBefore the introduction of first class AZs, each instance group was associated with a resource pool that typically defined some CPI specific AZ configuration in its \ncloud_properties\n. Typically there would be multiple instance groups that mostly differed by their name, for example \netcd_z1\n and \netcd_z2\n. With first class AZs, multiple instance groups typically should be collapsed to simplify the deployment.\n\n\n\n\n\n\nGiven following instance groups \netcd_z1\n and \netcd_z2\n with AZ specific resource pools and networks:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \netcd_z1\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \netcd\n,\n \nrelease\n:\n \netcd\n}\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nresource_pool\n:\n \nmedium_z1\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault_z1\n\n\n\n-\n \nname\n:\n \netcd_z2\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \netcd\n,\n \nrelease\n:\n \netcd\n}\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nresource_pool\n:\n \nmedium_z2\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault_z2\n\n\n\n\n\n\n\n\n\nCollapse both instance groups into a single instance group \netcd\n and use \nmigrated_from\n with previous group names.\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \netcd\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n  \ninstances\n:\n \n3\n\n  \njobs\n:\n\n  \n-\n \n{\nname\n:\n \netcd\n,\n \nrelease\n:\n \netcd\n}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10_240\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n  \nmigrated_from\n:\n\n  \n-\n \n{\nname\n:\n \netcd_z1\n,\n \naz\n:\n \nz1\n}\n\n  \n-\n \n{\nname\n:\n \netcd_z2\n,\n \naz\n:\n \nz2\n}\n\n\n\n\n\nNote that other referenced resources such as resource pool and network should be adjusted to work with AZs.\n\n\nNote: Migration from one AZ to a different AZ is not supported yet.\n\n\n\n\n\n\nDeploy.",
            "title": "To Availability Zones"
        },
        {
            "location": "/migrated-from/#schema",
            "text": "migrated_from  [Array, required]: The name and AZ of each instance group that should be used to form new instance group.   name  [String, required]: Name of an instance group that used to exist in the manifest.  az  [String, optional]: Availability zone that was used for the named instance group. This key is optional for instance groups that used first class AZs (via  azs  key). If first class AZ was not used, then this key must specify first class AZ that matches actual IaaS AZ configuration.",
            "title": "Schema "
        },
        {
            "location": "/migrated-from/#renaming-instance-groups",
            "text": "Given follow deployment instance group  etcd :  instance_groups :  -   name :   etcd-primary \n   instances :   2 \n   jobs : \n   -   { name :   etcd ,   release :   etcd } \n   vm_type :   default \n   stemcell :   default \n   persistent_disk :   10_240 \n   networks : \n   -   name :   default     Change instance group's name to  etcd  and add  migrated_from  with a previous name.  instance_groups :  -   name :   etcd \n   instances :   2 \n   jobs : \n   -   { name :   etcd ,   release :   etcd } \n   vm_type :   default \n   stemcell :   default \n   persistent_disk :   10_240 \n   networks : \n   -   name :   default \n   migrated_from : \n   -   name :   etcd-primary     Deploy.",
            "title": "Renaming Instance Groups "
        },
        {
            "location": "/migrated-from/#migrating-instance-groups-to-first-class-azs",
            "text": "Before the introduction of first class AZs, each instance group was associated with a resource pool that typically defined some CPI specific AZ configuration in its  cloud_properties . Typically there would be multiple instance groups that mostly differed by their name, for example  etcd_z1  and  etcd_z2 . With first class AZs, multiple instance groups typically should be collapsed to simplify the deployment.    Given following instance groups  etcd_z1  and  etcd_z2  with AZ specific resource pools and networks:  instance_groups :  -   name :   etcd_z1 \n   instances :   2 \n   jobs : \n   -   { name :   etcd ,   release :   etcd } \n   persistent_disk :   10_240 \n   resource_pool :   medium_z1 \n   networks : \n   -   name :   default_z1  -   name :   etcd_z2 \n   instances :   1 \n   jobs : \n   -   { name :   etcd ,   release :   etcd } \n   persistent_disk :   10_240 \n   resource_pool :   medium_z2 \n   networks : \n   -   name :   default_z2     Collapse both instance groups into a single instance group  etcd  and use  migrated_from  with previous group names.  instance_groups :  -   name :   etcd \n   azs :   [ z1 ,   z2 ] \n   instances :   3 \n   jobs : \n   -   { name :   etcd ,   release :   etcd } \n   vm_type :   default \n   stemcell :   default \n   persistent_disk :   10_240 \n   networks : \n   -   name :   default \n   migrated_from : \n   -   { name :   etcd_z1 ,   az :   z1 } \n   -   { name :   etcd_z2 ,   az :   z2 }   Note that other referenced resources such as resource pool and network should be adjusted to work with AZs.  Note: Migration from one AZ to a different AZ is not supported yet.    Deploy.",
            "title": "Migrating Instance Groups (to first class AZs) "
        },
        {
            "location": "/cli-v2-diff/",
            "text": "Note: Applies to CLI v2 v2.0.13+.\n\n\n\nGeneral \n\u00b6\n\n\nThe BOSH CLI v2 differs from v1 in two main ways: it is stateless, and it hyphenates single commands.\n\n\nStatelessness\n\n\nThe BOSH CLI v2 does not store values for a current environment or configuration. \nIn v1, you set the environment by passing a Director endpoint to \nbosh target\n and set the deployment by passing a manifest \nfile to \nbosh deployment\n. Then you could run \nbosh deploy\n with no arguments.\n\n\nIn contrast, the BOSH CLI v2 is stateless. To specify a Director instance and deployment manifest to run a command over, \nyou do one of the following:\n\n\n\n\nPass the BOSH environment in with the \n-e\n flag and the deployment in with the \n-d\n flag, or\n\n\nSet the command shell environment variable \nBOSH_ENVIRONMENT\n to your Director endpoint or alias and set \nBOSH_DEPLOYMENT\n to your deployment name. You can also use \nbosh alias-env\n to create an alias for your BOSH environment configuration, to avoid having to reference the Director endpoint and credential information for every command.\n\n\n\n\nHyphenation\n\n\nThe BOSH v2 CLI also hyphenates single commands that v1 represented as space-separated word pairs. \nFor example, \nbosh delete deployment\n in v1 corresponds to \nbosh delete-deployment\n in v2.\n\n\n\n\n\n\n\n\nBefore\n\n\nAfter\n\n\n\n\n\n\n\n\n\n\nbosh-init deploy \n\n\nbosh create-env \n\n\n\n\n\n\nbosh-init delete \n\n\nbosh delete-env \n\n\n\n\n\n\nbosh target \n\n\nbosh alias-env my-env -e \n\n\n\n\n\n\nbosh status\n\n\nbosh env\n\n\n\n\n\n\nbosh -t my-env ...\n\n\nbosh -e my-env ...\n\n\n\n\n\n\nbosh -d manifest-path ...\n\n\nbosh -d deployment-name ... [3]\n\n\n\n\n\n\nbosh deployment \n\n\nn/a\n\n\n\n\n\n\nbosh deploy\n\n\nbosh deploy \n\n\n\n\n\n\nbosh delete deployment\n\n\nbosh delete-deployment\n\n\n\n\n\n\nbosh tasks --no-filter\n\n\nbosh tasks\n\n\n\n\n\n\nbosh tasks recent 1000\n\n\nbosh tasks -r=1000\n\n\n\n\n\n\nbosh download manifest dep\n\n\nbosh manifest\n\n\n\n\n\n\nbosh vms my-dep\n\n\nbosh instances\n\n\n\n\n\n\nbosh vms my-dep\n\n\nbosh -d my-dep vms\n\n\n\n\n\n\n\n\n\n\nMost commands require (\n--environment\n) \n-e\n and \n--deployment\n (\n-d\n) flags\n\n\n--deployment\n (\n-d\n) flag accepts a deployment name instead of a manifest\n\n\nbosh-init CLI is now absorbed by the bosh CLI. One binary!\n\n\nVariety of commands (create-env/delete-env/etc.) accept simple interpolation flags (\n-v/-l\n)\n\n\nAll commands support friendlier non-TTY output, forceful TTY output and \n--json\n formatting\n\n\nAll command names now use dashes instead of spaces\n\n\nAll commands expect 'piece1/piece2' formatting for instances, releases, and stemcells\n\n\n^+C\n doesnt ask for task cancellation and just exits CLI command (task continue to run)\n\n\nSorts all tables in a more consistent manner\n\n\nStores configuration file in \n~/.bosh/config\n instead of \n~/.bosh_config\n\n\nMost of the output formatting have changed\n\n\n\n\n\n\nNotable differences per command \n\u00b6\n\n\n\n\nbosh alias-env\n and all commands\n\n\nonly allows connections to Director configured with verifiable certificates\n\n\n\n\nno longer asks to interactively log in\n\n\n\n\n\n\nbosh log-in\n\n\n\n\n\n\nno longer accepts username or password arguments\n\n\n\n\n\n\nbosh task\n\n\n\n\n\n\nremoved \n--no-track\n flag without replacement\n\n\n\n\n\n\nbosh tasks\n\n\n\n\n\n\nimproves argument syntax (\n-r\n for recent and \n-a\n for all)\n\n\n\n\n\n\nbosh deploy\n\n\n\n\n\n\nno longer checks or requires \ndirector_uuid\n in the deployment manifest\n\n\n\n\nto achieve similar safety make sure to give unique deployment names across environments\n\n\n\n\n\n\n\n\nbosh instances\n\n\n\n\n\n\nno longer accepts deployment name argument in favor of using global \n--deployment\n (\n-d\n) flag\n\n\n\n\n\n\nbosh vms\n\n\n\n\n\n\nno longer accepts deployment name argument in favor of using global \n--deployment\n (\n-d\n) flag\n\n\n\n\n\n\nbosh logs\n\n\n\n\n\n\nadds \n-f\n flag similar to \ntail -f\n (uses \nbosh ssh\n command internally)\n\n\n\n\n\n\nbosh ssh\n\n\n\n\nimproves argument syntax, use \nssh --help\n for more info\n\n\nsupport for running commands against multiple machines\n\n\nadds \n--opts\n flag to pass through options to ssh command for port forwarding etc.\n\n\n\n\nadds \n-r\n flag to collate results from multiple machines\n\n\n\n\n\n\nbosh scp\n\n\n\n\nimproves argument syntax\n\n\n\n\nsupport for running command against multiple machines\n\n\n\n\n\n\nbosh delete-deployment\n\n\n\n\n\n\nremoves explicit argument for specifying deployment in favor of global \n--deployment\n (\n-d\n) flag\n\n\n\n\n\n\nbosh add-blob\n\n\n\n\nrequires a path to its release destination\n\n\nno longer uses symlinks to manage blobs but rather places file directly into \nblobs/\n\n\n\n\n\n\nNext: \nGlobal Flags\n\n\nPrevious: \nCLI v2",
            "title": "From the Ruby CLI"
        },
        {
            "location": "/cli-v2-diff/#general",
            "text": "The BOSH CLI v2 differs from v1 in two main ways: it is stateless, and it hyphenates single commands.  Statelessness  The BOSH CLI v2 does not store values for a current environment or configuration. \nIn v1, you set the environment by passing a Director endpoint to  bosh target  and set the deployment by passing a manifest \nfile to  bosh deployment . Then you could run  bosh deploy  with no arguments.  In contrast, the BOSH CLI v2 is stateless. To specify a Director instance and deployment manifest to run a command over, \nyou do one of the following:   Pass the BOSH environment in with the  -e  flag and the deployment in with the  -d  flag, or  Set the command shell environment variable  BOSH_ENVIRONMENT  to your Director endpoint or alias and set  BOSH_DEPLOYMENT  to your deployment name. You can also use  bosh alias-env  to create an alias for your BOSH environment configuration, to avoid having to reference the Director endpoint and credential information for every command.   Hyphenation  The BOSH v2 CLI also hyphenates single commands that v1 represented as space-separated word pairs. \nFor example,  bosh delete deployment  in v1 corresponds to  bosh delete-deployment  in v2.     Before  After      bosh-init deploy   bosh create-env     bosh-init delete   bosh delete-env     bosh target   bosh alias-env my-env -e     bosh status  bosh env    bosh -t my-env ...  bosh -e my-env ...    bosh -d manifest-path ...  bosh -d deployment-name ... [3]    bosh deployment   n/a    bosh deploy  bosh deploy     bosh delete deployment  bosh delete-deployment    bosh tasks --no-filter  bosh tasks    bosh tasks recent 1000  bosh tasks -r=1000    bosh download manifest dep  bosh manifest    bosh vms my-dep  bosh instances    bosh vms my-dep  bosh -d my-dep vms      Most commands require ( --environment )  -e  and  --deployment  ( -d ) flags  --deployment  ( -d ) flag accepts a deployment name instead of a manifest  bosh-init CLI is now absorbed by the bosh CLI. One binary!  Variety of commands (create-env/delete-env/etc.) accept simple interpolation flags ( -v/-l )  All commands support friendlier non-TTY output, forceful TTY output and  --json  formatting  All command names now use dashes instead of spaces  All commands expect 'piece1/piece2' formatting for instances, releases, and stemcells  ^+C  doesnt ask for task cancellation and just exits CLI command (task continue to run)  Sorts all tables in a more consistent manner  Stores configuration file in  ~/.bosh/config  instead of  ~/.bosh_config  Most of the output formatting have changed",
            "title": "General "
        },
        {
            "location": "/cli-v2-diff/#notable-differences-per-command",
            "text": "bosh alias-env  and all commands  only allows connections to Director configured with verifiable certificates   no longer asks to interactively log in    bosh log-in    no longer accepts username or password arguments    bosh task    removed  --no-track  flag without replacement    bosh tasks    improves argument syntax ( -r  for recent and  -a  for all)    bosh deploy    no longer checks or requires  director_uuid  in the deployment manifest   to achieve similar safety make sure to give unique deployment names across environments     bosh instances    no longer accepts deployment name argument in favor of using global  --deployment  ( -d ) flag    bosh vms    no longer accepts deployment name argument in favor of using global  --deployment  ( -d ) flag    bosh logs    adds  -f  flag similar to  tail -f  (uses  bosh ssh  command internally)    bosh ssh   improves argument syntax, use  ssh --help  for more info  support for running commands against multiple machines  adds  --opts  flag to pass through options to ssh command for port forwarding etc.   adds  -r  flag to collate results from multiple machines    bosh scp   improves argument syntax   support for running command against multiple machines    bosh delete-deployment    removes explicit argument for specifying deployment in favor of global  --deployment  ( -d ) flag    bosh add-blob   requires a path to its release destination  no longer uses symlinks to manage blobs but rather places file directly into  blobs/    Next:  Global Flags  Previous:  CLI v2",
            "title": "Notable differences per command "
        },
        {
            "location": "/create-release/",
            "text": "A release contains one or more pieces of software that work together.\nFor example, you could create a release of a service with three pieces:\ntwo MySQL nodes and a dashboard app.\n\n\nThere are four fundamental elements in a release:\n\n\n\n\nJobs\n describe pieces of the service or application you are releasing\n\n\nPackages\n provide source code and dependencies to jobs\n\n\nSource\n provides non-binary files to packages\n\n\nBlobs\n provide binary files (other than those checked into a source code repository) to packages\n\n\n\n\nThe following instructions use an example release that includes two jobs:\na web UI and a background worker.\nThe two jobs split up the functionality provided by single Ruby app,\n\nardo_app\n (you can use simple \ngist\n as app).\n\n\n\n\nPreparation\n\u00b6\n\n\nThis section needs to be completed once.\nNext, you iterate through Steps 1 through 6 until your dev release is\nsatisfactory.\nThen you can do a final release.\n\n\nCreate the release directory\n\u00b6\n\n\nTo create the release directory, navigate into the workspace where you want the\nrelease to be, and run:\n\n\nbosh init-release --dir <release_name>\n\n\nYou can add the \n--git\n option to initialize a git repository.\nUse dashes in the release name.\nUse underscores for all other filenames in the release.\n\n\nView the release with \ntree\n:\n\n\n$ tree .\n.\n\u251c\u2500\u2500 blobs\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 blobs.yml\n\u251c\u2500\u2500 \njobs\n\n\u251c\u2500\u2500 packages\n\u2514\u2500\u2500 src\n\n\n5\n directories, \n1\n file\n\n\n\n\nWhen deploying your release, BOSH places compiled code and other resources\nin the \n/var/vcap/\n directory tree, which BOSH creates on the job VMs.\nThe four directories you just created, \njobs\n, \npackages\n, \nsrc\n, and \nblobs\n,\nappear on job VMs as \n/var/vcap/jobs\n, \n/var/vcap/packages\n, \n/var/vcap/src\n,\nand \n/var/vcap/blobs\n, respectively.\n\n\nPopulate the src directory ### \n\u00b6\n\n\nCopy your source code into the \nsrc\n directory.\nAlternatively, link your source code to the directory using a mechanism such as\na Git submodule or a Mercurial repo.\n\n\nChoose a work strategy ### \n\u00b6\n\n\nChoose whether you want to work one step at a time or one job at a time.\nFor releases with just a few jobs, going one step at a time is probably easiest.\nIf you have a larger number of jobs, going one job at a time may be more efficient.\n\n\n\n\nStep 1: Create Job Skeletons ## \n\u00b6\n\n\nNavigate into the release directory.\n\n\nFor each job, create a job skeleton:\n\n\nbosh generate-job <job_name>\n\n\nIn our example, we run \nbosh generate-job\n twice, once for the \nweb_ui\n job,\nand once for the \nbg_worker\n job.\n\n\nView the job skeletons with \ntree\n:\n\n\n$ tree .\n.\n\u251c\u2500\u2500 blobs\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 blobs.yml\n\u251c\u2500\u2500 \njobs\n\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bg_worker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 monit\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 web_ui\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 monit\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 templates\n\u251c\u2500\u2500 packages\n\u2514\u2500\u2500 src\n\n\n9\n directories, \n5\n files\n\n\n\n\nCreate control scripts  ### \n\u00b6\n\n\nEvery job needs a way to start and stop.\nYou provide that by writing a control script and updating the \nmonit\n file.\n\n\nThe control script:\n\n\n\n\nIncludes a start command and a stop command.\n\n\nIs an ERB template stored in the \ntemplates\n directory for the relevant job.\n\n\n\n\nFor each job, create a control script that configures the job to store logs in \n/var/vcap/sys/log/JOB_NAME\n. Save this script as \nctl.erb\n in the \ntemplates\n directory for its job.\n\n\nThe control script for the \nweb_ui\n job looks like this:\n\n\n#!/bin/bash\n\n\n\nRUN_DIR\n=\n/var/vcap/sys/run/web_ui\n\nLOG_DIR\n=\n/var/vcap/sys/log/web_ui\n\nPIDFILE\n=\n${\nRUN_DIR\n}\n/pid\n\n\ncase\n \n$1\n in\n\n  start\n)\n\n    mkdir -p \n$RUN_DIR\n \n$LOG_DIR\n\n    chown -R vcap:vcap \n$RUN_DIR\n \n$LOG_DIR\n\n\n    \necho\n \n$$\n > \n$PIDFILE\n\n\n    \ncd\n /var/vcap/packages/ardo_app\n\n    \nexport\n \nPATH\n=\n/var/vcap/packages/ruby_1.9.3/bin:\n$PATH\n\n\n    \nexec\n /var/vcap/packages/ruby_1.9.3/bin/bundle \nexec\n \n\\\n\n      rackup -p <%\n=\n properties.web_ui.port %> \n\\\n\n      >>  \n$LOG_DIR\n/web_ui.stdout.log \n\\\n\n      \n2\n>> \n$LOG_DIR\n/web_ui.stderr.log\n\n    \n;;\n\n\n  stop\n)\n\n    \nkill\n -9 \n`\ncat \n$PIDFILE\n`\n\n    rm -f \n$PIDFILE\n\n\n    \n;;\n\n\n  *\n)\n\n    \necho\n \n\"Usage: ctl {start|stop}\"\n \n;;\n\n\n\nesac\n\n\n\n\n\nIf your release needs templates other than the control script, create them now.\n\n\nUpdate monit files  ### \n\u00b6\n\n\nThe \nmonit\n file:\n\n\n\n\nSpecifies the process ID (pid) file for the job\n\n\nReferences each command provided by the templates for the job\n\n\nSpecifies that the job belongs to the \nvcap\n group\n\n\n\n\nOn a deployed release, a BOSH Agent runs on each job VM.\nBOSH communicates with the Agent, which in turn executes commands in the\ncontrol script.\nThe Agent does this using open source process monitoring software called\n\nMonit\n.\n\n\nThe \nmonit\n file for the \nweb_ui\n job looks like this:\n\n\ncheck process web_ui\n  with pidfile /var/vcap/sys/run/web_ui/pid\n  start program \"/var/vcap/jobs/web_ui/bin/ctl start\"\n  stop program \"/var/vcap/jobs/web_ui/bin/ctl stop\"\n  group vcap\n\n\n\n\nUpdate the \nmonit\n file for each of your jobs.\nUse \n/var/vcap\n paths as shown in the example.\n\n\nNote\n: BOSH requires a \nmonit\n file for each job in a release. When developing a release, you can use an empty \nmonit\n file to meet this requirement without having to first create a control script.\n\n\n\nUpdate job specs  ### \n\u00b6\n\n\nAt compile time, BOSH transforms templates into files, which it then replicates\non the job VMs.\n\n\nThe template names and file paths are among the metadata for each job that\nresides in the job \nspec\n file.\n\n\nIn the job \nspec\n file, the \ntemplates\n block contains key/value pairs where:\n\n\n\n\nEach key is template name\n\n\nEach value is the path to the corresponding file on a job VM\n\n\n\n\nThe file paths that you provide for templates are relative to\nthe \n/var/vcap/jobs/<job_name>\n directory on the VM.\nFor example, \nbin/ctl\n becomes \n/var/vcap/jobs/<job_name>/bin/ctl\n on the job VM.\nUsing \nbin\n as the directory where these files go is a convention.\n\n\nThe \ntemplates\n block of the updated \nspec\n files for the example jobs look\nlike this:\n\n\ntemplates\n:\n\n  \nctl.erb\n:\n \nbin/ctl\n\n\n\n\n\nFor each job, update the \nspec\n file with template names.\n\n\nCommit ### \n\u00b6\n\n\nYou have now created one or more job skeletons; this is a good time to commit.\n\n\nIf you used the \n--git\n option with \nbosh init-release\n (as recommended), the\ncorrect \n.gitignore\n file has been automatically created for you.\n\n\n\n\nStep 2: Make Dependency Graphs ## \n\u00b6\n\n\nThere are two kinds of dependencies in a BOSH release:\n\n\n\n\nThe \nruntime dependency\n, where a job depends on a package at runtime.\nFor example, the \nweb_ui\n job depends on Ruby.\n\n\nThe \ncompile-time dependency\n, where a package depends on another package at\ncompile time.\nFor example, Ruby depends on the YAML library.\n\n\n\n\nThree rules govern these dependencies:\n\n\n\n\nJobs never depend on other jobs.\n\n\nJobs can depend on packages.\n\n\nPackages can depend on other packages.\n\n\n\n\n \nBuilding the Dependency Graph\n\u00b6\n\n\nCreate a dependency graph to clarify your understanding of the\ndependencies between the jobs and packages in your release.\n\n\n \nIdentify runtime dependencies\n\u00b6\n\n\nWhenever a control script or other template cites a package name, the job\nthat the template belongs to depends on the cited package at runtime.\n\n\nFor each job, find all the cases where your control scripts cite packages.\nAdd these runtime dependencies to your dependency graph.\n\n\nIn our example, this line in both of our \nctl.erb\n scripts cites \nardo_app\n:\n\n\ncd /var/vcap/packages/ardo_app\n\n\n\n\nThis line cites Ruby:\n\n\nexec /var/vcap/packages/ruby_1.9.3/bin/bundle exec\n\n\n\n\nThis means that both the \nweb-ui\n and \nbg_worker\n jobs have runtime\ndependencies on both the \nardo_app\n and \nruby_1.9.3\n packages.\n\n\nWe add these four runtime dependencies to our example dependency graph.\n\n\n \nIdentify compile-time dependencies\n\u00b6\n\n\nUse your knowledge about the runtime dependencies you have already noted.\nConsider the packages you have identified as dependencies.\nDo any of them depend on other packages in turn?\n\n\nWhenever a package depends on another package, that is a compile-time\ndependency.\n\n\nFor each job, add the compile-time dependencies to your dependency graph.\nIf you miss a dependency, BOSH lets you know later, when you try to deploy.\n\n\nIn our example, we already noted a runtime dependency on Ruby 1.9.3.\nWe now ask ourselves whether Ruby 1.9.3 itself has any dependencies.\nThe answer is yes, it depends on libyaml 0.1.4.\n\n\nWe add this compile-time dependency to our example dependency graph.\n\n\n \nThe complete example dependency graph\n\u00b6\n\n\nThe complete dependency graph for \nardo-release\n looks like this:\n\n\n\n\nFor a large or complicated release, consider making more than one dependency\ngraph.\n\n\nStep 3: Create Package Skeletons ## \n\u00b6\n\n\nPackages give BOSH the information needed to prepare the binaries and\ndependencies for your jobs.\n\n\nCreate package skeletons starting from the bottom of your dependency graph.\n\n\nbosh generate-package <dependency_name>\n\n\nIn our example, we run this command three times.\nStarting from the bottom of the dependency graph,\nwe run it for \nlibyaml_0.1.4\n, \nruby_1.9.3\n, and \nardo_app\n.\n\n\nView the package skeletons with \ntree\n:\n\n\n$ tree packages\npackages\n\u251c\u2500\u2500 ardo_app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 packaging\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pre_packaging\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec\n\u251c\u2500\u2500 libyaml_0.1.4\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 packaging\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pre_packaging\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec\n\u2514\u2500\u2500 ruby_1.9.3\n    \u251c\u2500\u2500 packaging\n    \u251c\u2500\u2500 pre_packaging\n    \u2514\u2500\u2500 spec\n\n\n3\n directories, \n9\n files\n\n\n\n\nPutting each dependency in a separate package provides maximum reusability\nalong with a clear, modular structure. This is not mandatory; what packages\nto create is a matter of preference. You could even opt to put all the\ndependencies together in a single package, though that is not recommended.\n\n\nNote\n: Use of the \npre_packaging\n file is not recommended, and is not discussed in this tutorial.\n\n\n\nWithout using \npre_packaging\n for our \nardo_app\n we need to pack gems manually for further usage:\n\n\n$ \ncd\n src/ardo_app/\n$ bundle package\n\n\n\n\nUpdate packaging specs ### \n\u00b6\n\n\nWithin each package directory, there is a \nspec\n file which states:\n\n\n\n\nThe package name\n\n\nThe package's dependencies\n\n\nThe location where BOSH can find the binaries and other files that the package needs at compile time\n\n\n\n\nUse your dependency graph to determine which dependencies belong in each spec.\nDeveloper preferences and style play a role here.\nConsider our example: the spec for Ruby lists \nrubygems\n and \nbundler\n as dependencies along\nwith Ruby itself.\nSome Ruby developers would do it this way; others would not.\n\n\nTo maximize portability of your release across different versions of stemcells,\nnever depend on the presence of libraries or other software on stemcells.\n\n\nTo describe binary locations in the \nfiles\n block of the spec:\n\n\n\n\n\n\nFind the official site for the binary in question.\nFor example, Ruby might be at \nhttp://cache.ruby-lang.org/pub/ruby/1.9/ruby-1.9.3-p484.tar.gz\n.\n\n\n\n\n\n\nDownload the binary from the official location and make sure the file hash matches.\n\n\n\n\n\n\nRecord the binary name including version number, with a slash and the binary\nfilename concatenated to it.\nIt's a good idea to cite the official URL in a comment, in the same line.\n\n\n\n\n\n\nBOSH interprets the locations you record in the \nfiles\n section as being\neither in the \nsrc\n directory or in the \nblobs\n directory.\n(BOSH looks in \nsrc\n first.)\nWhen you add the actual blobs to a blobstore (see the next section),\nBOSH populates the \nblobs\n directory with the correct information.\n\n\nFor packages that depend on their own source code, use the globbing pattern\n\n<package_name>/**/*\n to deep-traverse the directory in \nsrc\n where\nthe source code should reside.\n\n\nUpdate the spec for each package.\nRefer to the example specs below for guidance.\n\n\nExample libyaml package spec #### \n\u00b6\n\n\n---\n\n\nname\n:\n \nlibyaml_0.1.4\n\n\n\ndependencies\n:\n \n[]\n\n\n\nfiles\n:\n\n\n-\n \nlibyaml_0.1.4/yaml-0.1.4.tar.gz\n \n# From http://pyyaml.org/download/libyaml/yaml-0.1.4.tar.gz\n\n\n\n\n\nExample Ruby package spec #### \n\u00b6\n\n\n---\n\n\nname\n:\n \nruby_1.9.3\n\n\n\ndependencies\n:\n\n\n-\n \nlibyaml_0.1.4\n\n\n\nfiles\n:\n\n\n-\n \nruby_1.9.3/ruby-1.9.3-p484.tar.gz\n \n# http://cache.ruby-lang.org/pub/ruby/1.9/ruby-1.9.3-p484.tar.gz\n\n\n-\n \nruby_1.9.3/rubygems-1.8.24.tgz\n    \n# http://production.cf.rubygems.org/rubygems/rubygems-1.8.24.tgz\n\n\n-\n \nruby_1.9.3/bundler-1.2.1.gem\n      \n# https://rubygems.org/downloads/bundler-1.2.1.gem\n\n\n\n\n\nExample ardo_app package spec #### \n\u00b6\n\n\n---\n\n\nname\n:\n \nardo_app\n\n\n\ndependencies\n:\n\n\n-\n \nruby_1.9.3\n\n\n\nfiles\n:\n\n\n-\n \nardo_app/**/*\n\n\n\n\n\nCreate packaging scripts ### \n\u00b6\n\n\nAt compile time, BOSH takes the source files referenced in the package specs,\n and renders them into the executable binaries and scripts that your deployed\njobs need.\n\n\nYou write packaging scripts to instruct BOSH how to do this.\nThe instructions may involve some combination of copying, compilation, and\nrelated procedures.\nFor example:\n\n\n\n\n\n\nFor a Ruby app like \nardo_app\n, BOSH must copy source files and install Ruby\ngems.\n\n\n\n\n\n\nFor Ruby itself, BOSH must compile source code into a binary.\n\n\n\n\n\n\nFor a Python app, BOSH must copy source files and install Python eggs.\n\n\n\n\n\n\nBOSH relies on you to write packaging scripts that perform the correct operation.\n\n\nAdhere to these principles when writing packaging scripts:\n\n\n\n\n\n\nUse your dependency graph to determine which dependencies belong in each\npackaging script.\n\n\n\n\n\n\nBegin each script with a \nset -e -x\n line.\nThis aids debugging at compile time by causing the script to exit immediately\nif a command exits with a non-zero exit code.\n\n\n\n\n\n\nEnsure that any copying, installing or compiling delivers resulting code to\n the install target directory (represented as the \nBOSH_INSTALL_TARGET\n\nenvironment variable). For \nmake\n commands, use \nconfigure\n or its equivalent\nto accomplish this.\n\n\n\n\n\n\nBe aware that BOSH ensures that dependencies cited in the \ndependencies\n\nblock of package \nspec\n files are available to the deployed binary.\nFor example, in the \nspec\n file for the Ruby package, we cite libyaml as a\ndependency.\nThis ensures that on the compilation VMs, the packaging script for Ruby has\naccess to the compiled libyaml package.\n\n\n\n\n\n\nIf the instructions you provide in the packaging scripts fail to deliver compiled\ncode to \nBOSH_INSTALL_TARGET\n, the job cannot function because the VM has no\nway to find the code to run.\nThis failure scenario can happen if, for example,\nyou use a \nmake\n command that delivers compiled code to some standard location\nby default.\nYou can fix the problem by configuring \nmake\n to compile into\n\nBOSH_INSTALL_TARGET\n.\nSee how this is done in the example packaging scripts.\n\n\nLike control scripts, writing packaging scripts is one of the heavier tasks\nentailed in creating a release.\nWrite your packaging scripts now.\nRefer to the examples below for guidance.\n\n\nExample libyaml packaging script #### \n\u00b6\n\n\nset -e -x\n\ntar xzf libyaml_0.1.4/yaml-0.1.4.tar.gz\npushd yaml-0.1.4\n  ./configure --prefix=\n${\nBOSH_INSTALL_TARGET\n}\n\n\n  make\n  make install\npopd\n\n\n\n\nExample Ruby packaging script #### \n\u00b6\n\n\nset -e -x\n\ntar xzf ruby_1.9.3/ruby-1.9.3-p484.tar.gz\npushd ruby-1.9.3-p484\n  ./configure \\\n    --prefix=\n${\nBOSH_INSTALL_TARGET\n}\n \\\n    --disable-install-doc \\\n    --with-opt-dir=/var/vcap/packages/libyaml_0.1.4\n\n  make\n  make install\npopd\n\ntar zxvf ruby_1.9.3/rubygems-1.8.24.tgz\npushd rubygems-1.8.24\n  \n${\nBOSH_INSTALL_TARGET\n}\n/bin/ruby setup.rb\npopd\n\n\n${\nBOSH_INSTALL_TARGET\n}\n/bin/gem install ruby_1.9.3/bundler-1.2.1.gem --no-ri --no-rdoc\n\n\n\n\nExample ardo_app packaging script #### \n\u00b6\n\n\nset -e -x\n\ncp -a ardo_app/* \n${\nBOSH_INSTALL_TARGET\n}\n\n\ncd \n${\nBOSH_INSTALL_TARGET\n}\n\n\n/var/vcap/packages/ruby_1.9.3/bin/bundle install \\\n  --local \\\n  --deployment \\\n  --without development test\n\n\n\n\nUpdate job specs with dependencies ### \n\u00b6\n\n\nThe dependency graph reveals runtime dependencies that\nneed to be added to the \npackages\n block of the job spec.\n\n\nEdit the job specs to include these dependencies.\n\n\nIn our example, the dependency graph shows that \nweb_ui\n job depends on\n\nardo_app\n and \nruby_1.9.3\n:\n\n\npackages\n:\n\n\n-\n \nardo_app\n\n\n-\n \nruby_1.9.3\n\n\n\n\n\n\n\nStep 4: Add Blobs ## \n\u00b6\n\n\nWhen creating a release, you will likely use a source code repository.\nBut releases often use tar files or other binaries, also known as blobs.\nChecking blobs into a repository is problematic if your repository\nunsuited to dealing with large binaries (as is true of Git, for example).\n\n\nBOSH lets you avoid checking blobs into a repository by doing the following:\n\n\n\n\n\n\nFor dev releases, use local copies of blobs.\n\n\n\n\n\n\nFor a final release, upload blobs to a blobstore, and direct BOSH to obtain the blobs from there.\n\n\n\n\n\n\nConfigure a blobstore  ### \n\u00b6\n\n\nIn the \nconfig\n directory, you record the information BOSH needs about the\nblobstore:\n\n\n\n\n\n\nThe \nfinal.yml\n file names the blobstore and declares its type, which is either \nlocal\n\nor one of several other types that specify blobstore providers.\n\n\n\n\n\n\nThe \nprivate.yml\n file specifies the blobstore path, along with a secret.\n\n\n\n\n\n\nprivate.yml\n contains keys for accessing the blobstore and should not be\nchecked into a repository.\n(If you used the \n--git\n option when running \nbosh init-release\n at the beginning\nof this tutorial, \nprivate.yml\n is automatically \ngitignored\n.)\n\n\nThe \nconfig\n directory also contains two files whose content is automatically\ngenerated: the \nblobs.yml\n file and the \ndev.yml\n file.\n\n\nAdapt the examples below to fit the specifics of your release.\nOur example release uses the \nlocal\n type blobstore because otherwise it would\nbe necessary to explain how to configure a public blobstore such as\nAmazon S3, which is too large a topic for this context.\n\n\nThe \nlocal\n type blobstore is suitable for learning but the resulting release\ncannot be shared.\nFor that reason, you should configure a non-local, publicly available blobstore\nfor releases that you intend to share.\nNormally, the blobstore you choose when you begin working on a release is used\nfor all subsequent versions of the release.\nChanging the blobstore that a release uses is beyond the scope of this tutorial.\n\n\nExample \nfinal.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \nprovider\n:\n \nlocal\n\n  \noptions\n:\n\n    \nblobstore_path\n:\n \n/tmp/ardo-blobs\n\n\nfinal_name\n:\n \nardo_app\n\n\n\n\n\nExample \nprivate.yml\n:\n\n\n---\n\n\nblobstore_secret\n:\n \n'does-not-matter'\n\n\nblobstore\n:\n\n  \nlocal\n:\n\n    \nblobstore_path\n:\n \n/tmp/ardo-blobs\n\n\n\n\n\nIf you have a \nprivate.yml\n file:\n\n\n\n\nRequired\n: Include the \nblobstore_path\n in the \nprivate.yml\n file.\n\n\nOptional\n: Include the \nblobstore_path\n in the \nfinal.yml\n file. Doing so allows you to \ngitignore\n the \nprivate.yml\n file but still allow the release to be downloaded and used on other systems.\n\n\n\n\nNote\n: The \nblobstore_secret\n is required for the \nlocal\n type blobstore.\nThis is true even though the \nblobstore_secret\n line is deprecated and its\ncontent does not matter.\nThere is never a \nblobstore_secret\n line for blobstores of types other than\n\nlocal\n.\n\n\n\nInform BOSH where blobs are ### \n\u00b6\n\n\nIn the package \nspec\n file, the \nfiles\n block lists any binaries you downloaded,\nalong with the URLs from which you downloaded them.\n(This assumes that you followed the directions in the \nUpdate package specs\n section.)\n\n\nThose files are blobs, and now you need the paths to the downloaded blobs on\nyour local system.\n\n\nIn our example, the \nspec\n file for the \nlibyaml_0.1.4\n package includes the line:\n\n\nfiles\n:\n\n\n-\n \nlibyaml_0.1.4/yaml-0.1.4.tar.gz\n \n# From http://pyyaml.org/download/libyaml/yaml-0.1.4.tar.gz\n\n\n\n\n\nIf you downloaded the blob, its local path might be:\n\n\n~/Downloads/yaml-0.1.4.tar.gz\n\n\nGo through all your packages and make a list of local paths to the blobs you downloaded.\nNow you are ready to inform BOSH about these blobs.\n\n\nFor each blob, run:\n\n\nbosh add-blob <path_to_blob_on_local_system> <package_name>\n\n\ne.g.\n\n\nbosh add-blob ~/Downloads/yaml-0.1.4.tar.gz libyaml_0.1.4\n\n\nThe \nbosh add-blob\n command adds a local blob to the collection your release\nrecognizes as BOSH blobs.\n\n\nThe usage shown above is a blend of requirement and convention.\nIt works like this:\n\n\n\n\nFor the first argument, you provide the path to the blob on your local system\n\n\nFor the second argument, you provide a destination within the \nblobs\n directory\nin your release\n\n\nBOSH goes into the \nblobs\n directory and creates a subdirectory with\nthe name of the package that the local blob represents\n\n\nIn the new subdirectory, BOSH creates a symbolic link to a copy of the blob\nwhich BOSH makes in a hidden directory\n\n\n\n\nUsing the package name as the second argument of the \nbosh add-blob\n command\nis recommended because it produces a cleanly-organized blobs directory.\n\n\nLater, when you upload blobs for a final release, BOSH uses the hidden directory\nas a staging area.\n\n\nDo not upload blobs for a dev release ### \n\u00b6\n\n\nOnce you have uploaded blobs to a non-local blobstore, those blobs may become\nessential to some other developer.\nFor this reason, uploading a blob and then removing it is considered poor practice.\n\n\nWhen creating dev releases, do not run \nbosh upload-blobs\n.\n(You only run it when you do a final release.)\n\n\n\n\nStep 5: Create Job Properties  ## \n\u00b6\n\n\nIf your service needs to be configurable at deployment time,\nyou create the desired inputs or controls and specify them in the release.\nEach input is a \nproperty\n that belongs to a particular job.\n\n\nCreating properties requires three steps:\n\n\n\n\n\n\nDefine properties and defaults in the \nproperties\n block of\nthe job spec.\n\n\n\n\n\n\nUse the property lookup helper \np()\n to reference properties in\nrelevant templates.\n\n\n\n\n\n\nFor example, a start command can take a property as an argument,\nusing the property lookup helper:\n\n\n   \n<%=\n \np\n(\n'<job_name>.<property_name>'\n)\n \n%>\n\n\n\n\n\n\n\n\nSpecify the property in the \ndeployment manifest\n.\n\n\n\n\nAdapt the example below to create any properties your release needs now.\n\n\nIn our example, we want the port that the web UI listens on to be a\nconfigurable property.\n\n\nWe edit the spec for the web UI job to look like this:\n\n\nproperties\n:\n\n   \nport\n:\n\n     \ndescription\n:\n \nPort that web_ui app listens on\n\n     \ndefault\n:\n \n80\n\n\n\n\n\n\n\nStep 6: Create a Dev Release  ## \n\u00b6\n\n\nAll the elements needed to create a dev release should now be in place.\n\n\nRelease  ### \n\u00b6\n\n\nFor the dev release, use the \n--force\n option with the \nbosh create-release\n\ncommand.\nThis forces BOSH to use the local copies of our blobs.\n\n\nWithout the \n--force\n option, BOSH requires blobs to be uploaded before you\nrun \nbosh create-release\n.\nFor a final release, we upload blobs, but not for a dev release.\n\n\nCreate the dev release:\n\n\nbosh create-release --force\n\n\nBOSH prompts for a release name, and assigns a dot-number version to the release.\n\n\nDeploy the Dev Release  ### \n\u00b6\n\n\nDeploying the release requires three or more steps, depending on whether\nBOSH is targeting the desired Director, and whether BOSH is already pointing\nto a release.\n\n\nSee what director BOSH is targeting:\n\n\nbosh env\n\n\nTarget a director:\n\n\nbosh -e <director_alias> log-in\n\n\nSee what releases are available:\n\n\nbosh releases\n\n\nIf BOSH is already pointing to a release, edit the BOSH deployment manifest.\nOtherwise, create a manifest. See \nBOSH Deployment Manifest\n for more information.\nSimple manifest for \nardo_app\n can be found \nhere\n (OpenStack) or \nhere\n (AWS).\n\n\nUpload the new dev release.\n\n\nbosh upload-release\n\n\nAssuming you are in the release directory, no path is needed with the above command.\n\n\nDeploy:\n\n\nbosh deploy\n\n\nTest the Dev Release  ### \n\u00b6\n\n\nWhat tests to run depends on the software you are releasing.\n\n\nStart by opening a separate terminal, logging in on the job VM, and observing\nlogging output as you test your release.\n\n\nIf your release fails tests, follow this pattern.\n\n\n\n\nFix the code.\n\n\nDo a new dev release.\n\n\nRun \nbosh deploy\n to see whether the new release deploys successfully.\n\n\n\n\nUsing \nbosh deploy --recreate\n can provide a clearer picture because with that option,\nBOSH deploys all the VMs from scratch.\n\n\n\n\nCreate a Final Release  ## \n\u00b6\n\n\nOnly proceed to this step if your latest dev release passes all tests.\n\n\nUpload blobs ### \n\u00b6\n\n\nWhen you use the \nbosh create-release --force\n command to create them, dev\nreleases depend on locally-stored blobs.\nTo do a final release, you must upload blobs first.\n\n\nIf files that you need to keep private are uploaded to a public blobstore,\nthere is no satisfactory way to undo the mistake.\nTo avoid this situation, complete the following steps immediately before\nyou upload blobs:\n\n\n\n\n\n\nRun \nbosh blobs\n to see the list of blobs BOSH is prepared to upload\n\n\n\n\n\n\nProofread the list of blobs displayed by the command\n\n\n\n\n\n\nThe list should include only the blobs you need for the final release\n\n\n\n\n\n\nIf the list includes any files that should not be uploaded, find and delete\nthe symbolic links to them in the \nblobs\n directory\n\n\n\n\n\n\nTo upload your blobs, run:\n\n\nbosh upload-blobs\n\n\nCommit ### \n\u00b6\n\n\nThe \nbosh upload-blobs\n command has now populated the \nblobs.yml\n file\nin the \nconfig\n directory with metadata for uploaded blobs.\n\n\nThis is a good reason to commit.\n\n\nRelease ### \n\u00b6\n\n\nRun:\n\n\nbosh create-release --final\n\n\nBOSH prompts you for a release name, and assigns a whole-number version to the release.\n\n\nThis is a good time to push your code to a shared repository to give others access to\nyour final release.\n\n\nCommit ### \n\u00b6\n\n\nDo one more commit before you deploy!\n\n\nDeploy the Final Release  ### \n\u00b6\n\n\nRun:\n\n\nbosh deploy",
            "title": "Creating a Release"
        },
        {
            "location": "/create-release/#populate-the-src-directory",
            "text": "Copy your source code into the  src  directory.\nAlternatively, link your source code to the directory using a mechanism such as\na Git submodule or a Mercurial repo.",
            "title": "Populate the src directory ### "
        },
        {
            "location": "/create-release/#choose-a-work-strategy",
            "text": "Choose whether you want to work one step at a time or one job at a time.\nFor releases with just a few jobs, going one step at a time is probably easiest.\nIf you have a larger number of jobs, going one job at a time may be more efficient.",
            "title": "Choose a work strategy ### "
        },
        {
            "location": "/create-release/#step-1-create-job-skeletons",
            "text": "Navigate into the release directory.  For each job, create a job skeleton:  bosh generate-job <job_name>  In our example, we run  bosh generate-job  twice, once for the  web_ui  job,\nand once for the  bg_worker  job.  View the job skeletons with  tree :  $ tree .\n.\n\u251c\u2500\u2500 blobs\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 blobs.yml\n\u251c\u2500\u2500  jobs \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bg_worker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 monit\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 web_ui\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 monit\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 templates\n\u251c\u2500\u2500 packages\n\u2514\u2500\u2500 src 9  directories,  5  files",
            "title": "Step 1: Create Job Skeletons ## "
        },
        {
            "location": "/create-release/#create-control-scripts",
            "text": "Every job needs a way to start and stop.\nYou provide that by writing a control script and updating the  monit  file.  The control script:   Includes a start command and a stop command.  Is an ERB template stored in the  templates  directory for the relevant job.   For each job, create a control script that configures the job to store logs in  /var/vcap/sys/log/JOB_NAME . Save this script as  ctl.erb  in the  templates  directory for its job.  The control script for the  web_ui  job looks like this:  #!/bin/bash  RUN_DIR = /var/vcap/sys/run/web_ui LOG_DIR = /var/vcap/sys/log/web_ui PIDFILE = ${ RUN_DIR } /pid case   $1  in\n\n  start ) \n    mkdir -p  $RUN_DIR   $LOG_DIR \n    chown -R vcap:vcap  $RUN_DIR   $LOG_DIR \n\n     echo   $$  >  $PIDFILE \n\n     cd  /var/vcap/packages/ardo_app\n\n     export   PATH = /var/vcap/packages/ruby_1.9.3/bin: $PATH \n\n     exec  /var/vcap/packages/ruby_1.9.3/bin/bundle  exec   \\ \n      rackup -p <% =  properties.web_ui.port %>  \\ \n      >>   $LOG_DIR /web_ui.stdout.log  \\ \n       2 >>  $LOG_DIR /web_ui.stderr.log\n\n     ;; \n\n  stop ) \n     kill  -9  ` cat  $PIDFILE ` \n    rm -f  $PIDFILE \n\n     ;; \n\n  * ) \n     echo   \"Usage: ctl {start|stop}\"   ;;  esac   If your release needs templates other than the control script, create them now.",
            "title": "Create control scripts  ### "
        },
        {
            "location": "/create-release/#update-monit-files",
            "text": "The  monit  file:   Specifies the process ID (pid) file for the job  References each command provided by the templates for the job  Specifies that the job belongs to the  vcap  group   On a deployed release, a BOSH Agent runs on each job VM.\nBOSH communicates with the Agent, which in turn executes commands in the\ncontrol script.\nThe Agent does this using open source process monitoring software called Monit .  The  monit  file for the  web_ui  job looks like this:  check process web_ui\n  with pidfile /var/vcap/sys/run/web_ui/pid\n  start program \"/var/vcap/jobs/web_ui/bin/ctl start\"\n  stop program \"/var/vcap/jobs/web_ui/bin/ctl stop\"\n  group vcap  Update the  monit  file for each of your jobs.\nUse  /var/vcap  paths as shown in the example.  Note : BOSH requires a  monit  file for each job in a release. When developing a release, you can use an empty  monit  file to meet this requirement without having to first create a control script.",
            "title": "Update monit files  ### "
        },
        {
            "location": "/create-release/#update-job-specs",
            "text": "At compile time, BOSH transforms templates into files, which it then replicates\non the job VMs.  The template names and file paths are among the metadata for each job that\nresides in the job  spec  file.  In the job  spec  file, the  templates  block contains key/value pairs where:   Each key is template name  Each value is the path to the corresponding file on a job VM   The file paths that you provide for templates are relative to\nthe  /var/vcap/jobs/<job_name>  directory on the VM.\nFor example,  bin/ctl  becomes  /var/vcap/jobs/<job_name>/bin/ctl  on the job VM.\nUsing  bin  as the directory where these files go is a convention.  The  templates  block of the updated  spec  files for the example jobs look\nlike this:  templates : \n   ctl.erb :   bin/ctl   For each job, update the  spec  file with template names.",
            "title": "Update job specs  ### "
        },
        {
            "location": "/create-release/#commit",
            "text": "You have now created one or more job skeletons; this is a good time to commit.  If you used the  --git  option with  bosh init-release  (as recommended), the\ncorrect  .gitignore  file has been automatically created for you.",
            "title": "Commit ### "
        },
        {
            "location": "/create-release/#step-2-make-dependency-graphs",
            "text": "There are two kinds of dependencies in a BOSH release:   The  runtime dependency , where a job depends on a package at runtime.\nFor example, the  web_ui  job depends on Ruby.  The  compile-time dependency , where a package depends on another package at\ncompile time.\nFor example, Ruby depends on the YAML library.   Three rules govern these dependencies:   Jobs never depend on other jobs.  Jobs can depend on packages.  Packages can depend on other packages.",
            "title": "Step 2: Make Dependency Graphs ## "
        },
        {
            "location": "/create-release/#building-the-dependency-graph",
            "text": "Create a dependency graph to clarify your understanding of the\ndependencies between the jobs and packages in your release.",
            "title": " "
        },
        {
            "location": "/create-release/#identify-runtime-dependencies",
            "text": "Whenever a control script or other template cites a package name, the job\nthat the template belongs to depends on the cited package at runtime.  For each job, find all the cases where your control scripts cite packages.\nAdd these runtime dependencies to your dependency graph.  In our example, this line in both of our  ctl.erb  scripts cites  ardo_app :  cd /var/vcap/packages/ardo_app  This line cites Ruby:  exec /var/vcap/packages/ruby_1.9.3/bin/bundle exec  This means that both the  web-ui  and  bg_worker  jobs have runtime\ndependencies on both the  ardo_app  and  ruby_1.9.3  packages.  We add these four runtime dependencies to our example dependency graph.",
            "title": " "
        },
        {
            "location": "/create-release/#identify-compile-time-dependencies",
            "text": "Use your knowledge about the runtime dependencies you have already noted.\nConsider the packages you have identified as dependencies.\nDo any of them depend on other packages in turn?  Whenever a package depends on another package, that is a compile-time\ndependency.  For each job, add the compile-time dependencies to your dependency graph.\nIf you miss a dependency, BOSH lets you know later, when you try to deploy.  In our example, we already noted a runtime dependency on Ruby 1.9.3.\nWe now ask ourselves whether Ruby 1.9.3 itself has any dependencies.\nThe answer is yes, it depends on libyaml 0.1.4.  We add this compile-time dependency to our example dependency graph.",
            "title": " "
        },
        {
            "location": "/create-release/#the-complete-example-dependency-graph",
            "text": "The complete dependency graph for  ardo-release  looks like this:   For a large or complicated release, consider making more than one dependency\ngraph.",
            "title": " "
        },
        {
            "location": "/create-release/#step-3-create-package-skeletons",
            "text": "Packages give BOSH the information needed to prepare the binaries and\ndependencies for your jobs.  Create package skeletons starting from the bottom of your dependency graph.  bosh generate-package <dependency_name>  In our example, we run this command three times.\nStarting from the bottom of the dependency graph,\nwe run it for  libyaml_0.1.4 ,  ruby_1.9.3 , and  ardo_app .  View the package skeletons with  tree :  $ tree packages\npackages\n\u251c\u2500\u2500 ardo_app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 packaging\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pre_packaging\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec\n\u251c\u2500\u2500 libyaml_0.1.4\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 packaging\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pre_packaging\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec\n\u2514\u2500\u2500 ruby_1.9.3\n    \u251c\u2500\u2500 packaging\n    \u251c\u2500\u2500 pre_packaging\n    \u2514\u2500\u2500 spec 3  directories,  9  files  Putting each dependency in a separate package provides maximum reusability\nalong with a clear, modular structure. This is not mandatory; what packages\nto create is a matter of preference. You could even opt to put all the\ndependencies together in a single package, though that is not recommended.  Note : Use of the  pre_packaging  file is not recommended, and is not discussed in this tutorial.  Without using  pre_packaging  for our  ardo_app  we need to pack gems manually for further usage:  $  cd  src/ardo_app/\n$ bundle package",
            "title": "Step 3: Create Package Skeletons ## "
        },
        {
            "location": "/create-release/#update-packaging-specs",
            "text": "Within each package directory, there is a  spec  file which states:   The package name  The package's dependencies  The location where BOSH can find the binaries and other files that the package needs at compile time   Use your dependency graph to determine which dependencies belong in each spec.\nDeveloper preferences and style play a role here.\nConsider our example: the spec for Ruby lists  rubygems  and  bundler  as dependencies along\nwith Ruby itself.\nSome Ruby developers would do it this way; others would not.  To maximize portability of your release across different versions of stemcells,\nnever depend on the presence of libraries or other software on stemcells.  To describe binary locations in the  files  block of the spec:    Find the official site for the binary in question.\nFor example, Ruby might be at  http://cache.ruby-lang.org/pub/ruby/1.9/ruby-1.9.3-p484.tar.gz .    Download the binary from the official location and make sure the file hash matches.    Record the binary name including version number, with a slash and the binary\nfilename concatenated to it.\nIt's a good idea to cite the official URL in a comment, in the same line.    BOSH interprets the locations you record in the  files  section as being\neither in the  src  directory or in the  blobs  directory.\n(BOSH looks in  src  first.)\nWhen you add the actual blobs to a blobstore (see the next section),\nBOSH populates the  blobs  directory with the correct information.  For packages that depend on their own source code, use the globbing pattern <package_name>/**/*  to deep-traverse the directory in  src  where\nthe source code should reside.  Update the spec for each package.\nRefer to the example specs below for guidance.",
            "title": "Update packaging specs ### "
        },
        {
            "location": "/create-release/#example-libyaml-package-spec",
            "text": "---  name :   libyaml_0.1.4  dependencies :   []  files :  -   libyaml_0.1.4/yaml-0.1.4.tar.gz   # From http://pyyaml.org/download/libyaml/yaml-0.1.4.tar.gz",
            "title": "Example libyaml package spec #### "
        },
        {
            "location": "/create-release/#example-ruby-package-spec",
            "text": "---  name :   ruby_1.9.3  dependencies :  -   libyaml_0.1.4  files :  -   ruby_1.9.3/ruby-1.9.3-p484.tar.gz   # http://cache.ruby-lang.org/pub/ruby/1.9/ruby-1.9.3-p484.tar.gz  -   ruby_1.9.3/rubygems-1.8.24.tgz      # http://production.cf.rubygems.org/rubygems/rubygems-1.8.24.tgz  -   ruby_1.9.3/bundler-1.2.1.gem        # https://rubygems.org/downloads/bundler-1.2.1.gem",
            "title": "Example Ruby package spec #### "
        },
        {
            "location": "/create-release/#example-ardo_app-package-spec",
            "text": "---  name :   ardo_app  dependencies :  -   ruby_1.9.3  files :  -   ardo_app/**/*",
            "title": "Example ardo_app package spec #### "
        },
        {
            "location": "/create-release/#create-packaging-scripts",
            "text": "At compile time, BOSH takes the source files referenced in the package specs,\n and renders them into the executable binaries and scripts that your deployed\njobs need.  You write packaging scripts to instruct BOSH how to do this.\nThe instructions may involve some combination of copying, compilation, and\nrelated procedures.\nFor example:    For a Ruby app like  ardo_app , BOSH must copy source files and install Ruby\ngems.    For Ruby itself, BOSH must compile source code into a binary.    For a Python app, BOSH must copy source files and install Python eggs.    BOSH relies on you to write packaging scripts that perform the correct operation.  Adhere to these principles when writing packaging scripts:    Use your dependency graph to determine which dependencies belong in each\npackaging script.    Begin each script with a  set -e -x  line.\nThis aids debugging at compile time by causing the script to exit immediately\nif a command exits with a non-zero exit code.    Ensure that any copying, installing or compiling delivers resulting code to\n the install target directory (represented as the  BOSH_INSTALL_TARGET \nenvironment variable). For  make  commands, use  configure  or its equivalent\nto accomplish this.    Be aware that BOSH ensures that dependencies cited in the  dependencies \nblock of package  spec  files are available to the deployed binary.\nFor example, in the  spec  file for the Ruby package, we cite libyaml as a\ndependency.\nThis ensures that on the compilation VMs, the packaging script for Ruby has\naccess to the compiled libyaml package.    If the instructions you provide in the packaging scripts fail to deliver compiled\ncode to  BOSH_INSTALL_TARGET , the job cannot function because the VM has no\nway to find the code to run.\nThis failure scenario can happen if, for example,\nyou use a  make  command that delivers compiled code to some standard location\nby default.\nYou can fix the problem by configuring  make  to compile into BOSH_INSTALL_TARGET .\nSee how this is done in the example packaging scripts.  Like control scripts, writing packaging scripts is one of the heavier tasks\nentailed in creating a release.\nWrite your packaging scripts now.\nRefer to the examples below for guidance.",
            "title": "Create packaging scripts ### "
        },
        {
            "location": "/create-release/#example-libyaml-packaging-script",
            "text": "set -e -x\n\ntar xzf libyaml_0.1.4/yaml-0.1.4.tar.gz\npushd yaml-0.1.4\n  ./configure --prefix= ${ BOSH_INSTALL_TARGET } \n\n  make\n  make install\npopd",
            "title": "Example libyaml packaging script #### "
        },
        {
            "location": "/create-release/#example-ruby-packaging-script",
            "text": "set -e -x\n\ntar xzf ruby_1.9.3/ruby-1.9.3-p484.tar.gz\npushd ruby-1.9.3-p484\n  ./configure \\\n    --prefix= ${ BOSH_INSTALL_TARGET }  \\\n    --disable-install-doc \\\n    --with-opt-dir=/var/vcap/packages/libyaml_0.1.4\n\n  make\n  make install\npopd\n\ntar zxvf ruby_1.9.3/rubygems-1.8.24.tgz\npushd rubygems-1.8.24\n   ${ BOSH_INSTALL_TARGET } /bin/ruby setup.rb\npopd ${ BOSH_INSTALL_TARGET } /bin/gem install ruby_1.9.3/bundler-1.2.1.gem --no-ri --no-rdoc",
            "title": "Example Ruby packaging script #### "
        },
        {
            "location": "/create-release/#example-ardo_app-packaging-script",
            "text": "set -e -x\n\ncp -a ardo_app/*  ${ BOSH_INSTALL_TARGET } \n\ncd  ${ BOSH_INSTALL_TARGET } \n\n/var/vcap/packages/ruby_1.9.3/bin/bundle install \\\n  --local \\\n  --deployment \\\n  --without development test",
            "title": "Example ardo_app packaging script #### "
        },
        {
            "location": "/create-release/#update-job-specs-with-dependencies",
            "text": "The dependency graph reveals runtime dependencies that\nneed to be added to the  packages  block of the job spec.  Edit the job specs to include these dependencies.  In our example, the dependency graph shows that  web_ui  job depends on ardo_app  and  ruby_1.9.3 :  packages :  -   ardo_app  -   ruby_1.9.3",
            "title": "Update job specs with dependencies ### "
        },
        {
            "location": "/create-release/#step-4-add-blobs",
            "text": "When creating a release, you will likely use a source code repository.\nBut releases often use tar files or other binaries, also known as blobs.\nChecking blobs into a repository is problematic if your repository\nunsuited to dealing with large binaries (as is true of Git, for example).  BOSH lets you avoid checking blobs into a repository by doing the following:    For dev releases, use local copies of blobs.    For a final release, upload blobs to a blobstore, and direct BOSH to obtain the blobs from there.",
            "title": "Step 4: Add Blobs ## "
        },
        {
            "location": "/create-release/#configure-a-blobstore",
            "text": "In the  config  directory, you record the information BOSH needs about the\nblobstore:    The  final.yml  file names the blobstore and declares its type, which is either  local \nor one of several other types that specify blobstore providers.    The  private.yml  file specifies the blobstore path, along with a secret.    private.yml  contains keys for accessing the blobstore and should not be\nchecked into a repository.\n(If you used the  --git  option when running  bosh init-release  at the beginning\nof this tutorial,  private.yml  is automatically  gitignored .)  The  config  directory also contains two files whose content is automatically\ngenerated: the  blobs.yml  file and the  dev.yml  file.  Adapt the examples below to fit the specifics of your release.\nOur example release uses the  local  type blobstore because otherwise it would\nbe necessary to explain how to configure a public blobstore such as\nAmazon S3, which is too large a topic for this context.  The  local  type blobstore is suitable for learning but the resulting release\ncannot be shared.\nFor that reason, you should configure a non-local, publicly available blobstore\nfor releases that you intend to share.\nNormally, the blobstore you choose when you begin working on a release is used\nfor all subsequent versions of the release.\nChanging the blobstore that a release uses is beyond the scope of this tutorial.  Example  final.yml :  ---  blobstore : \n   provider :   local \n   options : \n     blobstore_path :   /tmp/ardo-blobs  final_name :   ardo_app   Example  private.yml :  ---  blobstore_secret :   'does-not-matter'  blobstore : \n   local : \n     blobstore_path :   /tmp/ardo-blobs   If you have a  private.yml  file:   Required : Include the  blobstore_path  in the  private.yml  file.  Optional : Include the  blobstore_path  in the  final.yml  file. Doing so allows you to  gitignore  the  private.yml  file but still allow the release to be downloaded and used on other systems.   Note : The  blobstore_secret  is required for the  local  type blobstore.\nThis is true even though the  blobstore_secret  line is deprecated and its\ncontent does not matter.\nThere is never a  blobstore_secret  line for blobstores of types other than local .",
            "title": "Configure a blobstore  ### "
        },
        {
            "location": "/create-release/#inform-bosh-where-blobs-are",
            "text": "In the package  spec  file, the  files  block lists any binaries you downloaded,\nalong with the URLs from which you downloaded them.\n(This assumes that you followed the directions in the  Update package specs  section.)  Those files are blobs, and now you need the paths to the downloaded blobs on\nyour local system.  In our example, the  spec  file for the  libyaml_0.1.4  package includes the line:  files :  -   libyaml_0.1.4/yaml-0.1.4.tar.gz   # From http://pyyaml.org/download/libyaml/yaml-0.1.4.tar.gz   If you downloaded the blob, its local path might be:  ~/Downloads/yaml-0.1.4.tar.gz  Go through all your packages and make a list of local paths to the blobs you downloaded.\nNow you are ready to inform BOSH about these blobs.  For each blob, run:  bosh add-blob <path_to_blob_on_local_system> <package_name>  e.g.  bosh add-blob ~/Downloads/yaml-0.1.4.tar.gz libyaml_0.1.4  The  bosh add-blob  command adds a local blob to the collection your release\nrecognizes as BOSH blobs.  The usage shown above is a blend of requirement and convention.\nIt works like this:   For the first argument, you provide the path to the blob on your local system  For the second argument, you provide a destination within the  blobs  directory\nin your release  BOSH goes into the  blobs  directory and creates a subdirectory with\nthe name of the package that the local blob represents  In the new subdirectory, BOSH creates a symbolic link to a copy of the blob\nwhich BOSH makes in a hidden directory   Using the package name as the second argument of the  bosh add-blob  command\nis recommended because it produces a cleanly-organized blobs directory.  Later, when you upload blobs for a final release, BOSH uses the hidden directory\nas a staging area.",
            "title": "Inform BOSH where blobs are ### "
        },
        {
            "location": "/create-release/#do-not-upload-blobs-for-a-dev-release",
            "text": "Once you have uploaded blobs to a non-local blobstore, those blobs may become\nessential to some other developer.\nFor this reason, uploading a blob and then removing it is considered poor practice.  When creating dev releases, do not run  bosh upload-blobs .\n(You only run it when you do a final release.)",
            "title": "Do not upload blobs for a dev release ### "
        },
        {
            "location": "/create-release/#step-5-create-job-properties",
            "text": "If your service needs to be configurable at deployment time,\nyou create the desired inputs or controls and specify them in the release.\nEach input is a  property  that belongs to a particular job.  Creating properties requires three steps:    Define properties and defaults in the  properties  block of\nthe job spec.    Use the property lookup helper  p()  to reference properties in\nrelevant templates.    For example, a start command can take a property as an argument,\nusing the property lookup helper:      <%=   p ( '<job_name>.<property_name>' )   %>    Specify the property in the  deployment manifest .   Adapt the example below to create any properties your release needs now.  In our example, we want the port that the web UI listens on to be a\nconfigurable property.  We edit the spec for the web UI job to look like this:  properties : \n    port : \n      description :   Port that web_ui app listens on \n      default :   80",
            "title": "Step 5: Create Job Properties  ## "
        },
        {
            "location": "/create-release/#step-6-create-a-dev-release",
            "text": "All the elements needed to create a dev release should now be in place.",
            "title": "Step 6: Create a Dev Release  ## "
        },
        {
            "location": "/create-release/#release",
            "text": "For the dev release, use the  --force  option with the  bosh create-release \ncommand.\nThis forces BOSH to use the local copies of our blobs.  Without the  --force  option, BOSH requires blobs to be uploaded before you\nrun  bosh create-release .\nFor a final release, we upload blobs, but not for a dev release.  Create the dev release:  bosh create-release --force  BOSH prompts for a release name, and assigns a dot-number version to the release.",
            "title": "Release  ### "
        },
        {
            "location": "/create-release/#deploy-the-dev-release",
            "text": "Deploying the release requires three or more steps, depending on whether\nBOSH is targeting the desired Director, and whether BOSH is already pointing\nto a release.  See what director BOSH is targeting:  bosh env  Target a director:  bosh -e <director_alias> log-in  See what releases are available:  bosh releases  If BOSH is already pointing to a release, edit the BOSH deployment manifest.\nOtherwise, create a manifest. See  BOSH Deployment Manifest  for more information.\nSimple manifest for  ardo_app  can be found  here  (OpenStack) or  here  (AWS).  Upload the new dev release.  bosh upload-release  Assuming you are in the release directory, no path is needed with the above command.  Deploy:  bosh deploy",
            "title": "Deploy the Dev Release  ### "
        },
        {
            "location": "/create-release/#test-the-dev-release",
            "text": "What tests to run depends on the software you are releasing.  Start by opening a separate terminal, logging in on the job VM, and observing\nlogging output as you test your release.  If your release fails tests, follow this pattern.   Fix the code.  Do a new dev release.  Run  bosh deploy  to see whether the new release deploys successfully.   Using  bosh deploy --recreate  can provide a clearer picture because with that option,\nBOSH deploys all the VMs from scratch.",
            "title": "Test the Dev Release  ### "
        },
        {
            "location": "/create-release/#create-a-final-release",
            "text": "Only proceed to this step if your latest dev release passes all tests.",
            "title": "Create a Final Release  ## "
        },
        {
            "location": "/create-release/#upload-blobs",
            "text": "When you use the  bosh create-release --force  command to create them, dev\nreleases depend on locally-stored blobs.\nTo do a final release, you must upload blobs first.  If files that you need to keep private are uploaded to a public blobstore,\nthere is no satisfactory way to undo the mistake.\nTo avoid this situation, complete the following steps immediately before\nyou upload blobs:    Run  bosh blobs  to see the list of blobs BOSH is prepared to upload    Proofread the list of blobs displayed by the command    The list should include only the blobs you need for the final release    If the list includes any files that should not be uploaded, find and delete\nthe symbolic links to them in the  blobs  directory    To upload your blobs, run:  bosh upload-blobs",
            "title": "Upload blobs ### "
        },
        {
            "location": "/create-release/#commit_1",
            "text": "The  bosh upload-blobs  command has now populated the  blobs.yml  file\nin the  config  directory with metadata for uploaded blobs.  This is a good reason to commit.",
            "title": "Commit ### "
        },
        {
            "location": "/create-release/#release_1",
            "text": "Run:  bosh create-release --final  BOSH prompts you for a release name, and assigns a whole-number version to the release.  This is a good time to push your code to a shared repository to give others access to\nyour final release.",
            "title": "Release ### "
        },
        {
            "location": "/create-release/#commit_2",
            "text": "Do one more commit before you deploy!",
            "title": "Commit ### "
        },
        {
            "location": "/create-release/#deploy-the-final-release",
            "text": "Run:  bosh deploy",
            "title": "Deploy the Final Release  ### "
        },
        {
            "location": "/links-properties/",
            "text": "(See \nLinks\n for an introduction.)\n\n\nNote: This feature is available with bosh-release v255.5+.\n\n\n\nIn addition to sharing basic networking information (name, AZ, IP, etc.) links allow to show arbitrary information via properties. Most common example is sharing a port value. From our previous example here is a \nweb\n job that communicates with a database:\n\n\nname\n:\n \nweb\n\n\n\ntemplates\n:\n\n  \nconfig.erb\n:\n \nconfig/conf\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nprimary_db\n\n  \ntype\n:\n \ndb\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nincoming\n\n  \ntype\n:\n \nhttp\n\n\n\nproperties\n:\n \n{\n...\n}\n\n\n\n\n\nHere is an example Postgres job that provides a \nconn\n link of type \ndb\n, and now also includes server's port for client connections:\n\n\nname\n:\n \npostgres\n\n\n\ntemplates\n:\n \n{\n...\n}\n\n\n\nprovides\n:\n\n\n-\n \nname\n:\n \nconn\n\n  \ntype\n:\n \ndb\n\n  \nproperties\n:\n\n  \n-\n \nport\n\n  \n-\n \nadapter\n\n  \n-\n \nusername\n\n  \n-\n \npassword\n\n  \n-\n \nname\n\n\n\nproperties\n:\n\n  \nport\n:\n\n    \ndescription\n:\n \n\"Port\n \nfor\n \nPostgres\n \nto\n \nlisten\n \non\"\n\n    \ndefault\n:\n \n5432\n\n  \nadapter\n:\n\n    \ndescription\n:\n \n\"Type\n \nof\n \nthis\n \ndatabase\"\n\n    \ndefault\n:\n \npostgres\n\n  \nusername\n:\n\n    \ndescription\n:\n \n\"Username\"\n\n    \ndefault\n:\n \npostgres\n\n  \npassword\n:\n\n    \ndescription\n:\n \n\"Password\"\n\n  \nname\n:\n\n    \ndescription\n:\n \n\"Database\n \nname\"\n\n    \ndefault\n:\n \npostgres\n\n\n\n\n\nNote that all properties included in the \nconn\n link are defined by the job itself in the \nproperties\n section.\n\n\nAnd finally \nweb\n job can use the port and a few other properties in its ERB templates when configuring how to connect to the database:\n\n\n<\n%=\n\n\n\ndb =\n \nlink\n(\n\"primary_db\"\n)\n\n\n\nresult\n \n=\n \n{\n\n  \n\"production\"\n \n=>\n \n{\n\n    \n\"adapter\"\n \n=>\n \ndb\n.\np\n(\n\"adapter\"\n),\n\n    \n\"username\"\n \n=>\n \ndb\n.\np\n(\n\"username\"\n),\n\n    \n\"password\"\n \n=>\n \ndb\n.\np\n(\n\"password\"\n),\n\n    \n\"host\"\n \n=>\n \ndb\n.\ninstances\n.\nfirst\n.\naddress\n,\n\n    \n\"port\"\n \n=>\n \ndb\n.\np\n(\n\"port\"\n),\n\n    \n\"database\"\n \n=>\n \ndb\n.\np\n(\n\"name\"\n),\n\n    \n\"encoding\"\n \n=>\n \n\"utf8\"\n,\n\n    \n\"reconnect\"\n \n=>\n \nfalse\n,\n\n    \n\"pool\"\n \n=>\n \n5\n\n  \n}\n\n\n}\n\n\n\nJSON\n.\ndump\n(\nresult\n)\n\n\n\n%>\n\n\n\n\n\nSimilarly to how \np\n template accessor\n provides access to the job's top level properties, \nlink(\"...\").p(\"...)\n and \nlink(\"...\").if_p(\"...)\n accessors work with properties included in the link.\n\n\nif_p\n template accessor becomes very useful when trying to provide backwards compatibility around link properties as their interface changes. For example if Postgres job author decides to start including \nencoding\n property in the link and \nweb\n job's author wants to continue to support older links that don't include that information, they can use \ndb.if_p(\"encoding\") { ... }\n.\n\n\nAnd finally in the above example the operator needs to configure database password to deploy these two jobs since password property doesn't have a default:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nweb\n\n    \nrelease\n:\n \nmy-app\n\n    \nconsumes\n:\n\n      \nprimary_db\n:\n \n{\nfrom\n:\n \ndata_db\n}\n\n\n\n-\n \nname\n:\n \ndata_db\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \npostgres\n\n    \nrelease\n:\n \npostgres\n\n    \nprovides\n:\n\n      \nconn\n:\n \n{\nas\n:\n \ndata_db\n}\n\n    \nproperties\n:\n\n      \npassword\n:\n \nsome-password\n\n\n\n\n\n\n\nNext: \nManual linking\n\n\nBack to Table of Contents",
            "title": "Sharing Properties"
        },
        {
            "location": "/release-blobstore/",
            "text": "Note: Examples require CLI v2.\n\n\n\nA release blobstore contains \nrelease blob\n and created final releases.\n\n\nAccess to release blobstore is configured via two files:\n\n\n\n\nconfig/final.yml\n (checked into Git repository): contains blobstore location\n\n\nconfig/private.yml\n (is NOT checked into Git repository): contains blobstore credentials\n\n\n\n\nCLI supports three different blobstore providers: \ns3\n, \ngcs\n and \nlocal\n.\n\n\nS3 Configuration \n\u00b6\n\n\nS3 provider is used for most production releases. It's can be used with any S3-compatible blobstore (in compatibility mode) like Google Cloud Storage and Swift.\n\n\nconfig/final.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \nprovider\n:\n \ns3\n\n  \noptions\n:\n\n    \nbucket_name\n:\n \n<bucket_name>\n\n\n\n\n\nconfig/private.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \noptions\n:\n\n    \naccess_key_id\n:\n \n<access_key_id>\n\n    \nsecret_access_key\n:\n \n<secret_access_key>\n\n\n\n\n\nSee \nConfiguring S3 release blobstore\n for details and \nS3 CLI Usage\n for additional configuration options.\n\n\nGCS Configuration \n\u00b6\n\n\nGoogle Cloud Storage can be used without S3 compatibility mode.\n\n\nconfig/final.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \nprovider\n:\n \ngcs\n\n  \noptions\n:\n\n    \nbucket_name\n:\n \n<bucket_name>\n\n\n\n\n\nconfig/private.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \noptions\n:\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \n<json-key>\n\n\n\n\n\n\n\nLocal Configuration \n\u00b6\n\n\nLocal provider is useful for testing.\n\n\nconfig/final.yml\n:\n\n\n---\n\n\nblobstore\n:\n\n  \nprovider\n:\n \nlocal\n\n  \noptions\n:\n\n    \nblobstore_path\n:\n \n/tmp/test-blobs\n\n\n\n\n\nNothing in \nconfig/private.yml\n.\n\n\n\n\nMigrating blobs \n\u00b6\n\n\nCLI does not currently provide a builtin way to migrate blobs to a different blobstore. Suggested way to migrate blobs is to use third party tool like \ns3cmd\n to list and copy all blobs from current blobstore to another. Once copying of all blobs is complete, update \nconfig\n directory to with new blobstore location.",
            "title": "Release Blobstore"
        },
        {
            "location": "/release-blobstore/#s3-configuration",
            "text": "S3 provider is used for most production releases. It's can be used with any S3-compatible blobstore (in compatibility mode) like Google Cloud Storage and Swift.  config/final.yml :  ---  blobstore : \n   provider :   s3 \n   options : \n     bucket_name :   <bucket_name>   config/private.yml :  ---  blobstore : \n   options : \n     access_key_id :   <access_key_id> \n     secret_access_key :   <secret_access_key>   See  Configuring S3 release blobstore  for details and  S3 CLI Usage  for additional configuration options.",
            "title": "S3 Configuration "
        },
        {
            "location": "/release-blobstore/#gcs-configuration",
            "text": "Google Cloud Storage can be used without S3 compatibility mode.  config/final.yml :  ---  blobstore : \n   provider :   gcs \n   options : \n     bucket_name :   <bucket_name>   config/private.yml :  ---  blobstore : \n   options : \n     credentials_source :   static \n     json_key :   | \n       <json-key>",
            "title": "GCS Configuration "
        },
        {
            "location": "/release-blobstore/#local-configuration",
            "text": "Local provider is useful for testing.  config/final.yml :  ---  blobstore : \n   provider :   local \n   options : \n     blobstore_path :   /tmp/test-blobs   Nothing in  config/private.yml .",
            "title": "Local Configuration "
        },
        {
            "location": "/release-blobstore/#migrating-blobs",
            "text": "CLI does not currently provide a builtin way to migrate blobs to a different blobstore. Suggested way to migrate blobs is to use third party tool like  s3cmd  to list and copy all blobs from current blobstore to another. Once copying of all blobs is complete, update  config  directory to with new blobstore location.",
            "title": "Migrating blobs "
        },
        {
            "location": "/s3-release-blobstore/",
            "text": "Note: Examples require CLI v2.\n\n\n\nThis topic is written for release maintainers and describes how to set up a S3 bucket for storing release artifacts.\n\n\nCreating S3 Bucket \n\u00b6\n\n\nS3 bucket is used for storing release blobs and generated final release blobs. It's configured to be readable by everyone.\n\n\n\n\n\n\nCreate S3 bucket with a descriptive name. For example choose \nredis-blobs\n for \nredis-release\n release.\n\n\n\n\n\n\nUnder the bucket properties, click \nAdd bucket policies\n and add the following entry to make blobs publicly downloadable. Be sure to change \n<blobs_bucket_name>\n to a name you want to call your blobstore bucket.\n\n\n\n\n\n\n{\n\n  \n\"Statement\"\n:\n \n[{\n\n    \n\"Action\"\n:\n \n[\n \n\"s3:GetObject\"\n \n],\n\n    \n\"Effect\"\n:\n \n\"Allow\"\n,\n\n    \n\"Resource\"\n:\n \n\"arn:aws:s3:::<blobs_bucket_name>/*\"\n,\n\n    \n\"Principal\"\n:\n \n{\n \n\"AWS\"\n:\n \n[\n\"*\"\n]\n \n}\n\n  \n}]\n\n\n}\n\n\n\n\n\nNote\n: S3 buckets have a global namespace. If you create a bucket, that name has been forever consumed for everyone using S3. If you choose to delete that bucket, the name will not be added back to the global pool of names. It is gone forever.\n\n\n\nCreating IAM User for the Maintainer \n\u00b6\n\n\nAn IAM user is used to download and upload blobs to a created S3 bucket.\n\n\n\n\n\n\nCreate an AWS IAM user with a name that describes a specific purpose for this user -- uploading release blobs. For example: \nredis-blobs-upload\n.\n\n\n\n\n\n\nMake sure to save the credentials provided in the last step of user creation. These will need to be put in the \nconfig/private.yml\n file of your BOSH release. This file will look something like:\n\n\n\n\n\n\n---\n\n\nblobstore\n:\n\n  \noptions\n:\n\n    \naccess_key_id\n:\n \n<access_key_id>\n\n    \nsecret_access_key\n:\n \n<secret_access_key>\n\n\n\n\n\n\n\nRemember to also create/update \nconfig/final.yml\n:\n\n\n\n\n---\n\n\nblobstore\n:\n\n  \nprovider\n:\n \ns3\n\n  \noptions\n:\n\n    \nbucket_name\n:\n \n<blobs_bucket_name>\n\n\n\n\n\nNote\n: The \n.gitignore\n file in the BOSH release should include \nconfig/private.yml\n. This file should \nnot\n be committed to the release repo. It is only meant for the release maintainers. \nconfig/final.yml\n, on the other hand, should not be in the \n.gitignore\n file, and should be committed to the repository, as it is for users consuming and deploying the release.\n\n\n\n\n\nAttach a \nuser\n policy that would limit the user to permissions to read/write to the bucket that was just created:\n\n\n\n\n{\n\n  \n\"Version\"\n:\n \n\"2012-10-17\"\n,\n\n  \n\"Statement\"\n:\n \n[{\n\n    \n\"Effect\"\n:\n \n\"Allow\"\n,\n\n    \n\"Action\"\n:\n \n[\n \n\"s3:PutObject\"\n \n],\n\n    \n\"Resource\"\n:\n \n[\n \n\"arn:aws:s3:::<blobs_bucket_name>/*\"\n \n]\n\n  \n}]\n\n\n}\n\n\n\n\n\nNote\n: When you first create a bucket, it might take a little while for Amazon to be able to route requests correctly to the bucket and so downloads may fail with an obscure \"Broken Pipe\" error. The solution is to wait for some time before trying.\n\n## Setting S3 region \n\n\nBy default, Amazon S3 buckets resolve to the `us-east-1` (North Virginia) region. If your blobstore bucket resides in a different region, override the region and endpoint settings in `config/final.yml`. For example, a bucket in `eu-west-1` would be as follows:\n\n\u0002wzxhzdk:4\u0003\n\nA full list of S3 regions and endpoints is available [here](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).\n\nSee [S3 CLI Usage](https://github.com/pivotal-golang/s3cli#usage) for additional configuration options.\n\n## Usage \n\n\nOnce the S3 bucket and IAM user are configured with correct access rules, `bosh upload blobs` should succeed and the S3 bucket should contain uploaded blobs. Running `bosh create release --final` will also place additional blobs into the bucket.",
            "title": "Using S3 & IAM Policies"
        },
        {
            "location": "/s3-release-blobstore/#creating-s3-bucket",
            "text": "S3 bucket is used for storing release blobs and generated final release blobs. It's configured to be readable by everyone.    Create S3 bucket with a descriptive name. For example choose  redis-blobs  for  redis-release  release.    Under the bucket properties, click  Add bucket policies  and add the following entry to make blobs publicly downloadable. Be sure to change  <blobs_bucket_name>  to a name you want to call your blobstore bucket.    { \n   \"Statement\" :   [{ \n     \"Action\" :   [   \"s3:GetObject\"   ], \n     \"Effect\" :   \"Allow\" , \n     \"Resource\" :   \"arn:aws:s3:::<blobs_bucket_name>/*\" , \n     \"Principal\" :   {   \"AWS\" :   [ \"*\" ]   } \n   }]  }   Note : S3 buckets have a global namespace. If you create a bucket, that name has been forever consumed for everyone using S3. If you choose to delete that bucket, the name will not be added back to the global pool of names. It is gone forever.",
            "title": "Creating S3 Bucket "
        },
        {
            "location": "/s3-release-blobstore/#creating-iam-user-for-the-maintainer",
            "text": "An IAM user is used to download and upload blobs to a created S3 bucket.    Create an AWS IAM user with a name that describes a specific purpose for this user -- uploading release blobs. For example:  redis-blobs-upload .    Make sure to save the credentials provided in the last step of user creation. These will need to be put in the  config/private.yml  file of your BOSH release. This file will look something like:    ---  blobstore : \n   options : \n     access_key_id :   <access_key_id> \n     secret_access_key :   <secret_access_key>    Remember to also create/update  config/final.yml :   ---  blobstore : \n   provider :   s3 \n   options : \n     bucket_name :   <blobs_bucket_name>   Note : The  .gitignore  file in the BOSH release should include  config/private.yml . This file should  not  be committed to the release repo. It is only meant for the release maintainers.  config/final.yml , on the other hand, should not be in the  .gitignore  file, and should be committed to the repository, as it is for users consuming and deploying the release.   Attach a  user  policy that would limit the user to permissions to read/write to the bucket that was just created:   { \n   \"Version\" :   \"2012-10-17\" , \n   \"Statement\" :   [{ \n     \"Effect\" :   \"Allow\" , \n     \"Action\" :   [   \"s3:PutObject\"   ], \n     \"Resource\" :   [   \"arn:aws:s3:::<blobs_bucket_name>/*\"   ] \n   }]  }   Note : When you first create a bucket, it might take a little while for Amazon to be able to route requests correctly to the bucket and so downloads may fail with an obscure \"Broken Pipe\" error. The solution is to wait for some time before trying.\n\n## Setting S3 region  \n\nBy default, Amazon S3 buckets resolve to the `us-east-1` (North Virginia) region. If your blobstore bucket resides in a different region, override the region and endpoint settings in `config/final.yml`. For example, a bucket in `eu-west-1` would be as follows:\n\n\u0002wzxhzdk:4\u0003\n\nA full list of S3 regions and endpoints is available [here](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).\n\nSee [S3 CLI Usage](https://github.com/pivotal-golang/s3cli#usage) for additional configuration options.\n\n## Usage  \n\nOnce the S3 bucket and IAM user are configured with correct access rules, `bosh upload blobs` should succeed and the S3 bucket should contain uploaded blobs. Running `bosh create release --final` will also place additional blobs into the bucket.",
            "title": "Creating IAM User for the Maintainer "
        },
        {
            "location": "/release-blobs/",
            "text": "Note: Examples use CLI v2.\n\n\n\nA package may need to reference blobs (binary large objects) in addition to referencing other source files. For example when building a package for PostgreSQL server you may want to include \npostgresql-9.6.1.tar.gz\n from \nhttps://www.postgresql.org/ftp/source/\n. Typically it's not recommended to check in blobs directly into a Git repository because Git cannot efficiently track changes to such files. CLI provides a way to manage blobs in a reasonable manner with several commands:\n\n\n$ bosh -h\n|\ngrep blob\n  add-blob               Add blob\n  blobs                  List blobs\n  remove-blob            Remove blob\n  sync-blobs             Sync blobs\n  upload-blobs           Upload blobs\n\n\n\n\nAdding a blob \n\u00b6\n\n\nPackage can reference blobs via \nfiles\n directive in a package spec just like as other source files.\n\n\n---\n\n\nname\n:\n \ncockroachdb\n\n\nfiles\n:\n\n\n-\n \ncockroach-latest.linux-amd64.tgz\n\n\n\n\n\nCreating a release with above configuration causes following error:\n\n\n$ bosh create-release --force\nBuilding a release from directory \n'/Users/user/workspace/cockroachdb-release'\n:\n  - Constructing packages from directory:\n      - Reading package from \n'/Users/user/workspace/cockroachdb-release/packages/cockroachdb'\n:\n          Collecting package files:\n            Missing files \nfor\n pattern \n'cockroach-latest.linux-amd64.tgz'\n\n\n\n\n\nCLI expects to find \ncockroach-latest.linux-amd64.tgz\n in either \nblobs\n or \nsrc\n directory. Since it's a blob it should not be in \nsrc\n directory but rather added with the following command:\n\n\n$ bosh add-blob ~/Downloads/cockroach-latest.linux-amd64.tgz cockroach-latest.linux-amd64.tgz\n\n\n\n\nadd-blob\n command:\n\n\n\n\ncopies file into \nblobs\n directory (which should be in \n.gitignore\n)\n\n\nupdates \nconfig/blobs.yml\n to start tracking blobs\n\n\n\n\n\n\nListing blobs \n\u00b6\n\n\nTo list currently tracked blobs use \nbosh blobs\n command:\n\n\n$ bosh blobs\nPath                              Size    Blobstore ID                          SHA1\ncockroach-latest.linux-amd64.tgz  \n15\n MiB  \n(\nlocal\n)\n                               469004231a9ed1d87de798f12fe2f49cc6ff1d2f\ngo1.7.4.linux-amd64.tar.gz        \n80\n MiB  7e6431ba-f2c6-4e80-6a16-cd5cd8722b57  2e5baf03d1590e048c84d1d5b4b6f2540efaaea1\n\n\n2\n blobs\n\nSucceeded\n\n\n\n\nBlobs that have not been uploaded to release blobstore will be marked as \nlocal\n until they are uploaded.\n\n\n\n\nUploading blobs \n\u00b6\n\n\nBlobs should be saved into release blobstore before cutting a new final release so that others can rebuild a release at a future time.\n\n\nbosh upload-blobs\n command:\n\n\n\n\nuploads all local blobs to release blobstore\n\n\nupdates \nconfig/blobs.yml\n with blobstore IDs\n\n\n\n\nconfig/blobs.yml\n should be checked into a Git repository.\n\n\n\n\nRemoving blobs \n\u00b6\n\n\nOnce a blob is no longer needed by a package it can be stopped being tracked.\n\n\nbosh remove-blob\n command:\n\n\n\n\nremoves blob from \nconfig/blobs.yml\n\n\ndoes NOT remove blob from release blobstore so that new releases can be created from older revisions",
            "title": "Working with Blobs"
        },
        {
            "location": "/release-blobs/#adding-a-blob",
            "text": "Package can reference blobs via  files  directive in a package spec just like as other source files.  ---  name :   cockroachdb  files :  -   cockroach-latest.linux-amd64.tgz   Creating a release with above configuration causes following error:  $ bosh create-release --force\nBuilding a release from directory  '/Users/user/workspace/cockroachdb-release' :\n  - Constructing packages from directory:\n      - Reading package from  '/Users/user/workspace/cockroachdb-release/packages/cockroachdb' :\n          Collecting package files:\n            Missing files  for  pattern  'cockroach-latest.linux-amd64.tgz'   CLI expects to find  cockroach-latest.linux-amd64.tgz  in either  blobs  or  src  directory. Since it's a blob it should not be in  src  directory but rather added with the following command:  $ bosh add-blob ~/Downloads/cockroach-latest.linux-amd64.tgz cockroach-latest.linux-amd64.tgz  add-blob  command:   copies file into  blobs  directory (which should be in  .gitignore )  updates  config/blobs.yml  to start tracking blobs",
            "title": "Adding a blob "
        },
        {
            "location": "/release-blobs/#listing-blobs",
            "text": "To list currently tracked blobs use  bosh blobs  command:  $ bosh blobs\nPath                              Size    Blobstore ID                          SHA1\ncockroach-latest.linux-amd64.tgz   15  MiB   ( local )                                469004231a9ed1d87de798f12fe2f49cc6ff1d2f\ngo1.7.4.linux-amd64.tar.gz         80  MiB  7e6431ba-f2c6-4e80-6a16-cd5cd8722b57  2e5baf03d1590e048c84d1d5b4b6f2540efaaea1 2  blobs\n\nSucceeded  Blobs that have not been uploaded to release blobstore will be marked as  local  until they are uploaded.",
            "title": "Listing blobs "
        },
        {
            "location": "/release-blobs/#uploading-blobs",
            "text": "Blobs should be saved into release blobstore before cutting a new final release so that others can rebuild a release at a future time.  bosh upload-blobs  command:   uploads all local blobs to release blobstore  updates  config/blobs.yml  with blobstore IDs   config/blobs.yml  should be checked into a Git repository.",
            "title": "Uploading blobs "
        },
        {
            "location": "/release-blobs/#removing-blobs",
            "text": "Once a blob is no longer needed by a package it can be stopped being tracked.  bosh remove-blob  command:   removes blob from  config/blobs.yml  does NOT remove blob from release blobstore so that new releases can be created from older revisions",
            "title": "Removing blobs "
        },
        {
            "location": "/job-templates/",
            "text": "Unit Testing with \nbosh-template\n gem \n\u00b6\n\n\nbosh-template\n Ruby gem could be used for unit testing your job templates. Unit testing of job templates becomes even more important once they contain more complex ERB logic that may perform validation or data transformation.\n\n\nSee example of unit tests in a production release: \nhttps://github.com/cloudfoundry/bosh-dns-aliases-release/blob/master/spec/bosh-dns-aliases_spec.rb\n.\n\n\nAssuming we have a job \nweb-server\n with a following \nconfig.json\n ERB template:\n\n\n<\n%=\n\n\n\nport =\n \np\n(\n\"port\"\n)\n\n\n\nif\n \nport\n \n&\nlt\n;\n \n1024\n \nor\n \nport\n \n&\ngt\n;\n \n4000\n\n  \nraise\n \n\"Ports lower than 1024 or higher than 4000 are not allowed\"\n\n\nend\n\n\n\nJSON\n.\ndump\n(\n\"port\"\n \n=>\n \nport\n)\n\n\n\n%>\n\n\n\n\n\nTo start unit testing \nweb-server\n job, add \nGemfile\n to the root of your release so that \nbundler\n gem can install all dependencies necessary for testing:\n\n\nsource\n \n'https://rubygems.org'\n\n\n\ngroup\n \n:development\n,\n \n:test\n \ndo\n\n  \ngem\n \n'bosh-template'\n\n  \ngem\n \n'rspec'\n\n  \ngem\n \n'rspec-its'\n\n\nend\n\n\n\n\n\nThen run \nbundle install\n from the same directory to download and install specified gems.\n\n\nNow that necessary dependencies are installed, let's add \nspec/jobs/web_server_spec.rb\n to test if conditional within our \nconfig.json\n template:\n\n\nrequire\n \n'rspec'\n\n\nrequire\n \n'json'\n\n\nrequire\n \n'bosh/template/test'\n\n\n\ndescribe\n \n'web-server job'\n \ndo\n\n  \nlet\n(\n:release\n)\n \n{\n \nBosh\n::\nTemplate\n::\nTest\n::\nReleaseDir\n.\nnew\n(\nFile\n.\njoin\n(\nFile\n.\ndirname\n(\n__FILE__\n),\n \n'../..'\n))\n \n}\n\n  \nlet\n(\n:job\n)\n \n{\n \nrelease\n.\njob\n(\n'web-server'\n)\n \n}\n\n\n  \ndescribe\n \n'config.json'\n \ndo\n\n    \nlet\n(\n:template\n)\n \n{\n \njob\n.\ntemplate\n(\n'config/config.json'\n)\n \n}\n\n\n    \nit\n \n'raises error if given port is < 1024'\n \ndo\n\n      \nexpect\n \n{\n\n        \ntemplate\n.\nrender\n(\n\"port\"\n \n=>\n \n1023\n)\n\n      \n}\n.\nto\n \nraise_error\n \n\"Ports lower than 1024 or higher than 4000 are not allowed\"\n\n    \nend\n\n\n    \nit\n \n'raises error if given port is > 4000'\n \ndo\n\n      \nexpect\n \n{\n\n        \ntemplate\n.\nrender\n(\n\"port\"\n \n=>\n \n4001\n)\n\n      \n}\n.\nto\n \nraise_error\n \n\"Ports lower than 1024 or higher than 4000 are not allowed\"\n\n    \nend\n\n\n    \nit\n \n'configures port successfully'\n \ndo\n\n      \nconfig\n \n=\n \nJSON\n.\nparse\n(\ntemplate\n.\nrender\n(\n\"port\"\n \n=>\n \n1024\n))\n\n      \nexpect\n(\nconfig\n[\n'port'\n]\n)\n.\nto\n \neq\n(\n1024\n)\n\n\n      \nconfig\n \n=\n \nJSON\n.\nparse\n(\ntemplate\n.\nrender\n(\n\"port\"\n \n=>\n \n4000\n))\n\n      \nexpect\n(\nconfig\n[\n'port'\n]\n)\n.\nto\n \neq\n(\n4000\n)\n\n    \nend\n\n  \nend\n\n\nend\n\n\n\n\n\nAbove set of tests provides enough gurantee that our ERB template is validating and passing down correct configuration to the web server binary.\n\n\nAt this point release directory will look something like this:\n\n\nweb-server-release $ tree .\n.\n\u251c\u2500\u2500 Gemfile\n\u251c\u2500\u2500 Gemfile.lock\n\u251c\u2500\u2500 jobs\n\u2502   \u2514\u2500\u2500 web-server\n\u2502       \u251c\u2500\u2500templates\n\u2502       \u2502  \u2514\u2500\u2500 config.json\n\u2502       \u251c\u2500\u2500 spec\n\u2502       \u2514\u2500\u2500 monit\n\u2514\u2500\u2500 spec\n    \u2514\u2500\u2500 jobs\n        \u2514\u2500\u2500 web_server_spec.rb\n\n\n\n\nInstance configuration\n\u00b6\n\n\nbosh-template\n gem provides default values for \nspec.*\n ERB accessors:\n\n\nconfig\n \n=\n \nJSON\n.\nparse\n(\ntemplate\n.\nrender\n({\n...\n}))\n\n\nexpect\n(\nconfig\n[\n'address'\n]\n)\n.\nto\n \neq\n(\n'my.bosh.com'\n)\n\n\nexpect\n(\nconfig\n[\n'az'\n]\n)\n.\nto\n \neq\n(\n'az1'\n)\n\n\nexpect\n(\nconfig\n[\n'bootstrap'\n]\n)\n.\nto\n \neq\n(\nfalse\n)\n\n\nexpect\n(\nconfig\n[\n'deployment'\n]\n)\n.\nto\n \neq\n(\n'my-deployment'\n)\n\n\nexpect\n(\nconfig\n[\n'id'\n]\n)\n.\nto\n \neq\n(\n'xxxxxx-xxxxxxxx-xxxxx'\n)\n\n\nexpect\n(\nconfig\n[\n'index'\n]\n)\n.\nto\n \neq\n(\n0\n)\n\n\nexpect\n(\nconfig\n[\n'ip'\n]\n)\n.\nto\n \neq\n(\n'192.168.0.0'\n)\n\n\nexpect\n(\nconfig\n[\n'name'\n]\n)\n.\nto\n \neq\n(\n'me'\n)\n\n\nexpect\n(\nconfig\n[\n'network_data'\n]\n)\n.\nto\n \neq\n(\n'bar'\n)\n\n\nexpect\n(\nconfig\n[\n'network_ip'\n]\n)\n.\nto\n \neq\n(\n'192.168.0.0'\n)\n\n\nexpect\n(\nconfig\n[\n'job_name'\n]\n)\n.\nto\n \neq\n(\n'me'\n)\n\n\n\n\n\nThese values could be overriden to test particular behaviour:\n\n\nspec\n \n=\n \nBosh\n::\nTemplate\n::\nTest\n::\nInstanceSpec\n.\nnew\n(\naddress\n:\n \n'cloudfoundry.org'\n,\n \nbootstrap\n:\n \ntrue\n)\n\n\nconfig\n \n=\n \nJSON\n.\nparse\n(\ntemplate\n.\nrender\n({\n...\n},\n \nspec\n:\n \nspec\n))\n\n\nexpect\n(\nconfig\n[\n'address'\n]\n)\n.\nto\n \neq\n(\n'cloudfoundry.org'\n)\n\n\nexpect\n(\nconfig\n[\n'bootstrap'\n]\n)\n.\nto\n \neq\n(\ntrue\n)\n\n\n\n\n\nLinks configuration\n\u00b6\n\n\nBy default no links are provided to the job, but they can be provided via \nlinks:\n key to the \n#render\n function.\n\n\nlinks\n \n=\n \n[\n\n  \nBosh\n::\nTemplate\n::\nTest\n::\nLink\n.\nnew\n(\n\n    \nname\n:\n \n'primary_db'\n,\n\n    \ninstances\n:\n \n[\nBosh\n::\nTemplate\n::\nTest\n::\nLinkInstance\n.\nnew\n(\naddress\n:\n \n'my.database.com'\n)\n]\n,\n\n    \nproperties\n:\n \n{\n\n      \n'adapter'\n \n=>\n \n'sqlite'\n,\n\n      \n'username'\n \n=>\n \n'root'\n,\n\n      \n'password'\n \n=>\n \n'asdf1234'\n,\n\n      \n'port'\n \n=>\n \n4321\n,\n\n      \n'name'\n \n=>\n \n'webserverdb'\n,\n\n    \n}\n\n  \n)\n\n\n]\n\n\nconfig\n \n=\n \nJSON\n.\nparse\n(\ntemplate\n.\nrender\n({\n...\n},\n \nconsumes\n:\n \nlinks\n))\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'host'\n]\n)\n.\nto\n \neq\n(\n'my.database.com'\n)\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'adapter'\n]\n)\n.\nto\n \neq\n(\n'sqlite'\n)\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'username'\n]\n)\n.\nto\n \neq\n(\n'root'\n)\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'password'\n]\n)\n.\nto\n \neq\n(\n'asdf1234'\n)\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'port'\n]\n)\n.\nto\n \neq\n(\n4321\n)\n\n\nexpect\n(\nconfig\n[\n'db'\n][\n'database'\n]\n)\n.\nto\n \neq\n(\n'webserverdb'\n)",
            "title": "Unit Testing Job Templates"
        },
        {
            "location": "/job-templates/#unit-testing-with-bosh-template-gem",
            "text": "bosh-template  Ruby gem could be used for unit testing your job templates. Unit testing of job templates becomes even more important once they contain more complex ERB logic that may perform validation or data transformation.  See example of unit tests in a production release:  https://github.com/cloudfoundry/bosh-dns-aliases-release/blob/master/spec/bosh-dns-aliases_spec.rb .  Assuming we have a job  web-server  with a following  config.json  ERB template:  < %=  port =   p ( \"port\" )  if   port   & lt ;   1024   or   port   & gt ;   4000 \n   raise   \"Ports lower than 1024 or higher than 4000 are not allowed\"  end  JSON . dump ( \"port\"   =>   port )  %>   To start unit testing  web-server  job, add  Gemfile  to the root of your release so that  bundler  gem can install all dependencies necessary for testing:  source   'https://rubygems.org'  group   :development ,   :test   do \n   gem   'bosh-template' \n   gem   'rspec' \n   gem   'rspec-its'  end   Then run  bundle install  from the same directory to download and install specified gems.  Now that necessary dependencies are installed, let's add  spec/jobs/web_server_spec.rb  to test if conditional within our  config.json  template:  require   'rspec'  require   'json'  require   'bosh/template/test'  describe   'web-server job'   do \n   let ( :release )   {   Bosh :: Template :: Test :: ReleaseDir . new ( File . join ( File . dirname ( __FILE__ ),   '../..' ))   } \n   let ( :job )   {   release . job ( 'web-server' )   } \n\n   describe   'config.json'   do \n     let ( :template )   {   job . template ( 'config/config.json' )   } \n\n     it   'raises error if given port is < 1024'   do \n       expect   { \n         template . render ( \"port\"   =>   1023 ) \n       } . to   raise_error   \"Ports lower than 1024 or higher than 4000 are not allowed\" \n     end \n\n     it   'raises error if given port is > 4000'   do \n       expect   { \n         template . render ( \"port\"   =>   4001 ) \n       } . to   raise_error   \"Ports lower than 1024 or higher than 4000 are not allowed\" \n     end \n\n     it   'configures port successfully'   do \n       config   =   JSON . parse ( template . render ( \"port\"   =>   1024 )) \n       expect ( config [ 'port' ] ) . to   eq ( 1024 ) \n\n       config   =   JSON . parse ( template . render ( \"port\"   =>   4000 )) \n       expect ( config [ 'port' ] ) . to   eq ( 4000 ) \n     end \n   end  end   Above set of tests provides enough gurantee that our ERB template is validating and passing down correct configuration to the web server binary.  At this point release directory will look something like this:  web-server-release $ tree .\n.\n\u251c\u2500\u2500 Gemfile\n\u251c\u2500\u2500 Gemfile.lock\n\u251c\u2500\u2500 jobs\n\u2502   \u2514\u2500\u2500 web-server\n\u2502       \u251c\u2500\u2500templates\n\u2502       \u2502  \u2514\u2500\u2500 config.json\n\u2502       \u251c\u2500\u2500 spec\n\u2502       \u2514\u2500\u2500 monit\n\u2514\u2500\u2500 spec\n    \u2514\u2500\u2500 jobs\n        \u2514\u2500\u2500 web_server_spec.rb",
            "title": "Unit Testing with bosh-template gem "
        },
        {
            "location": "/job-templates/#instance-configuration",
            "text": "bosh-template  gem provides default values for  spec.*  ERB accessors:  config   =   JSON . parse ( template . render ({ ... }))  expect ( config [ 'address' ] ) . to   eq ( 'my.bosh.com' )  expect ( config [ 'az' ] ) . to   eq ( 'az1' )  expect ( config [ 'bootstrap' ] ) . to   eq ( false )  expect ( config [ 'deployment' ] ) . to   eq ( 'my-deployment' )  expect ( config [ 'id' ] ) . to   eq ( 'xxxxxx-xxxxxxxx-xxxxx' )  expect ( config [ 'index' ] ) . to   eq ( 0 )  expect ( config [ 'ip' ] ) . to   eq ( '192.168.0.0' )  expect ( config [ 'name' ] ) . to   eq ( 'me' )  expect ( config [ 'network_data' ] ) . to   eq ( 'bar' )  expect ( config [ 'network_ip' ] ) . to   eq ( '192.168.0.0' )  expect ( config [ 'job_name' ] ) . to   eq ( 'me' )   These values could be overriden to test particular behaviour:  spec   =   Bosh :: Template :: Test :: InstanceSpec . new ( address :   'cloudfoundry.org' ,   bootstrap :   true )  config   =   JSON . parse ( template . render ({ ... },   spec :   spec ))  expect ( config [ 'address' ] ) . to   eq ( 'cloudfoundry.org' )  expect ( config [ 'bootstrap' ] ) . to   eq ( true )",
            "title": "Instance configuration"
        },
        {
            "location": "/job-templates/#links-configuration",
            "text": "By default no links are provided to the job, but they can be provided via  links:  key to the  #render  function.  links   =   [ \n   Bosh :: Template :: Test :: Link . new ( \n     name :   'primary_db' , \n     instances :   [ Bosh :: Template :: Test :: LinkInstance . new ( address :   'my.database.com' ) ] , \n     properties :   { \n       'adapter'   =>   'sqlite' , \n       'username'   =>   'root' , \n       'password'   =>   'asdf1234' , \n       'port'   =>   4321 , \n       'name'   =>   'webserverdb' , \n     } \n   )  ]  config   =   JSON . parse ( template . render ({ ... },   consumes :   links ))  expect ( config [ 'db' ][ 'host' ] ) . to   eq ( 'my.database.com' )  expect ( config [ 'db' ][ 'adapter' ] ) . to   eq ( 'sqlite' )  expect ( config [ 'db' ][ 'username' ] ) . to   eq ( 'root' )  expect ( config [ 'db' ][ 'password' ] ) . to   eq ( 'asdf1234' )  expect ( config [ 'db' ][ 'port' ] ) . to   eq ( 4321 )  expect ( config [ 'db' ][ 'database' ] ) . to   eq ( 'webserverdb' )",
            "title": "Links configuration"
        },
        {
            "location": "/scheduled-procs/",
            "text": "Jobs can use cron installed on the stemcell to schedule processes. In a \npre-start script\n, copy a script from your job's \nbin\n directory to one of the following locations:\n\n\n\n\n/etc/cron.hourly\n\n\n/etc/cron.daily\n\n\n/etc/cron.weekly\n\n\n\n\nYou can also create a file in \n/etc/cron.d\n for full control over the cron schedule.",
            "title": "Scheduled Processes"
        },
        {
            "location": "/compiled-releases/",
            "text": "Note: This feature is available with bosh-release v210+.\n\n\n\nNote: CLI v2 is used in the examples.\n\n\n\nTypically release tarballs are distributed with source packages; however, there may be a requirement to use compiled packages in an environment (for example a production environment) where:\n\n\n\n\ncompilation is not permitted for security reasons\n\n\naccess to source packages is not permitted for legal reasons\n\n\nexact existing audited binary assets are expected to be used\n\n\n\n\nAny release can be exported as a compiled release by using the Director and \nbosh export-release\n command.\n\n\n\n\nUsing export-release command \n\u00b6\n\n\nTo export a release:\n\n\n\n\n\n\nCreate an empty deployment (or use an existing one). This deployment will hold compilation VMs if compilation is necessary.\n\n\nname\n:\n \ncompilation-workspace\n\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nuaa\n\n  \nversion\n:\n \n\"45\"\n\n\n\nstemcells\n:\n\n\n-\n \nalias\n:\n \ndefault\n\n  \nos\n:\n \nubuntu-trusty\n\n  \nversion\n:\n \nlatest\n\n\n\ninstance_groups\n:\n \n[]\n\n\n\nupdate\n:\n\n  \ncanaries\n:\n \n1\n\n  \nmax_in_flight\n:\n \n1\n\n  \ncanary_watch_time\n:\n \n1000-90000\n\n  \nupdate_watch_time\n:\n \n1000-90000\n\n\n\n\n\nNote: This example assumes you are using \ncloud config\n, hence no compilation, networks and other sections were defined. If you are not using cloud config you will have to define them.\n\n\n\n\n\n\nReference desired release versions you want to export.\n\n\n\n\n\n\nDeploy. Example manifest above does not allocate any resources when deployed.\n\n\n\n\n\n\nRun \nbosh export-release\n command. In our example: \nbosh export-release uaa/45 ubuntu-trusty/3197\n. If release is not already compiled it will create necessary compilation VMs and compile all packages.\n\n\n\n\n\n\nFind exported release tarball in the current directory. Compiled release tarball can be now imported into any other Director via \nbosh upload-release\n command.\n\n\n\n\n\n\nOptionally use \nbosh inspect-release\n command to view associated compiled packages on the Director. In our example: \nbosh inspect-release uaa/45\n.\n\n\n\n\n\n\n\n\nFloating stemcells \n\u00b6\n\n\nCompiled releases are built against a particular stemcell version. Director allows compiled releases to be installed on any minor version of the major stemcell version that the compiled release was exported against. \nbosh create-env\n command requires exact stemcell match unlike the Director.\n\n\nFor example UAA release 27 compiled against stemcell version 3233.10 will work on any 3233 stemcell, but the Director will refuse to install it on 3234.",
            "title": "Creating Compiled Releases"
        },
        {
            "location": "/compiled-releases/#using-export-release-command",
            "text": "To export a release:    Create an empty deployment (or use an existing one). This deployment will hold compilation VMs if compilation is necessary.  name :   compilation-workspace  releases :  -   name :   uaa \n   version :   \"45\"  stemcells :  -   alias :   default \n   os :   ubuntu-trusty \n   version :   latest  instance_groups :   []  update : \n   canaries :   1 \n   max_in_flight :   1 \n   canary_watch_time :   1000-90000 \n   update_watch_time :   1000-90000   Note: This example assumes you are using  cloud config , hence no compilation, networks and other sections were defined. If you are not using cloud config you will have to define them.    Reference desired release versions you want to export.    Deploy. Example manifest above does not allocate any resources when deployed.    Run  bosh export-release  command. In our example:  bosh export-release uaa/45 ubuntu-trusty/3197 . If release is not already compiled it will create necessary compilation VMs and compile all packages.    Find exported release tarball in the current directory. Compiled release tarball can be now imported into any other Director via  bosh upload-release  command.    Optionally use  bosh inspect-release  command to view associated compiled packages on the Director. In our example:  bosh inspect-release uaa/45 .",
            "title": "Using export-release command "
        },
        {
            "location": "/compiled-releases/#floating-stemcells",
            "text": "Compiled releases are built against a particular stemcell version. Director allows compiled releases to be installed on any minor version of the major stemcell version that the compiled release was exported against.  bosh create-env  command requires exact stemcell match unlike the Director.  For example UAA release 27 compiled against stemcell version 3233.10 will work on any 3233 stemcell, but the Director will refuse to install it on 3234.",
            "title": "Floating stemcells "
        },
        {
            "location": "/errands/",
            "text": "(See \nJobs\n for an introduction to jobs.)\n\n\nAny job that includes \nbin/run\n script in its spec file's templates section is considered to be an errand. Operator can trigger execution of an errand at any time after the deploy and receive back script's stdout, stderr and exit code upon its completion.\n\n\n\n\nRelease Definition \n\u00b6\n\n\nExample of an errand job \nsmoke-tests\n from Zookeeper release. \nbin/run\n script is specified in its templates section:\n\n\n---\n\n\nname\n:\n \nsmoke-tests\n\n\n\ntemplates\n:\n\n  \nrun.sh\n:\n \nbin/run\n\n\n\nconsumes\n:\n\n\n-\n \nname\n:\n \nconn\n\n  \ntype\n:\n \nzookeeper\n\n  \nproperties\n:\n\n  \n-\n \nclient_port\n\n\n\npackages\n:\n\n\n-\n \nsmoke-tests\n\n\n\nproperties\n:\n \n{}\n\n\n\n\n\nwith a \nrun.sh\n template:\n\n\n#!/bin/bash\n\n\nset\n -e\n<% \nconn\n \n=\n link\n(\n'conn'\n)\n %>\n\nexport\n \nZOOKEEPER_SERVERS\n=\n<%\n=\n conn.instances.map \n{\n \n|\ni\n|\n \n\"#{i.address}:#{conn.p('client_port')}\"\n \n}\n.join\n(\n\",\"\n)\n %>\n/var/vcap/packages/smoke-tests/bin/tests\n\n\n\n\n\n\nInclude in a Deployment \n\u00b6\n\n\nThere are two ways to add an errand to a deployment:\n\n\n\n\nadd it to a dedicated instance group\n\n\nadd it to an existing instance group (colocated) (available in bosh-release v263+)\n\n\n\n\nIn some cases it makes sense to place an errand in a dedicated instance group. You can add an instance group that specifies only an errand job in its jobs section:\n\n\n-\n \nname\n:\n \nsmoke-tests\n\n  \nazs\n:\n \n[\nz1\n]\n\n  \nlifecycle\n:\n \nerrand\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nsmoke-tests\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\nNote that above example uses \nlifecycle\n:\n \nerrand\n configuration to specify that \nsmoke-tests\n instances should only be present when the \nsmoke-tests\n errand is running. Cloud compute resources will be allocated right before errand is running and released when errand is finished.\n\n\nAlternatively, it might make sense to colocate an errand job with other jobs in an existing instance group. This might be useful if an errand is meant to perform work local to an instance or simply to avoid adding additional resources to your deployment:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n,\n \nz3\n]\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nzookeeper\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \n-\n \nname\n:\n \nstatus\n\n    \nrelease\n:\n \nzookeeper\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \ndefault\n\n  \npersistent_disk\n:\n \n10240\n\n  \nstemcell\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nExecution \n\u00b6\n\n\nUnlike regular jobs which run continiously and get automatically restarted on failure, errand jobs are executed upon operator's request some time after a deploy and if fail do not get restarted. There is no timeout on how long an errand can execute.\n\n\nNote that currently Director will acquire deployment lock for chosen deployment which will prevent execution of other commands that also require deployment lock (for example \nbosh deploy\n or another errand execution). This behaviour will be made more granular over time allowing more commands to run in parallel against a single deployment.\n\n\nAfter running \nbosh deploy\n command\n to update your deployment, you can inspect which errands are available within a deployment via \nbosh errands\n command\n:\n\n\n$ bosh -e vbox -d zookeeper errands\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nUsing deployment \n'zookeeper'\n\n\nName\nsmoke-tests\nstatus\n\n\n2\n errands\n\nSucceeded\n\n\n\n\nTo execute an errand, use \nbosh run-errand\n command\n.\n\n\n$ bosh -e vbox -d zookeeper run-errand status\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nUsing deployment \n'zookeeper'\n\n\nTask \n5609\n\n\nTask \n5609\n \n|\n \n01\n:31:57 \n|\n Preparing deployment: Preparing deployment \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n\nTask \n5609\n \n|\n \n01\n:31:58 \n|\n Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:31:59 \n|\n Fetching logs \nfor\n zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d \n(\n3\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n \n(\n00\n:00:02\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n \n(\n00\n:00:02\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8 \n(\n0\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd \n(\n2\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:00 \n|\n Fetching logs \nfor\n zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n: Finding and packing log files\nTask \n5609\n \n|\n \n01\n:32:01 \n|\n Fetching logs \nfor\n zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n(\n1\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\nTask \n5609\n \n|\n \n01\n:32:01 \n|\n Fetching logs \nfor\n zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 \n(\n4\n)\n: Finding and packing log files \n(\n00\n:00:01\n)\n\n\nTask \n5609\n Started  Mon Sep \n18\n \n01\n:31:57 UTC \n2017\n\nTask \n5609\n Finished Mon Sep \n18\n \n01\n:32:01 UTC \n2017\n\nTask \n5609\n Duration \n00\n:00:04\nTask \n5609\n \ndone\n\n\nInstance   zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d\nExit Code  \n0\n\nStdout     Mode: leader\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8\nExit Code  \n0\n\nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\n\n5\n errand\n(\ns\n)\n\n\nSucceeded\n\n\n\n\nIf an errand job is colocated on multiple instances (over one or more instance groups), by default \nbosh run-errand\n command will execute them all in parallel. You can limit number of instances used for execution via \n--instance\n flag:\n\n\n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\n\n\n\n\nSee \nbosh run-errand\n command\n description for additional ways to use \n--instance\n flag. One of those way is to use \n--instance group/first\n (where \nfirst\n is a literal value) so that errand only runs on one of the instances.\n\n\n\n\nPrevious: \nJobs",
            "title": "Using Errands"
        },
        {
            "location": "/errands/#release-definition",
            "text": "Example of an errand job  smoke-tests  from Zookeeper release.  bin/run  script is specified in its templates section:  ---  name :   smoke-tests  templates : \n   run.sh :   bin/run  consumes :  -   name :   conn \n   type :   zookeeper \n   properties : \n   -   client_port  packages :  -   smoke-tests  properties :   {}   with a  run.sh  template:  #!/bin/bash  set  -e\n<%  conn   =  link ( 'conn' )  %> export   ZOOKEEPER_SERVERS = <% =  conn.instances.map  {   | i |   \"#{i.address}:#{conn.p('client_port')}\"   } .join ( \",\" )  %>\n/var/vcap/packages/smoke-tests/bin/tests",
            "title": "Release Definition "
        },
        {
            "location": "/errands/#include-in-a-deployment",
            "text": "There are two ways to add an errand to a deployment:   add it to a dedicated instance group  add it to an existing instance group (colocated) (available in bosh-release v263+)   In some cases it makes sense to place an errand in a dedicated instance group. You can add an instance group that specifies only an errand job in its jobs section:  -   name :   smoke-tests \n   azs :   [ z1 ] \n   lifecycle :   errand \n   instances :   1 \n   jobs : \n   -   name :   smoke-tests \n     release :   zookeeper \n     properties :   {} \n   vm_type :   default \n   stemcell :   default \n   networks : \n   -   name :   default   Note that above example uses  lifecycle :   errand  configuration to specify that  smoke-tests  instances should only be present when the  smoke-tests  errand is running. Cloud compute resources will be allocated right before errand is running and released when errand is finished.  Alternatively, it might make sense to colocate an errand job with other jobs in an existing instance group. This might be useful if an errand is meant to perform work local to an instance or simply to avoid adding additional resources to your deployment:  -   name :   zookeeper \n   azs :   [ z1 ,   z2 ,   z3 ] \n   instances :   1 \n   jobs : \n   -   name :   zookeeper \n     release :   zookeeper \n     properties :   {} \n   -   name :   status \n     release :   zookeeper \n     properties :   {} \n   vm_type :   default \n   persistent_disk :   10240 \n   stemcell :   default \n   networks : \n   -   name :   default",
            "title": "Include in a Deployment "
        },
        {
            "location": "/errands/#execution",
            "text": "Unlike regular jobs which run continiously and get automatically restarted on failure, errand jobs are executed upon operator's request some time after a deploy and if fail do not get restarted. There is no timeout on how long an errand can execute.  Note that currently Director will acquire deployment lock for chosen deployment which will prevent execution of other commands that also require deployment lock (for example  bosh deploy  or another errand execution). This behaviour will be made more granular over time allowing more commands to run in parallel against a single deployment.  After running  bosh deploy  command  to update your deployment, you can inspect which errands are available within a deployment via  bosh errands  command :  $ bosh -e vbox -d zookeeper errands\nUsing environment  '192.168.56.6'  as client  'admin' \n\nUsing deployment  'zookeeper' \n\nName\nsmoke-tests\nstatus 2  errands\n\nSucceeded  To execute an errand, use  bosh run-errand  command .  $ bosh -e vbox -d zookeeper run-errand status\nUsing environment  '192.168.56.6'  as client  'admin' \n\nUsing deployment  'zookeeper' \n\nTask  5609 \n\nTask  5609   |   01 :31:57  |  Preparing deployment: Preparing deployment  ( 00 :00:01 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) \nTask  5609   |   01 :31:58  |  Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) \nTask  5609   |   01 :31:59  |  Running errand: zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) : Finding and packing log files\nTask  5609   |   01 :31:59  |  Running errand: zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) : Finding and packing log files\nTask  5609   |   01 :31:59  |  Running errand: zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 )   ( 00 :00:01 ) \nTask  5609   |   01 :31:59  |  Fetching logs  for  zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) : Finding and packing log files\nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d  ( 3 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Running errand: zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 )   ( 00 :00:02 ) \nTask  5609   |   01 :32:00  |  Running errand: zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 )   ( 00 :00:02 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8  ( 0 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd  ( 2 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) : Finding and packing log files\nTask  5609   |   01 :32:00  |  Fetching logs  for  zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) : Finding and packing log files\nTask  5609   |   01 :32:01  |  Fetching logs  for  zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  ( 1 ) : Finding and packing log files  ( 00 :00:01 ) \nTask  5609   |   01 :32:01  |  Fetching logs  for  zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  ( 4 ) : Finding and packing log files  ( 00 :00:01 ) \n\nTask  5609  Started  Mon Sep  18   01 :31:57 UTC  2017 \nTask  5609  Finished Mon Sep  18   01 :32:01 UTC  2017 \nTask  5609  Duration  00 :00:04\nTask  5609   done \n\nInstance   zookeeper/0015b995-5ec3-4519-8c11-6521b0aa079d\nExit Code   0 \nStdout     Mode: leader\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/d9e00366-8ab1-4ea2-bae3-14cd6bf562cd\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg\n\nInstance   zookeeper/e31944b9-8bf5-4a42-8d6b-3402a85d24d8\nExit Code   0 \nStdout     Mode: follower\nStderr     ZooKeeper JMX enabled by default\n           Using config: /var/vcap/jobs/zookeeper/config/zoo.cfg 5  errand ( s ) \n\nSucceeded  If an errand job is colocated on multiple instances (over one or more instance groups), by default  bosh run-errand  command will execute them all in parallel. You can limit number of instances used for execution via  --instance  flag:  $ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5  See  bosh run-errand  command  description for additional ways to use  --instance  flag. One of those way is to use  --instance group/first  (where  first  is a literal value) so that errand only runs on one of the instances.   Previous:  Jobs",
            "title": "Execution "
        },
        {
            "location": "/props-common/",
            "text": "TLS configuration \n\u00b6\n\n\nFollowing is a \nsuggested\n set of properties for TLS configuration:\n\n\n\n\ntls\n [Hash]: TLS configuration section.\n\n\nenabled\n [Boolean, optional]: Enable/disable TLS. Default should be \ntrue\n.\n\n\ncert\n [Hash]: Value described by \nceritificate\n variable type\n. Default is \nnil\n.\n\n\nprotocols\n [String, optional]: Space separated list of protocols to support. Example: \nTLSv1.2\n.\n\n\nciphers\n [String, optional]: OpenSSL formatted list of ciphers to support. Example: \n!DES:!RC4:!3DES:!MD5:!PSK\n.\n\n\n\n\n\n\n\n\nExample job spec:\n\n\nname\n:\n \napp-server\n\n\n\nproperties\n:\n\n  \ntls.enabled\n:\n\n    \ndescription\n:\n \n\"Enable/disable\n \nTLS\"\n\n    \ndefault\n:\n \ntrue\n\n  \ntls.cert\n:\n\n    \ntype\n:\n \ncertificate\n\n    \ndescription\n:\n \n\"Specify\n \ncertificate\"\n\n  \n...\n\n\n\n\n\nExample manifest usage:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp-server\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \napp-server\n\n    \nproperties\n:\n\n      \ntls\n:\n\n        \ncert\n:\n \n((app-server-tls))\n\n  \n...\n\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \napp-server-tls\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \n...\n\n\n\n\n\nNote that if your job requires multiple TLS configurations (for example, for a client and server TLS configurations), configuration above would be nested under particular context. For example:\n\n\nname\n:\n \napp-server\n\n\n\nproperties\n:\n\n  \nserver.tls.enabled\n:\n\n    \ndescription\n:\n \n\"Enable/disable\n \nTLS\"\n\n    \ndefault\n:\n \ntrue\n\n  \nserver.tls.cert\n:\n\n    \ntype\n:\n \ncertificate\n\n    \ndescription\n:\n \n\"Specify\n \nserver\n \ncertificate\"\n\n\n  \nclient.tls.enabled\n:\n\n    \ndescription\n:\n \n\"Enable/disable\n \nTLS\"\n\n    \ndefault\n:\n \ntrue\n\n  \nclient.tls.cert\n:\n\n    \ntype\n:\n \ncertificate\n\n    \ndescription\n:\n \n\"Specify\n \nclient\n \ncertificate\"\n\n  \n...\n\n\n\n\n\nExample manifest usage:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp-server\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \napp-server\n\n    \nproperties\n:\n\n      \nserver\n:\n\n        \ntls\n:\n\n          \ncert\n:\n \n((app-server-tls))\n\n      \nclient\n:\n\n        \ntls\n:\n\n          \ncert\n:\n \n((app-client-tls))\n\n  \n...\n\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \napp-server-tls\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \n...\n\n\n-\n \nname\n:\n \napp-client-tls\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \n...\n\n\n\n\n\n\n\nEnvironment proxy configuration \n\u00b6\n\n\nFollowing is a \nsuggested\n set of properties for environment proxy configuration:\n\n\n\n\nenv\n [Hash]\n\n\nhttp_proxy\n [String, optinal]: HTTP proxy that software should use. Default: not specified.\n\n\nhttps_proxy\n [String, optinal]: HTTPS proxy that software should use. Default: not specified.\n\n\nno_proxy\n [String, optinal]: List of comma-separated hosts that should skip connecting to the proxy in software. Default: not specified.\n\n\n\n\n\n\n\n\nExample job spec:\n\n\nname\n:\n \napp-server\n\n\n\nproperties\n:\n\n  \nenv.http_proxy\n:\n\n    \ndescription\n:\n \nHTTP proxy that the server should use\n\n  \nenv.https_proxy\n:\n\n    \ndescription\n:\n \nHTTPS proxy that the server should use\n\n  \nenv.no_proxy\n:\n\n    \ndescription\n:\n \nList of comma-separated hosts that should skip connecting to the proxy in the server\n\n  \n...\n\n\n\n\n\nExample manifest usage:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \napp-server\n\n  \ninstances\n:\n \n2\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \napp-server\n\n    \nproperties\n:\n\n      \nenv\n:\n\n        \nhttp_proxy\n:\n \nhttp://10.203.0.1:5187/\n\n        \nhttps_proxy\n:\n \nhttp://10.203.0.1:5187/\n\n        \nno_proxy\n:\n \nlocalhost,127.0.0.1\n\n  \n...",
            "title": "Recommended Property Types"
        },
        {
            "location": "/props-common/#tls-configuration",
            "text": "Following is a  suggested  set of properties for TLS configuration:   tls  [Hash]: TLS configuration section.  enabled  [Boolean, optional]: Enable/disable TLS. Default should be  true .  cert  [Hash]: Value described by  ceritificate  variable type . Default is  nil .  protocols  [String, optional]: Space separated list of protocols to support. Example:  TLSv1.2 .  ciphers  [String, optional]: OpenSSL formatted list of ciphers to support. Example:  !DES:!RC4:!3DES:!MD5:!PSK .     Example job spec:  name :   app-server  properties : \n   tls.enabled : \n     description :   \"Enable/disable   TLS\" \n     default :   true \n   tls.cert : \n     type :   certificate \n     description :   \"Specify   certificate\" \n   ...   Example manifest usage:  instance_groups :  -   name :   app-server \n   instances :   2 \n   jobs : \n   -   name :   app-server \n     properties : \n       tls : \n         cert :   ((app-server-tls)) \n   ...  variables :  -   name :   app-server-tls \n   type :   certificate \n   options : \n     ...   Note that if your job requires multiple TLS configurations (for example, for a client and server TLS configurations), configuration above would be nested under particular context. For example:  name :   app-server  properties : \n   server.tls.enabled : \n     description :   \"Enable/disable   TLS\" \n     default :   true \n   server.tls.cert : \n     type :   certificate \n     description :   \"Specify   server   certificate\" \n\n   client.tls.enabled : \n     description :   \"Enable/disable   TLS\" \n     default :   true \n   client.tls.cert : \n     type :   certificate \n     description :   \"Specify   client   certificate\" \n   ...   Example manifest usage:  instance_groups :  -   name :   app-server \n   instances :   2 \n   jobs : \n   -   name :   app-server \n     properties : \n       server : \n         tls : \n           cert :   ((app-server-tls)) \n       client : \n         tls : \n           cert :   ((app-client-tls)) \n   ...  variables :  -   name :   app-server-tls \n   type :   certificate \n   options : \n     ...  -   name :   app-client-tls \n   type :   certificate \n   options : \n     ...",
            "title": "TLS configuration "
        },
        {
            "location": "/props-common/#environment-proxy-configuration",
            "text": "Following is a  suggested  set of properties for environment proxy configuration:   env  [Hash]  http_proxy  [String, optinal]: HTTP proxy that software should use. Default: not specified.  https_proxy  [String, optinal]: HTTPS proxy that software should use. Default: not specified.  no_proxy  [String, optinal]: List of comma-separated hosts that should skip connecting to the proxy in software. Default: not specified.     Example job spec:  name :   app-server  properties : \n   env.http_proxy : \n     description :   HTTP proxy that the server should use \n   env.https_proxy : \n     description :   HTTPS proxy that the server should use \n   env.no_proxy : \n     description :   List of comma-separated hosts that should skip connecting to the proxy in the server \n   ...   Example manifest usage:  instance_groups :  -   name :   app-server \n   instances :   2 \n   jobs : \n   -   name :   app-server \n     properties : \n       env : \n         http_proxy :   http://10.203.0.1:5187/ \n         https_proxy :   http://10.203.0.1:5187/ \n         no_proxy :   localhost,127.0.0.1 \n   ...",
            "title": "Environment proxy configuration "
        },
        {
            "location": "/packages/",
            "text": "A package is a component of a BOSH release that contains a packaging \nspec\n file and a packaging script.\nEach package also references source code or pre-compiled software that you store in the \nsrc\n directory of a BOSH \nrelease\n\ndirectory\n.\n\n\nYou build BOSH packages in a BOSH release directory. Your release might contain one or more packages.\nThis topic describes how to create a BOSH package that includes either source code or pre-compiled software.\n\n\nThe information and procedures in this topic form \nStep 3: Create Package Skeletons\n of the Creating a Release topic. Refer to that topic to understand where BOSH packaging fits in the context of creating a BOSH release.\n\n\nPrerequisite\n\u00b6\n\n\nCreate a release directory. Refer to the \nCreate a Release Directory\n section in the Creating\na BOSH Release topic.\n\n\nEdit a Package Spec\n\u00b6\n\n\nYou specify package contents in the package \nspec\n file. BOSH automatically creates this file as a template with the following\nsections when you run the \nbosh generate package PACKAGE_NAME\n command:\n\n\n\n\nname\n: Defines the package name.\n\n\ndependencies\n: \n(Optional)\n Defines a list of other packages that this package depends on.\n\n\nfiles\n: Defines a list of files that this package contains. You can define this list explicitly or through pattern-matching.  \n\n\n\n\nTo edit a package spec file:\n\n\n\n\nIdentify all compile-time dependencies.\n    A compile-time dependency occurs when a package depends on another package.\n    For more information, refer to the \nMake  Dependency Graphs\n section of the Creating a BOSH\nRelease topic.\n\n\nRun \nbosh generate package PACKAGE_NAME\n for each compile-time dependency.\n\n\n\n\nCopy all files that the package requires to the \nsrc\n directory of the BOSH release directory.\n\n\nTypically, these files are source code. If you are including pre-compiled software, copy a compressed file that contains the\npre-compiled binary.\n\n\n\n\n\n\nEdit each package spec file as follows:\n\n\n\n\nAdd the names of the files for that package.\n\n\nAdd the names of any compile-time dependencies to each package spec file. Use \n[]\n to indicate an empty array if a package\nhas no compile-time dependencies.\n\n\n\n\nThe example shows an edited Ruby spec file with dependencies and file names.\nRuby 1.9.3 has a compile-time dependency on libyaml_0.1.4, and the ruby_1.9.3 source code consists of three files.\n\n\nExample Ruby package spec file:\n\n\n\n\n\n\nname\n:\n \nruby_1.9.3\n\n\n\ndependencies\n:\n\n\n-\n \nlibyaml_0.1.4\n\n\n\nfiles\n:\n\n\n-\n \nruby_1.9.3/ruby-1.9.3-p484.tar.gz\n\n\n-\n \nruby_1.9.3/rubygems-1.8.24.tgz\n\n\n-\n \nruby_1.9.3/bundler-1.2.1.gem\n\n\n\n\n\nCreate a Packaging Script\n\u00b6\n\n\nBOSH automatically creates a packaging script file template when you run the \nbosh generate package PACKAGE_NAME\n command. Each\npackaging script in a package must include a symlink in the format \n/var/vcap/packages/<package name>\n for each dependency and\ndeliver all compiled code to \nBOSH_INSTALL_TARGET\n. Store the script in the \npackages/<package name>/packaging\n directory.\n\n\nNote\n: If your package contains source code, the script must compile the code and deliver it to\n\nBOSH_INSTALL_TARGET\n. If your package contains pre-compiled software, the script must extract the binary from the compressed file and copy it to \nBOSH_INSTALL_TARGET\n.\n\n\nNote\n: If your package contains pre-compiled software, record the operating system that the pre-compiled software requires. Because a pre-compiled binary runs only on a specific operating system, any deployment using a package containing pre-compiled software requires a stemcell that contains that operating system.\n\n\nExample Ruby packaging script:\n\n\nset\n -e -x\n\ntar xzf ruby_1.9.3/ruby-1.9.3-p484.tar.gz\n\npushd\n ruby-1.9.3-p484\n  ./configure \n\\\n\n   --prefix\n=\n${\nBOSH_INSTALL_TARGET\n}\n \n\\\n\n   --disable-install-doc \n\\\n\n   --with-opt-dir\n=\n/var/vcap/packages/libyaml_0.1.4\n\n   make\n   make install\n\npopd\n\n\ntar zxvf ruby_1.9.3/rubygems-1.8.24.tgz\n\npushd\n rubygems-1.8.24\n  \n${\nBOSH_INSTALL_TARGET\n}\n/bin/ruby setup.rb\n\npopd\n\n\n\n${\nBOSH_INSTALL_TARGET\n}\n/bin/gem install ruby_1.9.3/bundler-1.2.1.gem --no-ri --no-rdoc\n\n\n\n\nExample script referencing pre-compiled code:\n\n\ntar zxf myfile.tar.gz\ncp -a myfile \n${\nBOSH_INSTALL_TARGET\n}",
            "title": "Creating Packages"
        },
        {
            "location": "/package-vendoring/",
            "text": "Note: Requires CLI v2.0.36+.\n\n\n\nWhile authoring a release, it is usually necessary to include some commonly used packages:\n\n\n\n\nlanguages and/or runtimes such as Ruby, Java JRE or Go\n\n\nCLIs such as CF and/or BOSH CLI\n\n\nsupporting packages for compilation such as Git\n\n\n\n\nIt's a recommended practice to make releases be self contained; however, that may force each release author to figure out how to make such common packages on their own. There are several solutions that solve this problem in their distinct way:\n\n\n\n\nuse \nbosh vendor-package\n command to copy over existing package\n\n\nuse job colocation (with necessary package dependencies)\n\n\ncopy over manually package source from a different release\n\n\n\n\nSections below describe steps, advantages and disadvantages for each approach.\n\n\nUsing \nbosh vendor-package\n \n\u00b6\n\n\nCLI v2 introduces new command for release authors to easily vendor final version of a package from another release. BOSH team has also created \nbosh-packages\n Github organization\n for tracking official commonly used packages. First additions to that organization are: \ngolang-release\n, \nruby-release\n, \njava-release\n and \nnginx-release\n. More may be added if deemed to be useful to a number of release authors.\n\n\nAs an example, if release encapsulates a Go application that needs to be compiled with Go compiler (as most Go apps do), release author, instead of figuring out how to make a \ngolang-1.x\n package on their own, can vendor in one from \nhttps://github.com/bosh-packages/golang-release\n. \ngolang-release\n currently contains packages for Golang versions 1.8 and 1.9, and is updated as new minor versions come out.\n\n\nSuch workflow may look like this:\n\n\n# Clone golang-release to your system\n\n$ git clone https://github.com/bosh-packages/golang-release ~/workspace/golang-release\n\n$ \ncd\n ~/workspace/my-app-release\n$ bosh generate-package my-app\n\n\n# Make sure final blobstore credentials are available\n\n$ vim config/private.yml\n\n\n# Perform vendoring of golang-1.8-linux package\n\n$ bosh vendor-package golang-1.8-linux ~/workspace/golang-release\n\n\n\n\nIn the above steps, CLI v2 vendors \ngolang-1.8-linux\n package into your \nmy-app-release\n release, and makes it available just like any other package as a dependency to other packages or jobs:\n\n\nname\n:\n \nmy-app\n\n\n\ndependencies\n:\n\n\n-\n \ngolang-1.8-linux\n\n\n\nfiles\n:\n\n\n-\n \n\"**/*.go\"\n\n\n-\n \n\"**/*.s\"\n\n\n\n\n\nAs a general convention, packages that need non-trivial configuration (via environment variables, or in some other ways) should include \nbosh/compile.env\n file that can be sourced by consumers to make use of that package much easier. Here is how \nmy-app\n package's packaging may look like:\n\n\nset\n -e -x\n\nsource\n /var/vcap/packages/golang-1.8-linux/bosh/compile.env\n\nmkdir ../src \n&&\n cp -a * ../src/ \n&&\n mv ../src ./src\nmkdir \n$BOSH_INSTALL_TARGET\n/bin\n\ngo build -o \n$BOSH_INSTALL_TARGET\n/bin/app src/github.com/company/my-app/main/*.go\n\n\n\n\nIn the above BASH script, \nsource /var/vcap/packages/golang-1.8-linux/bosh/compile.env\n makes available several Go specific environment variables (\nGOPATH\n and \nGOROOT\n) and adds \ngo\n binary to the \nPATH\n so that executing \ngo build\n just works.\n\n\nPackages may also include \nbosh/runtime.env\n for loading specific functionality at job runtime instead of during package compilation.\n\n\nAdditional notes about \nvendor-package\n command:\n\n\n\n\ncommand is idempotent hence could be run in the CI continuously tracking source release and automatically vendoring in updates\n\n\ncommand requires access to final blobstore (specified via \nconfig/private.yml\n) as it will download source release package blob and upload it into destination release's blobstore\n\n\ndependencies of a vendored packaged are vendored as well\n\n\nafter running the command, the \npackages\n directory will contain a directory named after the vendored package. That directory will have a \nspec.lock\n file which references the name and fingerprint of the vendored package. \nspec.lock\n and updates to the \n.final_builds\n directory must be saved (checked in).\n\n\n\n\nWhen to use this approach:\n\n\n\n\npackage is readily available from \nbosh-packages\n Github organization\n\n\npackage is an internal implementation detail of your release that cannot or should not be swappable by an operator\n\n\n\n\nWhen to be cautious with this approach:\n\n\n\n\nsource release is not explicitly stating that included packages are meant to be vendored\n\n\npackage's purpose or implementation is extremely specific to the source release\n\n\n\n\n\n\nUsing job colocation \n\u00b6\n\n\nJob colocation can provide a powerful way to make a release extensible and pluggable where necessary. Unlike vendoring approach, release author choosing job colocation as a way to consume dependent software is explicitly stating that there is not necessarily a single one implementation of a particular dependency but rather it could be chosen by on operator at the time of a deploy.\n\n\nHere are two examples:\n\n\n\n\n\n\nBOSH CPIs shipped as separate releases and colocated with a Director since Director has a very clear and stable API contract with CPIs\n\n\n\n\n\n\nBPM release\n making \n/var/vcap/jobs/bpm/bin/bpm\n available to all other releases so that operator can keep BPM release up to date without relying on individual release authors for an update\n\n\n\n\n\n\nWhen to use this approach:\n\n\n\n\npackage does not provide the only way to provide functionality\n\n\nrelease author needs to decouple particular package release cycle from the entire release cycle\n\n\n\n\nWhen to be cautious with this approach:\n\n\n\n\nif operators will incur unnecessary burden during a deploy\n\n\n\n\n\n\nCopying over package source \n\u00b6\n\n\nLastly, sometimes it may be necessary to actually copy over (\ncp\n) software bits from one release to another.\n\n\nTypically this approach means that release author have to keep very close eye on the upstream software include in this package, hence, it may require more effort to stay up to date.\n\n\nWhen to use this approach:\n\n\n\n\npackage is very similar but additional functionality must be added within that package",
            "title": "Reusing Packages"
        },
        {
            "location": "/package-vendoring/#using-bosh-vendor-package",
            "text": "CLI v2 introduces new command for release authors to easily vendor final version of a package from another release. BOSH team has also created  bosh-packages  Github organization  for tracking official commonly used packages. First additions to that organization are:  golang-release ,  ruby-release ,  java-release  and  nginx-release . More may be added if deemed to be useful to a number of release authors.  As an example, if release encapsulates a Go application that needs to be compiled with Go compiler (as most Go apps do), release author, instead of figuring out how to make a  golang-1.x  package on their own, can vendor in one from  https://github.com/bosh-packages/golang-release .  golang-release  currently contains packages for Golang versions 1.8 and 1.9, and is updated as new minor versions come out.  Such workflow may look like this:  # Clone golang-release to your system \n$ git clone https://github.com/bosh-packages/golang-release ~/workspace/golang-release\n\n$  cd  ~/workspace/my-app-release\n$ bosh generate-package my-app # Make sure final blobstore credentials are available \n$ vim config/private.yml # Perform vendoring of golang-1.8-linux package \n$ bosh vendor-package golang-1.8-linux ~/workspace/golang-release  In the above steps, CLI v2 vendors  golang-1.8-linux  package into your  my-app-release  release, and makes it available just like any other package as a dependency to other packages or jobs:  name :   my-app  dependencies :  -   golang-1.8-linux  files :  -   \"**/*.go\"  -   \"**/*.s\"   As a general convention, packages that need non-trivial configuration (via environment variables, or in some other ways) should include  bosh/compile.env  file that can be sourced by consumers to make use of that package much easier. Here is how  my-app  package's packaging may look like:  set  -e -x source  /var/vcap/packages/golang-1.8-linux/bosh/compile.env\n\nmkdir ../src  &&  cp -a * ../src/  &&  mv ../src ./src\nmkdir  $BOSH_INSTALL_TARGET /bin\n\ngo build -o  $BOSH_INSTALL_TARGET /bin/app src/github.com/company/my-app/main/*.go  In the above BASH script,  source /var/vcap/packages/golang-1.8-linux/bosh/compile.env  makes available several Go specific environment variables ( GOPATH  and  GOROOT ) and adds  go  binary to the  PATH  so that executing  go build  just works.  Packages may also include  bosh/runtime.env  for loading specific functionality at job runtime instead of during package compilation.  Additional notes about  vendor-package  command:   command is idempotent hence could be run in the CI continuously tracking source release and automatically vendoring in updates  command requires access to final blobstore (specified via  config/private.yml ) as it will download source release package blob and upload it into destination release's blobstore  dependencies of a vendored packaged are vendored as well  after running the command, the  packages  directory will contain a directory named after the vendored package. That directory will have a  spec.lock  file which references the name and fingerprint of the vendored package.  spec.lock  and updates to the  .final_builds  directory must be saved (checked in).   When to use this approach:   package is readily available from  bosh-packages  Github organization  package is an internal implementation detail of your release that cannot or should not be swappable by an operator   When to be cautious with this approach:   source release is not explicitly stating that included packages are meant to be vendored  package's purpose or implementation is extremely specific to the source release",
            "title": "Using bosh vendor-package "
        },
        {
            "location": "/package-vendoring/#using-job-colocation",
            "text": "Job colocation can provide a powerful way to make a release extensible and pluggable where necessary. Unlike vendoring approach, release author choosing job colocation as a way to consume dependent software is explicitly stating that there is not necessarily a single one implementation of a particular dependency but rather it could be chosen by on operator at the time of a deploy.  Here are two examples:    BOSH CPIs shipped as separate releases and colocated with a Director since Director has a very clear and stable API contract with CPIs    BPM release  making  /var/vcap/jobs/bpm/bin/bpm  available to all other releases so that operator can keep BPM release up to date without relying on individual release authors for an update    When to use this approach:   package does not provide the only way to provide functionality  release author needs to decouple particular package release cycle from the entire release cycle   When to be cautious with this approach:   if operators will incur unnecessary burden during a deploy",
            "title": "Using job colocation "
        },
        {
            "location": "/package-vendoring/#copying-over-package-source",
            "text": "Lastly, sometimes it may be necessary to actually copy over ( cp ) software bits from one release to another.  Typically this approach means that release author have to keep very close eye on the upstream software include in this package, hence, it may require more effort to stay up to date.  When to use this approach:   package is very similar but additional functionality must be added within that package",
            "title": "Copying over package source "
        },
        {
            "location": "/windows/",
            "text": "BOSH can deploy jobs on Windows VMs. There is open source tooling and documentation available to build \nAWS\n, \nAzure\n,\n\nvSphere\n and \nOpenstack\n stemcells for Windows.\n\n\nIn general Windows BOSH Releases work in the same way as a standard BOSH release. The main difference is that the \nmonit file\n for Linux Releases is structured differently on Windows. Below are specific concerns for jobs on Windows.\n\n\n\n\nReleases \n\u00b6\n\n\nThe structure of a BOSH release for Windows is identical to \nLinux BOSH Releases\n.  This means the structure of a Windows BOSH release will be:\n\n\n\n\nmetadata that specifies available configuration options\n\n\nERB configuration files\n\n\na Monit file that describes how to start, stop and monitor processes\n\n\nstart and stop scripts for each process\n\n\nadditional hook scripts\n\n\n\n\n\n\nJobs \n\u00b6\n\n\nThe structure of a BOSH job for Windows is similar to the \nStandard Linux BOSH Job Lifecycle\n, only with processes monitored by \nWindows Service Wrapper\n instead of monit.\n\n\nThe monit file for Windows is a JSON config file that describes processes to run:\n\n\n{\n\n  \n\"processes\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"say-hello\"\n,\n\n      \n\"executable\"\n:\n \n\"powershell\"\n,\n\n      \n\"args\"\n:\n \n[\n \n\"/var/vcap/jobs/say-hello/bin/start.ps1\"\n \n],\n\n      \n\"env\"\n:\n \n{\n\n        \n\"FOO\"\n:\n \n\"BAR\"\n\n      \n}\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\n\nThe above monit file will execute the file \nC:\\var\\vcap\\jobs\\say-hello\\bin\\start.ps1\n with the environment variable \nFOO\n set to \nBAR\n. The BOSH agent ensures the process is running by executing within a \nservice wrapper\n.\n\n\nAlso, note that Pre-Start, Post-Start, Drain, and Post-Deploy scripts (described in the \njob lifecycle\n) must be powershell scripts and end with the \n.ps1\n extension, i.e., \npre-start.ps1\n, \npost-start.ps1\n, \ndrain.ps1\n, and \npost-deploy.ps1\n.\n\n\n\n\nStop Scripts in Jobs \n\u00b6\n\n\nRelease job can have a stop script that will run when the job is restarted or stopped. This script allows the job to clean up and get into a state where it can be safely stopped.\n\n\nThe stop script replaces the standard mechanism for shutting down a BOSH job. If you use a stop script, \nwinsw\n will \nnot\n stop your job automatically. Instead it is the responsibility of the stop script to clean up resources and kill any processes that are part of the job. Winsw will wait for both the stop script and the main job process to exit before reporting to Windows that the service has terminated. For more details on how winsw handles a stop script, see \nwinsw documentation\n.\n\n\nTo use a stop script, a change to the job's \nmonit\n and \nspec\n file must be made. The actual script source is placed in the jobs template directory. eg: \njobs/job_name/templates\n\n\nStdout and Stderr are currently not preserved. It is recommended to use the Windows EventLog.\n\n\nMonit\n\u00b6\n\n\nMonit changes can refer to separate scripts for both stop and start actions.\nFor instance, to use separate scripts for start and stop:\n\n\n{\n\n  \n\"processes\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"say-hello\"\n,\n\n      \n\"executable\"\n:\n \n\"powershell.exe\"\n,\n\n      \n\"args\"\n:\n \n[\n \n\"/var/vcap/jobs/say-hello/bin/start.ps1\"\n \n],\n\n      \n\"stop\"\n:\n \n{\n\n        \n\"executable\"\n:\n \n\"powershell.exe\"\n,\n:\n\n        \n\"args\"\n:\n \n[\n \n\"/var/vcap/jobs/say-hello/bin/stop.ps1\"\n \n],\n\n      \n}\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\n\nSpec\n\u00b6\n\n\nThe \nspec\n file change is similar to linux. Here is an example:\n\n\n---\nname: simple-stop-example\ntemplates:\n  stop.ps1: bin/stop.ps1\n\n\u00b6\n\n\nSample BOSH Windows Release\n\u00b6\n\n\nPlease see \nthe next page\n for a sample BOSH Windows release.",
            "title": "Overview"
        },
        {
            "location": "/windows/#releases",
            "text": "The structure of a BOSH release for Windows is identical to  Linux BOSH Releases .  This means the structure of a Windows BOSH release will be:   metadata that specifies available configuration options  ERB configuration files  a Monit file that describes how to start, stop and monitor processes  start and stop scripts for each process  additional hook scripts",
            "title": "Releases "
        },
        {
            "location": "/windows/#jobs",
            "text": "The structure of a BOSH job for Windows is similar to the  Standard Linux BOSH Job Lifecycle , only with processes monitored by  Windows Service Wrapper  instead of monit.  The monit file for Windows is a JSON config file that describes processes to run:  { \n   \"processes\" :   [ \n     { \n       \"name\" :   \"say-hello\" , \n       \"executable\" :   \"powershell\" , \n       \"args\" :   [   \"/var/vcap/jobs/say-hello/bin/start.ps1\"   ], \n       \"env\" :   { \n         \"FOO\" :   \"BAR\" \n       } \n     } \n   ]  }   The above monit file will execute the file  C:\\var\\vcap\\jobs\\say-hello\\bin\\start.ps1  with the environment variable  FOO  set to  BAR . The BOSH agent ensures the process is running by executing within a  service wrapper .  Also, note that Pre-Start, Post-Start, Drain, and Post-Deploy scripts (described in the  job lifecycle ) must be powershell scripts and end with the  .ps1  extension, i.e.,  pre-start.ps1 ,  post-start.ps1 ,  drain.ps1 , and  post-deploy.ps1 .",
            "title": "Jobs "
        },
        {
            "location": "/windows/#stop-scripts-in-jobs",
            "text": "Release job can have a stop script that will run when the job is restarted or stopped. This script allows the job to clean up and get into a state where it can be safely stopped.  The stop script replaces the standard mechanism for shutting down a BOSH job. If you use a stop script,  winsw  will  not  stop your job automatically. Instead it is the responsibility of the stop script to clean up resources and kill any processes that are part of the job. Winsw will wait for both the stop script and the main job process to exit before reporting to Windows that the service has terminated. For more details on how winsw handles a stop script, see  winsw documentation .  To use a stop script, a change to the job's  monit  and  spec  file must be made. The actual script source is placed in the jobs template directory. eg:  jobs/job_name/templates  Stdout and Stderr are currently not preserved. It is recommended to use the Windows EventLog.",
            "title": "Stop Scripts in Jobs "
        },
        {
            "location": "/windows/#monit",
            "text": "Monit changes can refer to separate scripts for both stop and start actions.\nFor instance, to use separate scripts for start and stop:  { \n   \"processes\" :   [ \n     { \n       \"name\" :   \"say-hello\" , \n       \"executable\" :   \"powershell.exe\" , \n       \"args\" :   [   \"/var/vcap/jobs/say-hello/bin/start.ps1\"   ], \n       \"stop\" :   { \n         \"executable\" :   \"powershell.exe\" , : \n         \"args\" :   [   \"/var/vcap/jobs/say-hello/bin/stop.ps1\"   ], \n       } \n     } \n   ]  }",
            "title": "Monit"
        },
        {
            "location": "/windows/#spec",
            "text": "The  spec  file change is similar to linux. Here is an example:",
            "title": "Spec"
        },
        {
            "location": "/windows/#-name-simple-stop-example-templates-stopps1-binstopps1",
            "text": "",
            "title": "---"
        },
        {
            "location": "/windows/#sample-bosh-windows-release",
            "text": "Please see  the next page  for a sample BOSH Windows release.",
            "title": "Sample BOSH Windows Release"
        },
        {
            "location": "/windows-sample-release/",
            "text": "This is a sample BOSH release than can be deployed using a Windows stemcell. It has a single job called \nsay-hello\n that repeatedly prints out a message.\n\n\nAfter creating a deployment with this release and the \nsay-hello\n job you can access the job's standard out with the \nbosh log\n command (see documentation on \nlogs\n for more information).\n\n\n\n\nRelease Structure \n\u00b6\n\n\n$ mkdir sample-windows-release\n$ \ncd\n sample-windows-release\n$ bosh init-release --git\n$ bosh generate-job say-hello\n\n\n\n\njobs/\n  say-hello/\n    templates/\n      post-deploy.ps1\n      post-start.ps1\n      pre-start.ps1\n      start.ps1\n    monit\n    spec\npackages/\n\n\n\n\n\n\nspec\n \n\u00b6\n\n\nThe \nspec\n file specifies the job name and description. It also contains the templates to render, which may depend on zero or more packages. See the documentation on \njob spec files\n for more information.\n\n\n---\n\n\nname\n:\n \nsay-hello\n\n\n\ndescription\n:\n \n\"This\n \nis\n \na\n \nsimple\n \njob\"\n\n\n\ntemplates\n:\n\n  \nstart.ps1\n:\n \nbin/start.ps1\n\n\n\npackages\n:\n \n[]\n\n\n\n\n\n\n\nmonit\n \n\u00b6\n\n\nThe \nmonit\n file includes zero or more processes to run. Each process specifies an executable as well as any arguments and environment variables. See the documentation on \nmonit files\n for more information. Note, however, that Windows monit files are JSON config files for \nWindows service wrapper\n, not config files for the monit Unix utility.\n\n\n{\n\n  \n\"processes\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"say-hello\"\n,\n\n      \n\"executable\"\n:\n \n\"powershell\"\n,\n\n      \n\"args\"\n:\n \n[\n \n\"/var/vcap/jobs/say-hello/bin/start.ps1\"\n \n],\n\n      \n\"env\"\n:\n \n{\n\n        \n\"FOO\"\n:\n \n\"BAR\"\n\n      \n}\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\n\n\n\nstart.ps1\n \n\u00b6\n\n\nThe \nstart.ps1\n script executed by the \nservice-wrapper\n loops indefintely while printing out a message:\n\n\nwhile\n \n(\n$true\n)\n\n\n{\n\n  \nWrite-Host\n \n\"I am executing a BOSH job. FOO=${Env:FOO}\"\n\n  \nStart-Sleep\n \n1\n.\n0\n\n\n}\n\n\n\n\n\n\n\nCreating and Deploying the Sample Release \n\u00b6\n\n\nIf you have the Director with a Windows stemcell uploaded, you can create the above described release with an empty \nblobs.yml\n and \nfinal.yml\n, then try deploying it:\n\n\n$ \ncd\n sample-windows-release\n$ bosh create-release --force\n$ bosh upload-release\n$ bosh -d sample-windows-deployment deploy manifest.yml\n\n\n\n\nFor information about deployment basics, see the \nDeploy Workflow\n documenation.\n\n\nHere is a sample manifest. For information on manifest basics, see the \nDeployment Manifest\n documentation.\n\n\nname\n:\n \nsample-windows-deployment\n\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nsample-windows-release\n\n  \nversion\n:\n \nlatest\n\n\n\nstemcells\n:\n\n\n-\n \nalias\n:\n \nwindows\n\n  \nos\n:\n \nwindows2012R2\n\n  \nversion\n:\n \nlatest\n\n\n\nupdate\n:\n\n  \ncanaries\n:\n \n1\n\n  \nmax_in_flight\n:\n \n1\n\n  \ncanary_watch_time\n:\n \n30000-300000\n\n  \nupdate_watch_time\n:\n \n30000-300000\n\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nhello\n\n  \nazs\n:\n \n[\nz1\n]\n\n  \ninstances\n:\n \n1\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nsay-hello\n\n    \nrelease\n:\n \nsample-windows-release\n\n  \nstemcell\n:\n \nwindows\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault",
            "title": "Sample Release"
        },
        {
            "location": "/windows-sample-release/#release-structure",
            "text": "$ mkdir sample-windows-release\n$  cd  sample-windows-release\n$ bosh init-release --git\n$ bosh generate-job say-hello  jobs/\n  say-hello/\n    templates/\n      post-deploy.ps1\n      post-start.ps1\n      pre-start.ps1\n      start.ps1\n    monit\n    spec\npackages/",
            "title": "Release Structure "
        },
        {
            "location": "/windows-sample-release/#spec",
            "text": "The  spec  file specifies the job name and description. It also contains the templates to render, which may depend on zero or more packages. See the documentation on  job spec files  for more information.  ---  name :   say-hello  description :   \"This   is   a   simple   job\"  templates : \n   start.ps1 :   bin/start.ps1  packages :   []",
            "title": "spec "
        },
        {
            "location": "/windows-sample-release/#monit",
            "text": "The  monit  file includes zero or more processes to run. Each process specifies an executable as well as any arguments and environment variables. See the documentation on  monit files  for more information. Note, however, that Windows monit files are JSON config files for  Windows service wrapper , not config files for the monit Unix utility.  { \n   \"processes\" :   [ \n     { \n       \"name\" :   \"say-hello\" , \n       \"executable\" :   \"powershell\" , \n       \"args\" :   [   \"/var/vcap/jobs/say-hello/bin/start.ps1\"   ], \n       \"env\" :   { \n         \"FOO\" :   \"BAR\" \n       } \n     } \n   ]  }",
            "title": "monit "
        },
        {
            "location": "/windows-sample-release/#startps1",
            "text": "The  start.ps1  script executed by the  service-wrapper  loops indefintely while printing out a message:  while   ( $true )  { \n   Write-Host   \"I am executing a BOSH job. FOO=${Env:FOO}\" \n   Start-Sleep   1 . 0  }",
            "title": "start.ps1 "
        },
        {
            "location": "/windows-sample-release/#creating-and-deploying-the-sample-release",
            "text": "If you have the Director with a Windows stemcell uploaded, you can create the above described release with an empty  blobs.yml  and  final.yml , then try deploying it:  $  cd  sample-windows-release\n$ bosh create-release --force\n$ bosh upload-release\n$ bosh -d sample-windows-deployment deploy manifest.yml  For information about deployment basics, see the  Deploy Workflow  documenation.  Here is a sample manifest. For information on manifest basics, see the  Deployment Manifest  documentation.  name :   sample-windows-deployment  releases :  -   name :   sample-windows-release \n   version :   latest  stemcells :  -   alias :   windows \n   os :   windows2012R2 \n   version :   latest  update : \n   canaries :   1 \n   max_in_flight :   1 \n   canary_watch_time :   30000-300000 \n   update_watch_time :   30000-300000  instance_groups :  -   name :   hello \n   azs :   [ z1 ] \n   instances :   1 \n   jobs : \n   -   name :   say-hello \n     release :   sample-windows-release \n   stemcell :   windows \n   vm_type :   default \n   networks : \n   -   name :   default",
            "title": "Creating and Deploying the Sample Release "
        },
        {
            "location": "/events/",
            "text": "Note: This feature is available in bosh-release v256+.\n\n\n\nIn addition to keeping a historical list of \nDirector tasks\n for debugging history, the Director keeps detailed list of actions user and system took during its operation. Events are recorded into the Director database.\n\n\nCurrently following events are recorded:\n\n\n\n\ncloud config update\n\n\nruntime config update\n\n\ndeployment create/update/delete\n\n\nVM create/delete\n\n\ndisk create/delete\n\n\nbosh ssh\n events\n\n\n\n\nRun \nbosh events\n command\n to view 200 recent events:\n\n\n$ bosh events\n\n+--------------+------------------------------+-------+-------------+-------------+------------------------------------------------+------+-----------+------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n\n|\n ID           \n|\n Time                         \n|\n User  \n|\n Action      \n|\n Object \ntype\n \n|\n Object ID                                      \n|\n Task \n|\n Dep       \n|\n Inst                                           \n|\n Context                                                                                                                                            \n|\n\n+--------------+------------------------------+-------+-------------+-------------+------------------------------------------------+------+-----------+------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n\n|\n \n5223\n         \n|\n Thu May \n12\n \n23\n:49:41 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1084\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_d05e94d02fd049c0                                                                                                                        \n|\n\n\n|\n \n5222\n         \n|\n Thu May \n12\n \n23\n:49:38 UTC \n2016\n \n|\n admin \n|\n cleanup ssh \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1083\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: ^bosh_87ad9a84d61c4e57                                                                                                                       \n|\n\n\n|\n \n5221\n         \n|\n Thu May \n12\n \n23\n:49:16 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1082\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_87ad9a84d61c4e57                                                                                                                        \n|\n\n\n|\n \n5220\n         \n|\n Thu May \n12\n \n23\n:46:26 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1081\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_41c768618e734a72                                                                                                                        \n|\n\n\n|\n \n5219\n <- \n5216\n \n|\n Thu May \n12\n \n23\n:43:34 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1079\n \n|\n slow-nats \n|\n -                                              \n|\n before: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n, \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n after: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n   \n|\n\n\n|\n \n5218\n <- \n5217\n \n|\n Thu May \n12\n \n23\n:43:34 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1079\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5217\n         \n|\n Thu May \n12\n \n23\n:42:52 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1079\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n az: z3,                                                                                                                                            \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n changes: \n[\n\"configuration\"\n, \n\"job\"\n]\n                                                                                                                  \n|\n\n\n|\n \n5216\n         \n|\n Thu May \n12\n \n23\n:42:51 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1079\n \n|\n slow-nats \n|\n -                                              \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5215\n         \n|\n Thu May \n12\n \n21\n:43:14 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1076\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_e54b9915d292428e                                                                                                                        \n|\n\n\n|\n \n5214\n <- \n5211\n \n|\n Thu May \n12\n \n20\n:53:11 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1074\n \n|\n slow-nats \n|\n -                                              \n|\n before: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n, \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n after: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n   \n|\n\n\n|\n \n5213\n <- \n5212\n \n|\n Thu May \n12\n \n20\n:53:11 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1074\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5212\n         \n|\n Thu May \n12\n \n20\n:51:35 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1074\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n az: z3,                                                                                                                                            \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n changes: \n[\n\"configuration\"\n, \n\"job\"\n]\n                                                                                                                  \n|\n\n\n|\n \n5211\n         \n|\n Thu May \n12\n \n20\n:51:34 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1074\n \n|\n slow-nats \n|\n -                                              \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5210\n         \n|\n Thu May \n12\n \n20\n:35:52 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1073\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_c5d3b51c29f14e03                                                                                                                        \n|\n\n\n|\n \n5209\n         \n|\n Thu May \n12\n \n20\n:34:53 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1072\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_3a86d5bfbdec4fba                                                                                                                        \n|\n\n\n|\n \n5208\n <- \n5205\n \n|\n Thu May \n12\n \n20\n:32:20 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1068\n \n|\n slow-nats \n|\n -                                              \n|\n before: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n, \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n after: \n{\n\"releases\"\n=\n>\n[\n\"os-conf/5\"\n, \n\"syslog/5\"\n, \n\"bosh/256.2\"\n, \n\"bosh-aws-cpi/44\"\n]\n, \n\"stemcells\"\n=\n>\n[\n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3232.2\"\n]}\n   \n|\n\n\n|\n \n5207\n <- \n5206\n \n|\n Thu May \n12\n \n20\n:32:20 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1068\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5206\n         \n|\n Thu May \n12\n \n20\n:24:20 UTC \n2016\n \n|\n admin \n|\n update      \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1068\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n az: z3,                                                                                                                                            \n|\n\n\n|\n              \n|\n                              \n|\n       \n|\n             \n|\n             \n|\n                                                \n|\n      \n|\n           \n|\n                                                \n|\n changes: \n[\n\"dirty\"\n, \n\"configuration\"\n, \n\"state\"\n]\n                                                                                                       \n|\n\n\n|\n \n5205\n         \n|\n Thu May \n12\n \n20\n:24:18 UTC \n2016\n \n|\n admin \n|\n update      \n|\n deployment  \n|\n slow-nats                                      \n|\n \n1068\n \n|\n slow-nats \n|\n -                                              \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5204\n         \n|\n Thu May \n12\n \n20\n:19:24 UTC \n2016\n \n|\n admin \n|\n cleanup ssh \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1067\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: ^bosh_59b01a03f3574812                                                                                                                       \n|\n\n\n|\n \n5203\n         \n|\n Thu May \n12\n \n20\n:19:18 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1066\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_59b01a03f3574812                                                                                                                        \n|\n\n\n|\n \n5202\n         \n|\n Thu May \n12\n \n20\n:17:47 UTC \n2016\n \n|\n admin \n|\n setup ssh   \n|\n instance    \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n \n1064\n \n|\n slow-nats \n|\n bosh/54fc51cb-32b9-41f6-9d16-538e4fed1ef0      \n|\n user: bosh_de1f9ba9ab444b6d                                                                                                                        \n|\n\n\n|\n \n5201\n <- \n5002\n \n|\n Thu May \n12\n \n00\n:47:48 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n deployment  \n|\n tiny                                           \n|\n \n1059\n \n|\n tiny      \n|\n -                                              \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5200\n <- \n5095\n \n|\n Thu May \n12\n \n00\n:47:48 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n instance    \n|\n zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5199\n <- \n5096\n \n|\n Thu May \n12\n \n00\n:47:48 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n vm          \n|\n i-054a17a75c0c9b279                            \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5198\n <- \n5179\n \n|\n Thu May \n12\n \n00\n:47:23 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n instance    \n|\n zookeeper/c7741d2b-4f47-4e82-bf44-5226044da9a3 \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/c7741d2b-4f47-4e82-bf44-5226044da9a3 \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5197\n <- \n5180\n \n|\n Thu May \n12\n \n00\n:47:23 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n vm          \n|\n i-0609fb01d24bc9a31                            \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/c7741d2b-4f47-4e82-bf44-5226044da9a3 \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5196\n <- \n5175\n \n|\n Thu May \n12\n \n00\n:47:22 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n instance    \n|\n zookeeper/01f97065-6757-4c59-85e7-404c2e8418e9 \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/01f97065-6757-4c59-85e7-404c2e8418e9 \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5195\n <- \n5176\n \n|\n Thu May \n12\n \n00\n:47:22 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n vm          \n|\n i-0c9e15d1577a50d3a                            \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/01f97065-6757-4c59-85e7-404c2e8418e9 \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5194\n <- \n5167\n \n|\n Thu May \n12\n \n00\n:47:16 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n instance    \n|\n zookeeper/32b9aa25-0080-4a66-865a-777577e1727c \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/32b9aa25-0080-4a66-865a-777577e1727c \n|\n -                                                                                                                                                  \n|\n\n\n|\n \n5193\n <- \n5168\n \n|\n Thu May \n12\n \n00\n:47:16 UTC \n2016\n \n|\n admin \n|\n delete      \n|\n vm          \n|\n i-04eccec6f844e862e                            \n|\n \n1059\n \n|\n tiny      \n|\n zookeeper/32b9aa25-0080-4a66-865a-777577e1727c \n|\n -\n\n...\n\n\n\n\nList of events can be also filtered by a deployment name (\n--deployment\n), a task ID (\n--task\n), and/or an instance (\n--instance\n). Additionally you can paginate by specifying \n--before-id\n flag to view next 200 events matching viewed criteria. In an upcoming release we will also include filtering based on an event timestamp to quickly identify events happened during specific timeframe.\n\n\nExample query commands:\n\n\n$ bosh events --deployment slow-nats\n$ bosh events --deployment slow-nats --before-id \n5208\n\n$ bosh events --instance zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc\n\n\n\n\n\n\nEnding vs. Single Actions \n\u00b6\n\n\nEach event represents an action. Some actions take time to perform (e.g. delete a VM), and other actions are just one-off events (e.g. set up SSH access). Actions that take time are represented by two events (starting and ending one) instead of just one. In the example below delete VM action is recorded as starting in event #5096 and finishing in event #5199.\n\n\n|\n \n5199\n <- \n5096\n \n|\n Thu May \n12\n \n00\n:47:48 UTC \n2016\n \n|\n admin \n|\n delete \n|\n vm \n|\n i-054a17a75c0c9b279 \n|\n \n1059\n \n|\n tiny \n|\n zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc ...\n\n|\n \n5096\n         \n|\n Thu May \n12\n \n00\n:40:44 UTC \n2016\n \n|\n admin \n|\n delete \n|\n vm \n|\n i-054a17a75c0c9b279 \n|\n \n1059\n \n|\n tiny \n|\n zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc ...\n\n\n\n\n\n\nEnabling Event Collection \n\u00b6\n\n\nTo enable this feature:\n\n\n\n\n\n\nAdd \ndirector.events.record_events\n deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nevents\n:\n\n      \nrecord_events\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nOptionally change frequency and number of events to keep.\n\n\n\n\n\n\nRedeploy the Director.\n\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Auditing Events"
        },
        {
            "location": "/events/#ending-vs-single-actions",
            "text": "Each event represents an action. Some actions take time to perform (e.g. delete a VM), and other actions are just one-off events (e.g. set up SSH access). Actions that take time are represented by two events (starting and ending one) instead of just one. In the example below delete VM action is recorded as starting in event #5096 and finishing in event #5199.  |   5199  <-  5096   |  Thu May  12   00 :47:48 UTC  2016   |  admin  |  delete  |  vm  |  i-054a17a75c0c9b279  |   1059   |  tiny  |  zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc ... |   5096           |  Thu May  12   00 :40:44 UTC  2016   |  admin  |  delete  |  vm  |  i-054a17a75c0c9b279  |   1059   |  tiny  |  zookeeper/ca5f695a-eb81-49fd-a577-33825cb1b5fc ...",
            "title": "Ending vs. Single Actions "
        },
        {
            "location": "/events/#enabling-event-collection",
            "text": "To enable this feature:    Add  director.events.record_events  deployment manifest for the Director:  properties : \n   director : \n     events : \n       record_events :   true     Optionally change frequency and number of events to keep.    Redeploy the Director.     Back to Table of Contents",
            "title": "Enabling Event Collection "
        },
        {
            "location": "/director-tasks/",
            "text": "An operator uses the CLI to interact with the Director. Certain CLI commands result in complex and potentially long running operations against the IaaS, blobstore, or other resources. Such commands are associated with a Director task and continue running on the Director even if the CLI disconnects from the Director.\n\n\nTo find out if a CLI command has an associated Director task, look for \"Director task [NUM]\" in its output:\n\n\n$ bosh -d zookeeper deploy zookeeper.yml\nUsing deployment \n'zookeeper'\n\n\nTask \n766\n \n# <---\n\n\n...snip...\n\n\n\n\n\n\nCurrently active tasks \n\u00b6\n\n\nAt any time the Director might be performing multiple tasks at once. Active tasks can be in two states: \nqueued\n or \nprocessing\n.\n\n\nTo see all currently active tasks:\n\n\n$ bosh tasks --no-filter\n\n+-----+------------+-------------------------+-------+-------------------------------+--------+\n\n|\n \n#   | State      | Timestamp               | User  | Description                   | Result |\n\n+-----+------------+-------------------------+-------+-------------------------------+--------+\n\n|\n \n766\n \n|\n processing \n|\n \n2015\n-01-27 \n21\n:39:30 UTC \n|\n admin \n|\n create deployment             \n|\n        \n|\n\n\n|\n \n765\n \n|\n queued     \n|\n \n2015\n-01-27 \n21\n:35:02 UTC \n|\n admin \n|\n scheduled SnapshotDeployments \n|\n        \n|\n\n+-----+------------+-------------------------+-------+-------------------------------+--------+\n\nTotal tasks running now: \n2\n\n\n\n\n\nNote\n: \n--no-filter\n flag shows all tasks. Without that flag, the Director returns a subset of running tasks that it deems important.\n\n\n\nJoining tasks \n\u00b6\n\n\nSince Director tasks continue to run in the background even if the CLI has disconnected, you can rejoin a task at any time:\n\n\n$ bosh task \n766\n\n\nDirector task \n766\n\n\n...snip...\n\n\n\n\nTasks can be joined in different output modes:\n\n\n\n\nevent\n (default): human readable high-level events\n\n\ndebug\n: detailed logs showing all internal communication between the Director and the Agents\n\n\ncpi\n: detailed logs showing all requests and responses from the CPI\n\n\n\n\n$ bosh task \n766\n --debug\n\nDirector task \n766\n\n\nI, \n[\n2015\n-01-27T21:33:19.469158 \n#1769] [0x3fab30147330]  INFO -- TaskHelper: Director Version: 1.2811.0\n\nI, \n[\n2015\n-01-27T21:33:19.469212 \n#1769] [0x3fab30147330]  INFO -- TaskHelper: Enqueuing task: 766\n\nI, \n[\n2015\n-01-27 \n21\n:33:21 \n#2725] []  INFO -- DirectorJobRunner: Looking for task with task id 766\n\nD, \n[\n2015\n-01-27 \n21\n:33:21 \n#2725] [] DEBUG -- DirectorJobRunner: (0.001125s) SELECT * FROM \"tasks\" WHERE \"id\" = 766\n\n\n...snip...\n\n\n\n\nCanceling tasks \n\u00b6\n\n\nTasks can be cancelled before and while they are running. Canceling an active task will not take immediate effect; however, the Director will stop task execution at a next safe checkpoint. To cancel a task, either press \nCtrl+C\n while tracking the task or run:\n\n\n$ bosh cancel task \n766\n\n\n\n\n\n\n\nFinished tasks \n\u00b6\n\n\nThe Director keeps a record of tasks that have finished. Finished tasks can be in two states: \ndone\n or \nerror\n.\n\n\nTo view recently finished tasks:\n\n\n$ bosh tasks recent\n\n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+\n\n|\n \n#   | State | Timestamp               | User   | Description              | Result                                                    |\n\n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+\n\n|\n \n768\n \n|\n \ndone\n  \n|\n \n2015\n-01-27 \n21\n:39:30 UTC \n|\n admin  \n|\n run errand smoke-tests   \n|\n Errand \n`\nsmoke-tests\n' completed successfully (exit code 0) |\n\n\n| 766 | done  | 2015-01-27 21:33:42 UTC | admin  | create deployment        | /deployments/mysql-dep                                    |\n\n\n| 765 | error | 2015-01-27 21:27:48 UTC | admin  | create deployment        | Timed out pinging to 95206a0e-4dd9-4598-a074-2aee54793f0f |\n\n\n| 764 | done  | 2015-01-27 21:25:50 UTC | admin  | create stemcell          | /stemcells/bosh-aws-xen-ubuntu-trusty-go_agent/2827       |\n\n\n| 760 | done  | 2015-01-27 21:25:01 UTC | admin  | create release           | Created release `cf-mysql/16'\n                             \n|\n\n\n|\n \n759\n \n|\n error \n|\n \n2015\n-01-27 \n21\n:23:43 UTC \n|\n admin  \n|\n create release           \n|\n No space left on device @ io_write -...                   \n|\n\n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+\n\nShowing \n30\n recent tasks\n\n\n\n\nYou can also run \nbosh tasks recent [NUM]\n to retrieve more tasks.\n\n\nNote\n: \n--no-filter\n flag shows all tasks. Without that flag, the Director returns a subset of finished tasks that it deems important.\n\n\n\nJoining finished tasks \n\u00b6\n\n\nFinished tasks can be joined just like active tasks but only to view their output (see \nvarious output modes\n). Finished tasks cannot be cancelled.",
            "title": "Reviewing Tasks"
        },
        {
            "location": "/director-tasks/#currently-active-tasks",
            "text": "At any time the Director might be performing multiple tasks at once. Active tasks can be in two states:  queued  or  processing .  To see all currently active tasks:  $ bosh tasks --no-filter\n\n+-----+------------+-------------------------+-------+-------------------------------+--------+ |   #   | State      | Timestamp               | User  | Description                   | Result | \n+-----+------------+-------------------------+-------+-------------------------------+--------+ |   766   |  processing  |   2015 -01-27  21 :39:30 UTC  |  admin  |  create deployment              |          |  |   765   |  queued      |   2015 -01-27  21 :35:02 UTC  |  admin  |  scheduled SnapshotDeployments  |          | \n+-----+------------+-------------------------+-------+-------------------------------+--------+\n\nTotal tasks running now:  2   Note :  --no-filter  flag shows all tasks. Without that flag, the Director returns a subset of running tasks that it deems important.",
            "title": "Currently active tasks "
        },
        {
            "location": "/director-tasks/#joining-tasks",
            "text": "Since Director tasks continue to run in the background even if the CLI has disconnected, you can rejoin a task at any time:  $ bosh task  766 \n\nDirector task  766 \n\n...snip...  Tasks can be joined in different output modes:   event  (default): human readable high-level events  debug : detailed logs showing all internal communication between the Director and the Agents  cpi : detailed logs showing all requests and responses from the CPI   $ bosh task  766  --debug\n\nDirector task  766 \n\nI,  [ 2015 -01-27T21:33:19.469158  #1769] [0x3fab30147330]  INFO -- TaskHelper: Director Version: 1.2811.0 \nI,  [ 2015 -01-27T21:33:19.469212  #1769] [0x3fab30147330]  INFO -- TaskHelper: Enqueuing task: 766 \nI,  [ 2015 -01-27  21 :33:21  #2725] []  INFO -- DirectorJobRunner: Looking for task with task id 766 \nD,  [ 2015 -01-27  21 :33:21  #2725] [] DEBUG -- DirectorJobRunner: (0.001125s) SELECT * FROM \"tasks\" WHERE \"id\" = 766 \n\n...snip...",
            "title": "Joining tasks "
        },
        {
            "location": "/director-tasks/#canceling-tasks",
            "text": "Tasks can be cancelled before and while they are running. Canceling an active task will not take immediate effect; however, the Director will stop task execution at a next safe checkpoint. To cancel a task, either press  Ctrl+C  while tracking the task or run:  $ bosh cancel task  766",
            "title": "Canceling tasks "
        },
        {
            "location": "/director-tasks/#finished-tasks",
            "text": "The Director keeps a record of tasks that have finished. Finished tasks can be in two states:  done  or  error .  To view recently finished tasks:  $ bosh tasks recent\n\n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+ |   #   | State | Timestamp               | User   | Description              | Result                                                    | \n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+ |   768   |   done    |   2015 -01-27  21 :39:30 UTC  |  admin   |  run errand smoke-tests    |  Errand  ` smoke-tests ' completed successfully (exit code 0) |  | 766 | done  | 2015-01-27 21:33:42 UTC | admin  | create deployment        | /deployments/mysql-dep                                    |  | 765 | error | 2015-01-27 21:27:48 UTC | admin  | create deployment        | Timed out pinging to 95206a0e-4dd9-4598-a074-2aee54793f0f |  | 764 | done  | 2015-01-27 21:25:50 UTC | admin  | create stemcell          | /stemcells/bosh-aws-xen-ubuntu-trusty-go_agent/2827       |  | 760 | done  | 2015-01-27 21:25:01 UTC | admin  | create release           | Created release `cf-mysql/16'                               |  |   759   |  error  |   2015 -01-27  21 :23:43 UTC  |  admin   |  create release            |  No space left on device @ io_write -...                    | \n+-----+-------+-------------------------+--------+--------------------------+-----------------------------------------------------------+\n\nShowing  30  recent tasks  You can also run  bosh tasks recent [NUM]  to retrieve more tasks.  Note :  --no-filter  flag shows all tasks. Without that flag, the Director returns a subset of finished tasks that it deems important.",
            "title": "Finished tasks "
        },
        {
            "location": "/director-tasks/#joining-finished-tasks",
            "text": "Finished tasks can be joined just like active tasks but only to view their output (see  various output modes ). Finished tasks cannot be cancelled.",
            "title": "Joining finished tasks "
        },
        {
            "location": "/director-access-events/",
            "text": "Note: This feature is available in bosh-release v256+.\n\n\n\nDirector logs all API access events to syslog under \nvcap.bosh.director\n topic.\n\n\nHere is a log snipped found in \n/var/log/syslog\n in \nCommon Event Format (CEF)\n:\n\n\nMay 13 05:13:34 localhost vcap.bosh.director[16199]: CEF:0|CloudFoundry|BOSH|1.0000.0|director_api|/deployments|7|requestMethod=GET src=127.0.0.1 spt=25556 shost=36ff45a2-51a2-488d-af95-953c43de4cec cs1=10.10.0.36,fe80::80a:99ff:fed6:df7d%eth0 cs1Label=ips cs2=X_BOSH_UPLOAD_REQUEST_TIME=0.000&HOST=127.0.0.1&X_REAL_IP=127.0.0.1&X_FORWARDED_FOR=127.0.0.1&X_FORWARDED_PROTO=https&USER_AGENT=EventMachine HttpClient cs2Label=httpHeaders cs3=none cs3Label=authType cs4=401 cs4Label=responseStatus cs5=Not authorized: '/deployments' cs5Label=statusReason\n\n\n\n\nAnd in a more redable form:\n\n\nMay 13 05:13:34 localhost vcap.bosh.director[16199]:\nCEF:0\nCloudFoundry\nBOSH\n1.3232.0\ndirector_api\n/deployments\n7\n\nrequestMethod=GET\nsrc=127.0.0.1\nspt=25556\nshost=36ff45a2-51a2-488d-af95-953c43de4cec\n\ncs1=10.10.0.36,fe80::80a:99ff:fed6:df7d%eth0\ncs1Label=ips\n\ncs2=X_BOSH_UPLOAD_REQUEST_TIME=0.000&HOST=127.0.0.1&X_REAL_IP=127.0.0.1&X_FORWARDED_FOR=127.0.0.1&X_FORWARDED_PROTO=https&USER_AGENT=EventMachine HttpClient\ncs2Label=httpHeaders\n\ncs3=none\ncs3Label=authType\n\ncs4=401\ncs4Label=responseStatus\n\ncs5=Not authorized: '/deployments'\ncs5Label=statusReason\n\n\n\n\n\n\nEnabling Logging \n\u00b6\n\n\nTo enable this feature:\n\n\n\n\n\n\nAdd \ndirector.log_access_events_to_syslog\n deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nlog_access_events_to_syslog\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nOptionally colocate \nsyslog-release's \nsyslog_forwarder\n job\n with the Director to forward logs to a remote location.\n\n\n\n\n\n\nRedeploy the Director.\n\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Logging API Access"
        },
        {
            "location": "/director-access-events/#enabling-logging",
            "text": "To enable this feature:    Add  director.log_access_events_to_syslog  deployment manifest for the Director:  properties : \n   director : \n     log_access_events_to_syslog :   true     Optionally colocate  syslog-release's  syslog_forwarder  job  with the Director to forward logs to a remote location.    Redeploy the Director.     Back to Table of Contents",
            "title": "Enabling Logging "
        },
        {
            "location": "/director-users/",
            "text": "The Director provides a very simple built-in user management system for authentication of operators and internal services (for example, the Health Monitor). Alternatively, it can integrate with UAA for more advanced use cases.\n\n\n\n\nDefault Configuration \n\u00b6\n\n\nNote\n: We are planning to remove this configuration. We recommend configuring the Director as described below in \nPreconfigured Users\n section.\n\n\n\nOnce installed, the Director comes without any configured users by default. When there are no configured users you can use \nadmin\n / \nadmin\n credentials to login into the Director.\n\n\n$ bosh login admin\n\nEnter password: *****\nLogged in as \n`\nadmin\n'\n\n\n\n\n\nWhen the Director is configured with at least one user, default \nadmin\n / \nadmin\n credentials no longer work. To create a new user:\n\n\n$ bosh create user some-operator\n\nEnter new password: ********\nVerify new password: ********\nUser \n`\nsome-operator\n'\n has been created\n\n\n\n\nTo delete existing user:\n\n\n$ bosh delete user some-operator\n\nAre you sure you would like to delete the user \n`\nsome-operator\n'? (type '\nyes\n' to continue): yes\n\n\nUser `some-operator'\n has been deleted\n\n\n\n\n\n\nPreconfigured Users \n\u00b6\n\n\nNote\n: This feature is available with bosh-release v177+ (1.2999.0).\n\n\n\nIn this configuration the Director is configured in advance with a list of users. There is no way to add or remove users without redeploying the Director.\n\n\nTo configure the Director with a list of users:\n\n\n\n\n\n\nChange deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nuser_management\n:\n\n      \nprovider\n:\n \nlocal\n\n      \nlocal\n:\n\n        \nusers\n:\n\n        \n-\n \n{\nname\n:\n \nadmin\n,\n \npassword\n:\n \nadmin-password\n}\n\n        \n-\n \n{\nname\n:\n \nhm\n,\n \npassword\n:\n \nhm-password\n}\n\n\n\n\n\n\n\n\n\nRedeploy the Director with the updated manifest.\n\n\n\n\n\n\n\n\nUAA Integration \n\u00b6\n\n\nConfigure the Director with UAA user management\n.\n\n\n\n\nDirector Tasks \n\u00b6\n\n\nWhen a user initiates a \ndirector task\n, the director logs the user in the task audit log.\n\n\n\n\nHealth Monitor Authentication \n\u00b6\n\n\nThe Health Monitor is configured to use a custom user to query/submit requests to the Director. Since by default the Director does not come with any users, the Health Monitor is not able to successfully communicate with the Director. See the \nAutomatic repair with Resurrector\n topic for more details.\n\n\n\n\nBack to Table of Contents",
            "title": "Using Basic Users"
        },
        {
            "location": "/director-users/#default-configuration",
            "text": "Note : We are planning to remove this configuration. We recommend configuring the Director as described below in  Preconfigured Users  section.  Once installed, the Director comes without any configured users by default. When there are no configured users you can use  admin  /  admin  credentials to login into the Director.  $ bosh login admin\n\nEnter password: *****\nLogged in as  ` admin '   When the Director is configured with at least one user, default  admin  /  admin  credentials no longer work. To create a new user:  $ bosh create user some-operator\n\nEnter new password: ********\nVerify new password: ********\nUser  ` some-operator '  has been created  To delete existing user:  $ bosh delete user some-operator\n\nAre you sure you would like to delete the user  ` some-operator '? (type ' yes ' to continue): yes  User `some-operator'  has been deleted",
            "title": "Default Configuration "
        },
        {
            "location": "/director-users/#preconfigured-users",
            "text": "Note : This feature is available with bosh-release v177+ (1.2999.0).  In this configuration the Director is configured in advance with a list of users. There is no way to add or remove users without redeploying the Director.  To configure the Director with a list of users:    Change deployment manifest for the Director:  properties : \n   director : \n     user_management : \n       provider :   local \n       local : \n         users : \n         -   { name :   admin ,   password :   admin-password } \n         -   { name :   hm ,   password :   hm-password }     Redeploy the Director with the updated manifest.",
            "title": "Preconfigured Users "
        },
        {
            "location": "/director-users/#uaa-integration",
            "text": "Configure the Director with UAA user management .",
            "title": "UAA Integration "
        },
        {
            "location": "/director-users/#director-tasks",
            "text": "When a user initiates a  director task , the director logs the user in the task audit log.",
            "title": "Director Tasks "
        },
        {
            "location": "/director-users/#health-monitor-authentication",
            "text": "The Health Monitor is configured to use a custom user to query/submit requests to the Director. Since by default the Director does not come with any users, the Health Monitor is not able to successfully communicate with the Director. See the  Automatic repair with Resurrector  topic for more details.   Back to Table of Contents",
            "title": "Health Monitor Authentication "
        },
        {
            "location": "/director-users-uaa/",
            "text": "Note: This feature is available with bosh-release v209+ (1.3088.0) colocated with uaa v1+.\n\n\n\nIn this configuration the Director is configured to delegate user management to the \nUAA\n server. The UAA server can be configured to manage its own list of users or work with an LDAP server, or a SAML provider. Regardless how the UAA server is configured the BOSH CLI will ask appropriate credentials and forward them to the UAA to request a token.\n\n\n\n\nConfiguring the Director \n\u00b6\n\n\n\n\n\n\nChange deployment manifest for the Director and add UAA release:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/cloudfoundry/bosh?v=209\n\n  \nsha1\n:\n \na96833b6c68abda5aaa5d05ebdd0a5d394e6c15f\n\n\n# ...\n\n\n-\n \nname\n:\n \nuaa\n \n# <---\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/cloudfoundry/uaa-release?v=24\n\n  \nsha1\n:\n \nd0feb5494153217f3d62b346f426ad2b2f43511a\n\n\n\n\n\n\n\n\n\nColocate UAA next to the Director:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ninstances\n:\n \n1\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nnats\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \nredis\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \npostgres\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \nblobstore\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \ndirector\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \nhealth_monitor\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n-\n \n{\nname\n:\n \nuaa\n,\n \nrelease\n:\n \nuaa\n}\n \n# <---\n\n  \nresource_pool\n:\n \ndefault\n\n  \n# ...\n\n\n\n\n\n\n\n\n\nAdd \nuaa\n section to the deployment manifest:\n\n\nproperties\n:\n\n  \nuaa\n:\n\n    \nurl\n:\n \n\"https://54.236.100.56:8443\"\n\n\n    \nscim\n:\n\n      \nusers\n:\n\n      \n-\n \nname\n:\n \nadmin\n\n        \n# password: admin-password # <--- Uncomment & change\n\n        \ngroups\n:\n\n        \n-\n \nscim.write\n\n        \n-\n \nscim.read\n\n        \n-\n \nbosh.admin\n\n\n    \nclients\n:\n\n      \nbosh_cli\n:\n\n        \noverride\n:\n \ntrue\n\n        \nauthorized-grant-types\n:\n \npassword,refresh_token\n\n        \n# scopes the client may receive\n\n        \nscope\n:\n \nopenid,bosh.admin,bosh.read,bosh.*.admin,bosh.*.read\n\n        \nauthorities\n:\n \nuaa.none\n\n        \naccess-token-validity\n:\n \n120\n \n# 2 min\n\n        \nrefresh-token-validity\n:\n \n86400\n \n# re-login required once a day\n\n        \nsecret\n:\n \n\"\"\n \n# CLI expects this secret to be empty\n\n\n    \nadmin\n:\n\n      \n# client_secret: admin-password # <--- Uncomment & change\n\n    \nlogin\n:\n\n      \n# client_secret: login-password # <--- Uncomment & change\n\n    \nzones\n:\n \n{\ninternal\n:\n \n{\nhostnames\n:\n \n[]}}\n\n\n\n\n\nNote: Make sure UAA URL corresponds to the Director and UAA certificate subjects.\n\n\nTo configure UAA to use LDAP, SAML, etc. see \nuaa release job properties\n.\n\n\n\n\n\n\nConfigure the Director Postgres server to have an additional database called \nuaa\n:\n\n\nproperties\n:\n\n  \npostgres\n:\n \n&db\n\n    \nhost\n:\n \n127.0.0.1\n\n    \nport\n:\n \n5432\n\n    \nuser\n:\n \npostgres\n\n    \n# password: postgres-password # <--- Uncomment & change\n\n    \ndatabase\n:\n \nbosh\n\n    \nadditional_databases\n:\n \n[\nuaa\n]\n \n# <---\n\n    \nadapter\n:\n \npostgres\n\n\n\n\n\nNote: If you are using externally configured database, you should skip this section.\n\n\n\n\n\n\nConfigure the UAA server to point to that database:\n\n\nproperties\n:\n\n  \nuaadb\n:\n\n    \naddress\n:\n \n127.0.0.1\n\n    \nport\n:\n \n5432\n\n    \ndb_scheme\n:\n \npostgresql\n\n    \ndatabases\n:\n\n    \n-\n \n{\ntag\n:\n \nuaa\n,\n \nname\n:\n \nuaa\n}\n\n    \nroles\n:\n\n    \n-\n \ntag\n:\n \nadmin\n\n      \nname\n:\n \npostgres\n\n      \n# password: postgres-password # <--- Uncomment & change\n\n\n\n\n\n\n\n\n\nChange Director configuration to specify how to contact the UAA server and how to verify an access token. Since UAA will be on the same server we can use the same IP as the one used for the Director.\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nuser_management\n:\n\n      \nprovider\n:\n \nuaa\n\n      \nuaa\n:\n\n        \nurl\n:\n \n\"https://54.236.100.56:8443\"\n\n\n\n\n\nNote: The UAA URL given to the Director will be advertised to the CLI and the CLI will use it to ask for an access token. This means that the CLI must be able to reach that IP.\n\n\nNote: Make sure UAA URL corresponds to the UAA certificate subject.\n\n\n\n\n\n\nConfigure Certificates and Keys\n\n\nSee \nDirector certificates configuration doc\n to find out how to generate necessary certificates.\nNote, however, that \nlogin.saml.serviceProviderKeyPassword\n may need to be set to \"\", \nsee\n.\n\n\nTo generate UAA signing (private key) and verification key (public key) in PEM format:\n\n\n$ ssh-keygen -t rsa -b \n4096\n -f uaa\n$ openssl rsa -in uaa -pubout > uaa.pub\n\n\n\n\nPut the keys in the Director deployment manifest:\n- \nuaa.jwt.signing_key\n\n    - Private key used to sign authorization tokens (e.g. \n./uaa\n)\n- \nuaa.jwt.verification_key\n\n    - Public key used to verify tokens (e.g. \n./uaa.pub\n)\n- \ndirector.user_management.uaa.public_key\n\n    - Public key used by the Director to verify tokens without contacting the UAA (e.g. \n./uaa.pub\n)\n\n\n\n\n\n\nAllow access to port 8443 on the Director VM from your IaaS so that the CLI can access the UAA server.\n\n\n\n\n\n\nRedeploy the Director with the updated manifest.\n\n\n\n\n\n\n\n\nLogging into the Director as a user \n\u00b6\n\n\nDepending on how the UAA is configured different prompts may be shown.\n\n\n$ bosh login\nEmail: admin\nPassword: **************\n\n\n\n\nAdding/removing Users and Permissions \n\u00b6\n\n\nAn example of how to use \nUAA CLI\n to add a new user that has readonly access on any Director. Enter the client secret provided for the UAA admin client in the manifest at \nuaa.admin.client_secret\n.\n\n\n$ uaac target https://54.236.100.56:8443 --ca-cert certs/rootCA.pem\n$ uaac token client get admin\nClient secret:  **************\n$ uaac user add some-new-user --emails new.user@example.com\n\n\n\n\nNote: Use UAA CLI v3.1.4+ to specify custom CA certificate.\n\n\n\nYou can add permissions to users by defining a group and adding users to that group:\n\n\n$ uaac group add bosh.read\n$ uaac member add bosh.read some-new-user\n\n\n\n\nRemove permission by removing users from a group:\n\n\n$ uaac member delete bosh.read some-new-user\n\n\n\n\nRemove users to revoke authentication completely:\n\n\n$ uaac user delete some-new-user\n\n\n\n\nNote that changing group membership will take effect when a new access token is created for that user. New access are granted when their existing access token expires or when user logs out and logs in again. Hence it's recommended to set access token validity to a minute or so.\n\n\n\n\n\nLogging into the Director as a UAA client \n\u00b6\n\n\nNon-interactive login, e.g. for scripts during a CI build is supported by the UAA by using a different UAA client allowing \nclient_credentials\n grant type.\n\n\n$ \nexport\n \nBOSH_CLIENT\n=\nci\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\nci-password\n$ bosh status\n\n\n\n\nSee \nthe resurrector UAA client configuration\n for an example to set up an additional client.\n\n\n\n\nPermissions \n\u00b6\n\n\nSee \nUAA permissions\n to limit access to resources.\n\n\n\n\nErrors \n\u00b6\n\n\nHTTP 401: Not authorized: '/deployments' requires one of the scopes: bosh.admin, bosh.UUID.admin, bosh.read, bosh.UUID.read\n\n\n\n\nThis error occurs if the user doesn't have the right scopes for the requested command. It might be the case that you created a user without adding it to any groups. See \nAdding/removing users and scopes\n above.\n\n\n\n\nBack to Table of Contents",
            "title": "Configuring Director"
        },
        {
            "location": "/director-users-uaa/#configuring-the-director",
            "text": "Change deployment manifest for the Director and add UAA release:  releases :  -   name :   bosh \n   url :   https://bosh.io/d/github.com/cloudfoundry/bosh?v=209 \n   sha1 :   a96833b6c68abda5aaa5d05ebdd0a5d394e6c15f  # ...  -   name :   uaa   # <--- \n   url :   https://bosh.io/d/github.com/cloudfoundry/uaa-release?v=24 \n   sha1 :   d0feb5494153217f3d62b346f426ad2b2f43511a     Colocate UAA next to the Director:  jobs :  -   name :   bosh \n   instances :   1 \n   templates : \n   -   { name :   nats ,   release :   bosh } \n   -   { name :   redis ,   release :   bosh } \n   -   { name :   postgres ,   release :   bosh } \n   -   { name :   blobstore ,   release :   bosh } \n   -   { name :   director ,   release :   bosh } \n   -   { name :   health_monitor ,   release :   bosh } \n   -   { name :   uaa ,   release :   uaa }   # <--- \n   resource_pool :   default \n   # ...     Add  uaa  section to the deployment manifest:  properties : \n   uaa : \n     url :   \"https://54.236.100.56:8443\" \n\n     scim : \n       users : \n       -   name :   admin \n         # password: admin-password # <--- Uncomment & change \n         groups : \n         -   scim.write \n         -   scim.read \n         -   bosh.admin \n\n     clients : \n       bosh_cli : \n         override :   true \n         authorized-grant-types :   password,refresh_token \n         # scopes the client may receive \n         scope :   openid,bosh.admin,bosh.read,bosh.*.admin,bosh.*.read \n         authorities :   uaa.none \n         access-token-validity :   120   # 2 min \n         refresh-token-validity :   86400   # re-login required once a day \n         secret :   \"\"   # CLI expects this secret to be empty \n\n     admin : \n       # client_secret: admin-password # <--- Uncomment & change \n     login : \n       # client_secret: login-password # <--- Uncomment & change \n     zones :   { internal :   { hostnames :   []}}   Note: Make sure UAA URL corresponds to the Director and UAA certificate subjects.  To configure UAA to use LDAP, SAML, etc. see  uaa release job properties .    Configure the Director Postgres server to have an additional database called  uaa :  properties : \n   postgres :   &db \n     host :   127.0.0.1 \n     port :   5432 \n     user :   postgres \n     # password: postgres-password # <--- Uncomment & change \n     database :   bosh \n     additional_databases :   [ uaa ]   # <--- \n     adapter :   postgres   Note: If you are using externally configured database, you should skip this section.    Configure the UAA server to point to that database:  properties : \n   uaadb : \n     address :   127.0.0.1 \n     port :   5432 \n     db_scheme :   postgresql \n     databases : \n     -   { tag :   uaa ,   name :   uaa } \n     roles : \n     -   tag :   admin \n       name :   postgres \n       # password: postgres-password # <--- Uncomment & change     Change Director configuration to specify how to contact the UAA server and how to verify an access token. Since UAA will be on the same server we can use the same IP as the one used for the Director.  properties : \n   director : \n     user_management : \n       provider :   uaa \n       uaa : \n         url :   \"https://54.236.100.56:8443\"   Note: The UAA URL given to the Director will be advertised to the CLI and the CLI will use it to ask for an access token. This means that the CLI must be able to reach that IP.  Note: Make sure UAA URL corresponds to the UAA certificate subject.    Configure Certificates and Keys  See  Director certificates configuration doc  to find out how to generate necessary certificates.\nNote, however, that  login.saml.serviceProviderKeyPassword  may need to be set to \"\",  see .  To generate UAA signing (private key) and verification key (public key) in PEM format:  $ ssh-keygen -t rsa -b  4096  -f uaa\n$ openssl rsa -in uaa -pubout > uaa.pub  Put the keys in the Director deployment manifest:\n-  uaa.jwt.signing_key \n    - Private key used to sign authorization tokens (e.g.  ./uaa )\n-  uaa.jwt.verification_key \n    - Public key used to verify tokens (e.g.  ./uaa.pub )\n-  director.user_management.uaa.public_key \n    - Public key used by the Director to verify tokens without contacting the UAA (e.g.  ./uaa.pub )    Allow access to port 8443 on the Director VM from your IaaS so that the CLI can access the UAA server.    Redeploy the Director with the updated manifest.",
            "title": "Configuring the Director "
        },
        {
            "location": "/director-users-uaa/#logging-into-the-director-as-a-user",
            "text": "Depending on how the UAA is configured different prompts may be shown.  $ bosh login\nEmail: admin\nPassword: **************",
            "title": "Logging into the Director as a user "
        },
        {
            "location": "/director-users-uaa/#addingremoving-users-and-permissions",
            "text": "An example of how to use  UAA CLI  to add a new user that has readonly access on any Director. Enter the client secret provided for the UAA admin client in the manifest at  uaa.admin.client_secret .  $ uaac target https://54.236.100.56:8443 --ca-cert certs/rootCA.pem\n$ uaac token client get admin\nClient secret:  **************\n$ uaac user add some-new-user --emails new.user@example.com  Note: Use UAA CLI v3.1.4+ to specify custom CA certificate.  You can add permissions to users by defining a group and adding users to that group:  $ uaac group add bosh.read\n$ uaac member add bosh.read some-new-user  Remove permission by removing users from a group:  $ uaac member delete bosh.read some-new-user  Remove users to revoke authentication completely:  $ uaac user delete some-new-user  Note that changing group membership will take effect when a new access token is created for that user. New access are granted when their existing access token expires or when user logs out and logs in again. Hence it's recommended to set access token validity to a minute or so.",
            "title": "Adding/removing Users and Permissions "
        },
        {
            "location": "/director-users-uaa/#logging-into-the-director-as-a-uaa-client",
            "text": "Non-interactive login, e.g. for scripts during a CI build is supported by the UAA by using a different UAA client allowing  client_credentials  grant type.  $  export   BOSH_CLIENT = ci\n$  export   BOSH_CLIENT_SECRET = ci-password\n$ bosh status  See  the resurrector UAA client configuration  for an example to set up an additional client.",
            "title": "Logging into the Director as a UAA client "
        },
        {
            "location": "/director-users-uaa/#permissions",
            "text": "See  UAA permissions  to limit access to resources.",
            "title": "Permissions "
        },
        {
            "location": "/director-users-uaa/#errors",
            "text": "HTTP 401: Not authorized: '/deployments' requires one of the scopes: bosh.admin, bosh.UUID.admin, bosh.read, bosh.UUID.read  This error occurs if the user doesn't have the right scopes for the requested command. It might be the case that you created a user without adding it to any groups. See  Adding/removing users and scopes  above.   Back to Table of Contents",
            "title": "Errors "
        },
        {
            "location": "/director-users-uaa-perms/",
            "text": "All UAA users can log into all Directors which can verify the access token. However, user actions will be limited based on the presence of the following scopes in their UAA token:\n\n\nWarning: If you use the same private key to sign keys on different UAAs, users might obtain a token from one UAA and use it on the Director configured with a different UAA. It is therefore highly recommended to lock down scopes to individual Directors and not re-use your private key used for signing on the UAA.\n\n\n\nAnonymous \n\u00b6\n\n\nCan access:\n\n\n\n\nbosh status\n: show general information about targeted Director (authentication is not required)\n\n\n\n\n\n\nFull Admin \n\u00b6\n\n\nScopes:\n\n\n\n\nbosh.admin\n: user has admin access on any Director\n\n\nbosh.<DIRECTOR-UUID>.admin\n: user has admin access on the Director with the corresponding UUID\n\n\n\n\nCan use all commands on all deployments.\n\n\n\n\nFull Read-only \n\u00b6\n\n\nScopes:\n\n\n\n\nbosh.read\n: user has read access on any Director\n\n\nbosh.<DIRECTOR-UUID>.read\n: user has read access on the Director with the corresponding UUID\n\n\n\n\nCannot modify any resource.\n\n\nCan access in read-only capacity:\n\n\n\n\nbosh deployments\n: list of \nall\n deployments and releases/stemcells used\n\n\nbosh releases\n: list of \nall\n releases and their versions\n\n\nbosh stemcells\n: list of \nall\n stemcells and their versions\n\n\nbosh vms\n: list of all VMs which includes job names, IPs, vitals, details, etc.\n\n\nbosh tasks\n: list of all tasks summaries which includes task descriptions without access to debug logs\n\n\n\n\n\n\nTeam Admin \n\u00b6\n\n\nNote: This feature is available with bosh-release v255.4+.\n\n\n\nThe Director has a concept of a team so that set of users can only manage specific deployments. When a user creates a deployment, created deployment will be \nmanaged\n by the teams that that user is part of. There is currently no way to assign or reassign deployment's teams.\n\n\nScopes:\n\n\n\n\nbosh.teams.<team>.admin\n: user has admin access for deployments managed by the team\n\n\n\n\nCan modify team managed deployments' associated resources:\n\n\n\n\nbosh deploy\n: create or update deployment\n\n\nbosh delete deployment\n: delete deployment\n\n\nbosh start/stop/recreate\n: manage VMs\n\n\nbosh cck\n: diagnose deployment problems\n\n\nbosh ssh\n: SSH into a VM\n\n\nbosh logs\n: fetch logs from a VM\n\n\nbosh run errand\n: run an errand\n\n\n\n\nCan view shared resources:\n\n\n\n\nbosh deployments\n: list of team managed deployments and releases/stemcells used\n\n\nbosh releases\n: list of \nall\n releases and their versions\n\n\nbosh stemcells\n: list of \nall\n stemcells and their versions\n\n\nbosh vms\n: list of team managed deployments' VMs which includes job names, IPs, vitals, details, etc.\n\n\nbosh tasks\n: list of team managed deployments' tasks and their full details\n\n\n\n\nTeam admin cannot upload releases and stemcells.\n\n\n\n\nStemcell uploader \n\u00b6\n\n\nNote: This feature is available with bosh-release v261.2+.\n\n\n\nScopes:\n\n\n\n\nbosh.stemcells.upload\n: user can upload new stemcells\n\n\n\n\nNote that CLI will try to list stemcells before uploading given stemcell, hence \nbosh upload stemcell\n CLI command requires users/clients to have \nbosh.read\n scope as well.\n\n\n\n\nRelease uploader \n\u00b6\n\n\nNote: This feature is available with bosh-release v261.2+.\n\n\n\nScopes:\n\n\n\n\nbosh.releases.upload\n: user can upload new releases\n\n\n\n\nNote that CLI will try to list releases before uploading given release, hence \nbosh upload release\n CLI command requires users/clients to have \nbosh.read\n scope as well.\n\n\n\n\nErrors \n\u00b6\n\n\nHTTP 401: Not authorized: '/deployments' requires one of the scopes: bosh.admin, bosh.UUID.admin, bosh.read, bosh.UUID.read\n\n\n\n\nThis error occurs if the user doesn't have the right scopes for the requested command.\n\n\n\n\nBack to Table of Contents",
            "title": "Using Teams"
        },
        {
            "location": "/director-users-uaa-perms/#anonymous",
            "text": "Can access:   bosh status : show general information about targeted Director (authentication is not required)",
            "title": "Anonymous "
        },
        {
            "location": "/director-users-uaa-perms/#full-admin",
            "text": "Scopes:   bosh.admin : user has admin access on any Director  bosh.<DIRECTOR-UUID>.admin : user has admin access on the Director with the corresponding UUID   Can use all commands on all deployments.",
            "title": "Full Admin "
        },
        {
            "location": "/director-users-uaa-perms/#full-read-only",
            "text": "Scopes:   bosh.read : user has read access on any Director  bosh.<DIRECTOR-UUID>.read : user has read access on the Director with the corresponding UUID   Cannot modify any resource.  Can access in read-only capacity:   bosh deployments : list of  all  deployments and releases/stemcells used  bosh releases : list of  all  releases and their versions  bosh stemcells : list of  all  stemcells and their versions  bosh vms : list of all VMs which includes job names, IPs, vitals, details, etc.  bosh tasks : list of all tasks summaries which includes task descriptions without access to debug logs",
            "title": "Full Read-only "
        },
        {
            "location": "/director-users-uaa-perms/#team-admin",
            "text": "Note: This feature is available with bosh-release v255.4+.  The Director has a concept of a team so that set of users can only manage specific deployments. When a user creates a deployment, created deployment will be  managed  by the teams that that user is part of. There is currently no way to assign or reassign deployment's teams.  Scopes:   bosh.teams.<team>.admin : user has admin access for deployments managed by the team   Can modify team managed deployments' associated resources:   bosh deploy : create or update deployment  bosh delete deployment : delete deployment  bosh start/stop/recreate : manage VMs  bosh cck : diagnose deployment problems  bosh ssh : SSH into a VM  bosh logs : fetch logs from a VM  bosh run errand : run an errand   Can view shared resources:   bosh deployments : list of team managed deployments and releases/stemcells used  bosh releases : list of  all  releases and their versions  bosh stemcells : list of  all  stemcells and their versions  bosh vms : list of team managed deployments' VMs which includes job names, IPs, vitals, details, etc.  bosh tasks : list of team managed deployments' tasks and their full details   Team admin cannot upload releases and stemcells.",
            "title": "Team Admin "
        },
        {
            "location": "/director-users-uaa-perms/#stemcell-uploader",
            "text": "Note: This feature is available with bosh-release v261.2+.  Scopes:   bosh.stemcells.upload : user can upload new stemcells   Note that CLI will try to list stemcells before uploading given stemcell, hence  bosh upload stemcell  CLI command requires users/clients to have  bosh.read  scope as well.",
            "title": "Stemcell uploader "
        },
        {
            "location": "/director-users-uaa-perms/#release-uploader",
            "text": "Note: This feature is available with bosh-release v261.2+.  Scopes:   bosh.releases.upload : user can upload new releases   Note that CLI will try to list releases before uploading given release, hence  bosh upload release  CLI command requires users/clients to have  bosh.read  scope as well.",
            "title": "Release uploader "
        },
        {
            "location": "/director-users-uaa-perms/#errors",
            "text": "HTTP 401: Not authorized: '/deployments' requires one of the scopes: bosh.admin, bosh.UUID.admin, bosh.read, bosh.UUID.read  This error occurs if the user doesn't have the right scopes for the requested command.   Back to Table of Contents",
            "title": "Errors "
        },
        {
            "location": "/director-certs/",
            "text": "Note: See \nDirector SSL Certificate Configuration with OpenSSL\n if you prefer to generate certs with OpenSSL config.\n\n\n\nDepending on you configuration, there are up to three endpoints to be secured using SSL certificates: The Director, the UAA, and the SAML Service Provider on the UAA.\n\n\nNote: If you are using the UAA for user management, an SSL certificate is mandatory for the Director and the UAA.\n\n\n\nNote: Unless you are using a configuration server, your SSL certificates will be stored in the Director's database.\n\n\n\nGenerate SSL certificates \n\u00b6\n\n\nYou can use CLI v2 \ninterpolate\n command to generate self signed certificates. Even if you use CLI v2 to generate certificates, you can still continue using CLI v1 with the Director.\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \ndefault_ca\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nis_ca\n:\n \ntrue\n\n    \ncommon_name\n:\n \nbosh_ca\n\n\n-\n \nname\n:\n \ndirector_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ndefault_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n-\n \nname\n:\n \nuaa_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ndefault_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n-\n \nname\n:\n \nuaa_service_provider_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ndefault_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n\n\n\n$ bosh interpolate tpl.yml -v \ninternal_ip\n=\n10\n.244.4.2 --vars-store certs.yml\n$ cat certs.yml\n\n\n\n\nConfigure the Director to use certificates \n\u00b6\n\n\nUpdate the Director deployment manifest:\n\n\n\n\ndirector.ssl.key\n\n\nPrivate key for the Director (content of \nbosh int certs.yml --path /director_ssl/private_key\n)\n\n\n\n\n\n\ndirector.ssl.cert\n\n\nAssociated certificate for the Director (content of \nbosh int certs.yml --path /director_ssl/certificate\n)\n\n\nInclude all intermediate certificates if necessary\n\n\n\n\n\n\nhm.director_account.ca_cert\n\n\nCA certificate used by the HM to verify the Director's certificate (content of \nbosh int certs.yml --path /director_ssl/ca\n)\n\n\n\n\n\n\n\n\nExample manifest excerpt:\n\n\n...\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \nproperties\n:\n\n    \ndirector\n:\n\n      \nssl\n:\n\n        \nkey\n:\n \n|\n\n          \n-----BEGIN RSA PRIVATE KEY-----\n\n          \nMII...\n\n          \n-----END RSA PRIVATE KEY-----\n\n        \ncert\n:\n \n|\n\n          \n-----BEGIN CERTIFICATE-----\n\n          \nMII...\n\n          \n-----END CERTIFICATE-----\n\n\n...\n\n\n\n\n\nNote: A `path` to the key or certificate file is not supported.\n\n\n\nIf you are using the UAA for user management, additionally put certificates in these properties:\n\n\n\n\nuaa.sslPrivateKey\n\n\nPrivate key for the UAA (content of \nbosh int certs.yml --path /uaa_ssl/private_key\n)\n\n\n\n\n\n\nuaa.sslCertificate\n\n\nAssociated certificate for the UAA (content of \nbosh int certs.yml --path /uaa_ssl/certificate\n)\n\n\nInclude all intermediate certificates if necessary\n\n\n\n\n\n\nlogin.saml.serviceProviderKey\n\n\nPrivate key for the UAA (content of \nbosh int certs.yml --path /uaa_service_provider_ssl/private_key\n)\n\n\n\n\n\n\nlogin.saml.serviceProviderCertificate\n\n\nAssociated certificate for the UAA (content of \nbosh int certs.yml --path /uaa_service_provider_ssl/certificate\n)\n\n\n\n\n\n\n\n\n\n\nTarget the Director \n\u00b6\n\n\nAfter you deployed your Director with the above changes, you need to specify \n--ca-cert\n when targeting the Director:\n\n\n$ bosh --ca-cert <\n(\nbosh int certs.yml --path /director_ssl/ca\n)\n target \n10\n.244.4.2\n\n\n\n\nNote: If your certificates are trusted via system installed CA certificates, there is no need to provide `--ca-cert` option.\n\n\n\n\n\nBack to Table of Contents",
            "title": "Configuring SSL Certificates"
        },
        {
            "location": "/director-certs/#generate-ssl-certificates",
            "text": "You can use CLI v2  interpolate  command to generate self signed certificates. Even if you use CLI v2 to generate certificates, you can still continue using CLI v1 with the Director.  variables :  -   name :   default_ca \n   type :   certificate \n   options : \n     is_ca :   true \n     common_name :   bosh_ca  -   name :   director_ssl \n   type :   certificate \n   options : \n     ca :   default_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]  -   name :   uaa_ssl \n   type :   certificate \n   options : \n     ca :   default_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]  -   name :   uaa_service_provider_ssl \n   type :   certificate \n   options : \n     ca :   default_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]   $ bosh interpolate tpl.yml -v  internal_ip = 10 .244.4.2 --vars-store certs.yml\n$ cat certs.yml",
            "title": "Generate SSL certificates "
        },
        {
            "location": "/director-certs/#configure-the-director-to-use-certificates",
            "text": "Update the Director deployment manifest:   director.ssl.key  Private key for the Director (content of  bosh int certs.yml --path /director_ssl/private_key )    director.ssl.cert  Associated certificate for the Director (content of  bosh int certs.yml --path /director_ssl/certificate )  Include all intermediate certificates if necessary    hm.director_account.ca_cert  CA certificate used by the HM to verify the Director's certificate (content of  bosh int certs.yml --path /director_ssl/ca )     Example manifest excerpt:  ...  jobs :  -   name :   bosh \n   properties : \n     director : \n       ssl : \n         key :   | \n           -----BEGIN RSA PRIVATE KEY----- \n           MII... \n           -----END RSA PRIVATE KEY----- \n         cert :   | \n           -----BEGIN CERTIFICATE----- \n           MII... \n           -----END CERTIFICATE-----  ...   Note: A `path` to the key or certificate file is not supported.  If you are using the UAA for user management, additionally put certificates in these properties:   uaa.sslPrivateKey  Private key for the UAA (content of  bosh int certs.yml --path /uaa_ssl/private_key )    uaa.sslCertificate  Associated certificate for the UAA (content of  bosh int certs.yml --path /uaa_ssl/certificate )  Include all intermediate certificates if necessary    login.saml.serviceProviderKey  Private key for the UAA (content of  bosh int certs.yml --path /uaa_service_provider_ssl/private_key )    login.saml.serviceProviderCertificate  Associated certificate for the UAA (content of  bosh int certs.yml --path /uaa_service_provider_ssl/certificate )",
            "title": "Configure the Director to use certificates "
        },
        {
            "location": "/director-certs/#target-the-director",
            "text": "After you deployed your Director with the above changes, you need to specify  --ca-cert  when targeting the Director:  $ bosh --ca-cert < ( bosh int certs.yml --path /director_ssl/ca )  target  10 .244.4.2  Note: If your certificates are trusted via system installed CA certificates, there is no need to provide `--ca-cert` option.   Back to Table of Contents",
            "title": "Target the Director "
        },
        {
            "location": "/director-configure-db/",
            "text": "The Director stores VM, persistent disk and other information in a database. An internal database might be sufficient for your deployment; however, a highly-available external database can improve performance, scalability and protect against data loss.\n\n\nIncluded Postgres (default) \n\u00b6\n\n\n\n\n\n\nAdd postgres release job and make sure that persistent disk is enabled:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \npostgres\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n25_000\n\n  \n# ...\n\n\n\n\n\n\n\n\n\nConfigure postgres job, and let the Director and the Registry (if configured) use the database:\n\n\nproperties\n:\n\n  \npostgres\n:\n \n&database\n\n    \nhost\n:\n \n127.0.0.1\n\n    \nuser\n:\n \npostgres\n\n    \npassword\n:\n \npostgres-password\n\n    \ndatabase\n:\n \nbosh\n\n    \nadapter\n:\n \npostgres\n\n\n  \ndirector\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n  \nregistry\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n\n\n\n\n\n\n\n\n\nExternal \n\u00b6\n\n\nThe Director is tested to be compatible with MySQL and Postgresql databases.\n\n\n\n\n\n\nModify deployment manifest for the Director\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \ndb\n:\n \n&database\n\n      \nhost\n:\n \nDB-HOST\n\n      \nport\n:\n \nDB-PORT\n\n      \nuser\n:\n \nDB-USER\n\n      \npassword\n:\n \nDB-PASSWORD\n\n      \ndatabase\n:\n \nbosh\n\n      \nadapter\n:\n \npostgres\n\n\n  \nregistry\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n\n\n\nSee \ndirector.db job configuration\n for more details.",
            "title": "Builtin PostgreSQL"
        },
        {
            "location": "/director-configure-db/#included-postgres-default",
            "text": "Add postgres release job and make sure that persistent disk is enabled:  jobs :  -   name :   bosh \n   templates : \n   -   { name :   postgres ,   release :   bosh } \n   # ... \n   persistent_disk :   25_000 \n   # ...     Configure postgres job, and let the Director and the Registry (if configured) use the database:  properties : \n   postgres :   &database \n     host :   127.0.0.1 \n     user :   postgres \n     password :   postgres-password \n     database :   bosh \n     adapter :   postgres \n\n   director : \n     db :   *database \n     # ... \n\n   registry : \n     db :   *database \n     # ...",
            "title": "Included Postgres (default) "
        },
        {
            "location": "/director-configure-db/#external",
            "text": "The Director is tested to be compatible with MySQL and Postgresql databases.    Modify deployment manifest for the Director  properties : \n   director : \n     db :   &database \n       host :   DB-HOST \n       port :   DB-PORT \n       user :   DB-USER \n       password :   DB-PASSWORD \n       database :   bosh \n       adapter :   postgres \n\n   registry : \n     db :   *database \n     # ...   See  director.db job configuration  for more details.",
            "title": "External "
        },
        {
            "location": "/director-configure-db/",
            "text": "The Director stores VM, persistent disk and other information in a database. An internal database might be sufficient for your deployment; however, a highly-available external database can improve performance, scalability and protect against data loss.\n\n\nIncluded Postgres (default) \n\u00b6\n\n\n\n\n\n\nAdd postgres release job and make sure that persistent disk is enabled:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \npostgres\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n25_000\n\n  \n# ...\n\n\n\n\n\n\n\n\n\nConfigure postgres job, and let the Director and the Registry (if configured) use the database:\n\n\nproperties\n:\n\n  \npostgres\n:\n \n&database\n\n    \nhost\n:\n \n127.0.0.1\n\n    \nuser\n:\n \npostgres\n\n    \npassword\n:\n \npostgres-password\n\n    \ndatabase\n:\n \nbosh\n\n    \nadapter\n:\n \npostgres\n\n\n  \ndirector\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n  \nregistry\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n\n\n\n\n\n\n\n\n\nExternal \n\u00b6\n\n\nThe Director is tested to be compatible with MySQL and Postgresql databases.\n\n\n\n\n\n\nModify deployment manifest for the Director\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \ndb\n:\n \n&database\n\n      \nhost\n:\n \nDB-HOST\n\n      \nport\n:\n \nDB-PORT\n\n      \nuser\n:\n \nDB-USER\n\n      \npassword\n:\n \nDB-PASSWORD\n\n      \ndatabase\n:\n \nbosh\n\n      \nadapter\n:\n \npostgres\n\n\n  \nregistry\n:\n\n    \ndb\n:\n \n*database\n\n    \n# ...\n\n\n\n\n\nSee \ndirector.db job configuration\n for more details.",
            "title": "External MySQL"
        },
        {
            "location": "/director-configure-db/#included-postgres-default",
            "text": "Add postgres release job and make sure that persistent disk is enabled:  jobs :  -   name :   bosh \n   templates : \n   -   { name :   postgres ,   release :   bosh } \n   # ... \n   persistent_disk :   25_000 \n   # ...     Configure postgres job, and let the Director and the Registry (if configured) use the database:  properties : \n   postgres :   &database \n     host :   127.0.0.1 \n     user :   postgres \n     password :   postgres-password \n     database :   bosh \n     adapter :   postgres \n\n   director : \n     db :   *database \n     # ... \n\n   registry : \n     db :   *database \n     # ...",
            "title": "Included Postgres (default) "
        },
        {
            "location": "/director-configure-db/#external",
            "text": "The Director is tested to be compatible with MySQL and Postgresql databases.    Modify deployment manifest for the Director  properties : \n   director : \n     db :   &database \n       host :   DB-HOST \n       port :   DB-PORT \n       user :   DB-USER \n       password :   DB-PASSWORD \n       database :   bosh \n       adapter :   postgres \n\n   registry : \n     db :   *database \n     # ...   See  director.db job configuration  for more details.",
            "title": "External "
        },
        {
            "location": "/director-configure-blobstore/",
            "text": "The Director stores uploaded releases, configuration files, logs and other data in a blobstore. A default DAV blobstore is sufficient for most BOSH environments; however, a highly-available external blobstore may be desired.\n\n\nIncluded DAV (default) \n\u00b6\n\n\nBy default the Director is configured to use included DAV blobstore job (see \nInstalling BOSH section\n for example manifests). Here is how to configure it:\n\n\n\n\n\n\nAdd blobstore release job and make sure that persistent disk is enabled:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nblobstore\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n25_000\n\n  \n# ...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n    \nstatic_ips\n:\n \n[\n10.0.0.6\n]\n\n\n\n\n\n\n\n\n\nConfigure blobstore job. The blobstore's address must be reachable by the Agents:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ndav\n\n    \naddress\n:\n \n10.0.0.6\n\n    \nport\n:\n \n25250\n\n    \ndirector\n:\n\n      \nuser\n:\n \ndirector\n\n      \npassword\n:\n \nDIRECTOR-PASSWORD\n\n    \nagent\n:\n\n      \nuser\n:\n \nagent\n\n      \npassword\n:\n \nAGENT-PASSWORD\n\n\n\n\n\n\n\n\n\nAbove configuration is used by the Director and the Agents.\n\n\n\n\nS3 \n\u00b6\n\n\nThe Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a \nprivate\n S3 bucket\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify S3 credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n\n\n\n\n\n\n\n\nFor an S3 compatible blobstore you need to additionally specify the host:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nhost\n:\n \nobjects.dreamhost.com\n\n\n\n\n\n\n\n\n\n\n\nGoogle Cloud Storage (GCS) \n\u00b6\n\n\nNote: Available in bosh release v263+ and Linux stemcells 3450+.\n\n\n\nThe Director and the Agents can use GCS as a blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a GCS bucket\n.\n\n\n\n\n\n\nFollow the steps on how to create service accounts and configure them with the \nminimum set of permissions\n.\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify GCS credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use \nCustomer Supplied Encryption Keys\n\nto encrypt blobstore contents instead of server-side encryption keys, specify \nencryption_key\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nencryption_key\n:\n \nBASE64-ENCODED-32-BYTES\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use an explicit \nStorage Class\n\nto store blobstore contents instead of the bucket default, specify \nstorage_class\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nstorage_class\n:\n \nREGIONAL\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Builtin DAV Server"
        },
        {
            "location": "/director-configure-blobstore/#included-dav-default",
            "text": "By default the Director is configured to use included DAV blobstore job (see  Installing BOSH section  for example manifests). Here is how to configure it:    Add blobstore release job and make sure that persistent disk is enabled:  jobs :  -   name :   bosh \n   templates : \n   -   { name :   blobstore ,   release :   bosh } \n   # ... \n   persistent_disk :   25_000 \n   # ... \n   networks : \n   -   name :   default \n     static_ips :   [ 10.0.0.6 ]     Configure blobstore job. The blobstore's address must be reachable by the Agents:  properties : \n   blobstore : \n     provider :   dav \n     address :   10.0.0.6 \n     port :   25250 \n     director : \n       user :   director \n       password :   DIRECTOR-PASSWORD \n     agent : \n       user :   agent \n       password :   AGENT-PASSWORD     Above configuration is used by the Director and the Agents.",
            "title": "Included DAV (default) "
        },
        {
            "location": "/director-configure-blobstore/#s3",
            "text": "The Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:    Create a  private  S3 bucket    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify S3 credentials and bucket name:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket     For an S3 compatible blobstore you need to additionally specify the host:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket \n     host :   objects.dreamhost.com",
            "title": "S3 "
        },
        {
            "location": "/director-configure-blobstore/#google-cloud-storage-gcs",
            "text": "Note: Available in bosh release v263+ and Linux stemcells 3450+.  The Director and the Agents can use GCS as a blobstore. Here is how to configure it:    Create a GCS bucket .    Follow the steps on how to create service accounts and configure them with the  minimum set of permissions .    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify GCS credentials and bucket name:  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n   agent : \n     blobstore : \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use  Customer Supplied Encryption Keys \nto encrypt blobstore contents instead of server-side encryption keys, specify  encryption_key :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     encryption_key :   BASE64-ENCODED-32-BYTES \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use an explicit  Storage Class \nto store blobstore contents instead of the bucket default, specify  storage_class :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     storage_class :   REGIONAL \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Google Cloud Storage (GCS) "
        },
        {
            "location": "/director-configure-blobstore/",
            "text": "The Director stores uploaded releases, configuration files, logs and other data in a blobstore. A default DAV blobstore is sufficient for most BOSH environments; however, a highly-available external blobstore may be desired.\n\n\nIncluded DAV (default) \n\u00b6\n\n\nBy default the Director is configured to use included DAV blobstore job (see \nInstalling BOSH section\n for example manifests). Here is how to configure it:\n\n\n\n\n\n\nAdd blobstore release job and make sure that persistent disk is enabled:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nblobstore\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n25_000\n\n  \n# ...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n    \nstatic_ips\n:\n \n[\n10.0.0.6\n]\n\n\n\n\n\n\n\n\n\nConfigure blobstore job. The blobstore's address must be reachable by the Agents:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ndav\n\n    \naddress\n:\n \n10.0.0.6\n\n    \nport\n:\n \n25250\n\n    \ndirector\n:\n\n      \nuser\n:\n \ndirector\n\n      \npassword\n:\n \nDIRECTOR-PASSWORD\n\n    \nagent\n:\n\n      \nuser\n:\n \nagent\n\n      \npassword\n:\n \nAGENT-PASSWORD\n\n\n\n\n\n\n\n\n\nAbove configuration is used by the Director and the Agents.\n\n\n\n\nS3 \n\u00b6\n\n\nThe Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a \nprivate\n S3 bucket\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify S3 credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n\n\n\n\n\n\n\n\nFor an S3 compatible blobstore you need to additionally specify the host:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nhost\n:\n \nobjects.dreamhost.com\n\n\n\n\n\n\n\n\n\n\n\nGoogle Cloud Storage (GCS) \n\u00b6\n\n\nNote: Available in bosh release v263+ and Linux stemcells 3450+.\n\n\n\nThe Director and the Agents can use GCS as a blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a GCS bucket\n.\n\n\n\n\n\n\nFollow the steps on how to create service accounts and configure them with the \nminimum set of permissions\n.\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify GCS credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use \nCustomer Supplied Encryption Keys\n\nto encrypt blobstore contents instead of server-side encryption keys, specify \nencryption_key\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nencryption_key\n:\n \nBASE64-ENCODED-32-BYTES\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use an explicit \nStorage Class\n\nto store blobstore contents instead of the bucket default, specify \nstorage_class\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nstorage_class\n:\n \nREGIONAL\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Amazon S3"
        },
        {
            "location": "/director-configure-blobstore/#included-dav-default",
            "text": "By default the Director is configured to use included DAV blobstore job (see  Installing BOSH section  for example manifests). Here is how to configure it:    Add blobstore release job and make sure that persistent disk is enabled:  jobs :  -   name :   bosh \n   templates : \n   -   { name :   blobstore ,   release :   bosh } \n   # ... \n   persistent_disk :   25_000 \n   # ... \n   networks : \n   -   name :   default \n     static_ips :   [ 10.0.0.6 ]     Configure blobstore job. The blobstore's address must be reachable by the Agents:  properties : \n   blobstore : \n     provider :   dav \n     address :   10.0.0.6 \n     port :   25250 \n     director : \n       user :   director \n       password :   DIRECTOR-PASSWORD \n     agent : \n       user :   agent \n       password :   AGENT-PASSWORD     Above configuration is used by the Director and the Agents.",
            "title": "Included DAV (default) "
        },
        {
            "location": "/director-configure-blobstore/#s3",
            "text": "The Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:    Create a  private  S3 bucket    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify S3 credentials and bucket name:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket     For an S3 compatible blobstore you need to additionally specify the host:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket \n     host :   objects.dreamhost.com",
            "title": "S3 "
        },
        {
            "location": "/director-configure-blobstore/#google-cloud-storage-gcs",
            "text": "Note: Available in bosh release v263+ and Linux stemcells 3450+.  The Director and the Agents can use GCS as a blobstore. Here is how to configure it:    Create a GCS bucket .    Follow the steps on how to create service accounts and configure them with the  minimum set of permissions .    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify GCS credentials and bucket name:  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n   agent : \n     blobstore : \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use  Customer Supplied Encryption Keys \nto encrypt blobstore contents instead of server-side encryption keys, specify  encryption_key :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     encryption_key :   BASE64-ENCODED-32-BYTES \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use an explicit  Storage Class \nto store blobstore contents instead of the bucket default, specify  storage_class :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     storage_class :   REGIONAL \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Google Cloud Storage (GCS) "
        },
        {
            "location": "/director-configure-blobstore/",
            "text": "The Director stores uploaded releases, configuration files, logs and other data in a blobstore. A default DAV blobstore is sufficient for most BOSH environments; however, a highly-available external blobstore may be desired.\n\n\nIncluded DAV (default) \n\u00b6\n\n\nBy default the Director is configured to use included DAV blobstore job (see \nInstalling BOSH section\n for example manifests). Here is how to configure it:\n\n\n\n\n\n\nAdd blobstore release job and make sure that persistent disk is enabled:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nblobstore\n,\n \nrelease\n:\n \nbosh\n}\n\n  \n# ...\n\n  \npersistent_disk\n:\n \n25_000\n\n  \n# ...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n    \nstatic_ips\n:\n \n[\n10.0.0.6\n]\n\n\n\n\n\n\n\n\n\nConfigure blobstore job. The blobstore's address must be reachable by the Agents:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ndav\n\n    \naddress\n:\n \n10.0.0.6\n\n    \nport\n:\n \n25250\n\n    \ndirector\n:\n\n      \nuser\n:\n \ndirector\n\n      \npassword\n:\n \nDIRECTOR-PASSWORD\n\n    \nagent\n:\n\n      \nuser\n:\n \nagent\n\n      \npassword\n:\n \nAGENT-PASSWORD\n\n\n\n\n\n\n\n\n\nAbove configuration is used by the Director and the Agents.\n\n\n\n\nS3 \n\u00b6\n\n\nThe Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a \nprivate\n S3 bucket\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify S3 credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n\n\n\n\n\n\n\n\nFor an S3 compatible blobstore you need to additionally specify the host:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ns3\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nhost\n:\n \nobjects.dreamhost.com\n\n\n\n\n\n\n\n\n\n\n\nGoogle Cloud Storage (GCS) \n\u00b6\n\n\nNote: Available in bosh release v263+ and Linux stemcells 3450+.\n\n\n\nThe Director and the Agents can use GCS as a blobstore. Here is how to configure it:\n\n\n\n\n\n\nCreate a GCS bucket\n.\n\n\n\n\n\n\nFollow the steps on how to create service accounts and configure them with the \nminimum set of permissions\n.\n\n\n\n\n\n\nEnsure that access to the bucket is protected, as the Director may store sensitive information.\n\n\n\n\n\n\nModify deployment manifest for the Director and specify GCS credentials and bucket name:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use \nCustomer Supplied Encryption Keys\n\nto encrypt blobstore contents instead of server-side encryption keys, specify \nencryption_key\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nencryption_key\n:\n \nBASE64-ENCODED-32-BYTES\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE\n\n\n\n\n\n\n\n\n\nTo use an explicit \nStorage Class\n\nto store blobstore contents instead of the bucket default, specify \nstorage_class\n:\n\n\nproperties\n:\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \ngcs\n\n    \ncredentials_source\n:\n \nstatic\n\n    \njson_key\n:\n \n|\n\n      \nDIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE\n\n    \nbucket_name\n:\n \ntest-bosh-bucket\n\n    \nstorage_class\n:\n \nREGIONAL\n\n  \nagent\n:\n\n    \nblobstore\n:\n\n      \ncredentials_source\n:\n \nstatic\n\n      \njson_key\n:\n \n|\n\n        \nAGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Google Cloud Storage"
        },
        {
            "location": "/director-configure-blobstore/#included-dav-default",
            "text": "By default the Director is configured to use included DAV blobstore job (see  Installing BOSH section  for example manifests). Here is how to configure it:    Add blobstore release job and make sure that persistent disk is enabled:  jobs :  -   name :   bosh \n   templates : \n   -   { name :   blobstore ,   release :   bosh } \n   # ... \n   persistent_disk :   25_000 \n   # ... \n   networks : \n   -   name :   default \n     static_ips :   [ 10.0.0.6 ]     Configure blobstore job. The blobstore's address must be reachable by the Agents:  properties : \n   blobstore : \n     provider :   dav \n     address :   10.0.0.6 \n     port :   25250 \n     director : \n       user :   director \n       password :   DIRECTOR-PASSWORD \n     agent : \n       user :   agent \n       password :   AGENT-PASSWORD     Above configuration is used by the Director and the Agents.",
            "title": "Included DAV (default) "
        },
        {
            "location": "/director-configure-blobstore/#s3",
            "text": "The Director and the Agents can use an S3 compatible blobstore. Here is how to configure it:    Create a  private  S3 bucket    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify S3 credentials and bucket name:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket     For an S3 compatible blobstore you need to additionally specify the host:  properties : \n   blobstore : \n     provider :   s3 \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     bucket_name :   test-bosh-bucket \n     host :   objects.dreamhost.com",
            "title": "S3 "
        },
        {
            "location": "/director-configure-blobstore/#google-cloud-storage-gcs",
            "text": "Note: Available in bosh release v263+ and Linux stemcells 3450+.  The Director and the Agents can use GCS as a blobstore. Here is how to configure it:    Create a GCS bucket .    Follow the steps on how to create service accounts and configure them with the  minimum set of permissions .    Ensure that access to the bucket is protected, as the Director may store sensitive information.    Modify deployment manifest for the Director and specify GCS credentials and bucket name:  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n   agent : \n     blobstore : \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use  Customer Supplied Encryption Keys \nto encrypt blobstore contents instead of server-side encryption keys, specify  encryption_key :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     encryption_key :   BASE64-ENCODED-32-BYTES \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE     To use an explicit  Storage Class \nto store blobstore contents instead of the bucket default, specify  storage_class :  properties : \n   blobstore : \n     provider :   gcs \n     credentials_source :   static \n     json_key :   | \n       DIRECTOR-BLOBSTORE-SERVICE-ACCOUNT-FILE \n     bucket_name :   test-bosh-bucket \n     storage_class :   REGIONAL \n   agent : \n     blobstore : \n       credentials_source :   static \n       json_key :   | \n         AGENT-SERVICE-ACCOUNT-BLOBSTORE-FILE",
            "title": "Google Cloud Storage (GCS) "
        },
        {
            "location": "/managing-releases/",
            "text": "(See \nWhat is a Release?\n and \nUploading releases\n for an introduction.)\n\n\n\n\nJobs and Packages \n\u00b6\n\n\nEach job and package is uniquely identified by its name and fingerprint. A fingerprint is calculated based on the contents of all associated files, their permissions. A release captures set of job and package versions that depend on each other and gives it a name and a version. The CLI also records additional metadata when creating releases such as Git SHA.\n\n\nFor example here is a release description (e.g. \nreleases/zookeeper/zookeeper-0.0.5.yml\n) in an example \nZookeeper release\n:\n\n\nname\n:\n \nzookeeper\n\n\nversion\n:\n \n0.0.5\n\n\ncommit_hash\n:\n \n9f0bb43\n\n\nuncommitted_changes\n:\n \nfalse\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nsmoke-tests\n\n  \nversion\n:\n \n840b14bc609483bb03cf87a938bc69e76a6e2d88\n\n  \nfingerprint\n:\n \n840b14bc609483bb03cf87a938bc69e76a6e2d88\n\n  \nsha1\n:\n \nabafa9fc0c4d35fc818cc55438cbf19bd029a418\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nversion\n:\n \n2b29580fbc390762956826f4cb0d3517b6a01ca9\n\n  \nfingerprint\n:\n \n2b29580fbc390762956826f4cb0d3517b6a01ca9\n\n  \nsha1\n:\n \n8087993b361eee28ec779700c60ad26edaa37f0f\n\n\npackages\n:\n\n\n-\n \nname\n:\n \ngolang-1.7\n\n  \nversion\n:\n \n482e72c8435a11e1d1c3c25e4ee86ced53cc8739\n\n  \nfingerprint\n:\n \n482e72c8435a11e1d1c3c25e4ee86ced53cc8739\n\n  \nsha1\n:\n \n12917d086a9d92d208abfba279acd11aa627eec1\n\n  \ndependencies\n:\n \n[]\n\n\n-\n \nname\n:\n \njava\n\n  \nversion\n:\n \nc524e46e61b37894935ae28016973e0e8644fcde\n\n  \nfingerprint\n:\n \nc524e46e61b37894935ae28016973e0e8644fcde\n\n  \nsha1\n:\n \na1f0001e124f33ad6c9258e6113a3e730c7e82b9\n\n  \ndependencies\n:\n \n[]\n\n\n-\n \nname\n:\n \nsmoke-tests\n\n  \nversion\n:\n \n2a8864e206d64ac968c19c6883b7043cb8d3b880\n\n  \nfingerprint\n:\n \n2a8864e206d64ac968c19c6883b7043cb8d3b880\n\n  \nsha1\n:\n \n40d38f6c3cfa4712bfcdca15a67c459a1672a027\n\n  \ndependencies\n:\n\n  \n-\n \ngolang-1.7\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \nversion\n:\n \nca455273c83e828eb50a21d21811684eceda2603\n\n  \nfingerprint\n:\n \nca455273c83e828eb50a21d21811684eceda2603\n\n  \nsha1\n:\n \nd42e0023eb3d493e165ed0e36ae8643c8bfe535d\n\n  \ndependencies\n:\n \n[]\n\n\nlicense\n:\n\n  \nversion\n:\n \ne79e93c49714f52c0a231c78d480ea1ca757c8f9\n\n  \nfingerprint\n:\n \ne79e93c49714f52c0a231c78d480ea1ca757c8f9\n\n  \nsha1\n:\n \n09d28a6f4fc5b6725733add015e79928f7546a32\n\n\n\n\n\nJob and package names and fingerprints are used throughout the system to identify if certain actions need to be taken. Two primary uses are:\n\n\n\n\nto determine if packages need to be uploaded to the Director when uploading a release\n\n\nto determine if instances need to be updated during a \ndeployment procedure\n\n\n\n\n\n\nVersion uniqueness \n\u00b6\n\n\nThere exists an implicit trust between a release author and an operator that different release (a different set of jobs and packages) will not be published under the same version. The Director will reject new set of jobs and packages during the upload if it does not match with already uploaded contents.\n\n\nSometimes however two different releases (e.g. 1.0.1 and 1.0.3) may end up with exactly same set of jobs and packages. In fact running \nbosh create-release\n command\n a few times in a row will produce new releases with only version being different (e.g. 1.0.1+dev.0 and 1.0.1+dev.1). Operators should be aware that even if the deployment procedure shows release version changing there is a chance that no instances will be updated. This initially may be a surprising behaviour; however, given that the Director correctly determines that there are no changes to apply to the instances, there is really nothing to do. After the deploy is finished, \nbosh deployments\n command\n will of course state that new release is used.\n\n\n\n\nInspecting uploaded releases \n\u00b6\n\n\nOnce release is uploaded to the Director, it can be inspected via \nbosh inspect-release\n command\n. It will show names and fingerprints of jobs and packages. It will also show job details such as consumed and provided links.\n\n\n$ bosh -e vbox inspect-release zookeeper/0.0.5\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nJob                                                   Blobstore ID                          Digest                                    Links Consumed           Links Provided\nsmoke-tests/840b14bc609483bb03cf87a938bc69e76a6e2d88  98a1fb64-9851-4c28-bac5-df8a96d76449  abafa9fc0c4d35fc818cc55438cbf19bd029a418  - name: conn             -\n                                                                                                                                        type: zookeeper\nzookeeper/2b29580fbc390762956826f4cb0d3517b6a01ca9    97f299f8-7abf-4393-b3db-1bf33880d154  8087993b361eee28ec779700c60ad26edaa37f0f  - name: peers            - name: conn\n                                                                                                                                        type: zookeeper_peers    type: zookeeper\n                                                                                                                                                               - name: peers\n                                                                                                                                                                 type: zookeeper_peers\n\n\n2\n \njobs\n\n\nPackage                                               Compiled \nfor\n          Blobstore ID                          Digest\ngolang-1.7/482e72c8435a11e1d1c3c25e4ee86ced53cc8739   \n(\nsource\n)\n              d96c7916-d852-4e8a-ab48-404b0be1fdce  12917d086a9d92d208abfba279acd11aa627eec1\n~                                                     ubuntu-trusty/3421.4  bb02fe9d-9bc6-46ab-4319-8b68262c76cb  b8b55284bc386d279dce1056249f9f511b03e26a\njava/c524e46e61b37894935ae28016973e0e8644fcde         \n(\nsource\n)\n              68e0a834-3da4-4e74-a34e-0452cec61574  a1f0001e124f33ad6c9258e6113a3e730c7e82b9\n~                                                     ubuntu-trusty/3421.4  94c5f41b-1167-4a9e-5c25-e8e985baa5c8  e61c93539b557c7f8833dc73aa9dd84fd4a1c7b5\nsmoke-tests/2a8864e206d64ac968c19c6883b7043cb8d3b880  \n(\nsource\n)\n              18dad544-9a4d-4160-bf5c-bcfc70b103a4  40d38f6c3cfa4712bfcdca15a67c459a1672a027\n~                                                     ubuntu-trusty/3421.4  bb48ec91-af8c-4d29-6edd-fcbff4d322fe  52445ef444b0da7a14bf7bb05612c3cbc1de5e29\nzookeeper/ca455273c83e828eb50a21d21811684eceda2603    \n(\nsource\n)\n              99911f20-f806-41c6-bd33-8d34d7cd94e3  d42e0023eb3d493e165ed0e36ae8643c8bfe535d\n~                                                     ubuntu-trusty/3421.4  7c65efb0-9d99-4fe2-6283-157bfd231b18  b9c279eb3a99cc3bc27b6dfe1d6d372d2b86854e\n\n\n8\n packages\n\nSucceeded\n\n\n\n\nFor debugging command also shows blobstore information (ID and SHA1) for each job and package. The Director uses blobstore references when deploying jobs and compiling packages.\n\n\n\n\nFixing corrupted releases (experimental) \n\u00b6\n\n\nAssuming that somehow the Director blobstore loses referenced asset (job, source or compiled package), it's possible to fix the corrupted asset. \nbosh upload-release\n commmand\n provides a \n--fix\n flag which allows to reupload same release contents into the Director.\n\n\n\n\nCleaning up uploaded releases \n\u00b6\n\n\nOver time the Director accumulates releases, hence it uses more blobstore space. Releases could be deleted manually via \nbosh delete-release\n command or be cleaned up via \nbosh cleanup\n command\n.",
            "title": "Managing Releases"
        },
        {
            "location": "/managing-releases/#jobs-and-packages",
            "text": "Each job and package is uniquely identified by its name and fingerprint. A fingerprint is calculated based on the contents of all associated files, their permissions. A release captures set of job and package versions that depend on each other and gives it a name and a version. The CLI also records additional metadata when creating releases such as Git SHA.  For example here is a release description (e.g.  releases/zookeeper/zookeeper-0.0.5.yml ) in an example  Zookeeper release :  name :   zookeeper  version :   0.0.5  commit_hash :   9f0bb43  uncommitted_changes :   false  jobs :  -   name :   smoke-tests \n   version :   840b14bc609483bb03cf87a938bc69e76a6e2d88 \n   fingerprint :   840b14bc609483bb03cf87a938bc69e76a6e2d88 \n   sha1 :   abafa9fc0c4d35fc818cc55438cbf19bd029a418  -   name :   zookeeper \n   version :   2b29580fbc390762956826f4cb0d3517b6a01ca9 \n   fingerprint :   2b29580fbc390762956826f4cb0d3517b6a01ca9 \n   sha1 :   8087993b361eee28ec779700c60ad26edaa37f0f  packages :  -   name :   golang-1.7 \n   version :   482e72c8435a11e1d1c3c25e4ee86ced53cc8739 \n   fingerprint :   482e72c8435a11e1d1c3c25e4ee86ced53cc8739 \n   sha1 :   12917d086a9d92d208abfba279acd11aa627eec1 \n   dependencies :   []  -   name :   java \n   version :   c524e46e61b37894935ae28016973e0e8644fcde \n   fingerprint :   c524e46e61b37894935ae28016973e0e8644fcde \n   sha1 :   a1f0001e124f33ad6c9258e6113a3e730c7e82b9 \n   dependencies :   []  -   name :   smoke-tests \n   version :   2a8864e206d64ac968c19c6883b7043cb8d3b880 \n   fingerprint :   2a8864e206d64ac968c19c6883b7043cb8d3b880 \n   sha1 :   40d38f6c3cfa4712bfcdca15a67c459a1672a027 \n   dependencies : \n   -   golang-1.7  -   name :   zookeeper \n   version :   ca455273c83e828eb50a21d21811684eceda2603 \n   fingerprint :   ca455273c83e828eb50a21d21811684eceda2603 \n   sha1 :   d42e0023eb3d493e165ed0e36ae8643c8bfe535d \n   dependencies :   []  license : \n   version :   e79e93c49714f52c0a231c78d480ea1ca757c8f9 \n   fingerprint :   e79e93c49714f52c0a231c78d480ea1ca757c8f9 \n   sha1 :   09d28a6f4fc5b6725733add015e79928f7546a32   Job and package names and fingerprints are used throughout the system to identify if certain actions need to be taken. Two primary uses are:   to determine if packages need to be uploaded to the Director when uploading a release  to determine if instances need to be updated during a  deployment procedure",
            "title": "Jobs and Packages "
        },
        {
            "location": "/managing-releases/#version-uniqueness",
            "text": "There exists an implicit trust between a release author and an operator that different release (a different set of jobs and packages) will not be published under the same version. The Director will reject new set of jobs and packages during the upload if it does not match with already uploaded contents.  Sometimes however two different releases (e.g. 1.0.1 and 1.0.3) may end up with exactly same set of jobs and packages. In fact running  bosh create-release  command  a few times in a row will produce new releases with only version being different (e.g. 1.0.1+dev.0 and 1.0.1+dev.1). Operators should be aware that even if the deployment procedure shows release version changing there is a chance that no instances will be updated. This initially may be a surprising behaviour; however, given that the Director correctly determines that there are no changes to apply to the instances, there is really nothing to do. After the deploy is finished,  bosh deployments  command  will of course state that new release is used.",
            "title": "Version uniqueness "
        },
        {
            "location": "/managing-releases/#inspecting-uploaded-releases",
            "text": "Once release is uploaded to the Director, it can be inspected via  bosh inspect-release  command . It will show names and fingerprints of jobs and packages. It will also show job details such as consumed and provided links.  $ bosh -e vbox inspect-release zookeeper/0.0.5\nUsing environment  '192.168.56.6'  as  '?' \n\nJob                                                   Blobstore ID                          Digest                                    Links Consumed           Links Provided\nsmoke-tests/840b14bc609483bb03cf87a938bc69e76a6e2d88  98a1fb64-9851-4c28-bac5-df8a96d76449  abafa9fc0c4d35fc818cc55438cbf19bd029a418  - name: conn             -\n                                                                                                                                        type: zookeeper\nzookeeper/2b29580fbc390762956826f4cb0d3517b6a01ca9    97f299f8-7abf-4393-b3db-1bf33880d154  8087993b361eee28ec779700c60ad26edaa37f0f  - name: peers            - name: conn\n                                                                                                                                        type: zookeeper_peers    type: zookeeper\n                                                                                                                                                               - name: peers\n                                                                                                                                                                 type: zookeeper_peers 2   jobs \n\nPackage                                               Compiled  for           Blobstore ID                          Digest\ngolang-1.7/482e72c8435a11e1d1c3c25e4ee86ced53cc8739    ( source )               d96c7916-d852-4e8a-ab48-404b0be1fdce  12917d086a9d92d208abfba279acd11aa627eec1\n~                                                     ubuntu-trusty/3421.4  bb02fe9d-9bc6-46ab-4319-8b68262c76cb  b8b55284bc386d279dce1056249f9f511b03e26a\njava/c524e46e61b37894935ae28016973e0e8644fcde          ( source )               68e0a834-3da4-4e74-a34e-0452cec61574  a1f0001e124f33ad6c9258e6113a3e730c7e82b9\n~                                                     ubuntu-trusty/3421.4  94c5f41b-1167-4a9e-5c25-e8e985baa5c8  e61c93539b557c7f8833dc73aa9dd84fd4a1c7b5\nsmoke-tests/2a8864e206d64ac968c19c6883b7043cb8d3b880   ( source )               18dad544-9a4d-4160-bf5c-bcfc70b103a4  40d38f6c3cfa4712bfcdca15a67c459a1672a027\n~                                                     ubuntu-trusty/3421.4  bb48ec91-af8c-4d29-6edd-fcbff4d322fe  52445ef444b0da7a14bf7bb05612c3cbc1de5e29\nzookeeper/ca455273c83e828eb50a21d21811684eceda2603     ( source )               99911f20-f806-41c6-bd33-8d34d7cd94e3  d42e0023eb3d493e165ed0e36ae8643c8bfe535d\n~                                                     ubuntu-trusty/3421.4  7c65efb0-9d99-4fe2-6283-157bfd231b18  b9c279eb3a99cc3bc27b6dfe1d6d372d2b86854e 8  packages\n\nSucceeded  For debugging command also shows blobstore information (ID and SHA1) for each job and package. The Director uses blobstore references when deploying jobs and compiling packages.",
            "title": "Inspecting uploaded releases "
        },
        {
            "location": "/managing-releases/#fixing-corrupted-releases-experimental",
            "text": "Assuming that somehow the Director blobstore loses referenced asset (job, source or compiled package), it's possible to fix the corrupted asset.  bosh upload-release  commmand  provides a  --fix  flag which allows to reupload same release contents into the Director.",
            "title": "Fixing corrupted releases (experimental) "
        },
        {
            "location": "/managing-releases/#cleaning-up-uploaded-releases",
            "text": "Over time the Director accumulates releases, hence it uses more blobstore space. Releases could be deleted manually via  bosh delete-release  command or be cleaned up via  bosh cleanup  command .",
            "title": "Cleaning up uploaded releases "
        },
        {
            "location": "/managing-stemcells/",
            "text": "(See \nWhat is a Stemcell?\n and \nUploading stemcells\n for an introduction.)\n\n\n\n\nStemcell versioning \n\u00b6\n\n\nEach stemcell is uniquely identified by its name and version. Currently stemcell versions are in MAJOR.MINOR format. Major version is incremented when new features are added to stemcells (or any components that stemcells typically include such as BOSH Agent). Minor versions are incremented if certain security fixes and/or features are backported on top of existing stemcell line. We recommend to continiously bump to the latest major stemcell version to receive latest updates.\n\n\n\n\nOverview \n\u00b6\n\n\nYou can identify stemcell version from inside the VM via following files:\n\n\n\n\n/var/vcap/bosh/etc/stemcell_version\n: Example: \n3232.1\n\n\n/var/vcap/bosh/etc/stemcell_git_sha1\n: Example: \n8c8a6bd2ac5cacb11c421a97e90b888be9612ecb+\n\n\n\n\nNote: Release authors should not use the contents of these files in their releases.\n\n\n\nSee \nStemcell Building\n to find stemcell archive structure.\n\n\n\n\nFixing corrupted stemcells \n\u00b6\n\n\nOccasionally stemcells are deleted from the IaaS outside of the Director. For example your vSphere administrator decided to clean up your vSphere VMs folder. The Director of course will continue to reference deleted IaaS asset and CPI will eventually raise an error when trying to create new VM. \nbosh upload-stemcell\n command\n provides a \n--fix\n flag which allows to reupload stemcell with the same name and version into the Director fixing this problem.\n\n\n\n\nCleaning up uploaded stemcells \n\u00b6\n\n\nOver time the Director accumulates stemcells. Stemcells could be deleted manually via \nbosh delete-stemcell\n command\n or be cleaned up via \nbosh cleanup\n command\n.",
            "title": "Managing Stemcells"
        },
        {
            "location": "/managing-stemcells/#stemcell-versioning",
            "text": "Each stemcell is uniquely identified by its name and version. Currently stemcell versions are in MAJOR.MINOR format. Major version is incremented when new features are added to stemcells (or any components that stemcells typically include such as BOSH Agent). Minor versions are incremented if certain security fixes and/or features are backported on top of existing stemcell line. We recommend to continiously bump to the latest major stemcell version to receive latest updates.",
            "title": "Stemcell versioning "
        },
        {
            "location": "/managing-stemcells/#overview",
            "text": "You can identify stemcell version from inside the VM via following files:   /var/vcap/bosh/etc/stemcell_version : Example:  3232.1  /var/vcap/bosh/etc/stemcell_git_sha1 : Example:  8c8a6bd2ac5cacb11c421a97e90b888be9612ecb+   Note: Release authors should not use the contents of these files in their releases.  See  Stemcell Building  to find stemcell archive structure.",
            "title": "Overview "
        },
        {
            "location": "/managing-stemcells/#fixing-corrupted-stemcells",
            "text": "Occasionally stemcells are deleted from the IaaS outside of the Director. For example your vSphere administrator decided to clean up your vSphere VMs folder. The Director of course will continue to reference deleted IaaS asset and CPI will eventually raise an error when trying to create new VM.  bosh upload-stemcell  command  provides a  --fix  flag which allows to reupload stemcell with the same name and version into the Director fixing this problem.",
            "title": "Fixing corrupted stemcells "
        },
        {
            "location": "/managing-stemcells/#cleaning-up-uploaded-stemcells",
            "text": "Over time the Director accumulates stemcells. Stemcells could be deleted manually via  bosh delete-stemcell  command  or be cleaned up via  bosh cleanup  command .",
            "title": "Cleaning up uploaded stemcells "
        },
        {
            "location": "/resurrector/",
            "text": "The Resurrector is a plugin to the \nHealth Monitor\n. It's responsible for automatically recreating VMs that become inaccessible.\n\n\nThe Resurrector continuously cross-references VMs expected to be running against the VMs that are sending heartbeats. When resurrector does not receive heartbeats for a VM for a certain period of time, it will kick off a task on the Director (scan and fix task) to try to \"resurrect\" that VM. The Director may do one of two things:\n\n\n\n\ncreate a new VM if the old VM is missing from the IaaS\n\n\nreplace a VM if the Agent on that VM is not responding to commands\n\n\n\n\nUnder certain conditions the Resurrector will consider the system in the \"meltdown\" and will stop sending requests to the Director. It will resume submitting scan and fix tasks to the Director once the conditions change.\n\n\nResurrection can be turned off per specific deployment job instance or for all VMs managed by the Director via \nbosh vm resurrection\n CLI command\n.\n\n\nNote\n: The Health Monitor deploys with the Resurrector plugin disabled by default. To use it, you must enable the Resurrector plugin in your BOSH deployment manifest.\n\n\n\n\n\nEnabling and Configuring the Resurrector \n\u00b6\n\n\nTo enable the Resurrector:\n\n\n\n\n\n\nChange deployment manifest for the Health Monitor (typically colocated with the Director):\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \nresurrector_enabled\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nOptionally change configuration values:\n\n\n\n\nminimum_down_jobs\n [Integer, optional]: If the total number of instances that are down in a deployment (within time interval T) is below this number, the Resurrector will \nalways\n request to fix instances. This decision takes precedence to the \npercent_threshold\n check when the # of down instances \u2264 \nminimum_down_jobs\n. Default is 5.\n\n\npercent_threshold\n [Float, optional]: If the percentage of instances that are down in a deployment (within time interval T) is greater than the threshold percentage, the Resurrector will \nnot\n request to fix any instance. Going over this threshold is called \"meltdown\". Default is 0.2 (20%).\n\n\ntime_threshold\n [Integer, optional]: Time interval (in seconds) used in the above calculations. Default is 600.\n\n\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \nresurrector_enabled\n:\n \ntrue\n\n    \nresurrector\n:\n\n      \nminimum_down_jobs\n:\n \n5\n\n      \npercent_threshold\n:\n \n0.2\n\n      \ntime_threshold\n:\n \n600\n\n\n\n\n\n\n\n\n\nDepending on how you configured \nDirector user management\n, credentials are specified in the \nuser\n and \npassword\n properties or using a custom \nclient\n to authenticate with the UAA.\n\n\n Option a) Using UAA User Management\n\u00b6\n\n\nDefine an additional client in the \nuaa.clients\n section of your manifest:\n\n\nproperties\n:\n\n  \nuaa\n:\n\n    \nclients\n:\n\n      \nhm\n:\n\n        \noverride\n:\n \ntrue\n\n        \nauthorized-grant-types\n:\n \nclient_credentials\n\n        \nscope\n:\n \n\"\"\n\n        \nauthorities\n:\n \nbosh.admin\n\n        \nsecret\n:\n \n\"hm-password\"\n\n\n\n\n\nConfigure the Health Monitor to use the client with the defined secret to authenticate with the Director:\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ndirector_account\n:\n\n      \nclient_id\n:\n \nhm\n\n      \nclient_secret\n:\n \n\"hm-password\"\n\n\n\n\n\n Option b) Using Preconfigured Users\n\u00b6\n\n\nCreate new Director user so that the Resurrector plugin can communicate with the Director and query/submit information about deployments.\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nuser_management\n:\n\n      \nprovider\n:\n \nlocal\n\n      \nlocal\n:\n\n        \nusers\n:\n\n        \n-\n \n{\nname\n:\n \nadmin\n,\n \npassword\n:\n \nadmin-password\n}\n\n        \n-\n \n{\nname\n:\n \nhm\n,\n \npassword\n:\n \nhm-password\n}\n\n\n\n\n\nConfigure the Health Monitor to use the HM user and password to authenticate with the Director:\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ndirector_account\n:\n\n      \nuser\n:\n \nhm\n\n      \npassword\n:\n \nhm-password\n\n\n\n\n\n\n\n\n\nDeploy.\n\n\n\n\n\n\nCustomizing for Your Deployment \n\u00b6\n\n\nFor most deployments, you can use the default configuration values. In very small or very large deployments, the default values may need customization, as discussed in the following examples.\n\n\nSmall Deployment\n\u00b6\n\n\nIf your deployment consists of only five VMs, you may not want the Resurrector to attempt to recreate your entire deployment in the event of a catastrophic failure. In this scenario, we recommend that you set \nminimum_down_jobs\n to 1 or 2.\n\n\nLarge Deployment\n\u00b6\n\n\nIf your deployment consists of 1000 VMs, and you use the defaults, the Resurrector notifies the Director to recreate at least five VMs and up to 200 VMs. Depending on your deployment, you may consider even 100 down instances a catastrophic failure. In this scenario, set \npercent_threshold\n to 5% so that the Director resurrects 50 instances or fewer.\n\n\n\n\nDisabling the Resurrector \n\u00b6\n\n\nTo disable the Resurrector:\n\n\n\n\n\n\nChange deployment manifest for the Health Monitor (typically colocated with the Director):\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \nresurrector_enabled\n:\n \nfalse\n\n\n\n\n\n\n\n\n\nRedeploy.\n\n\n\n\n\n\nOptionally remove Director user created for the Health Monitor.\n\n\n\n\n\n\n\n\nViewing the Resurrector's Activity \n\u00b6\n\n\nSince scan and fix tasks on the Director are regular tasks, you can use \nbosh tasks --all -d ''\n command to view currently running/queued Resurrector's activity and \nbosh tasks --recent --all -d ''\n to also view finished tasks.\n\n\n\n\nNext: \nPersistent disk snapshotting\n\n\nPrevious: \nManual repair with Cloud Check",
            "title": "Auto-healing Capabilities"
        },
        {
            "location": "/resurrector/#enabling-and-configuring-the-resurrector",
            "text": "To enable the Resurrector:    Change deployment manifest for the Health Monitor (typically colocated with the Director):  properties : \n   hm : \n     resurrector_enabled :   true     Optionally change configuration values:   minimum_down_jobs  [Integer, optional]: If the total number of instances that are down in a deployment (within time interval T) is below this number, the Resurrector will  always  request to fix instances. This decision takes precedence to the  percent_threshold  check when the # of down instances \u2264  minimum_down_jobs . Default is 5.  percent_threshold  [Float, optional]: If the percentage of instances that are down in a deployment (within time interval T) is greater than the threshold percentage, the Resurrector will  not  request to fix any instance. Going over this threshold is called \"meltdown\". Default is 0.2 (20%).  time_threshold  [Integer, optional]: Time interval (in seconds) used in the above calculations. Default is 600.   properties : \n   hm : \n     resurrector_enabled :   true \n     resurrector : \n       minimum_down_jobs :   5 \n       percent_threshold :   0.2 \n       time_threshold :   600     Depending on how you configured  Director user management , credentials are specified in the  user  and  password  properties or using a custom  client  to authenticate with the UAA.",
            "title": "Enabling and Configuring the Resurrector "
        },
        {
            "location": "/resurrector/#customizing-for-your-deployment",
            "text": "For most deployments, you can use the default configuration values. In very small or very large deployments, the default values may need customization, as discussed in the following examples.",
            "title": "Customizing for Your Deployment "
        },
        {
            "location": "/resurrector/#small-deployment",
            "text": "If your deployment consists of only five VMs, you may not want the Resurrector to attempt to recreate your entire deployment in the event of a catastrophic failure. In this scenario, we recommend that you set  minimum_down_jobs  to 1 or 2.",
            "title": "Small Deployment"
        },
        {
            "location": "/resurrector/#large-deployment",
            "text": "If your deployment consists of 1000 VMs, and you use the defaults, the Resurrector notifies the Director to recreate at least five VMs and up to 200 VMs. Depending on your deployment, you may consider even 100 down instances a catastrophic failure. In this scenario, set  percent_threshold  to 5% so that the Director resurrects 50 instances or fewer.",
            "title": "Large Deployment"
        },
        {
            "location": "/resurrector/#disabling-the-resurrector",
            "text": "To disable the Resurrector:    Change deployment manifest for the Health Monitor (typically colocated with the Director):  properties : \n   hm : \n     resurrector_enabled :   false     Redeploy.    Optionally remove Director user created for the Health Monitor.",
            "title": "Disabling the Resurrector "
        },
        {
            "location": "/resurrector/#viewing-the-resurrectors-activity",
            "text": "Since scan and fix tasks on the Director are regular tasks, you can use  bosh tasks --all -d ''  command to view currently running/queued Resurrector's activity and  bosh tasks --recent --all -d ''  to also view finished tasks.   Next:  Persistent disk snapshotting  Previous:  Manual repair with Cloud Check",
            "title": "Viewing the Resurrector's Activity "
        },
        {
            "location": "/hm-config/",
            "text": "Sections below only show minimum configuration options to enable plugins. Add them to the deployment manifest for the Health Monitor. See \nhealth_monitor release job properties\n for more details.\n\n\n\n\n Event Logger\n\u00b6\n\n\nEnabled by default. No way to turn it off.\n\n\n\n\n Resurrector\n\u00b6\n\n\nRestarts VMs that have stopped heartbeating. See \nAutomatic repair with Resurrector\n for more details.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \nresurrector_enabled\n:\n \ntrue\n\n\n\n\n\n\n\n Emailer\n\u00b6\n\n\nPlugin that sends configurable e-mails on events reciept.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \nemail_notifications\n:\n \ntrue\n\n    \nemail_recipients\n:\n \n[\nemail@gmail.com\n]\n\n    \nsmtp\n:\n\n      \nfrom\n:\n\n      \nhost\n:\n\n      \nport\n:\n\n      \ndomain\n:\n\n      \ntls\n:\n\n      \nauth\n:\n\n      \nuser\n:\n\n      \npassword\n:\n\n\n\n\n\n\n\n JSON\n\u00b6\n\n\nEnabled by default.\n\n\nPlugin that sends alerts and heartbeats as json to programs installed on the director over stdin. The plugin will start and manage a process for each executable matching the glob \n/var/vcap/jobs/*/bin/bosh-monitor/*\n.\n\n\n\n\n OpenTSDB\n\u00b6\n\n\nPlugin that forwards alerts and heartbeats to \nOpenTSDB\n.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ntsdb_enabled\n:\n \ntrue\n\n    \ntsdb\n:\n\n      \naddress\n:\n \ntsdb.your.org\n\n      \nport\n:\n \n4242\n\n\n\n\n\n\n\n Graphite\n\u00b6\n\n\nPlugin that forwards heartbeats to \nGraphite\n.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ngraphite_enabled\n:\n \ntrue\n\n    \ngraphite\n:\n\n      \naddress\n:\n \ngraphite.your.org\n\n      \nport\n:\n \n2003\n\n\n\n\n\n\n\n PagerDuty\n\u00b6\n\n\nPlugin that sends various events to \nPagerDuty.com\n using their API.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \npagerduty_enabled\n:\n\n    \npagerduty\n:\n\n      \nservice_key\n:\n\n      \nhttp_proxy\n:\n\n\n\n\n\n\n\n DataDog\n\u00b6\n\n\nPlugin that sends various events to \nDataDog.com\n using their API.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ndatadog_enabled\n:\n \ntrue\n\n    \ndatadog\n:\n\n      \napi_key\n:\n\n      \napplication_key\n:\n\n      \npagerduty_service_name\n:\n\n\n\n\n\n\n\n AWS CloudWatch\n\u00b6\n\n\nPlugin that sends various events to \nAmazon's CloudWatch\n using their API.\n\n\nproperties\n:\n\n  \nhm\n:\n\n    \ncloud_watch_enabled\n:\n \ntrue\n\n  \naws\n:\n\n    \naccess_key_id\n:\n\n    \nsecret_access_key\n:",
            "title": "Using Health Monitor"
        },
        {
            "location": "/addons-common/",
            "text": "(See \nruntime config\n for an introduction to addons.)\n\n\nSyslog forwarding \n\u00b6\n\n\nNeed: Configure syslog on all machines to forward system logs to a remote location.\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nsyslog\n\n  \nversion\n:\n \n3\n\n\n\naddons\n:\n\n\n-\n \nname\n:\n \nlogs\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nsyslog_forwarder\n\n    \nrelease\n:\n \nsyslog\n\n  \nproperties\n:\n\n    \nsyslog\n:\n\n      \naddress\n:\n \nlogs4.papertrail.com\n\n      \ntransport\n:\n \ntcp\n\n      \nport\n:\n \n38559\n\n      \ntls_enabled\n:\n \ntrue\n\n      \npermitted_peer\n:\n \n\"*.papertrail.com\"\n\n      \nca_cert\n:\n \n|\n\n        \n-----BEGIN CERTIFICATE-----\n\n        \nMIIClTCCAf4CCQDc6hJtvGB8RjANBgkqhkiG9w0BAQUFADCBjjELMAk...\n\n        \n-----END CERTIFICATE-----\n\n\n\n\n\nSee \nsyslog_forwarder job\n.\n\n\n\n\nCustom SSH login banner \n\u00b6\n\n\nNote: This job work with 3232+ stemcell series due to how sshd is configured.\n\n\n\nNeed: Configure custom login banner to comply with organizational regulations.\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nos-conf\n\n  \nversion\n:\n \n3\n\n\n\naddons\n:\n\n\n-\n \nname\n:\n \nmisc\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nlogin_banner\n\n    \nrelease\n:\n \nos-conf\n\n  \nproperties\n:\n\n    \nlogin_banner\n:\n\n      \ntext\n:\n \n|\n\n        \nThis computer system is for authorized use only. All activity is logged and\n\n        \nregularly checked by system administrators. Individuals attempting to connect to,\n\n        \nport-scan, deface, hack, or otherwise interfere with any services on this system\n\n        \nwill be reported.\n\n\n\n\n\nSee \nlogin_banner job\n.\n\n\n\n\nCustom SSH users \n\u00b6\n\n\nWarning: This job does not remove users from the VM when user is removed from the manifest.\n\n\n\nNeed: Provide SSH access to all VMs for a third party automation system.\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nos-conf\n\n  \nversion\n:\n \n3\n\n\n\naddons\n:\n\n\n-\n \nname\n:\n \nmisc\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nuser_add\n\n    \nrelease\n:\n \nos-conf\n\n  \nproperties\n:\n\n    \nusers\n:\n\n    \n-\n \nname\n:\n \nnessus\n\n      \npublic_key\n:\n \n\"ssh-rsa\n \nAAAAB3NzaC1yc2EAQCyKb5nLZv...oYPkLlOGyAFLk6Id75Xr\n \nhostname\"\n\n    \n-\n \nname\n:\n \nteleport\n\n      \npublic_key\n:\n \n\"ssh-rsa\n \nAAAAB3NzaC1yc2dfgJKkb5nLZv...dkjfLlOGyAFLk6kfbgYG\n \nhostname\"\n\n\n\n\n\nSee \nuser_add job\n.",
            "title": "Common Addons"
        },
        {
            "location": "/addons-common/#syslog-forwarding",
            "text": "Need: Configure syslog on all machines to forward system logs to a remote location.  releases :  -   name :   syslog \n   version :   3  addons :  -   name :   logs \n   jobs : \n   -   name :   syslog_forwarder \n     release :   syslog \n   properties : \n     syslog : \n       address :   logs4.papertrail.com \n       transport :   tcp \n       port :   38559 \n       tls_enabled :   true \n       permitted_peer :   \"*.papertrail.com\" \n       ca_cert :   | \n         -----BEGIN CERTIFICATE----- \n         MIIClTCCAf4CCQDc6hJtvGB8RjANBgkqhkiG9w0BAQUFADCBjjELMAk... \n         -----END CERTIFICATE-----   See  syslog_forwarder job .",
            "title": "Syslog forwarding "
        },
        {
            "location": "/addons-common/#custom-ssh-login-banner",
            "text": "Note: This job work with 3232+ stemcell series due to how sshd is configured.  Need: Configure custom login banner to comply with organizational regulations.  releases :  -   name :   os-conf \n   version :   3  addons :  -   name :   misc \n   jobs : \n   -   name :   login_banner \n     release :   os-conf \n   properties : \n     login_banner : \n       text :   | \n         This computer system is for authorized use only. All activity is logged and \n         regularly checked by system administrators. Individuals attempting to connect to, \n         port-scan, deface, hack, or otherwise interfere with any services on this system \n         will be reported.   See  login_banner job .",
            "title": "Custom SSH login banner "
        },
        {
            "location": "/addons-common/#custom-ssh-users",
            "text": "Warning: This job does not remove users from the VM when user is removed from the manifest.  Need: Provide SSH access to all VMs for a third party automation system.  releases :  -   name :   os-conf \n   version :   3  addons :  -   name :   misc \n   jobs : \n   -   name :   user_add \n     release :   os-conf \n   properties : \n     users : \n     -   name :   nessus \n       public_key :   \"ssh-rsa   AAAAB3NzaC1yc2EAQCyKb5nLZv...oYPkLlOGyAFLk6Id75Xr   hostname\" \n     -   name :   teleport \n       public_key :   \"ssh-rsa   AAAAB3NzaC1yc2dfgJKkb5nLZv...dkjfLlOGyAFLk6kfbgYG   hostname\"   See  user_add job .",
            "title": "Custom SSH users "
        },
        {
            "location": "/variable-types/",
            "text": "(See \nVariable Interpolation\n for introduction.)\n\n\nCurrently CLI supports \ncertificate\n, \npassword\n, \nrsa\n, and \nssh\n types. The Director (connected to a config server) may support additional types known by the config server.\n\n\nNote that \n<value>\n indicates value obtained via \n((var))\n variable syntax.\n\n\n\n\nPassword \n\u00b6\n\n\n [String]: Password value. When generated defaults to 20 chars (from \na-z0-9\n).\n\n\n\n\nCertificate \n\u00b6\n\n\n [Hash]: Certificate.\n\n\n\n\nca\n [String]: Certificate's CA (PEM encoded).\n\n\ncertificate\n [String]: Certificate (PEM encoded).\n\n\nprivate_key\n [String]: Private key (PEM encoded).\n\n\n\n\nGeneration options:\n\n\n\n\ncommon_name\n [String, required]: Common name. Example: \nfoo.com\n.\n\n\nalternative_names\n [Array, options]: Subject alternative names. Example: \n[\"foo.com\", \"*.foo.com\"]\n.\n\n\nis_ca\n [Boolean, required]: Indicates whether this is a CA certificate (root or intermediate). Defaults to \nfalse\n.\n\n\nca\n [String, optional]: Specifies name of a CA certificate to use for making this certificate. Can be specified in conjuction with \nis_ca\n to produce an intermediate certificate.\n\n\nextended_key_usage\n [Array, optional]: List of extended key usage. Possible values: \nclient_auth\n and/or \nserver_auth\n. Default: empty. Example: \n[client_auth]\n.\n\n\n\n\nExample:\n\n\n-\n \nname\n:\n \nbosh_ca\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nis_ca\n:\n \ntrue\n\n    \ncommon_name\n:\n \nbosh\n\n\n-\n \nname\n:\n \nmbus_bootstrap_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \nbosh_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n\n\n\nExample of certificates used for mutual TLS:\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \ncockroachdb_ca\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nis_ca\n:\n \ntrue\n\n    \ncommon_name\n:\n \ncockroachdb\n\n\n-\n \nname\n:\n \ncockroachdb_server_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ncockroachdb_ca\n\n    \ncommon_name\n:\n \nnode\n\n    \nalternative_names\n:\n \n[\n\"*.cockroachdb.default.cockroachdb.bosh\"\n]\n\n    \nextended_key_usage\n:\n\n    \n-\n \nserver_auth\n\n    \n-\n \nclient_auth\n\n\n-\n \nname\n:\n \ncockroachdb_user_root\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ncockroachdb_ca\n\n    \ncommon_name\n:\n \nroot\n\n    \nextended_key_usage\n:\n\n    \n-\n \nclient_auth\n\n\n-\n \nname\n:\n \ncockroachdb_user_test\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ncockroachdb_ca\n\n    \ncommon_name\n:\n \ntest\n\n    \nextended_key_usage\n:\n\n    \n-\n \nclient_auth\n\n\n\n\n\n\n\nRSA \n\u00b6\n\n\n [Hash]: RSA key. When generated defaults to 2048 bits.\n\n\n\n\nprivate_key\n [String]: Private key (PEM encoded).\n\n\npublic_key\n [String]: Public key (PEM encoded).\n\n\n\n\n\n\nSSH \n\u00b6\n\n\n [Hash]: SSH key. When generated defaults to RSA 2048 bits.\n\n\n\n\nprivate_key\n [String]: Private key (PEM encoded).\n\n\npublic_key\n [String]: Public key (OpenSSH format, \"ssh-rsa ...\").\n\n\npublic_key_fingerprint\n [String]: Public key's MD5 fingerprint. Example: \nc3\n:\nae\n:\n51\n:\nec\n:\ncb\n:\na8\n:\n09\n:\nac\n:\n43\n:\nfd\n:\n84\n:\ndd\n:\n11\n:\ndd\n:\nfe\n:\nc7\n.",
            "title": "Variable Types"
        },
        {
            "location": "/variable-types/#password",
            "text": "[String]: Password value. When generated defaults to 20 chars (from  a-z0-9 ).",
            "title": "Password "
        },
        {
            "location": "/variable-types/#certificate",
            "text": "[Hash]: Certificate.   ca  [String]: Certificate's CA (PEM encoded).  certificate  [String]: Certificate (PEM encoded).  private_key  [String]: Private key (PEM encoded).   Generation options:   common_name  [String, required]: Common name. Example:  foo.com .  alternative_names  [Array, options]: Subject alternative names. Example:  [\"foo.com\", \"*.foo.com\"] .  is_ca  [Boolean, required]: Indicates whether this is a CA certificate (root or intermediate). Defaults to  false .  ca  [String, optional]: Specifies name of a CA certificate to use for making this certificate. Can be specified in conjuction with  is_ca  to produce an intermediate certificate.  extended_key_usage  [Array, optional]: List of extended key usage. Possible values:  client_auth  and/or  server_auth . Default: empty. Example:  [client_auth] .   Example:  -   name :   bosh_ca \n   type :   certificate \n   options : \n     is_ca :   true \n     common_name :   bosh  -   name :   mbus_bootstrap_ssl \n   type :   certificate \n   options : \n     ca :   bosh_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]   Example of certificates used for mutual TLS:  variables :  -   name :   cockroachdb_ca \n   type :   certificate \n   options : \n     is_ca :   true \n     common_name :   cockroachdb  -   name :   cockroachdb_server_ssl \n   type :   certificate \n   options : \n     ca :   cockroachdb_ca \n     common_name :   node \n     alternative_names :   [ \"*.cockroachdb.default.cockroachdb.bosh\" ] \n     extended_key_usage : \n     -   server_auth \n     -   client_auth  -   name :   cockroachdb_user_root \n   type :   certificate \n   options : \n     ca :   cockroachdb_ca \n     common_name :   root \n     extended_key_usage : \n     -   client_auth  -   name :   cockroachdb_user_test \n   type :   certificate \n   options : \n     ca :   cockroachdb_ca \n     common_name :   test \n     extended_key_usage : \n     -   client_auth",
            "title": "Certificate "
        },
        {
            "location": "/variable-types/#rsa",
            "text": "[Hash]: RSA key. When generated defaults to 2048 bits.   private_key  [String]: Private key (PEM encoded).  public_key  [String]: Public key (PEM encoded).",
            "title": "RSA "
        },
        {
            "location": "/variable-types/#ssh",
            "text": "[Hash]: SSH key. When generated defaults to RSA 2048 bits.   private_key  [String]: Private key (PEM encoded).  public_key  [String]: Public key (OpenSSH format, \"ssh-rsa ...\").  public_key_fingerprint  [String]: Public key's MD5 fingerprint. Example:  c3 : ae : 51 : ec : cb : a8 : 09 : ac : 43 : fd : 84 : dd : 11 : dd : fe : c7 .",
            "title": "SSH "
        },
        {
            "location": "/flush-arp/",
            "text": "Note: This feature is available with bosh-release v256+ and 3232+ stemcell series.\n\n\n\nCertain IaaSes may limit and/or disable gratuitous ARP for security reasons (for example AWS). Linux kernel performs periodic garbage collection of stale ARP entries; however, if there are open or stale connections these entries will not be cleared causing new connections to fail since they just use an existing \noutdated\n MAC address.\n\n\nThe Director is fully in control of when the VMs are created so it's able to communicate with the other VMs it manages and issues an explicit \ndelete_arp_entries\n Agent RPC call to clear stale ARP entries.\n\n\nTo enable this feature:\n\n\n\n\n\n\nAdd \ndirector.flush_arp\n deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nflush_arp\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nRedeploy the Director.\n\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Explicit ARP Flushing"
        },
        {
            "location": "/remove-dev-tools/",
            "text": "Note: This feature is available with bosh-release v255.4+ and on 3213+ stemcell series.\n\n\n\nIt's typically unnecessary to have development tools installed on all VMs in a production environment. All stemcells come with a minimal set of development tools for compilation workers to successfully compile packages.\n\n\nTo see which files and directories will be removed from the stemcell on bootup, unpack stemcell tarball and view \ndev_tools_file_list.txt\n.\n\n\nTo remove development tools from all non-compilation VMs:\n\n\n\n\n\n\nChange deployment manifest for the Director:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \nremove_dev_tools\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nRedeploy Director.\n\n\n\n\n\n\nRun \nbosh recreate\n for your deployments.",
            "title": "Removing Dev Tools from VMs"
        },
        {
            "location": "/trusted-certs/",
            "text": "Note: This feature is available with bosh-release v176+ (1.2992.0) and stemcells v2992+.\n\n\n\nThis document describes how to configure the Director to add a set of trusted certificates to all VMs managed by that Director. Configured trusted certificates are added to the default certificate store on each VM and will be automatically seen by the majority of software (e.g. curl).\n\n\n\n\nConfiguring Trusted Certificates \n\u00b6\n\n\nTo configure the Director with trusted certificates:\n\n\n\n\n\n\nChange deployment manifest for the Director to include one or more certificates:\n\n\nproperties\n:\n\n  \ndirector\n:\n\n    \ntrusted_certs\n:\n \n|\n\n      \n# Comments are allowed in between certificate boundaries\n\n      \n-----BEGIN CERTIFICATE-----\n\n      \nMIICsjCCAhugAwIBAgIJAMcyGWdRwnFlMA0GCSqGSIb3DQEBBQUAMEUxCzAJBgNV\n\n      \nBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEwHwYDVQQKExhJbnRlcm5ldCBX\n\n      \n...\n\n      \nItuuqKphqhSb6PEcFMzuVpTbN09ko54cHYIIULrSj3lEkoY9KJ1ONzxKjeGMHrOP\n\n      \nKS+vQr1+OCpxozj1qdBzvHgCS0DrtA==\n\n      \n-----END CERTIFICATE-----\n\n      \n# Some other certificate below\n\n      \n-----BEGIN CERTIFICATE-----\n\n      \nMIIB8zCCAVwCCQCLgU6CRfFs5jANBgkqhkiG9w0BAQUFADBFMQswCQYDVQQGEwJB\n\n      \nVTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0\n\n      \n...\n\n      \nVhORg7+d5moBrryXFJfeiybtuIEA+1AOwEkdp1MAKBhRZYmeoQXPAieBrCp6l+Ax\n\n      \nBaLg0R513H6KdlpsIOh6Ywa1r/ID0As=\n\n      \n-----END CERTIFICATE-----\n\n\n\n\n\n\n\n\n\nRedeploy the Director with the updated manifest.\n\n\nNote\n: Currently only VMs managed by the Director will be updated with the trusted certificates. The Director VM will not have trusted certificates installed.\n\n\n\n\n\n\nRedeploy each deployment to immediately update deployment's VMs with trusted certificates. Otherwise trusted certificate changes will be picked up next time you run \nbosh deploy\n for that deployment.\n\n\n$ bosh deployment ~/deployments/cf-mysql.yml\n$ bosh deploy\n...\n\n$ bosh deployment ~/deployments/cf-rabbitmq.yml\n$ bosh deploy\n...\n\n\n\n\n\n\n\n\nConfiguration Format \n\u00b6\n\n\nThe Director allows to specify one or more certificates concatenated together in the PEM format. Any text before, between and after certificate boundaries is ignored when importing the certificates, but may be useful for leaving notes about the certificate purpose.\n\n\nProviding multiple certificates makes downtimeless certificate rotation possible; however, it involves redeploying the Director and all deployments twice -- first to add a new certificate and second to remove an old certificate.\n\n\n\n\nBack to Table of Contents",
            "title": "Installing Certificates on VMs"
        },
        {
            "location": "/trusted-certs/#configuring-trusted-certificates",
            "text": "To configure the Director with trusted certificates:    Change deployment manifest for the Director to include one or more certificates:  properties : \n   director : \n     trusted_certs :   | \n       # Comments are allowed in between certificate boundaries \n       -----BEGIN CERTIFICATE----- \n       MIICsjCCAhugAwIBAgIJAMcyGWdRwnFlMA0GCSqGSIb3DQEBBQUAMEUxCzAJBgNV \n       BAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEwHwYDVQQKExhJbnRlcm5ldCBX \n       ... \n       ItuuqKphqhSb6PEcFMzuVpTbN09ko54cHYIIULrSj3lEkoY9KJ1ONzxKjeGMHrOP \n       KS+vQr1+OCpxozj1qdBzvHgCS0DrtA== \n       -----END CERTIFICATE----- \n       # Some other certificate below \n       -----BEGIN CERTIFICATE----- \n       MIIB8zCCAVwCCQCLgU6CRfFs5jANBgkqhkiG9w0BAQUFADBFMQswCQYDVQQGEwJB \n       VTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0 \n       ... \n       VhORg7+d5moBrryXFJfeiybtuIEA+1AOwEkdp1MAKBhRZYmeoQXPAieBrCp6l+Ax \n       BaLg0R513H6KdlpsIOh6Ywa1r/ID0As= \n       -----END CERTIFICATE-----     Redeploy the Director with the updated manifest.  Note : Currently only VMs managed by the Director will be updated with the trusted certificates. The Director VM will not have trusted certificates installed.    Redeploy each deployment to immediately update deployment's VMs with trusted certificates. Otherwise trusted certificate changes will be picked up next time you run  bosh deploy  for that deployment.  $ bosh deployment ~/deployments/cf-mysql.yml\n$ bosh deploy\n...\n\n$ bosh deployment ~/deployments/cf-rabbitmq.yml\n$ bosh deploy\n...",
            "title": "Configuring Trusted Certificates "
        },
        {
            "location": "/trusted-certs/#configuration-format",
            "text": "The Director allows to specify one or more certificates concatenated together in the PEM format. Any text before, between and after certificate boundaries is ignored when importing the certificates, but may be useful for leaving notes about the certificate purpose.  Providing multiple certificates makes downtimeless certificate rotation possible; however, it involves redeploying the Director and all deployments twice -- first to add a new certificate and second to remove an old certificate.   Back to Table of Contents",
            "title": "Configuration Format "
        },
        {
            "location": "/tips/",
            "text": "This document lists several common problems. If you are looking for CPI specific errors see:\n\n\n\n\nAWS CPI errors\n\n\nAzure CPI errors\n\n\nOpenStack CPI errors\n\n\nvSphere CPI errors\n\n\n\n\n\n\nTimed out pinging to ... after 600 seconds \n\u00b6\n\n\n$ bosh deploy\n...\n\n  Failed creating bound missing vms > cloud_controller_worker/0: Timed out pinging to 013ce5c9-e7fc-4f1d-ac24 after \n600\n seconds \n(\n00\n:16:03\n)\n\n  Failed creating bound missing vms > uaa/0: Timed out pinging to b029652d-14c3-4d68-98c7 after \n600\n seconds \n(\n00\n:16:12\n)\n\n  Failed creating bound missing vms > uaa/0: Timed out pinging to 1f56ddd1-7f2d-4afc-ae43 after \n600\n seconds \n(\n00\n:16:23\n)\n\n  Failed creating bound missing vms > loggregator_trafficcontroller/0: Timed out pinging to 28790bac-99a2-4703-89ad after \n600\n seconds \n(\n00\n:16:25\n)\n\n  Failed creating bound missing vms > health_manager/0: Timed out pinging to 720b805b-928c-4bb7-b6dd after \n600\n seconds \n(\n00\n:16:52\n)\n\n  Failed creating bound missing vms \n(\n00\n:16:53\n)\n\n\nError \n450002\n: Timed out pinging to 013ce5c9-e7fc-4f1d-ac24 after \n600\n seconds\n\nTask \n45\n error\n\nFor a more detailed error report, run: bosh task \n45\n --debug\n\n\n\n\nThis problem can occur due to:\n\n\n\n\nblocked network connectivity between the Agent on a new VM and NATS (typically the Director VM)\n\n\nbootstrapping problem on the VM and/or wrong configuration of the Agent\n\n\nblocked or slow boot times of the VM\n\n\n\n\nIt's recommended to start a deploy again and SSH into one of the VMs and look at \nthe Agent logs\n while the Director waits for VMs to become accessible. See \ndirector.debug.keep_unreachable_vms\n property\n to let Director know to leave unreachable VMs for easier debugging.\n\n\n\n\n...is not running after update \n\u00b6\n\n\n$ bosh deploy\n...\n\n  Started updating job access_z1 > access_z1/0 \n(\ncanary\n)\n\n     Done updating job route_emitter_z1 > route_emitter_z1/0 \n(\ncanary\n)\n \n(\n00\n:00:13\n)\n\n     Done updating job cc_bridge_z1 > cc_bridge_z1/0 \n(\ncanary\n)\n \n(\n00\n:00:20\n)\n\n     Done updating job cell_z1 > cell_z1/0 \n(\ncanary\n)\n \n(\n00\n:00:40\n)\n\n   Failed updating job access_z1 > access_z1/0 \n(\ncanary\n)\n: \n`\naccess_z1/0\n' is not running after update (00:02:13)\n\n\n\nError 400007: `access_z1/0'\n is not running after update\n\nTask \n47\n error\n\nFor a more detailed error report, run: bosh task \n47\n --debug\n\n\n\n\nThis problem occurs when one of the release jobs on a VM did not successfully start in a given amount of time. You can use \nbosh instances --ps\n command to find out which process on the VM is failing. You can also \naccess logs\n to view additional information.\n\n\nThis problem may also arise when deployment manifest specifies too small of a \ncanary/update watch time\n which may not be large enough for a process to successfully start.\n\n\n\n\numount: /var/vcap/store: device is busy \n\u00b6\n\n\nL Error: Action Failed get_task: Task 5be893c6-7a2c-4f3f-420b-433fd23528a1 result: Migrating persistent disk: Remounting persistent disk as readonly: Unmounting /var/vcap/store: Running command: \n'umount /var/vcap/store'\n, stdout: \n''\n, stderr: \n'umount: /var/vcap/store: device is busy.\n\n\n        (In some cases useful info about processes that use\n\n\n         the device is found by lsof(8) or fuser(1))\n\n\n'\n: \nexit\n status \n1\n\n\n\n\n\nThis process occurs when one of the processes (from one of the installed jobs) did not fully shutdown and continues to retain a reference to the persistent disk. Agent tries to unmount persistent disk and unmount command fails. You can use \nbosh ssh\n command to get inside the VM and diagnose which process is holding onto the persistent disk via \nlsof | grep /var/vcap/store\n (ran as root).\n\n\n\n\nRunning command: bosh-blobstore-dav -c ... 500 Internal Server Error \n\u00b6\n\n\n$ bosh deploy\n...\n\nFailed compiling packages > dea_next/3e95ef8425be45468e044c05cc9aa65494281ab5: Action Failed get_task: Task bd35f7c1-2144-4045-763e-40beeafc9fa3 result: Compiling package dea_next: Uploading compiled package: Creating blob in inner blobstore: Making put command: Shelling out to bosh-blobstore-dav cli: Running command: \n'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json put /var/vcap/data/tmp/bosh-platform-disk-TarballCompressor-CompressFilesInDir949066221 cd91a1c5-a034-4c69-4608-6b18cc3fcb2b'\n, stdout: \n'Error running app - Putting dav blob cd91a1c5-a034-4c69-4608-6b18cc3fcb2b: Wrong response code: 500; body: &lt;html&gt;\n\n\n&lt;head&gt;&lt;title&gt;500 Internal Server Error&lt;/title&gt;&lt;/head&gt;\n\n\n&lt;body bgcolor=\"white\"&gt;\n\n\n&lt;center&gt;&lt;h1&gt;500 Internal Server Error&lt;/h1&gt;&lt;/center&gt;\n\n\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n\n\n&lt;/body&gt;\n\n\n&lt;/html&gt;\n\n\n'\n, stderr: \n''\n: \nexit\n status \n1\n \n(\n00\n:03:16\n)\n\n\n\n\n\nThis problem can occur if the Director is configured to use built-in blobstore and does not have enough space on its persistent disk. You can redeploy the Director with a larger persistent disk. Alternatively you can remove unused releases by running \nbosh clean-up\n command.\n\n\nIf \nbosh clean-up\n command fails with 500 Internal Server Error, consider removing \n/var/vcap/store/director/tasks\n to free up a little bit of persistent disk space before running \nbosh clean-up\n command again. (Deleting that directory will delete Director task debug logs.)\n\n\n\n\nDebugging Director database \n\u00b6\n\n\nRarely it's necessary to dive into the Director DB. The easiest way to do so is to SSH into the Director VM and use \ndirector_ctl console\n. For example:\n\n\n$ ssh vcap@DIRECTOR-IP\n\n$ /var/vcap/jobs/director/bin/director_ctl \nconsole\n\n\n=\n> Loading /var/vcap/jobs/director/config/director.yml\n\n=\n> ruby-debug not found, debugger \ndisabled\n\n\n=\n> Welcome to BOSH Director \nconsole\n\n\n=\n> You can use \n'app'\n to access REST \nAPI\n\n\n=\n> You can also use \n'cloud'\n, \n'blobstore'\n, \n'nats'\n helpers to query these services\nirb\n(\nmain\n)\n:001:0> Bosh::Director::Models::RenderedTemplatesArchive.count\n\n=\n> \n3\n\n\n\n\n\nNote: It's not recommended to modify the Director database via this or other manual methods. Please let us know via GitHub issue if you need a certain feature in the BOSH CLI to do some operation.\n\n\n\n\n\nTask X cancelled \n\u00b6\n\n\n$ bosh deploy\n...\n\n  Started preparing package compilation > Finding packages to compile. Done \n(\n00\n:00:01\n)\n\n  Started preparing dns > Binding DNS. Done \n(\n00\n:00:05\n)\n\n\nError \n10001\n: Task \n106\n cancelled\n\nTask \n106\n cancelled\n\n\n\n\nThis problem typically occurs if the Director's system time is out of sync, or if the Director machine is underpowered.\n\n\n\n\nUpload release fails \n\u00b6\n\n\n$ bosh upload release blah.tgz\n...\n  Started creating new packages > blah_package/f9098f452f46fb072a6000b772166f349ffe27da. Failed: Could not create object, \n413\n/\n&\nlt\n;\nhtml\n&\ngt\n;\n\n\n&\nlt\n;\nhead\n&\ngt\n;&\nlt\n;\ntitle\n&\ngt\n;\n413\n Request Entity Too Large\n&\nlt\n;\n/title\n&\ngt\n;&\nlt\n;\n/head\n&\ngt\n;\n\n\n&\nlt\n;\nbody \nbgcolor\n=\n\"white\"\n&\ngt\n;\n\n\n&\nlt\n;\ncenter\n&\ngt\n;&\nlt\n;\nh1\n&\ngt\n;\n413\n Request Entity Too Large\n&\nlt\n;\n/h1\n&\ngt\n;&\nlt\n;\n/center\n&\ngt\n;\n\n\n&\nlt\n;\nhr\n&\ngt\n;&\nlt\n;\ncenter\n&\ngt\n;\nnginx\n&\nlt\n;\n/center\n&\ngt\n;\n\n\n&\nlt\n;\n/body\n&\ngt\n;\n\n\n&\nlt\n;\n/html\n&\ngt\n;\n\n \n(\n00\n:02:10\n)\n\n\nError \n100\n: Could not create object, \n413\n/\n&\nlt\n;\nhtml\n&\ngt\n;\n\n\n&\nlt\n;\nhead\n&\ngt\n;&\nlt\n;\ntitle\n&\ngt\n;\n413\n Request Entity Too Large\n&\nlt\n;\n/title\n&\ngt\n;&\nlt\n;\n/head\n&\ngt\n;\n\n\n&\nlt\n;\nbody \nbgcolor\n=\n\"white\"\n&\ngt\n;\n\n\n&\nlt\n;\ncenter\n&\ngt\n;&\nlt\n;\nh1\n&\ngt\n;\n413\n Request Entity Too Large\n&\nlt\n;\n/h1\n&\ngt\n;&\nlt\n;\n/center\n&\ngt\n;\n\n\n&\nlt\n;\nhr\n&\ngt\n;&\nlt\n;\ncenter\n&\ngt\n;\nnginx\n&\nlt\n;\n/center\n&\ngt\n;\n\n\n&\nlt\n;\n/body\n&\ngt\n;\n\n\n&\nlt\n;\n/html\n&\ngt\n;\n\n...\n\n\n\n\nThis failure occurs due to nginx configuration problems with the director and the nginx blobstore. This can be remedied by configuring the \nmax_upload_size\n property on the director and blobstore jobs.\n\n\n\n\nPersistent Disk with id \n not found \n\u00b6\n\n\n$ bosh create-env\n\n(\n...\n)\n\nCommand \n'deploy'\n failed:\n  Deploying:\n    Creating instance \n'bosh/0'\n:\n      Updating instance disks:\n        Updating disks:\n          Deploying disk:\n            Mounting disk:\n              Sending \n'get_task'\n to the agent:\n                Agent responded with error: Action Failed get_task: Task 7e4d289d-b97c-4464-40d4-ecc90cc2a94b result: Persistent disk with volume id \n'14128b61-e046-48ae-b48a-fc0324716b83'\n could not be found\n\n\n\n\nThe SSH tunnel between your machine and the VM in the cloud can be terminated prematurly, see \ncorresponding bug\n. Update CLI v2 to version >= v2.0.2 to fix this.",
            "title": "Troubleshooting"
        },
        {
            "location": "/tips/#timed-out-pinging-to-after-600-seconds",
            "text": "$ bosh deploy\n...\n\n  Failed creating bound missing vms > cloud_controller_worker/0: Timed out pinging to 013ce5c9-e7fc-4f1d-ac24 after  600  seconds  ( 00 :16:03 ) \n  Failed creating bound missing vms > uaa/0: Timed out pinging to b029652d-14c3-4d68-98c7 after  600  seconds  ( 00 :16:12 ) \n  Failed creating bound missing vms > uaa/0: Timed out pinging to 1f56ddd1-7f2d-4afc-ae43 after  600  seconds  ( 00 :16:23 ) \n  Failed creating bound missing vms > loggregator_trafficcontroller/0: Timed out pinging to 28790bac-99a2-4703-89ad after  600  seconds  ( 00 :16:25 ) \n  Failed creating bound missing vms > health_manager/0: Timed out pinging to 720b805b-928c-4bb7-b6dd after  600  seconds  ( 00 :16:52 ) \n  Failed creating bound missing vms  ( 00 :16:53 ) \n\nError  450002 : Timed out pinging to 013ce5c9-e7fc-4f1d-ac24 after  600  seconds\n\nTask  45  error\n\nFor a more detailed error report, run: bosh task  45  --debug  This problem can occur due to:   blocked network connectivity between the Agent on a new VM and NATS (typically the Director VM)  bootstrapping problem on the VM and/or wrong configuration of the Agent  blocked or slow boot times of the VM   It's recommended to start a deploy again and SSH into one of the VMs and look at  the Agent logs  while the Director waits for VMs to become accessible. See  director.debug.keep_unreachable_vms  property  to let Director know to leave unreachable VMs for easier debugging.",
            "title": "Timed out pinging to ... after 600 seconds "
        },
        {
            "location": "/tips/#is-not-running-after-update",
            "text": "$ bosh deploy\n...\n\n  Started updating job access_z1 > access_z1/0  ( canary ) \n     Done updating job route_emitter_z1 > route_emitter_z1/0  ( canary )   ( 00 :00:13 ) \n     Done updating job cc_bridge_z1 > cc_bridge_z1/0  ( canary )   ( 00 :00:20 ) \n     Done updating job cell_z1 > cell_z1/0  ( canary )   ( 00 :00:40 ) \n   Failed updating job access_z1 > access_z1/0  ( canary ) :  ` access_z1/0 ' is not running after update (00:02:13)  Error 400007: `access_z1/0'  is not running after update\n\nTask  47  error\n\nFor a more detailed error report, run: bosh task  47  --debug  This problem occurs when one of the release jobs on a VM did not successfully start in a given amount of time. You can use  bosh instances --ps  command to find out which process on the VM is failing. You can also  access logs  to view additional information.  This problem may also arise when deployment manifest specifies too small of a  canary/update watch time  which may not be large enough for a process to successfully start.",
            "title": "...is not running after update "
        },
        {
            "location": "/tips/#umount-varvcapstore-device-is-busy",
            "text": "L Error: Action Failed get_task: Task 5be893c6-7a2c-4f3f-420b-433fd23528a1 result: Migrating persistent disk: Remounting persistent disk as readonly: Unmounting /var/vcap/store: Running command:  'umount /var/vcap/store' , stdout:  '' , stderr:  'umount: /var/vcap/store: device is busy.          (In some cases useful info about processes that use           the device is found by lsof(8) or fuser(1))  ' :  exit  status  1   This process occurs when one of the processes (from one of the installed jobs) did not fully shutdown and continues to retain a reference to the persistent disk. Agent tries to unmount persistent disk and unmount command fails. You can use  bosh ssh  command to get inside the VM and diagnose which process is holding onto the persistent disk via  lsof | grep /var/vcap/store  (ran as root).",
            "title": "umount: /var/vcap/store: device is busy "
        },
        {
            "location": "/tips/#running-command-bosh-blobstore-dav-c-500-internal-server-error",
            "text": "$ bosh deploy\n...\n\nFailed compiling packages > dea_next/3e95ef8425be45468e044c05cc9aa65494281ab5: Action Failed get_task: Task bd35f7c1-2144-4045-763e-40beeafc9fa3 result: Compiling package dea_next: Uploading compiled package: Creating blob in inner blobstore: Making put command: Shelling out to bosh-blobstore-dav cli: Running command:  'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json put /var/vcap/data/tmp/bosh-platform-disk-TarballCompressor-CompressFilesInDir949066221 cd91a1c5-a034-4c69-4608-6b18cc3fcb2b' , stdout:  'Error running app - Putting dav blob cd91a1c5-a034-4c69-4608-6b18cc3fcb2b: Wrong response code: 500; body: &lt;html&gt;  &lt;head&gt;&lt;title&gt;500 Internal Server Error&lt;/title&gt;&lt;/head&gt;  &lt;body bgcolor=\"white\"&gt;  &lt;center&gt;&lt;h1&gt;500 Internal Server Error&lt;/h1&gt;&lt;/center&gt;  &lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;  &lt;/body&gt;  &lt;/html&gt;  ' , stderr:  '' :  exit  status  1   ( 00 :03:16 )   This problem can occur if the Director is configured to use built-in blobstore and does not have enough space on its persistent disk. You can redeploy the Director with a larger persistent disk. Alternatively you can remove unused releases by running  bosh clean-up  command.  If  bosh clean-up  command fails with 500 Internal Server Error, consider removing  /var/vcap/store/director/tasks  to free up a little bit of persistent disk space before running  bosh clean-up  command again. (Deleting that directory will delete Director task debug logs.)",
            "title": "Running command: bosh-blobstore-dav -c ... 500 Internal Server Error "
        },
        {
            "location": "/tips/#debugging-director-database",
            "text": "Rarely it's necessary to dive into the Director DB. The easiest way to do so is to SSH into the Director VM and use  director_ctl console . For example:  $ ssh vcap@DIRECTOR-IP\n\n$ /var/vcap/jobs/director/bin/director_ctl  console  = > Loading /var/vcap/jobs/director/config/director.yml = > ruby-debug not found, debugger  disabled  = > Welcome to BOSH Director  console  = > You can use  'app'  to access REST  API  = > You can also use  'cloud' ,  'blobstore' ,  'nats'  helpers to query these services\nirb ( main ) :001:0> Bosh::Director::Models::RenderedTemplatesArchive.count = >  3   Note: It's not recommended to modify the Director database via this or other manual methods. Please let us know via GitHub issue if you need a certain feature in the BOSH CLI to do some operation.",
            "title": "Debugging Director database "
        },
        {
            "location": "/tips/#task-x-cancelled",
            "text": "$ bosh deploy\n...\n\n  Started preparing package compilation > Finding packages to compile. Done  ( 00 :00:01 ) \n  Started preparing dns > Binding DNS. Done  ( 00 :00:05 ) \n\nError  10001 : Task  106  cancelled\n\nTask  106  cancelled  This problem typically occurs if the Director's system time is out of sync, or if the Director machine is underpowered.",
            "title": "Task X cancelled "
        },
        {
            "location": "/tips/#upload-release-fails",
            "text": "$ bosh upload release blah.tgz\n...\n  Started creating new packages > blah_package/f9098f452f46fb072a6000b772166f349ffe27da. Failed: Could not create object,  413 / & lt ; html & gt ;  & lt ; head & gt ;& lt ; title & gt ; 413  Request Entity Too Large & lt ; /title & gt ;& lt ; /head & gt ;  & lt ; body  bgcolor = \"white\" & gt ;  & lt ; center & gt ;& lt ; h1 & gt ; 413  Request Entity Too Large & lt ; /h1 & gt ;& lt ; /center & gt ;  & lt ; hr & gt ;& lt ; center & gt ; nginx & lt ; /center & gt ;  & lt ; /body & gt ;  & lt ; /html & gt ; \n  ( 00 :02:10 ) \n\nError  100 : Could not create object,  413 / & lt ; html & gt ;  & lt ; head & gt ;& lt ; title & gt ; 413  Request Entity Too Large & lt ; /title & gt ;& lt ; /head & gt ;  & lt ; body  bgcolor = \"white\" & gt ;  & lt ; center & gt ;& lt ; h1 & gt ; 413  Request Entity Too Large & lt ; /h1 & gt ;& lt ; /center & gt ;  & lt ; hr & gt ;& lt ; center & gt ; nginx & lt ; /center & gt ;  & lt ; /body & gt ;  & lt ; /html & gt ; \n...  This failure occurs due to nginx configuration problems with the director and the nginx blobstore. This can be remedied by configuring the  max_upload_size  property on the director and blobstore jobs.",
            "title": "Upload release fails "
        },
        {
            "location": "/tips/#persistent-disk-with-id-not-found",
            "text": "$ bosh create-env ( ... ) \nCommand  'deploy'  failed:\n  Deploying:\n    Creating instance  'bosh/0' :\n      Updating instance disks:\n        Updating disks:\n          Deploying disk:\n            Mounting disk:\n              Sending  'get_task'  to the agent:\n                Agent responded with error: Action Failed get_task: Task 7e4d289d-b97c-4464-40d4-ecc90cc2a94b result: Persistent disk with volume id  '14128b61-e046-48ae-b48a-fc0324716b83'  could not be found  The SSH tunnel between your machine and the VM in the cloud can be terminated prematurly, see  corresponding bug . Update CLI v2 to version >= v2.0.2 to fix this.",
            "title": "Persistent Disk with id  not found "
        },
        {
            "location": "/cli-v2/",
            "text": "Note: Applies to CLI v3.0.1+.\n\n\n\nThe BOSH Command Line Interface (CLI) is what you use to run BOSH commands. CLI v2 is a new major version of CLI.\n\n\n\n\nInstall \n\u00b6\n\n\n\n\n\n\nDownload the binary for your platform and place it on your \nPATH\n:\n\n\n\n\n\n\nbosh-cli-3.0.1-darwin-amd64\n \nsha1: d2fea20210a47b8c8f1f7dbb27ffb5808d47ce87\n\n\n\n\nbosh-cli-3.0.1-linux-amd64\n \nsha1: ccc893bab8b219e9e4a628ed044ebca6c6de9ca0\n\n\n\n\nbosh-cli-3.0.1-windows-amd64.exe\n \nsha1: 41c23c90cab9dc62fa0a1275dcaf64670579ed33\n (Windows CLI support is partial)\n\n\n$ chmod +x ~/Downloads/bosh-cli-*\n$ sudo mv ~/Downloads/bosh-cli-* /usr/local/bin/bosh\n\n\n\n\n\n\n\n\nCheck \nbosh\n version to make sure it is properly installed:\n\n\n$ bosh -v\nversion \n3\n.0.1-712bfd7-2018-03-13T23:26:42Z\n\n\n\n\nIf the output does not begin with \nversion 2.0...\n (or \n3.0\n) you are probably executing CLI v1 (Ruby based).\n\n\n\n\n\n\nInstall OS specified dependencies\n for \nbosh create-env\n command\n\n\n\n\n\n\nAlternatively, refer to \ncloudfoundry/homebrew-tap\n to install CLI via Homebrew on OS X. We currently do not publish CLI via apt or yum repositories.\n\n\n\n\nRelease Notes \n\u00b6\n\n\nCLI release notes can be found \non Github\n.\n\n\n\n\nGlobal Flags \n\u00b6\n\n\nSee \nGlobal flags\n for more details on how to enable different output formats, debug logging, etc.\n\n\n\n\nCommands \n\u00b6\n\n\nEnvironments \n\u00b6\n\n\nSee \nEnvironments\n.\n\n\n\n\n\n\n \nbosh environments\n (Alias: \nenvs\n)\n\n\nLists aliased environments known to the CLI. Aliasing is done via \nalias-env\n command.\n\n\n$ bosh envs\nURL              Alias\n\n104\n.154.171.255  gcp\n\n192\n.168.56.6     vbox\n\n\n2\n environments\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh create-env manifest.yml [--state path] [-v ...] [-o ...] [--vars-store path]\n\n\nCreates single VM based on the manifest. Typically used to create a Director environment. \nOperation files\n and \nvariables\n can be provided to adjust and fill in manifest before doing a deploy.\n\n\ncreate-env\n command replaces \nbosh-init deploy\n CLI command.\n\n\n$ bosh create-env ~/workspace/bosh-deployment/bosh.yml \n\\\n\n  --state state.json \n\\\n\n  --vars-store ./creds.yml \n\\\n\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml \n\\\n\n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml \n\\\n\n  -o ~/workspace/bosh-deployment/bosh-lite.yml \n\\\n\n  -o ~/workspace/bosh-deployment/jumpbox-user.yml \n\\\n\n  -v \ndirector_name\n=\nvbox \n\\\n\n  -v \ninternal_ip\n=\n192\n.168.56.6 \n\\\n\n  -v \ninternal_gw\n=\n192\n.168.56.1 \n\\\n\n  -v \ninternal_cidr\n=\n192\n.168.56.0/24 \n\\\n\n  -v \nnetwork_name\n=\nvboxnet0 \n\\\n\n  -v \noutbound_network_name\n=\nNatNetwork\n\n\n\n\n\n\n\n\n \nbosh alias-env name -e location [--ca-cert=path]\n\n\nAssigns a name to the created environment for easier access in subsequent CLI commands. Instead of specifying Director location and possibly a CA certificate, subsequent commands can just take given name via \n--environment\n flag (\n-e\n).\n\n\n$ bosh alias-env gcp -e bosh.corp.com\n$ bosh alias-env gcp -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int creds.yml --path /director_ssl/ca\n)\n\n\n\n\n\n\n\n\n\n \nbosh -e location environment\n (Alias: \nenv\n)\n\n\nShows Director information in the deployment environment.\n\n\n$ bosh -e vbox env\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nName      vbox\nUUID      eeb27cc6-467e-4c1d-a8f9-f1a8de759f52\nVersion   \n260\n.5.0 \n(\n00000000\n)\n\nCPI       warden_cpi\nFeatures  compiled_package_cache: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh delete-env manifest.yml [--state path] [-v ...] [-o ...] [--vars-store path]\n\n\nDeletes previously created VM based on the manifest. Same flags provided to \ncreate-env\n command should be given to the \ndelete-env\n command.\n\n\ndelete-env\n command replaces \nbosh-init delete\n CLI command.\n\n\n$ bosh delete-env ~/workspace/bosh-deployment/bosh.yml \n\\\n\n  --state state.json \n\\\n\n  --vars-store ./creds.yml \n\\\n\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml \n\\\n\n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml \n\\\n\n  -o ~/workspace/bosh-deployment/bosh-lite.yml \n\\\n\n  -o ~/workspace/bosh-deployment/jumpbox-user.yml \n\\\n\n  -v \ndirector_name\n=\nvbox \n\\\n\n  -v \ninternal_ip\n=\n192\n.168.56.6 \n\\\n\n  -v \ninternal_gw\n=\n192\n.168.56.1 \n\\\n\n  -v \ninternal_cidr\n=\n192\n.168.56.0/24 \n\\\n\n  -v \nnetwork_name\n=\nvboxnet0 \n\\\n\n  -v \noutbound_network_name\n=\nNatNetwork\n\n\n\n\n\n\n\n\n\n\nSession \n\u00b6\n\n\n\n\n\n\n \nbosh log-in\n (Alias: \nl\n, \nlogin\n)\n\n\nLogs in given user into the Director.\n\n\nThis command can only be used interactively. If non-interactive use is necessary (for example in scripts) please set \nBOSH_CLIENT\n and \nBOSH_CLIENT_SECRET\n environment variables instead of using this command. Note that if the Director is configured with UAA authentication you cannot use UAA users with BOSH_* environment variables but rather have to use UAA clients.\n\n\n$ bosh -e my-env l\nUser \n()\n: admin\nPassword \n()\n:\n\n\n\n\n\n\n\n\n \nbosh log-out\n (Alias: \nlogout\n)\n\n\nLogs out currently logged in user.\n\n\n\n\n\n\n\n\nStemcells \n\u00b6\n\n\nSee \nUploading Stemcells\n.\n\n\n\n\n\n\n \nbosh -e my-env stemcells\n (Alias: \nss\n)\n\n\nLists stemcells previously uploaded into the Director. Shows their names, versions and CIDs.\n\n\n$ bosh -e my-env ss\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nName                                         Version  OS             CPI  CID\nbosh-warden-boshlite-ubuntu-trusty-go_agent  \n3363\n*    ubuntu-trusty  -    6cbb176a-6a43-42...\n~                                            \n3312\n     ubuntu-trusty  -    43r3496a-4rt3-52...\nbosh-warden-boshlite-centos-7-go_agent       \n3363\n*    centos-7       -    38yr83gg-349r-94...\n\n\n(\n*\n)\n Currently deployed\n\n\n3\n stemcells\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env upload-stemcell location [--sha1=digest] [--fix]\n (Alias: \nus\n)\n\n\nUploads stemcell to the Director. Succeeds even if stemcell is already imported.\n\n\nStemcell location may be local file system path or an HTTP/HTTPS URL.\n\n\n--fix\n flag allows operator to replace previously uploaded stemcell with the same name and version to repair stemcells that might have been corrupted in the cloud.\n\n\n$ bosh -e my-env us ~/Downloads/bosh-stemcell-3468.17-warden-boshlite-ubuntu-trusty-go_agent.tgz\n$ bosh -e my-env us https://bosh.io/d/stemcells/bosh-stemcell-warden-boshlite-ubuntu-trusty-go_agent?v\n=\n3468\n.17\n\n\n\n\n\n\n\n\n \nbosh -e my-env delete-stemcell name/version\n\n\nDeletes uploaded stemcell from the Director. Succeeds even if stemcell is not found.\n\n\n$ bosh -e my-env delete-stemcell bosh-warden-boshlite-ubuntu-trusty-go_agent/3468.17\n\n\n\n\n\n\n\n\n \nbosh repack-stemcell src.tgz dst.tgz [--name=name] [--version=ver] [--cloud-properties=json-string]\n\n\nProduces new stemcell tarball with updated properties such as name, version, and cloud properties.\n\n\nSee \nRepacking stemcells\n for details.\n\n\n\n\n\n\n\n\nRelease creation \n\u00b6\n\n\n\n\n\n\n \nbosh init-release [--git] [--dir=dir]\n\n\nCreates an empty release skeleton for a release in \ndir\n. By default \ndir\n is the current directory.\n\n\n--git\n flag initializes release skeleton as a Git repository, adding appropriate \n.gitignore\n file.\n\n\n$ bosh init-release --git --dir release-dir\n$ \ncd\n release-dir\n\n\n\n\n\n\n\n\n \nbosh generate-job name [--dir=dir]\n\n\nCreates an empty job skeleton for a release in \ndir\n. Includes bare \nspec\n and an empty \nmonit\n file.\n\n\n\n\n\n\n \nbosh generate-package name [--dir=dir]\n\n\nCreates an empty package skeleton for a release in \ndir\n. Includes bare \nspec\n and an empty \npackaging\n file.\n\n\n\n\n\n\n \nbosh vendor-package name src-dir [--dir=dir]\n (v2.0.36+)\n\n\nVendors a package from a different release into a release in \ndir\n. It includes \nspec.lock\n in the package directory so that CLI will reference specific package by its fingerprint when creating releases.\n\n\nSee \nPackage vendoring\n for details.\n\n\n\n\n\n\n \nbosh create-release [--force] [--version=ver] [--timestamp-version] [--final] [--tarball=path] [--dir=dir]\n (Alias: \ncr\n)\n\n\nCreates new version of a release stored in \ndir\n\n\n\n\n--force\n flag specifies to ignore uncommitted changes in the release directory; it should only be used when building dev releases\n\n\n--version\n flag allows operator to provide custom release version\n\n\n--timestamp-version\n flag will produce timestamp based dev release version\n\n\n--tarball\n flag specifies destination of a release tarball; if not specified, release tarball will not be produced\n\n\n--sha2\n flag to use SHA256 checksum\n\n\n\n\nWhile iterating on a release it's common to run \nbosh create-release --force && bosh -e my-env upload-release && bosh -e my-env -d my-dep deploy manifest.yml\n command sequence.\n\n\nIn a CI pipeline it's common to use this command to create a release tarball and pass it into acceptance or end-to-end tests. Once release tarball goes through appropriate tests it can be finalized with a \nfinalize-release\n command and shared with release consumers.\n\n\n$ \ncd\n release-dir\n$ bosh create-release --force\n$ bosh create-release --timestamp-version\n$ bosh create-release --version \n9\n+dev.10\n$ bosh create-release --tarball /tmp/my-release.tgz\n$ bosh create-release --final\n$ bosh create-release --version \n20\n --final\n$ bosh create-release releases/zookeeper/zookeeper-3.yml --tarball /tmp/my-release.tgz\n\n\n\n\n\n\n\n\n \nbosh finalize-release release.tgz [--force] [--version=ver] [--dir=dir]\n\n\nRecords contents of a release tarball as a final release with an optionally given version. Once \n.final_builds\n and \nreleases\n directories are updated, it's strongly recommended to commit your changes to version control.\n\n\nTypically this command is used as a final step in the CI pipeline to save the final artifact once it passed appropriate tests.\n\n\n$ \ncd\n release-dir\n$ bosh finalize-release /tmp/my-release.tgz\n$ bosh finalize-release /tmp/my-release.tgz --version \n20\n\n$ git commit -am \n'Final release 20'\n\n$ git push origin master\n\n\n\n\n\n\n\n\n \nbosh reset-release [--dir=dir]\n\n\nRemoves temporary artifacts such as dev releases, blobs, etc. kept in the release directory \ndir\n.\n\n\n\n\n\n\n\n\nRelease blobs \n\u00b6\n\n\nSee \nRelease Blobs\n for a detailed workflow.\n\n\n\n\n\n\n \nbosh blobs\n\n\nLists tracked blobs from \nconfig/blobs.yml\n. Shows uploaded and not-yet-uploaded blobs.\n\n\n$ \ncd\n release-dir\n$ bosh blobs\nPath                               Size     Blobstore ID         Digest\ngolang/go1.6.2.linux-amd64.tar.gz  \n81\n MiB   f1833f76-ad8b-4b...  b8318b0...\nstress/stress-1.0.4.tar.gz         \n187\n KiB  \n(\nlocal\n)\n              e1533bc...\n\n\n2\n blobs\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh add-blob src-path dst-path\n\n\nSarts tracking blob in \nconfig/blobs.yml\n for inclusion in packages.\n\n\n$ \ncd\n release-dir\n$ bosh add-blob ~/Downloads/stress-1.0.4.tar.gz stress/stress-1.0.4.tar.gz\n\n\n\n\n\n\n\n\n \nbosh remove-blob blob-path\n\n\nStops tracking blob in \nconfig/blobs.yml\n. Does not remove previously uploaded copies from the blobstore as older release versions may still want to reference it.\n\n\n$ \ncd\n release-dir\n$ bosh remove-blob stress/stress-1.0.4.tar.gz\n\n\n\n\n\n\n\n\n \nbosh upload-blobs\n\n\nUploads previously added blobs that were not yet uploaded to the blobstore. Updates \nconfig/blobs.yml\n with returned blobstore IDs. Before creating a final release it's strongly recommended to upload blobs so that other release contributors can rebuild a release from scratch.\n\n\n$ \ncd\n release-dir\n$ bosh upload-blobs\n\n\n\n\n\n\n\n\n \nbosh sync-blobs\n\n\nDownloads blobs into \nblobs/\n based on \nconfig/blobs.yml\n.\n\n\n$ \ncd\n release-dir\n$ bosh sync-blobs\n\n\n\n\n\n\n\n\n\n\nReleases \n\u00b6\n\n\nSee \nUploading Releases\n.\n\n\n\n\n\n\n \nbosh -e my-env releases\n (Alias: \nrs\n)\n\n\nLists releases previously uploaded into the Director. Shows their names and versions.\n\n\n$ bosh -e my-env rs\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nName               Version   Commit Hash\ncapi               \n1\n.21.0*   716aa812\ncf-mysql           \n34\n*       e0508b5\ncf-smoke-tests     \n11\n*       a6dad6e\ncflinuxfs2-rootfs  \n1\n.52.0*   4827ef51+\nconsul             \n155\n*      22515a98+\ndiego              \n1\n.8.1*    0cca668e\ndns                \n3\n*        57e27da\netcd               \n94\n*       57c81e16\ngarden-runc        \n1\n.2.0*    2b3dedc5\nloggregator        \n78\n*       773a3ba\nnats               \n15\n*       d4dfc4c1+\nrouting            \n0\n.145.0*  dfb44c41+\nstatsd-injector    \n1\n.0.20*   552926d\nsyslog             \n9\n         ac2172f\nuaa                \n25\n*       86ec7568\n\n\n(\n*\n)\n Currently deployed\n\n(\n+\n)\n Uncommitted changes\n\n\n15\n releases\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env upload-release [location] [--version=ver] [--sha1=digest] [--fix]\n (Alias: \nur\n)\n\n\nUploads release to the Director. Succeeds even if release is already imported.\n\n\nRelease location may be local file system path, HTTP/HTTPS URL or a git URL.\n\n\n--fix\n flag allows replacement of previously uploaded release with the same name and version to repair releases that might have been corrupted.\n\n\n$ bosh -e my-env ur\n$ bosh -e my-env ur https://bosh.io/d/github.com/concourse/concourse?v\n=\n2\n.7.3\n$ bosh -e my-env ur git+https://github.com/concourse/concourse --version \n2\n.7.3\n\n\n\n\n\n\n\n\n \nbosh -e my-env delete-release name/version\n\n\nDeletes uploaded release from the Director. Succeeds even if release is not found.\n\n\n$ bosh -e my-env delete-release cf-smoke-tests/94\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep export-release name/version os/version [--dir=dir]\n\n\nCompiles and exports a release against a particular stemcell version.\n\n\nRequires to operate with a deployment so that compilation resources (VMs) are properly tracked.\n\n\nDestination directory default to the current directory.\n\n\n$ bosh -e my-env -d my-dep export-release cf-smoke-tests/94 ubuntu-trusty/3369\n\n\n\n\n\n\n\n\n \nbosh -e my-env inspect-release name/version\n\n\nLists all jobs, job metadata (such as links), packages, and compiled packages associated with a release version.\n\n\n$ bosh -e gcp-test inspect-release consul/155\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nJob                                                                    Blobstore ID       Digest       Links Consumed    Links Provided\nacceptance-tests/943c6083581e623dc66c7d9126d8e5989c4c2b31              0f3cd013-1d3d-...  17e5e4fc...  -                 -\nconsul-test-consumer-windows/6748c0675da2292c680da03e89b738a9d5818370  7461c74c-745d-...  9809861c...  -                 -\nconsul-test-consumer/7263db87ba85eaf0dd41bd198359c8597e961839          8bde4572-8e8b-...  7b08b059...  -                 -\nconsul_agent/b4872109282347700eaa884dcfe93f3a03dc22dd                  e41f705e-2cb7-...  a8db2c76...  - name: consul    - name: consul\n                                                                                                         type: consul      type: consul\n                                                                                                         optional: \ntrue\n\nconsul_agent_windows/a0b91cb0aa1029734d77fcf064dafdb67f14ada6          3a8755d0-7a39-...  17f07ec0...  - name: consul    - name: consul\n                                                                                                         type: consul      type: consul\n                                                                                                         optional: \ntrue\n\nfake-dns-server/a1ea5f64de0860512470ace7ce2376aa9470f9b1               5bb53f17-eba9-...  0565f9af...  -                 -\n\n\n6\n \njobs\n\n\nPackage                                                            Compiled \nfor\n          Blobstore ID            Digest\nacceptance-tests-windows/e36cef763e5cfd4e28738ad314807e6d1e13b960  \n(\nsource\n)\n              \n03589024\n-2596-49fc-...  96eaaf4ba...\nacceptance-tests/9d56ac03d7410dcdfd96a8c96bbc79eb4b53c864          \n(\nsource\n)\n              79fb9ba7-cd23-4b93-...  e08ee88f5...\nconfab-windows/52b117effcd95138eca94c789530bcd6499cff9d            \n(\nsource\n)\n              53d4b206-b064-462d-...  43f92c8d0...\nconfab/b2ff0bbd68b7d600ecb1ffaf41f59af073e894fd                    \n(\nsource\n)\n              b93214eb-a816-4029-...  4b627d264...\n~                                                                  ubuntu-trusty/3363.9  f66fe541-8c21-4fe3-...  8e662c2e2...\nconsul-windows/2a8e0b7ce1424d1d5efe5c7184791481a0c26424            \n(\nsource\n)\n              9516870b-801e-42ea-...  19db18127...\nconsul/6049d3016cd34ac64ccbf7837b06b6db81942102                    \n(\nsource\n)\n              04aa38af-e883-4842-...  c42cacfc7...\n~                                                                  ubuntu-trusty/3363.9  ab4afda6-881e-46b1-...  27c1390fa...\ngolang1.7-windows/1a80382e081cd429cf518f0c783f4e4172cac79e         \n(\nsource\n)\n              d7670210-7038-4749-...  b91caa06a...\ngolang1.7/181f7537c2ec17ac2406d9f2eb3322fd80fa2a1c                 \n(\nsource\n)\n              ac8aa36a-8965-46e9-...  ca440d716...\n~                                                                  ubuntu-trusty/3363.9  9d40794f-0c50-4d0c-...  9d6e29221...\n\n\n11\n packages\n\nSucceeded\n\n\n\n\n\n\n\n\n\n\nConfigs \n\u00b6\n\n\nSee \nConfigs\n.\n\n\n\n\n\n\n \nbosh -e my-env configs [--type=my-type] [--name=my-name]\n\n\nLists all the configs on the Director.\n\n\n$ bosh -e my-env configs\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nType     Name\ncloud    default\n~        custom-vm-types\ncpi      default\nruntime  default\n\n\n3\n configs\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env config [id] [--type=my-type] [--name=my-name]\n\n\nEither show config by \nid\n or by \nname\n and \ntype\n on the Director.\n\n\n$ bosh -e my-env config --type\n=\nmy-type --name\n=\nmy-name\n$ bosh -e my-env config \n5\n\n\n\n\n\n\n\n\n\n \nbosh -e my-env update-config config.yml --type=my-type [--name=my-name]\n\n\nUpdate config on the Director.\n\n\n\n\n--type\n (required) flag allows to specify config type\n\n\n--name\n flag allows to specify custom config name\n\n\n\n\n$ bosh -e my-env update-config config.yml --type\n=\ncloud\n$ bosh -e my-env update-config config.yml --type\n=\ncloud --name\n=\nnetwork1\n\n\n\n\n\n\n\n\n \nbosh -e my-env delete-config --type=my-type [--name=my-name]\n\n\nDelete config on the Director.\n\n\n\n\n--type\n (required) flag allows to specify config type\n\n\n--name\n flag allows to specify custom config name\n\n\n\n\n$ bosh -e my-env delete-config --type\n=\nmy-type\n$ bosh -e my-env delete-config --type\n=\nmy-type --name\n=\nmy-name\n\n\n\n\n\n\n\n\n\n\nCloud config \n\u00b6\n\n\nSee \nCloud config\n.\n\n\n\n\n\n\n \nbosh -e my-env cloud-config\n (Alias: \ncc\n)\n\n\nShow current cloud config on the Director.\n\n\n\n\n\n\n \nbosh -e my-env update-cloud-config config.yml [-v ...] [-o ...]\n (Alias: \nucc\n)\n\n\nUpdate current cloud config on the Director.\n\n\n$ bosh -e my-env ucc cc.yml\n\n\n\n\n\n\n\n\n\n\nRuntime config \n\u00b6\n\n\nSee \nRuntime config\n.\n\n\n\n\n\n\n \nbosh -e my-env runtime-config\n (Alias: \nrc\n)\n\n\nShow current runtime config on the Director.\n\n\n\n\n\n\n \nbosh -e my-env update-runtime-config config.yml [-v ...] [-o ...]\n (Alias: \nurc\n)\n\n\nUpdate current runtime config on the Director.\n\n\n$ bosh -e my-env urc runtime.yml\n\n\n\n\n\n\n\n\n\n\nCPI config \n\u00b6\n\n\nSee \nCPI config\n.\n\n\n\n\n\n\n \nbosh -e my-env cpi-config\n\n\nShow current CPI config on the Director.\n\n\n\n\n\n\n \nbosh -e my-env update-cpi-config config.yml [-v ...] [-o ...]\n\n\nUpdate current CPI config on the Director.\n\n\n$ bosh -e my-env update-cpi-config cpis.yml\n\n\n\n\n\n\n\n\n\n\nDeployments \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env deployments\n (Alias: \nds\n)\n\n\nLists deployments tracked by the Director. Shows their names, used releases and stemcells.\n\n\n$ bosh -e my-env ds\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nName                                Release\n(\ns\n)\n                Stemcell\n(\ns\n)\n                                         Team\n(\ns\n)\n  Cloud Config\ncf                                  binary-buildpack/1.0.9    bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest\n                                    capi/1.21.0\n                                    cf-mysql/34\n                                    cf-smoke-tests/11\n                                    cflinuxfs2-rootfs/1.52.0\n                                    consul/155\n                                    diego/1.8.1\n                                    etcd/94\n                                    garden-runc/1.2.0\n                                    loggregator/78\n                                    nats/15\n                                    routing/0.145.0\n                                    statsd-injector/1.0.20\n                                    uaa/25\nservice-instance_0d4140a0-42b7-...  mysql/0.6.0               bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest\n\n\n2\n deployments\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep deployment\n (Alias: \ndep\n)\n\n\nShows general deployment information for a given deployment.\n\n\nCan be used to determine if Director has a deployment with a given name.\n\n\n$ bosh -e vbox -d cf dep\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nName  Release\n(\ns\n)\n              Stemcell\n(\ns\n)\n                                         Team\n(\ns\n)\n  Cloud Config\ncf    binary-buildpack/1.0.9  bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest\n      capi/1.21.0\n      cf-mysql/34\n      cf-smoke-tests/11\n      uaa/25\n      dns/3\n      ...\n\n\n1\n deployments\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep deploy manifest.yml [-v ...] [-o ...]\n\n\nCreate or update specified deployment according to the provided manifest. Operation files and variables can be provided to adjust and fill in manifest before deploy begins.\n\n\nCurrently name provided via \n--deployment\n (\n-d\n) flag must match name specified in the manifest.\n\n\n$ bosh -e vbox -d cf deploy cf.yml -v \nsystem_domain\n=\nsys.example.com -o large-footprint.yml\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep delete-deployment [--force]\n (Alias: \ndeld\n)\n\n\nDeletes specified deployment. If \n--force\n is provided, ignores variety of errors (from IaaS, blobstore, database) when deleting.\n\n\nNote that if you've deleted your deployment, not all resources may have been freed. For example \"deleted\" persistent disks will be deleted after a few days to avoid accidental data loss. See \nPersistent and Orphaned Disks\n for more details.\n\n\nSucceeds even if deployment is not found.\n\n\n$ bosh -e vbox -d cf deld\n$ bosh -e vbox -d cf deld --force\n\n\n\n\n\n\n\n\n \nbosh -e my-env [-d my-dep] instances [--ps] [--details] [--vitals] [--failing]\n (Alias: \nis\n)\n\n\nLists all instances managed by the Director or in a single deployment. Show instance names, IPs, and VM and process health.\n\n\n\n\n--details\n (\n-i\n) flag includes VM CID, persistent disk CIDs, and other instance level details\n\n\n--ps\n flag includes per process health information\n\n\n--vitals\n flag shows basic VM and process usage such RAM, CPU and disk.\n\n\n--failing\n flag hides all healthy instances and processes leaving only non-healthy ones; useful for scripting\n\n\n\n\n$ bosh -e vbox is -i\n$ bosh -e vbox is --ps --vitals\n$ bosh -e vbox -d cf is\n$ bosh -e vbox -d cf is --ps\n$ bosh -e vbox -d cf is --ps --vitals\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep manifest\n (Alias: \nman\n)\n\n\nPrints deployment manifest to \nstdout\n.\n\n\n$ bosh -e vbox -d cf man > /tmp/manifest.yml\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep recreate [group[/instance-id]] [--skip-drain] [--fix] [--canaries=] [--max-in-flight=] [--dry-run]\n\n\nRecreates VMs for specified instances. Follows typical instance lifecycle.\n\n\n\n\n--skip-drain\n flag skips running drain scripts\n\n\n--fix\n flag replaces unresponsive VMs\n\n\n--canaries=\n flag overrides manifest values for \ncanaries\n\n\n--max-in-flight=\n flag overrides manifest values for \nmax_in_flight\n\n\n--dry-run\n flag runs through as many operations without altering deployment\n\n\n\n\n$ bosh -e vbox -d cf recreate\n$ bosh -e vbox -d cf recreate --fix\n$ bosh -e vbox -d cf recreate diego-cell\n$ bosh -e vbox -d cf recreate diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox -d cf recreate diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --skip-drain\n$ bosh -e vbox -d cf recreate diego-cell --canaries\n=\n0\n --max-in-flight\n=\n100\n%\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep restart [group[/instance-id]] [--skip-drain] [--canaries=] [--max-in-flight=]\n\n\nRestarts jobs (processes) on specified instances. Does not affect VM state.\n\n\n\n\n--skip-drain\n flag skips running drain scripts\n\n\n--canaries=\n flag overrides manifest values for \ncanaries\n\n\n--max-in-flight=\n flag overrides manifest values for \nmax_in_flight\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep start [group[/instance-id]] [--canaries=] [--max-in-flight=]\n\n\nStarts jobs (processes) on specified instances. Does not affect VM state.\n\n\n\n\n--canaries=\n flag overrides manifest values for \ncanaries\n\n\n--max-in-flight=\n flag overrides manifest values for \nmax_in_flight\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep stop [group[/instance-id]] [--skip-drain] [--canaries=] [--max-in-flight=]\n\n\nStops jobs (processes) on specified instances. Does not affect VM state unless \n--hard\n flag is specified.\n\n\n\n\n--hard\n flag forces VM deletion (keeping persistent disk)\n\n\n--skip-drain\n flag skips running drain scripts\n\n\n--canaries=\n flag overrides manifest values for \ncanaries\n\n\n--max-in-flight=\n flag overrides manifest values for \nmax_in_flight\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep ignore group/instance-id\n\n\nIgnores instance from being affected by other commands such as \nbosh deploy\n.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep unignore group/instance-id\n\n\nUnignores instance from being affected by other commands such as \nbosh deploy\n.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep logs [group[/instance-id]] [--follow] ...\n\n\nDownloads logs from one or more instances.\n\n\n\n\n--dir=\n flag specifies destination directory\n\n\n--job=\n flag includes only specific jobs logs\n\n\n--only=\n flag filters logs (comma-separated)\n\n\n--agent\n flag includes only BOSH Agent logs\n\n\n\n\nAdditional flags for following logs via SSH:\n\n\n\n\n--follow\n (\n-f\n) flag to turn on log following\n\n\n--num\n flag shows last number of lines immediately\n\n\n--quiet\n (\n-q\n) flag suppresses printing of headers when multiple files are examined\n\n\n--gw-*\n flags allow to configure SSH gateway configuration\n\n\n\n\nSee \nLocation and use of logs\n for details.\n\n\n$ bosh -e vbox -d cf logs diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox -d cf logs diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --job\n=\nrep --job\n=\nsilkd\n$ bosh -e vbox -d cf logs -f\n$ bosh -e vbox -d cf logs -f --num\n=\n1000\n\n\n\n\n\n\n\n\n\n \nbosh -e my-env [-d my-dep] events [--* ...]\n\n\nLists events.\n\n\nSee \nEvents\n for details.\n\n\n\n\n--before-id=\n flag shows events with ID less than the given ID\n\n\n--before=\n flag shows events before the given timestamp (ex: 2016-05-08 17:26:32)\n\n\n--after=\n flag shows events after the given timestamp (ex: 2016-05-08 17:26:32)\n\n\n--task=\n flag shows events with the given task ID\n\n\n--instance=\n flag shows events with given instance\n\n\n--event-user=\n flag shows events with given user\n\n\n--action=\n flag shows events with given action\n\n\n--object-type=\n flag shows events with given object type\n\n\n--object-id=\n flag shows events with given object ID\n\n\n\n\n$ bosh -e vbox events --instance diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox events --instance diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --task \n281\n\n$ bosh -e vbox events -d my-dep\n$ bosh -e vbox events --before-id\n=\n1298284\n\n$ bosh -e vbox events --before\n=\n\"2016-05-08 17:26:32 UTC\"\n --after\n=\n\"2016-05-07 UTC\"\n\n\n\n\n\n\n\n\n\n \nbosh -e my-env event id\n\n\nShows single event details.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep variables\n (Alias: \nvars\n)\n\n\nList variables referenced by the deployment.\n\n\n\n\n\n\n\n\nVMs \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env [-d my-dep] vms [--vitals]\n\n\nLists all VMs managed by the Director or VMs in a single deployment. Show instance names, IPs and VM CIDs.\n\n\n--vitals\n flag shows basic VM usage such RAM, CPU and disk.\n\n\n$ bosh -e vbox vms\n$ bosh -e vbox -d cf vms\n$ bosh -e vbox -d cf vms --vitals\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep delete-vm cid\n\n\nDeletes VM without going through typical instance lifecycle. Clears out VM reference from a Director database if referenced by any instance.\n\n\n$ bosh -e vbox -d cf delete-vm i-fs384238fjwjf8\n\n\n\n\n\n\n\n\n\n\nDisks \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep disks [--orphaned]\n\n\nLists disks. Currently only supports \n--orphaned\n flag.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep attach-disk group/instance-id disk-cid\n\n\nAttaches disk to an instance, replacing currently attached disk (if any).\n\n\n$ bosh -e vbox -d cf attach-disk postgres/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 vol-shw8f293f2f2\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep delete-disk cid\n\n\nDeletes orphaned disk.\n\n\n$ bosh -e vbox -d cf delete-disk vol-shw8f293f2f2\n\n\n\n\n\n\n\n\n\n\nSSH \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep ssh [destination] [-r] [-c=cmd] [--opts=opts] [--gw-* ...]\n\n\nSSH into one or more instances.\n\n\n\n\n--opts\n flag allows operator to pass through options to \nssh\n; useful for port forwarding\n\n\n--gw-*\n flags allows configuration of SSH gateway\n\n\n\n\n# execute command on all instances in a deployment\n\n$ bosh -e vbox -d cf ssh -c \n'uptime'\n\n\n\n# execute command on one instance group\n\n$ bosh -e vbox -d cf ssh diego-cell -c \n'uptime'\n\n\n\n# execute command on a single instance\n\n$ bosh -e vbox -d cf ssh diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 -c \n'uptime'\n\n\n\n# execute command with passwordless sudo\n\n$ bosh -e vbox -d cf ssh diego-cell -c \n'sudo lsof -i|grep LISTEN'\n\n\n\n# present output in a table by instance\n\n$ bosh -e vbox -d cf ssh -c \n'uptime'\n -r\n\n\n# port forward UAA port locally\n\n$ bosh -e vbox -d cf ssh uaa/0 --opts \n' -L 8080:localhost:8080'\n\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep scp src/dst:[file] src/dst:[file] [-r] [--gw-* ...]\n\n\nSCP to/from one or more instances.\n\n\n\n\n--recursive\n (\n-r\n) flag allow to copy directory recursively\n\n\n--gw-*\n flags allow to configure gateway configuration\n\n\n\n\n# copy file from this machine to machines a deployment\n\n$ bosh -e vbox -d cf scp ~/Downloads/script.sh :/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.sh diego-cell:/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.sh diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0:/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.ps1 windows_diego_cell:c:/temp/script/script.ps1\n\n\n# copy file from remote machines in a deployment to this machine\n\n$ bosh -e vbox -d cf scp :/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp diego-cell:/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0:/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp windows_diego_cell:c:/temp/script/script.ps1:~/Downloads/script.ps1\n\n\n# copy files from each instance into instance specific local directory\n\n$ bosh -e vbox -d cf scp diego-cell:/tmp/logs/ /tmp/logs/\n((\ninstance_id\n))\n\n\n\n\n\n\n\n\n\n\n\nErrands \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep errands\n (Alias: \nes\n)\n\n\nLists all errands defined by the deployment.\n\n\n$ bosh -e vbox -d cf es\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nUsing deployment \n'cf'\n\n\nName\nsmoke-tests\n\n\n1\n errands\n\nSucceeded\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep run-errand name [--keep-alive] [--when-changed] [--download-logs] [--logs-dir=dir] [--instance=instance-group/instance-id]\n\n\nRuns errand job by name.\n\n\n\n\n--keep-alive\n flag keeps VM around where errand was executing\n\n\n--when-changed\n flag indicates whether to skip running an errand if it previously ran (successfully finished) and errand job configuration did not change\n\n\n--download-logs\n flag indicates whether to download full errand logs to a directory specified by \n--logs-dir\n (defaults to the current directory)\n\n\n--instance=\n flag select which instances to use for errand execution (v2.0.31+)\n\n\n\n\nSee \nErrands\n for details.\n\n\n$ bosh -e vbox -d cf run-errand smoke-tests\n$ bosh -e vbox -d cf run-errand smoke-tests --keep-alive\n$ bosh -e vbox -d cf run-errand smoke-tests --when-changed\n\n\n# execute errand on all instances that have colocated status errand\n\n$ bosh -e vbox -d zookeeper run-errand status\n\n\n# execute errand on one instance\n\n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\n\n\n# execute errand on one instance within an instance group\n\n\n# (note that select instance may not necessarily be first based on its index)\n\n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/first\n\n\n# execute errand on all instance in an instance group\n\n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper\n\n\n# execute errand on two instances\n\n$ bosh -e vbox -d zookeeper run-errand status \n\\\n\n  --instance zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc \n\\\n\n  --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5\n\n\n\n\n\n\n\n\n\n\nTasks \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env tasks [--recent[=num]] [--all]\n (Alias: \nts\n)\n\n\nLists active and previously ran tasks.\n\n\n\n\n--deployment\n (\n-d\n) flag filters tasks by a deployment\n\n\n\n\n# currently active tasks\n\n$ bosh -e vbox ts\n\n\n# currently active tasks for my-dep deployment\n\n$ bosh -e vbox -d my-dep ts\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\n\n#   State  Started At                    Last Activity At              User   Deployment   Description                   Result\n\n\n\n27\n  \ndone\n   Thu Feb \n16\n \n19\n:16:15 UTC \n2017\n  Thu Feb \n16\n \n19\n:20:33 UTC \n2017\n  admin  cockroachdb  create deployment             /deployments/cockroachdb\n\n26\n  \ndone\n   Thu Feb \n16\n \n18\n:54:32 UTC \n2017\n  Thu Feb \n16\n \n18\n:55:27 UTC \n2017\n  admin  cockroachdb  delete deployment cockroachd  /deployments/cockroachdb\n...\n\n\n110\n tasks\n\nSucceeded\n\n\n# show last 30 tasks\n\n$ bosh -e vbox ts -r --all\n\n\n# show last 1000 tasks\n\n$ bosh -e vbox ts -r\n=\n1000\n\n\n\n\n\n\n\n\n\n \nbosh -e my-env task id [--debug] [--result] [--event] [--cpi]\n (Alias: \nt\n)\n\n\nShows single task details. Continues to follow task if it did not finish. \nCtrl^C\n does not cancel task.\n\n\n$ bosh -e vbox t \n281\n\n$ bosh -e vbox t \n281\n --debug\n\n\n\n\n\n\n\n\n \nbosh -e my-env cancel-task id\n (Alias: \nct\n)\n\n\nCancel task at its next checkpoint. Does not wait until task is cancelled.\n\n\n$ bosh -e vbox ct \n281\n\n\n\n\n\n\n\n\n\n\n\nSnapshots \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep snapshots\n\n\nLists disk snapshots for given deployment.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep take-snapshot [group/instance-id]\n\n\nTakes snapshot for an instance or an entire deployment.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep delete-snapshot cid\n\n\nDeletes snapshot.\n\n\n$ bosh -e vbox -d cf delete-snapshot snap-shw38ty83f2f2\n\n\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep delete-snapshots\n\n\nDeletes snapshots for an entire deployment.\n\n\n\n\n\n\n\n\nDeployment recovery \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env update-resurrection on/off\n\n\nEnables or disables resurrection globally. This state is not reflected in the \nbosh instances\n command's \nResurrection\n column.\n\n\nSee \nAutomatic repair with Resurrector\n for details.\n\n\n\n\n\n\n \nbosh -e my-env -d my-dep cloud-check [--report] [--auto]\n (Alias: \ncck\n)\n\n\nChecks for resource consistency and allows interactive repair.\n\n\nSee \nManual repair with Cloud Check\n for details.\n\n\n\n\n\n\n \nbosh -e my-env locks\n\n\nLists current locks.\n\n\n\n\n\n\n\n\nMisc \n\u00b6\n\n\n\n\n\n\n \nbosh -e my-env clean-up [--all]\n\n\nCleans up releases, stemcells, orphaned disks, and other unused resources.\n\n\n\n\n--all\n flag forces cleanup for orphaned disks\n\n\n\n\n\n\n\n\n \nbosh help\n\n\nShows list of available commands and global options. Consider using \n-h\n flag for individual commands.\n\n\n\n\n\n\n \nbosh interpolate manifest.yml [-v ...] [-o ...] [--vars-store path] [--path op-path]\n (Alias: \nint\n)\n\n\nInterpolates variables into a manifest sending result to stdout. \nOperation files\n and \nvariables\n can be provided to adjust and fill in manifest before doing a deploy.\n\n\n--path\n flag can be used to extract portion of a YAML document.\n\n\n$ bosh int bosh-deployment/bosh.yml \n\\\n\n  --vars-store ./creds.yml \n\\\n\n  -o bosh-deployment/virtualbox/cpi.yml \n\\\n\n  -o bosh-deployment/virtualbox/outbound-network.yml \n\\\n\n  -o bosh-deployment/bosh-lite.yml \n\\\n\n  -o bosh-deployment/jumpbox-user.yml \n\\\n\n  -v \ndirector_name\n=\nvbox \n\\\n\n  -v \ninternal_ip\n=\n192\n.168.56.6 \n\\\n\n  -v \ninternal_gw\n=\n192\n.168.56.1 \n\\\n\n  -v \ninternal_cidr\n=\n192\n.168.56.0/24 \n\\\n\n  -v \nnetwork_name\n=\nvboxnet0 \n\\\n\n  -v \noutbound_network_name\n=\nNatNetwork\n\n$ bosh int creds.yml --path /admin_password\nskh32i7rdfji4387hg\n\n$ bosh int creds.yml --path /director_ssl/ca\n-----BEGIN CERTIFICATE-----\n...\n\n\n\n\nCommand can be used in a generic way to generate CA and leaf certificates.\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \ndefault_ca\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \ncommon_name\n:\n \nca\n\n\n-\n \nname\n:\n \nservice_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ndefault_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n\n\n\n$ bosh interpolate certs-tpl.yml -v \ninternal_ip\n=\n1\n.2.3.4 --vars-store certs.yml --var-errs\n$ bosh interpolate certs.yml --path /service_ssl/ca\n$ bosh interpolate certs.yml --path /service_ssl/certificate\n$ bosh interpolate certs.yml --path /service_ssl/private_key\n\n\n\n\n\n\n\n\n\n\nNext: \nDifferences between CLI v2 vs v1",
            "title": "Commands"
        },
        {
            "location": "/cli-v2/#install",
            "text": "Download the binary for your platform and place it on your  PATH :    bosh-cli-3.0.1-darwin-amd64   sha1: d2fea20210a47b8c8f1f7dbb27ffb5808d47ce87   bosh-cli-3.0.1-linux-amd64   sha1: ccc893bab8b219e9e4a628ed044ebca6c6de9ca0   bosh-cli-3.0.1-windows-amd64.exe   sha1: 41c23c90cab9dc62fa0a1275dcaf64670579ed33  (Windows CLI support is partial)  $ chmod +x ~/Downloads/bosh-cli-*\n$ sudo mv ~/Downloads/bosh-cli-* /usr/local/bin/bosh    Check  bosh  version to make sure it is properly installed:  $ bosh -v\nversion  3 .0.1-712bfd7-2018-03-13T23:26:42Z  If the output does not begin with  version 2.0...  (or  3.0 ) you are probably executing CLI v1 (Ruby based).    Install OS specified dependencies  for  bosh create-env  command    Alternatively, refer to  cloudfoundry/homebrew-tap  to install CLI via Homebrew on OS X. We currently do not publish CLI via apt or yum repositories.",
            "title": "Install "
        },
        {
            "location": "/cli-v2/#release-notes",
            "text": "CLI release notes can be found  on Github .",
            "title": "Release Notes "
        },
        {
            "location": "/cli-v2/#global-flags",
            "text": "See  Global flags  for more details on how to enable different output formats, debug logging, etc.",
            "title": "Global Flags "
        },
        {
            "location": "/cli-v2/#commands",
            "text": "",
            "title": "Commands "
        },
        {
            "location": "/cli-v2/#environments",
            "text": "See  Environments .      bosh environments  (Alias:  envs )  Lists aliased environments known to the CLI. Aliasing is done via  alias-env  command.  $ bosh envs\nURL              Alias 104 .154.171.255  gcp 192 .168.56.6     vbox 2  environments\n\nSucceeded      bosh create-env manifest.yml [--state path] [-v ...] [-o ...] [--vars-store path]  Creates single VM based on the manifest. Typically used to create a Director environment.  Operation files  and  variables  can be provided to adjust and fill in manifest before doing a deploy.  create-env  command replaces  bosh-init deploy  CLI command.  $ bosh create-env ~/workspace/bosh-deployment/bosh.yml  \\ \n  --state state.json  \\ \n  --vars-store ./creds.yml  \\ \n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml  \\ \n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml  \\ \n  -o ~/workspace/bosh-deployment/bosh-lite.yml  \\ \n  -o ~/workspace/bosh-deployment/jumpbox-user.yml  \\ \n  -v  director_name = vbox  \\ \n  -v  internal_ip = 192 .168.56.6  \\ \n  -v  internal_gw = 192 .168.56.1  \\ \n  -v  internal_cidr = 192 .168.56.0/24  \\ \n  -v  network_name = vboxnet0  \\ \n  -v  outbound_network_name = NatNetwork      bosh alias-env name -e location [--ca-cert=path]  Assigns a name to the created environment for easier access in subsequent CLI commands. Instead of specifying Director location and possibly a CA certificate, subsequent commands can just take given name via  --environment  flag ( -e ).  $ bosh alias-env gcp -e bosh.corp.com\n$ bosh alias-env gcp -e  10 .0.0.6 --ca-cert < ( bosh int creds.yml --path /director_ssl/ca )       bosh -e location environment  (Alias:  env )  Shows Director information in the deployment environment.  $ bosh -e vbox env\nUsing environment  '192.168.56.6'  as  '?' \n\nName      vbox\nUUID      eeb27cc6-467e-4c1d-a8f9-f1a8de759f52\nVersion    260 .5.0  ( 00000000 ) \nCPI       warden_cpi\nFeatures  compiled_package_cache: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\n\nSucceeded      bosh delete-env manifest.yml [--state path] [-v ...] [-o ...] [--vars-store path]  Deletes previously created VM based on the manifest. Same flags provided to  create-env  command should be given to the  delete-env  command.  delete-env  command replaces  bosh-init delete  CLI command.  $ bosh delete-env ~/workspace/bosh-deployment/bosh.yml  \\ \n  --state state.json  \\ \n  --vars-store ./creds.yml  \\ \n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml  \\ \n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml  \\ \n  -o ~/workspace/bosh-deployment/bosh-lite.yml  \\ \n  -o ~/workspace/bosh-deployment/jumpbox-user.yml  \\ \n  -v  director_name = vbox  \\ \n  -v  internal_ip = 192 .168.56.6  \\ \n  -v  internal_gw = 192 .168.56.1  \\ \n  -v  internal_cidr = 192 .168.56.0/24  \\ \n  -v  network_name = vboxnet0  \\ \n  -v  outbound_network_name = NatNetwork",
            "title": "Environments "
        },
        {
            "location": "/cli-v2/#session",
            "text": "bosh log-in  (Alias:  l ,  login )  Logs in given user into the Director.  This command can only be used interactively. If non-interactive use is necessary (for example in scripts) please set  BOSH_CLIENT  and  BOSH_CLIENT_SECRET  environment variables instead of using this command. Note that if the Director is configured with UAA authentication you cannot use UAA users with BOSH_* environment variables but rather have to use UAA clients.  $ bosh -e my-env l\nUser  () : admin\nPassword  () :      bosh log-out  (Alias:  logout )  Logs out currently logged in user.",
            "title": "Session "
        },
        {
            "location": "/cli-v2/#stemcells",
            "text": "See  Uploading Stemcells .      bosh -e my-env stemcells  (Alias:  ss )  Lists stemcells previously uploaded into the Director. Shows their names, versions and CIDs.  $ bosh -e my-env ss\nUsing environment  '192.168.56.6'  as  '?' \n\nName                                         Version  OS             CPI  CID\nbosh-warden-boshlite-ubuntu-trusty-go_agent   3363 *    ubuntu-trusty  -    6cbb176a-6a43-42...\n~                                             3312      ubuntu-trusty  -    43r3496a-4rt3-52...\nbosh-warden-boshlite-centos-7-go_agent        3363 *    centos-7       -    38yr83gg-349r-94... ( * )  Currently deployed 3  stemcells\n\nSucceeded      bosh -e my-env upload-stemcell location [--sha1=digest] [--fix]  (Alias:  us )  Uploads stemcell to the Director. Succeeds even if stemcell is already imported.  Stemcell location may be local file system path or an HTTP/HTTPS URL.  --fix  flag allows operator to replace previously uploaded stemcell with the same name and version to repair stemcells that might have been corrupted in the cloud.  $ bosh -e my-env us ~/Downloads/bosh-stemcell-3468.17-warden-boshlite-ubuntu-trusty-go_agent.tgz\n$ bosh -e my-env us https://bosh.io/d/stemcells/bosh-stemcell-warden-boshlite-ubuntu-trusty-go_agent?v = 3468 .17      bosh -e my-env delete-stemcell name/version  Deletes uploaded stemcell from the Director. Succeeds even if stemcell is not found.  $ bosh -e my-env delete-stemcell bosh-warden-boshlite-ubuntu-trusty-go_agent/3468.17      bosh repack-stemcell src.tgz dst.tgz [--name=name] [--version=ver] [--cloud-properties=json-string]  Produces new stemcell tarball with updated properties such as name, version, and cloud properties.  See  Repacking stemcells  for details.",
            "title": "Stemcells "
        },
        {
            "location": "/cli-v2/#release-creation",
            "text": "bosh init-release [--git] [--dir=dir]  Creates an empty release skeleton for a release in  dir . By default  dir  is the current directory.  --git  flag initializes release skeleton as a Git repository, adding appropriate  .gitignore  file.  $ bosh init-release --git --dir release-dir\n$  cd  release-dir      bosh generate-job name [--dir=dir]  Creates an empty job skeleton for a release in  dir . Includes bare  spec  and an empty  monit  file.      bosh generate-package name [--dir=dir]  Creates an empty package skeleton for a release in  dir . Includes bare  spec  and an empty  packaging  file.      bosh vendor-package name src-dir [--dir=dir]  (v2.0.36+)  Vendors a package from a different release into a release in  dir . It includes  spec.lock  in the package directory so that CLI will reference specific package by its fingerprint when creating releases.  See  Package vendoring  for details.      bosh create-release [--force] [--version=ver] [--timestamp-version] [--final] [--tarball=path] [--dir=dir]  (Alias:  cr )  Creates new version of a release stored in  dir   --force  flag specifies to ignore uncommitted changes in the release directory; it should only be used when building dev releases  --version  flag allows operator to provide custom release version  --timestamp-version  flag will produce timestamp based dev release version  --tarball  flag specifies destination of a release tarball; if not specified, release tarball will not be produced  --sha2  flag to use SHA256 checksum   While iterating on a release it's common to run  bosh create-release --force && bosh -e my-env upload-release && bosh -e my-env -d my-dep deploy manifest.yml  command sequence.  In a CI pipeline it's common to use this command to create a release tarball and pass it into acceptance or end-to-end tests. Once release tarball goes through appropriate tests it can be finalized with a  finalize-release  command and shared with release consumers.  $  cd  release-dir\n$ bosh create-release --force\n$ bosh create-release --timestamp-version\n$ bosh create-release --version  9 +dev.10\n$ bosh create-release --tarball /tmp/my-release.tgz\n$ bosh create-release --final\n$ bosh create-release --version  20  --final\n$ bosh create-release releases/zookeeper/zookeeper-3.yml --tarball /tmp/my-release.tgz      bosh finalize-release release.tgz [--force] [--version=ver] [--dir=dir]  Records contents of a release tarball as a final release with an optionally given version. Once  .final_builds  and  releases  directories are updated, it's strongly recommended to commit your changes to version control.  Typically this command is used as a final step in the CI pipeline to save the final artifact once it passed appropriate tests.  $  cd  release-dir\n$ bosh finalize-release /tmp/my-release.tgz\n$ bosh finalize-release /tmp/my-release.tgz --version  20 \n$ git commit -am  'Final release 20' \n$ git push origin master      bosh reset-release [--dir=dir]  Removes temporary artifacts such as dev releases, blobs, etc. kept in the release directory  dir .",
            "title": "Release creation "
        },
        {
            "location": "/cli-v2/#release-blobs",
            "text": "See  Release Blobs  for a detailed workflow.      bosh blobs  Lists tracked blobs from  config/blobs.yml . Shows uploaded and not-yet-uploaded blobs.  $  cd  release-dir\n$ bosh blobs\nPath                               Size     Blobstore ID         Digest\ngolang/go1.6.2.linux-amd64.tar.gz   81  MiB   f1833f76-ad8b-4b...  b8318b0...\nstress/stress-1.0.4.tar.gz          187  KiB   ( local )               e1533bc... 2  blobs\n\nSucceeded      bosh add-blob src-path dst-path  Sarts tracking blob in  config/blobs.yml  for inclusion in packages.  $  cd  release-dir\n$ bosh add-blob ~/Downloads/stress-1.0.4.tar.gz stress/stress-1.0.4.tar.gz      bosh remove-blob blob-path  Stops tracking blob in  config/blobs.yml . Does not remove previously uploaded copies from the blobstore as older release versions may still want to reference it.  $  cd  release-dir\n$ bosh remove-blob stress/stress-1.0.4.tar.gz      bosh upload-blobs  Uploads previously added blobs that were not yet uploaded to the blobstore. Updates  config/blobs.yml  with returned blobstore IDs. Before creating a final release it's strongly recommended to upload blobs so that other release contributors can rebuild a release from scratch.  $  cd  release-dir\n$ bosh upload-blobs      bosh sync-blobs  Downloads blobs into  blobs/  based on  config/blobs.yml .  $  cd  release-dir\n$ bosh sync-blobs",
            "title": "Release blobs "
        },
        {
            "location": "/cli-v2/#releases",
            "text": "See  Uploading Releases .      bosh -e my-env releases  (Alias:  rs )  Lists releases previously uploaded into the Director. Shows their names and versions.  $ bosh -e my-env rs\nUsing environment  '192.168.56.6'  as client  'admin' \n\nName               Version   Commit Hash\ncapi                1 .21.0*   716aa812\ncf-mysql            34 *       e0508b5\ncf-smoke-tests      11 *       a6dad6e\ncflinuxfs2-rootfs   1 .52.0*   4827ef51+\nconsul              155 *      22515a98+\ndiego               1 .8.1*    0cca668e\ndns                 3 *        57e27da\netcd                94 *       57c81e16\ngarden-runc         1 .2.0*    2b3dedc5\nloggregator         78 *       773a3ba\nnats                15 *       d4dfc4c1+\nrouting             0 .145.0*  dfb44c41+\nstatsd-injector     1 .0.20*   552926d\nsyslog              9          ac2172f\nuaa                 25 *       86ec7568 ( * )  Currently deployed ( + )  Uncommitted changes 15  releases\n\nSucceeded      bosh -e my-env upload-release [location] [--version=ver] [--sha1=digest] [--fix]  (Alias:  ur )  Uploads release to the Director. Succeeds even if release is already imported.  Release location may be local file system path, HTTP/HTTPS URL or a git URL.  --fix  flag allows replacement of previously uploaded release with the same name and version to repair releases that might have been corrupted.  $ bosh -e my-env ur\n$ bosh -e my-env ur https://bosh.io/d/github.com/concourse/concourse?v = 2 .7.3\n$ bosh -e my-env ur git+https://github.com/concourse/concourse --version  2 .7.3      bosh -e my-env delete-release name/version  Deletes uploaded release from the Director. Succeeds even if release is not found.  $ bosh -e my-env delete-release cf-smoke-tests/94      bosh -e my-env -d my-dep export-release name/version os/version [--dir=dir]  Compiles and exports a release against a particular stemcell version.  Requires to operate with a deployment so that compilation resources (VMs) are properly tracked.  Destination directory default to the current directory.  $ bosh -e my-env -d my-dep export-release cf-smoke-tests/94 ubuntu-trusty/3369      bosh -e my-env inspect-release name/version  Lists all jobs, job metadata (such as links), packages, and compiled packages associated with a release version.  $ bosh -e gcp-test inspect-release consul/155\nUsing environment  '192.168.56.6'  as client  'admin' \n\nJob                                                                    Blobstore ID       Digest       Links Consumed    Links Provided\nacceptance-tests/943c6083581e623dc66c7d9126d8e5989c4c2b31              0f3cd013-1d3d-...  17e5e4fc...  -                 -\nconsul-test-consumer-windows/6748c0675da2292c680da03e89b738a9d5818370  7461c74c-745d-...  9809861c...  -                 -\nconsul-test-consumer/7263db87ba85eaf0dd41bd198359c8597e961839          8bde4572-8e8b-...  7b08b059...  -                 -\nconsul_agent/b4872109282347700eaa884dcfe93f3a03dc22dd                  e41f705e-2cb7-...  a8db2c76...  - name: consul    - name: consul\n                                                                                                         type: consul      type: consul\n                                                                                                         optional:  true \nconsul_agent_windows/a0b91cb0aa1029734d77fcf064dafdb67f14ada6          3a8755d0-7a39-...  17f07ec0...  - name: consul    - name: consul\n                                                                                                         type: consul      type: consul\n                                                                                                         optional:  true \nfake-dns-server/a1ea5f64de0860512470ace7ce2376aa9470f9b1               5bb53f17-eba9-...  0565f9af...  -                 - 6   jobs \n\nPackage                                                            Compiled  for           Blobstore ID            Digest\nacceptance-tests-windows/e36cef763e5cfd4e28738ad314807e6d1e13b960   ( source )                03589024 -2596-49fc-...  96eaaf4ba...\nacceptance-tests/9d56ac03d7410dcdfd96a8c96bbc79eb4b53c864           ( source )               79fb9ba7-cd23-4b93-...  e08ee88f5...\nconfab-windows/52b117effcd95138eca94c789530bcd6499cff9d             ( source )               53d4b206-b064-462d-...  43f92c8d0...\nconfab/b2ff0bbd68b7d600ecb1ffaf41f59af073e894fd                     ( source )               b93214eb-a816-4029-...  4b627d264...\n~                                                                  ubuntu-trusty/3363.9  f66fe541-8c21-4fe3-...  8e662c2e2...\nconsul-windows/2a8e0b7ce1424d1d5efe5c7184791481a0c26424             ( source )               9516870b-801e-42ea-...  19db18127...\nconsul/6049d3016cd34ac64ccbf7837b06b6db81942102                     ( source )               04aa38af-e883-4842-...  c42cacfc7...\n~                                                                  ubuntu-trusty/3363.9  ab4afda6-881e-46b1-...  27c1390fa...\ngolang1.7-windows/1a80382e081cd429cf518f0c783f4e4172cac79e          ( source )               d7670210-7038-4749-...  b91caa06a...\ngolang1.7/181f7537c2ec17ac2406d9f2eb3322fd80fa2a1c                  ( source )               ac8aa36a-8965-46e9-...  ca440d716...\n~                                                                  ubuntu-trusty/3363.9  9d40794f-0c50-4d0c-...  9d6e29221... 11  packages\n\nSucceeded",
            "title": "Releases "
        },
        {
            "location": "/cli-v2/#configs",
            "text": "See  Configs .      bosh -e my-env configs [--type=my-type] [--name=my-name]  Lists all the configs on the Director.  $ bosh -e my-env configs\nUsing environment  '192.168.50.6'  as client  'admin' \n\nType     Name\ncloud    default\n~        custom-vm-types\ncpi      default\nruntime  default 3  configs\n\nSucceeded      bosh -e my-env config [id] [--type=my-type] [--name=my-name]  Either show config by  id  or by  name  and  type  on the Director.  $ bosh -e my-env config --type = my-type --name = my-name\n$ bosh -e my-env config  5       bosh -e my-env update-config config.yml --type=my-type [--name=my-name]  Update config on the Director.   --type  (required) flag allows to specify config type  --name  flag allows to specify custom config name   $ bosh -e my-env update-config config.yml --type = cloud\n$ bosh -e my-env update-config config.yml --type = cloud --name = network1      bosh -e my-env delete-config --type=my-type [--name=my-name]  Delete config on the Director.   --type  (required) flag allows to specify config type  --name  flag allows to specify custom config name   $ bosh -e my-env delete-config --type = my-type\n$ bosh -e my-env delete-config --type = my-type --name = my-name",
            "title": "Configs "
        },
        {
            "location": "/cli-v2/#cloud-config",
            "text": "See  Cloud config .      bosh -e my-env cloud-config  (Alias:  cc )  Show current cloud config on the Director.      bosh -e my-env update-cloud-config config.yml [-v ...] [-o ...]  (Alias:  ucc )  Update current cloud config on the Director.  $ bosh -e my-env ucc cc.yml",
            "title": "Cloud config "
        },
        {
            "location": "/cli-v2/#runtime-config",
            "text": "See  Runtime config .      bosh -e my-env runtime-config  (Alias:  rc )  Show current runtime config on the Director.      bosh -e my-env update-runtime-config config.yml [-v ...] [-o ...]  (Alias:  urc )  Update current runtime config on the Director.  $ bosh -e my-env urc runtime.yml",
            "title": "Runtime config "
        },
        {
            "location": "/cli-v2/#cpi-config",
            "text": "See  CPI config .      bosh -e my-env cpi-config  Show current CPI config on the Director.      bosh -e my-env update-cpi-config config.yml [-v ...] [-o ...]  Update current CPI config on the Director.  $ bosh -e my-env update-cpi-config cpis.yml",
            "title": "CPI config "
        },
        {
            "location": "/cli-v2/#deployments",
            "text": "bosh -e my-env deployments  (Alias:  ds )  Lists deployments tracked by the Director. Shows their names, used releases and stemcells.  $ bosh -e my-env ds\nUsing environment  '192.168.56.6'  as client  'admin' \n\nName                                Release ( s )                 Stemcell ( s )                                          Team ( s )   Cloud Config\ncf                                  binary-buildpack/1.0.9    bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest\n                                    capi/1.21.0\n                                    cf-mysql/34\n                                    cf-smoke-tests/11\n                                    cflinuxfs2-rootfs/1.52.0\n                                    consul/155\n                                    diego/1.8.1\n                                    etcd/94\n                                    garden-runc/1.2.0\n                                    loggregator/78\n                                    nats/15\n                                    routing/0.145.0\n                                    statsd-injector/1.0.20\n                                    uaa/25\nservice-instance_0d4140a0-42b7-...  mysql/0.6.0               bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest 2  deployments\n\nSucceeded      bosh -e my-env -d my-dep deployment  (Alias:  dep )  Shows general deployment information for a given deployment.  Can be used to determine if Director has a deployment with a given name.  $ bosh -e vbox -d cf dep\nUsing environment  '192.168.56.6'  as client  'admin' \n\nName  Release ( s )               Stemcell ( s )                                          Team ( s )   Cloud Config\ncf    binary-buildpack/1.0.9  bosh-warden-boshlite-ubuntu-trusty-go_agent/3363.9  -        latest\n      capi/1.21.0\n      cf-mysql/34\n      cf-smoke-tests/11\n      uaa/25\n      dns/3\n      ... 1  deployments\n\nSucceeded      bosh -e my-env -d my-dep deploy manifest.yml [-v ...] [-o ...]  Create or update specified deployment according to the provided manifest. Operation files and variables can be provided to adjust and fill in manifest before deploy begins.  Currently name provided via  --deployment  ( -d ) flag must match name specified in the manifest.  $ bosh -e vbox -d cf deploy cf.yml -v  system_domain = sys.example.com -o large-footprint.yml      bosh -e my-env -d my-dep delete-deployment [--force]  (Alias:  deld )  Deletes specified deployment. If  --force  is provided, ignores variety of errors (from IaaS, blobstore, database) when deleting.  Note that if you've deleted your deployment, not all resources may have been freed. For example \"deleted\" persistent disks will be deleted after a few days to avoid accidental data loss. See  Persistent and Orphaned Disks  for more details.  Succeeds even if deployment is not found.  $ bosh -e vbox -d cf deld\n$ bosh -e vbox -d cf deld --force      bosh -e my-env [-d my-dep] instances [--ps] [--details] [--vitals] [--failing]  (Alias:  is )  Lists all instances managed by the Director or in a single deployment. Show instance names, IPs, and VM and process health.   --details  ( -i ) flag includes VM CID, persistent disk CIDs, and other instance level details  --ps  flag includes per process health information  --vitals  flag shows basic VM and process usage such RAM, CPU and disk.  --failing  flag hides all healthy instances and processes leaving only non-healthy ones; useful for scripting   $ bosh -e vbox is -i\n$ bosh -e vbox is --ps --vitals\n$ bosh -e vbox -d cf is\n$ bosh -e vbox -d cf is --ps\n$ bosh -e vbox -d cf is --ps --vitals      bosh -e my-env -d my-dep manifest  (Alias:  man )  Prints deployment manifest to  stdout .  $ bosh -e vbox -d cf man > /tmp/manifest.yml      bosh -e my-env -d my-dep recreate [group[/instance-id]] [--skip-drain] [--fix] [--canaries=] [--max-in-flight=] [--dry-run]  Recreates VMs for specified instances. Follows typical instance lifecycle.   --skip-drain  flag skips running drain scripts  --fix  flag replaces unresponsive VMs  --canaries=  flag overrides manifest values for  canaries  --max-in-flight=  flag overrides manifest values for  max_in_flight  --dry-run  flag runs through as many operations without altering deployment   $ bosh -e vbox -d cf recreate\n$ bosh -e vbox -d cf recreate --fix\n$ bosh -e vbox -d cf recreate diego-cell\n$ bosh -e vbox -d cf recreate diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox -d cf recreate diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --skip-drain\n$ bosh -e vbox -d cf recreate diego-cell --canaries = 0  --max-in-flight = 100 %      bosh -e my-env -d my-dep restart [group[/instance-id]] [--skip-drain] [--canaries=] [--max-in-flight=]  Restarts jobs (processes) on specified instances. Does not affect VM state.   --skip-drain  flag skips running drain scripts  --canaries=  flag overrides manifest values for  canaries  --max-in-flight=  flag overrides manifest values for  max_in_flight       bosh -e my-env -d my-dep start [group[/instance-id]] [--canaries=] [--max-in-flight=]  Starts jobs (processes) on specified instances. Does not affect VM state.   --canaries=  flag overrides manifest values for  canaries  --max-in-flight=  flag overrides manifest values for  max_in_flight       bosh -e my-env -d my-dep stop [group[/instance-id]] [--skip-drain] [--canaries=] [--max-in-flight=]  Stops jobs (processes) on specified instances. Does not affect VM state unless  --hard  flag is specified.   --hard  flag forces VM deletion (keeping persistent disk)  --skip-drain  flag skips running drain scripts  --canaries=  flag overrides manifest values for  canaries  --max-in-flight=  flag overrides manifest values for  max_in_flight       bosh -e my-env -d my-dep ignore group/instance-id  Ignores instance from being affected by other commands such as  bosh deploy .      bosh -e my-env -d my-dep unignore group/instance-id  Unignores instance from being affected by other commands such as  bosh deploy .      bosh -e my-env -d my-dep logs [group[/instance-id]] [--follow] ...  Downloads logs from one or more instances.   --dir=  flag specifies destination directory  --job=  flag includes only specific jobs logs  --only=  flag filters logs (comma-separated)  --agent  flag includes only BOSH Agent logs   Additional flags for following logs via SSH:   --follow  ( -f ) flag to turn on log following  --num  flag shows last number of lines immediately  --quiet  ( -q ) flag suppresses printing of headers when multiple files are examined  --gw-*  flags allow to configure SSH gateway configuration   See  Location and use of logs  for details.  $ bosh -e vbox -d cf logs diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox -d cf logs diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --job = rep --job = silkd\n$ bosh -e vbox -d cf logs -f\n$ bosh -e vbox -d cf logs -f --num = 1000       bosh -e my-env [-d my-dep] events [--* ...]  Lists events.  See  Events  for details.   --before-id=  flag shows events with ID less than the given ID  --before=  flag shows events before the given timestamp (ex: 2016-05-08 17:26:32)  --after=  flag shows events after the given timestamp (ex: 2016-05-08 17:26:32)  --task=  flag shows events with the given task ID  --instance=  flag shows events with given instance  --event-user=  flag shows events with given user  --action=  flag shows events with given action  --object-type=  flag shows events with given object type  --object-id=  flag shows events with given object ID   $ bosh -e vbox events --instance diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0\n$ bosh -e vbox events --instance diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 --task  281 \n$ bosh -e vbox events -d my-dep\n$ bosh -e vbox events --before-id = 1298284 \n$ bosh -e vbox events --before = \"2016-05-08 17:26:32 UTC\"  --after = \"2016-05-07 UTC\"       bosh -e my-env event id  Shows single event details.      bosh -e my-env -d my-dep variables  (Alias:  vars )  List variables referenced by the deployment.",
            "title": "Deployments "
        },
        {
            "location": "/cli-v2/#vms",
            "text": "bosh -e my-env [-d my-dep] vms [--vitals]  Lists all VMs managed by the Director or VMs in a single deployment. Show instance names, IPs and VM CIDs.  --vitals  flag shows basic VM usage such RAM, CPU and disk.  $ bosh -e vbox vms\n$ bosh -e vbox -d cf vms\n$ bosh -e vbox -d cf vms --vitals      bosh -e my-env -d my-dep delete-vm cid  Deletes VM without going through typical instance lifecycle. Clears out VM reference from a Director database if referenced by any instance.  $ bosh -e vbox -d cf delete-vm i-fs384238fjwjf8",
            "title": "VMs "
        },
        {
            "location": "/cli-v2/#disks",
            "text": "bosh -e my-env -d my-dep disks [--orphaned]  Lists disks. Currently only supports  --orphaned  flag.      bosh -e my-env -d my-dep attach-disk group/instance-id disk-cid  Attaches disk to an instance, replacing currently attached disk (if any).  $ bosh -e vbox -d cf attach-disk postgres/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 vol-shw8f293f2f2      bosh -e my-env -d my-dep delete-disk cid  Deletes orphaned disk.  $ bosh -e vbox -d cf delete-disk vol-shw8f293f2f2",
            "title": "Disks "
        },
        {
            "location": "/cli-v2/#ssh",
            "text": "bosh -e my-env -d my-dep ssh [destination] [-r] [-c=cmd] [--opts=opts] [--gw-* ...]  SSH into one or more instances.   --opts  flag allows operator to pass through options to  ssh ; useful for port forwarding  --gw-*  flags allows configuration of SSH gateway   # execute command on all instances in a deployment \n$ bosh -e vbox -d cf ssh -c  'uptime'  # execute command on one instance group \n$ bosh -e vbox -d cf ssh diego-cell -c  'uptime'  # execute command on a single instance \n$ bosh -e vbox -d cf ssh diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0 -c  'uptime'  # execute command with passwordless sudo \n$ bosh -e vbox -d cf ssh diego-cell -c  'sudo lsof -i|grep LISTEN'  # present output in a table by instance \n$ bosh -e vbox -d cf ssh -c  'uptime'  -r # port forward UAA port locally \n$ bosh -e vbox -d cf ssh uaa/0 --opts  ' -L 8080:localhost:8080'       bosh -e my-env -d my-dep scp src/dst:[file] src/dst:[file] [-r] [--gw-* ...]  SCP to/from one or more instances.   --recursive  ( -r ) flag allow to copy directory recursively  --gw-*  flags allow to configure gateway configuration   # copy file from this machine to machines a deployment \n$ bosh -e vbox -d cf scp ~/Downloads/script.sh :/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.sh diego-cell:/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.sh diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0:/tmp/script.sh\n$ bosh -e vbox -d cf scp ~/Downloads/script.ps1 windows_diego_cell:c:/temp/script/script.ps1 # copy file from remote machines in a deployment to this machine \n$ bosh -e vbox -d cf scp :/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp diego-cell:/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp diego-cell/209c42e5-3c1a-432a-8445-ab8d7c9f69b0:/tmp/script.sh ~/Downloads/script.sh\n$ bosh -e vbox -d cf scp windows_diego_cell:c:/temp/script/script.ps1:~/Downloads/script.ps1 # copy files from each instance into instance specific local directory \n$ bosh -e vbox -d cf scp diego-cell:/tmp/logs/ /tmp/logs/ (( instance_id ))",
            "title": "SSH "
        },
        {
            "location": "/cli-v2/#errands",
            "text": "bosh -e my-env -d my-dep errands  (Alias:  es )  Lists all errands defined by the deployment.  $ bosh -e vbox -d cf es\nUsing environment  '192.168.56.6'  as  '?' \n\nUsing deployment  'cf' \n\nName\nsmoke-tests 1  errands\n\nSucceeded      bosh -e my-env -d my-dep run-errand name [--keep-alive] [--when-changed] [--download-logs] [--logs-dir=dir] [--instance=instance-group/instance-id]  Runs errand job by name.   --keep-alive  flag keeps VM around where errand was executing  --when-changed  flag indicates whether to skip running an errand if it previously ran (successfully finished) and errand job configuration did not change  --download-logs  flag indicates whether to download full errand logs to a directory specified by  --logs-dir  (defaults to the current directory)  --instance=  flag select which instances to use for errand execution (v2.0.31+)   See  Errands  for details.  $ bosh -e vbox -d cf run-errand smoke-tests\n$ bosh -e vbox -d cf run-errand smoke-tests --keep-alive\n$ bosh -e vbox -d cf run-errand smoke-tests --when-changed # execute errand on all instances that have colocated status errand \n$ bosh -e vbox -d zookeeper run-errand status # execute errand on one instance \n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5 # execute errand on one instance within an instance group  # (note that select instance may not necessarily be first based on its index) \n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper/first # execute errand on all instance in an instance group \n$ bosh -e vbox -d zookeeper run-errand status --instance zookeeper # execute errand on two instances \n$ bosh -e vbox -d zookeeper run-errand status  \\ \n  --instance zookeeper/671d5b1d-0310-4735-8f58-182fdad0e8bc  \\ \n  --instance zookeeper/3e977542-d53e-4630-bc40-72011f853cb5",
            "title": "Errands "
        },
        {
            "location": "/cli-v2/#tasks",
            "text": "bosh -e my-env tasks [--recent[=num]] [--all]  (Alias:  ts )  Lists active and previously ran tasks.   --deployment  ( -d ) flag filters tasks by a deployment   # currently active tasks \n$ bosh -e vbox ts # currently active tasks for my-dep deployment \n$ bosh -e vbox -d my-dep ts\nUsing environment  '192.168.56.6'  as  '?'  #   State  Started At                    Last Activity At              User   Deployment   Description                   Result  27    done    Thu Feb  16   19 :16:15 UTC  2017   Thu Feb  16   19 :20:33 UTC  2017   admin  cockroachdb  create deployment             /deployments/cockroachdb 26    done    Thu Feb  16   18 :54:32 UTC  2017   Thu Feb  16   18 :55:27 UTC  2017   admin  cockroachdb  delete deployment cockroachd  /deployments/cockroachdb\n... 110  tasks\n\nSucceeded # show last 30 tasks \n$ bosh -e vbox ts -r --all # show last 1000 tasks \n$ bosh -e vbox ts -r = 1000       bosh -e my-env task id [--debug] [--result] [--event] [--cpi]  (Alias:  t )  Shows single task details. Continues to follow task if it did not finish.  Ctrl^C  does not cancel task.  $ bosh -e vbox t  281 \n$ bosh -e vbox t  281  --debug      bosh -e my-env cancel-task id  (Alias:  ct )  Cancel task at its next checkpoint. Does not wait until task is cancelled.  $ bosh -e vbox ct  281",
            "title": "Tasks "
        },
        {
            "location": "/cli-v2/#snapshots",
            "text": "bosh -e my-env -d my-dep snapshots  Lists disk snapshots for given deployment.      bosh -e my-env -d my-dep take-snapshot [group/instance-id]  Takes snapshot for an instance or an entire deployment.      bosh -e my-env -d my-dep delete-snapshot cid  Deletes snapshot.  $ bosh -e vbox -d cf delete-snapshot snap-shw38ty83f2f2      bosh -e my-env -d my-dep delete-snapshots  Deletes snapshots for an entire deployment.",
            "title": "Snapshots "
        },
        {
            "location": "/cli-v2/#deployment-recovery",
            "text": "bosh -e my-env update-resurrection on/off  Enables or disables resurrection globally. This state is not reflected in the  bosh instances  command's  Resurrection  column.  See  Automatic repair with Resurrector  for details.      bosh -e my-env -d my-dep cloud-check [--report] [--auto]  (Alias:  cck )  Checks for resource consistency and allows interactive repair.  See  Manual repair with Cloud Check  for details.      bosh -e my-env locks  Lists current locks.",
            "title": "Deployment recovery "
        },
        {
            "location": "/cli-v2/#misc",
            "text": "bosh -e my-env clean-up [--all]  Cleans up releases, stemcells, orphaned disks, and other unused resources.   --all  flag forces cleanup for orphaned disks       bosh help  Shows list of available commands and global options. Consider using  -h  flag for individual commands.      bosh interpolate manifest.yml [-v ...] [-o ...] [--vars-store path] [--path op-path]  (Alias:  int )  Interpolates variables into a manifest sending result to stdout.  Operation files  and  variables  can be provided to adjust and fill in manifest before doing a deploy.  --path  flag can be used to extract portion of a YAML document.  $ bosh int bosh-deployment/bosh.yml  \\ \n  --vars-store ./creds.yml  \\ \n  -o bosh-deployment/virtualbox/cpi.yml  \\ \n  -o bosh-deployment/virtualbox/outbound-network.yml  \\ \n  -o bosh-deployment/bosh-lite.yml  \\ \n  -o bosh-deployment/jumpbox-user.yml  \\ \n  -v  director_name = vbox  \\ \n  -v  internal_ip = 192 .168.56.6  \\ \n  -v  internal_gw = 192 .168.56.1  \\ \n  -v  internal_cidr = 192 .168.56.0/24  \\ \n  -v  network_name = vboxnet0  \\ \n  -v  outbound_network_name = NatNetwork\n\n$ bosh int creds.yml --path /admin_password\nskh32i7rdfji4387hg\n\n$ bosh int creds.yml --path /director_ssl/ca\n-----BEGIN CERTIFICATE-----\n...  Command can be used in a generic way to generate CA and leaf certificates.  variables :  -   name :   default_ca \n   type :   certificate \n   options : \n     common_name :   ca  -   name :   service_ssl \n   type :   certificate \n   options : \n     ca :   default_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]   $ bosh interpolate certs-tpl.yml -v  internal_ip = 1 .2.3.4 --vars-store certs.yml --var-errs\n$ bosh interpolate certs.yml --path /service_ssl/ca\n$ bosh interpolate certs.yml --path /service_ssl/certificate\n$ bosh interpolate certs.yml --path /service_ssl/private_key     Next:  Differences between CLI v2 vs v1",
            "title": "Misc "
        },
        {
            "location": "/cli-global-flags/",
            "text": "Note: Applies to CLI v2.\n\n\n\nHelp flag \n\u00b6\n\n\n\n\nbosh -h\n shows global flags described here and all available commands\n\n\nbosh <command> -h\n shows command specific options\n\n\n\n\n\n\nVersion flag \n\u00b6\n\n\n\n\n-v\n flag shows CLI version.\n\n\n\n\nNote: To see Director version use `bosh env` command.\n\n\n\n\n\nEnvironment flags \n\u00b6\n\n\n\n\n--environment\n (\n-e\n) flag allows to specify Director VM address or environment alias (\nBOSH_ENVIRONMENT\n environment variable)\n\n\n--ca-cert\n flag allows to specify CA certificate used for connecting for Director and UAA (\nBOSH_CA_CERT\n environment variable)\n\n\n\n\nCLI does not provide a way to skip SSL certificate validation to encourage secure Director configuration.\n\n\nSee \nCLI environments\n for details.\n\n\n\n\nAuthentication flags \n\u00b6\n\n\n\n\n--client\n flag allows to specify basic auth username or UAA client ID (\nBOSH_CLIENT\n environment variable)\n\n\n--client-secret\n flag allows to specify basic auth password or UAA client secret (\nBOSH_CLIENT_SECRET\n environment variable)\n\n\n\n\nCLI does not provide a way to specify UAA user login information since all non-interactive use (in scripts) should use UAA clients. \nbosh log-in\n command allows to log in interactively as a UAA user.\n\n\n\n\nOutput flags \n\u00b6\n\n\n\n\n-n\n flag affirms any confirmation that typically requires use input (\nBOSH_NON_INTERACTIVE=true\n environment variable)\n\n\n--json\n flag changes output format to JSON\n\n\n--tty\n flag forces output to include all decorative text typically visible when command is not redirected\n\n\n--no-color\n flag disables colors (enabled by default when command is redirected)\n\n\n\n\nCLI makes a distinction between decorative text (table headings) and primary content (such as tables). To make it eas easy to parse command output via other tools (such as grep) when decorative text is automatically hidden when command output is redirected.\n\n\n\n\nDeployment flag \n\u00b6\n\n\n\n\n--deployment\n (\n-d\n) flag allows to specify deployment for a command (\nBOSH_DEPLOYMENT\n environment variable)\n\n\n\n\nSeveral commands that can operate in a Director and a deployment context (such as \nbosh tasks\n command) account for presence of this flag and filter their output based on a deployment.\n\n\n\n\nSOCKS5 Tunneling \n\u00b6\n\n\nSee \ntunneling\n for details.\n\n\n\n\nLogging \n\u00b6\n\n\nAlong with the UI output (stdout) and UI errors (stderr), CLI can output more verbose logs.\n\n\nLogging is disabled by default (\nBOSH_LOG_LEVEL\n defaults to \nnone\n).\n\n\nTo enable logging, set the \nBOSH_LOG_LEVEL\n environment variable to one of the following values: \ndebug\n, \ninfo\n, \nwarn\n, \nerror\n, \nnone\n (default)\n\n\nLogs write to stdout (debug & info) & stderr (warn & error) by default.\n\n\nTo write logs to a file, set the \nBOSH_LOG_PATH\n environment variable to the path of the file to create and/or append to.",
            "title": "Global Flags"
        },
        {
            "location": "/cli-global-flags/#help-flag",
            "text": "bosh -h  shows global flags described here and all available commands  bosh <command> -h  shows command specific options",
            "title": "Help flag "
        },
        {
            "location": "/cli-global-flags/#version-flag",
            "text": "-v  flag shows CLI version.   Note: To see Director version use `bosh env` command.",
            "title": "Version flag "
        },
        {
            "location": "/cli-global-flags/#environment-flags",
            "text": "--environment  ( -e ) flag allows to specify Director VM address or environment alias ( BOSH_ENVIRONMENT  environment variable)  --ca-cert  flag allows to specify CA certificate used for connecting for Director and UAA ( BOSH_CA_CERT  environment variable)   CLI does not provide a way to skip SSL certificate validation to encourage secure Director configuration.  See  CLI environments  for details.",
            "title": "Environment flags "
        },
        {
            "location": "/cli-global-flags/#authentication-flags",
            "text": "--client  flag allows to specify basic auth username or UAA client ID ( BOSH_CLIENT  environment variable)  --client-secret  flag allows to specify basic auth password or UAA client secret ( BOSH_CLIENT_SECRET  environment variable)   CLI does not provide a way to specify UAA user login information since all non-interactive use (in scripts) should use UAA clients.  bosh log-in  command allows to log in interactively as a UAA user.",
            "title": "Authentication flags "
        },
        {
            "location": "/cli-global-flags/#output-flags",
            "text": "-n  flag affirms any confirmation that typically requires use input ( BOSH_NON_INTERACTIVE=true  environment variable)  --json  flag changes output format to JSON  --tty  flag forces output to include all decorative text typically visible when command is not redirected  --no-color  flag disables colors (enabled by default when command is redirected)   CLI makes a distinction between decorative text (table headings) and primary content (such as tables). To make it eas easy to parse command output via other tools (such as grep) when decorative text is automatically hidden when command output is redirected.",
            "title": "Output flags "
        },
        {
            "location": "/cli-global-flags/#deployment-flag",
            "text": "--deployment  ( -d ) flag allows to specify deployment for a command ( BOSH_DEPLOYMENT  environment variable)   Several commands that can operate in a Director and a deployment context (such as  bosh tasks  command) account for presence of this flag and filter their output based on a deployment.",
            "title": "Deployment flag "
        },
        {
            "location": "/cli-global-flags/#socks5-tunneling",
            "text": "See  tunneling  for details.",
            "title": "SOCKS5 Tunneling "
        },
        {
            "location": "/cli-global-flags/#logging",
            "text": "Along with the UI output (stdout) and UI errors (stderr), CLI can output more verbose logs.  Logging is disabled by default ( BOSH_LOG_LEVEL  defaults to  none ).  To enable logging, set the  BOSH_LOG_LEVEL  environment variable to one of the following values:  debug ,  info ,  warn ,  error ,  none  (default)  Logs write to stdout (debug & info) & stderr (warn & error) by default.  To write logs to a file, set the  BOSH_LOG_PATH  environment variable to the path of the file to create and/or append to.",
            "title": "Logging "
        },
        {
            "location": "/cli-envs/",
            "text": "Note: Applies to CLI v2.\n\n\n\nAn environment consists of a Director and deployments that it orchestrates.\n\n\ncreate-env\n command \n\u00b6\n\n\nbosh create-env\n command takes a deployment manifest as its input and idempotently converges the VM to its desired configuration:\n\n\n\n\nit may recreate a VM due to a stemcell difference, and\n\n\nit may recreate persistent disks and migrate the data\n\n\n\n\nHere is an example output which shows creation of a new VM and deployment of the Director onto it:\n\n\n$ bosh create-env bosh.yml --state\n=\nbosh.json -o ... -v ...\n\nDeployment manifest: \n'bosh.yml'\n\nDeployment state: \n'bosh.json'\n\n\nStarted validating\n  Downloading stemcell...  Finished \n(\n00\n:00:02\n)\n\n  Validating stemcell... Finished \n(\n00\n:00:04\n)\n\n  Downloading release \n'bosh'\n...  Finished \n(\n00\n:00:01\n)\n\n  Downloading release \n'bosh-warden-cpi'\n...  Finished \n(\n00\n:00:01\n)\n\n  Validating releases... Finished \n(\n00\n:00:03\n)\n\n  Validating deployment manifest... Finished \n(\n00\n:00:00\n)\n\n  Validating cpi release... Finished \n(\n00\n:00:00\n)\n\nFinished validating \n(\n00\n:00:07\n)\n\n\nStarted installing CPI\n  Compiling package \n'golang_1.3/fc3bc1b4431e8913d91362c1183c9852809d35f6'\n... Finished \n(\n00\n:00:10\n)\n\n  Compiling package \n'cpi/6f5b7e1d1050764cd14da9cc8e8683a03a502996'\n... Finished \n(\n00\n:00:04\n)\n\n  Rendering job templates... Finished \n(\n00\n:00:00\n)\n\n  Installing packages... Finished \n(\n00\n:00:01\n)\n\n  Installing job \n'cpi'\n... Finished \n(\n00\n:00:00\n)\n\nFinished installing CPI \n(\n00\n:00:16\n)\n\n\nStarting registry... Finished \n(\n00\n:00:00\n)\n\n\nUploading stemcell \n'bosh-warden-boshlite-ubuntu-trusty-go_agent/0000'\n... Finished \n(\n00\n:00:14\n)\n\n\nStarted deploying\n  Creating VM \nfor\n instance \n'bosh/0'\n from stemcell \n'47017a4e-4a81-41cf-4afc-1121346d46b4'\n... Finished \n(\n00\n:00:00\n)\n\n  Waiting \nfor\n the agent on VM \n'1987aaea-eb8a-4905-54d3-88202ce550d4'\n to be ready... Finished \n(\n00\n:00:01\n)\n\n  Creating disk... Finished \n(\n00\n:00:00\n)\n\n  Attaching disk \n'030015fc-4148-4313-5e17-608dc4b7aa76'\n to VM \n'1987aaea-eb8a-4905-54d3-88202ce550d4'\n... Finished \n(\n00\n:00:01\n)\n\n  Compiling package \n'ruby/8c1c0bba2f15f89e3129213e3877dd40e339592f'\n... Finished \n(\n00\n:01:32\n)\n\n  Compiling package \n'postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce'\n... Finished \n(\n00\n:00:04\n)\n\n  Compiling package \n'nginx/8f131f14088764682ebd9ff399707f8adb9a5038'\n... Finished \n(\n00\n:00:33\n)\n\n  Compiling package \n'libpq/6aa19afb153dc276924693dc724760664ce61593'\n... Finished \n(\n00\n:00:14\n)\n\n  Compiling package \n'mysql/e5309aed88f5cc662bc77988a31874461f7c4fb8'\n... Finished \n(\n00\n:00:06\n)\n\n  Compiling package \n'redis/ec27a0b7849863bc160ac54ce667ecacd07fc4cb'\n... Finished \n(\n00\n:00:24\n)\n\n  Compiling package \n'powerdns/e41baf8e236b5fed52ba3c33cf646e4b2e0d5a4e'\n... Finished \n(\n00\n:00:01\n)\n\n  Compiling package \n'genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201'\n... Finished \n(\n00\n:00:16\n)\n\n  Compiling package \n'director/a59aa6cf382b0c6df4206219f9f661b86dfc6103'\n... Finished \n(\n00\n:00:37\n)\n\n  Compiling package \n'nats/6a31c7bb0d5ffa2a9f43c7fd7193193438e20e92'\n... Finished \n(\n00\n:00:09\n)\n\n  Compiling package \n'health_monitor/a8a4a1cb04f924f17f9944845f5f4a73ecd4b895'\n... Finished \n(\n00\n:00:18\n)\n\n  Rendering job templates... Finished \n(\n00\n:00:00\n)\n\n  Updating instance \n'bosh/0'\n... Finished \n(\n00\n:00:09\n)\n\n  Waiting \nfor\n instance \n'bosh/0'\n to be running... Finished \n(\n00\n:00:07\n)\n\nFinished deploying \n(\n00\n:04:37\n)\n\n\n\n\n\nOnce Director VM is created you can check its basic information:\n\n\n$ bosh -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int creds.yml --path /director_ssl/ca\n)\n env\n\n\n\n\nInstead of specifying Director VM address via \n--environment\n (\n-e\n) flag and a CA certificate via \n--ca-cert\n flag in subsequent commands, a local alias can be created for environment with \nbosh alias-env\n command.\n\n\n$ bosh alias-env aws -e \n10\n.0.0.6 --ca-cert <\n(\nbosh int creds.yml --path /director_ssl/ca\n)\n\n\n\n\n\nUse \nbosh envs\n command to list local aliases:\n\n\n$ bosh envs\nURL            Alias\n\n10\n.0.0.6       aws\n\n192\n.168.50.6   vbox\n\n\n2\n environments\n\nSucceeded\n\n\n\n\nSubsequent commands can just reference created alias.\n\n\n$ bosh -e aws env\n\n\n\n\nAlternatively you can set \nexport BOSH_ENVIRONMENT=aws\n once instead of using \n--environment\n flag for each command.\n\n\n\n\nDeployment State \n\u00b6\n\n\nbosh create-env\n command needs to remember resources it creates in the IaaS so that it can re-use or delete them at a later time. The deploy command stores current state of your deployment in a given state file (via \n--state\n flag) or implicitly in \n<manifest>-state.json\n file in the same directory as your deployment manifest.\n\n\nThis allows you to deploy multiple deployments with different manifests.\n\n\nDo not delete state file unless you have already deleted your deployment (with \nbosh delete-env <manifest>\n or by manually removing the VM, disk(s), & stemcell from the IaaS). We recommend placing the deployment state file and the deployment manifest under version control and saving changes any time after running the deploy or delete commands.\n\n\nRecovering Deployment State \n\u00b6\n\n\nIf for some reason you've lost your deployment state file, or have not saved the updates from the last run of the deploy command:\n\n\n\n\nCreate a file a new deployment state file as \nstate.json\n. Write out following contents:\n\n\n\n\n{\n\n    \n\"current_vm_cid\"\n:\n \n\"<VM_ID>\"\n,\n\n    \n\"current_disk_id\"\n:\n \n\"disk1\"\n,\n\n    \n\"disks\"\n:\n \n[{\n \n\"id\"\n:\n \n\"disk1\"\n,\n \n\"cid\"\n:\n \n\"<DISK_ID>\"\n \n}]\n\n\n}\n\n\n\n\n\n\n\n\n\nReplace \n<VM_ID>\n with the ID of the VM found in the IaaS. For example on AWS it may be \ni-f62df90b\n.\n\n\n\n\n\n\nReplace \n<DISK_ID>\n with the ID of the persistent disk found in the IaaS. For example on AWS it may be \nvol-6370ec29\n.\n\n\n\n\n\n\nRun \nbosh create-env bosh.yml --state=state.json -o ... -v ...\n which will recreate the VM and migrate the disk contents.\n\n\n\n\n\n\nSave the deployment state file.\n\n\n\n\n\n\n\n\ndelete-env\n command \n\u00b6\n\n\nbosh delete-env\n command idempotently deletes all previously created IaaS resources (VMs, disks, and stemcells). The command will try its best to not return an error, for example it ignores resources that were already deleted and retries on certain operations.\n\n\nHere is an example output:\n\n\n$ bosh delete-env bosh.yml --state\n=\nbosh.json -o ... -v ...\n\nDeployment manifest: \n'bosh.yml'\n\nDeployment state: \n'state.json'\n\n\nStarted validating\n  Downloading stemcell...  Finished \n(\n00\n:00:02\n)\n\n  Validating stemcell... Finished \n(\n00\n:00:04\n)\n\n  Downloading release \n'bosh'\n...  Finished \n(\n00\n:00:01\n)\n\n  Downloading release \n'bosh-warden-cpi'\n...  Finished \n(\n00\n:00:01\n)\n\n  Validating releases... Finished \n(\n00\n:00:03\n)\n\n  Validating deployment manifest... Finished \n(\n00\n:00:00\n)\n\n  Validating cpi release... Finished \n(\n00\n:00:00\n)\n\nFinished validating \n(\n00\n:00:07\n)\n\n\nStarted installing CPI\n  Compiling package \n'golang_1.3/fc3bc1b4431e8913d91362c1183c9852809d35f6'\n... Finished \n(\n00\n:00:10\n)\n\n  Compiling package \n'cpi/6f5b7e1d1050764cd14da9cc8e8683a03a502996'\n... Finished \n(\n00\n:00:04\n)\n\n  Rendering job templates... Finished \n(\n00\n:00:00\n)\n\n  Installing packages... Finished \n(\n00\n:00:01\n)\n\n  Installing job \n'cpi'\n... Finished \n(\n00\n:00:00\n)\n\nFinished installing CPI \n(\n00\n:00:16\n)\n\n\nStarting registry... Finished \n(\n00\n:00:00\n)\n\n\nStarted deleting deployment\n  Waiting \nfor\n the agent on VM \n'eadd5540-2816-41c1-5ca3-672818e4f829'\n... Finished \n(\n00\n:00:00\n)\n\n  Stopping \njobs\n on instance \n'unknown/0'\n... Finished \n(\n00\n:00:01\n)\n\n  Unmounting disk \n'030015fc-4148-4313-5e17-608dc4b7aa76'\n... Finished \n(\n00\n:00:01\n)\n\n  Deleting VM \n'1987aaea-eb8a-4905-54d3-88202ce550d4'\n... Finished \n(\n00\n:00:00\n)\n\n  Deleting disk \n'030015fc-4148-4313-5e17-608dc4b7aa76'\n... Finished \n(\n00\n:00:00\n)\n\n  Deleting stemcell \n'47017a4e-4a81-41cf-4afc-1121346d46b4'\n... Finished \n(\n00\n:00:01\n)\n\nFinished deleting deployment \n(\n00\n:00:04\n)",
            "title": "Environments"
        },
        {
            "location": "/cli-envs/#create-env-command",
            "text": "bosh create-env  command takes a deployment manifest as its input and idempotently converges the VM to its desired configuration:   it may recreate a VM due to a stemcell difference, and  it may recreate persistent disks and migrate the data   Here is an example output which shows creation of a new VM and deployment of the Director onto it:  $ bosh create-env bosh.yml --state = bosh.json -o ... -v ...\n\nDeployment manifest:  'bosh.yml' \nDeployment state:  'bosh.json' \n\nStarted validating\n  Downloading stemcell...  Finished  ( 00 :00:02 ) \n  Validating stemcell... Finished  ( 00 :00:04 ) \n  Downloading release  'bosh' ...  Finished  ( 00 :00:01 ) \n  Downloading release  'bosh-warden-cpi' ...  Finished  ( 00 :00:01 ) \n  Validating releases... Finished  ( 00 :00:03 ) \n  Validating deployment manifest... Finished  ( 00 :00:00 ) \n  Validating cpi release... Finished  ( 00 :00:00 ) \nFinished validating  ( 00 :00:07 ) \n\nStarted installing CPI\n  Compiling package  'golang_1.3/fc3bc1b4431e8913d91362c1183c9852809d35f6' ... Finished  ( 00 :00:10 ) \n  Compiling package  'cpi/6f5b7e1d1050764cd14da9cc8e8683a03a502996' ... Finished  ( 00 :00:04 ) \n  Rendering job templates... Finished  ( 00 :00:00 ) \n  Installing packages... Finished  ( 00 :00:01 ) \n  Installing job  'cpi' ... Finished  ( 00 :00:00 ) \nFinished installing CPI  ( 00 :00:16 ) \n\nStarting registry... Finished  ( 00 :00:00 ) \n\nUploading stemcell  'bosh-warden-boshlite-ubuntu-trusty-go_agent/0000' ... Finished  ( 00 :00:14 ) \n\nStarted deploying\n  Creating VM  for  instance  'bosh/0'  from stemcell  '47017a4e-4a81-41cf-4afc-1121346d46b4' ... Finished  ( 00 :00:00 ) \n  Waiting  for  the agent on VM  '1987aaea-eb8a-4905-54d3-88202ce550d4'  to be ready... Finished  ( 00 :00:01 ) \n  Creating disk... Finished  ( 00 :00:00 ) \n  Attaching disk  '030015fc-4148-4313-5e17-608dc4b7aa76'  to VM  '1987aaea-eb8a-4905-54d3-88202ce550d4' ... Finished  ( 00 :00:01 ) \n  Compiling package  'ruby/8c1c0bba2f15f89e3129213e3877dd40e339592f' ... Finished  ( 00 :01:32 ) \n  Compiling package  'postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce' ... Finished  ( 00 :00:04 ) \n  Compiling package  'nginx/8f131f14088764682ebd9ff399707f8adb9a5038' ... Finished  ( 00 :00:33 ) \n  Compiling package  'libpq/6aa19afb153dc276924693dc724760664ce61593' ... Finished  ( 00 :00:14 ) \n  Compiling package  'mysql/e5309aed88f5cc662bc77988a31874461f7c4fb8' ... Finished  ( 00 :00:06 ) \n  Compiling package  'redis/ec27a0b7849863bc160ac54ce667ecacd07fc4cb' ... Finished  ( 00 :00:24 ) \n  Compiling package  'powerdns/e41baf8e236b5fed52ba3c33cf646e4b2e0d5a4e' ... Finished  ( 00 :00:01 ) \n  Compiling package  'genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201' ... Finished  ( 00 :00:16 ) \n  Compiling package  'director/a59aa6cf382b0c6df4206219f9f661b86dfc6103' ... Finished  ( 00 :00:37 ) \n  Compiling package  'nats/6a31c7bb0d5ffa2a9f43c7fd7193193438e20e92' ... Finished  ( 00 :00:09 ) \n  Compiling package  'health_monitor/a8a4a1cb04f924f17f9944845f5f4a73ecd4b895' ... Finished  ( 00 :00:18 ) \n  Rendering job templates... Finished  ( 00 :00:00 ) \n  Updating instance  'bosh/0' ... Finished  ( 00 :00:09 ) \n  Waiting  for  instance  'bosh/0'  to be running... Finished  ( 00 :00:07 ) \nFinished deploying  ( 00 :04:37 )   Once Director VM is created you can check its basic information:  $ bosh -e  10 .0.0.6 --ca-cert < ( bosh int creds.yml --path /director_ssl/ca )  env  Instead of specifying Director VM address via  --environment  ( -e ) flag and a CA certificate via  --ca-cert  flag in subsequent commands, a local alias can be created for environment with  bosh alias-env  command.  $ bosh alias-env aws -e  10 .0.0.6 --ca-cert < ( bosh int creds.yml --path /director_ssl/ca )   Use  bosh envs  command to list local aliases:  $ bosh envs\nURL            Alias 10 .0.0.6       aws 192 .168.50.6   vbox 2  environments\n\nSucceeded  Subsequent commands can just reference created alias.  $ bosh -e aws env  Alternatively you can set  export BOSH_ENVIRONMENT=aws  once instead of using  --environment  flag for each command.",
            "title": "create-env command "
        },
        {
            "location": "/cli-envs/#deployment-state",
            "text": "bosh create-env  command needs to remember resources it creates in the IaaS so that it can re-use or delete them at a later time. The deploy command stores current state of your deployment in a given state file (via  --state  flag) or implicitly in  <manifest>-state.json  file in the same directory as your deployment manifest.  This allows you to deploy multiple deployments with different manifests.  Do not delete state file unless you have already deleted your deployment (with  bosh delete-env <manifest>  or by manually removing the VM, disk(s), & stemcell from the IaaS). We recommend placing the deployment state file and the deployment manifest under version control and saving changes any time after running the deploy or delete commands.",
            "title": "Deployment State "
        },
        {
            "location": "/cli-envs/#recovering-deployment-state",
            "text": "If for some reason you've lost your deployment state file, or have not saved the updates from the last run of the deploy command:   Create a file a new deployment state file as  state.json . Write out following contents:   { \n     \"current_vm_cid\" :   \"<VM_ID>\" , \n     \"current_disk_id\" :   \"disk1\" , \n     \"disks\" :   [{   \"id\" :   \"disk1\" ,   \"cid\" :   \"<DISK_ID>\"   }]  }     Replace  <VM_ID>  with the ID of the VM found in the IaaS. For example on AWS it may be  i-f62df90b .    Replace  <DISK_ID>  with the ID of the persistent disk found in the IaaS. For example on AWS it may be  vol-6370ec29 .    Run  bosh create-env bosh.yml --state=state.json -o ... -v ...  which will recreate the VM and migrate the disk contents.    Save the deployment state file.",
            "title": "Recovering Deployment State "
        },
        {
            "location": "/cli-envs/#delete-env-command",
            "text": "bosh delete-env  command idempotently deletes all previously created IaaS resources (VMs, disks, and stemcells). The command will try its best to not return an error, for example it ignores resources that were already deleted and retries on certain operations.  Here is an example output:  $ bosh delete-env bosh.yml --state = bosh.json -o ... -v ...\n\nDeployment manifest:  'bosh.yml' \nDeployment state:  'state.json' \n\nStarted validating\n  Downloading stemcell...  Finished  ( 00 :00:02 ) \n  Validating stemcell... Finished  ( 00 :00:04 ) \n  Downloading release  'bosh' ...  Finished  ( 00 :00:01 ) \n  Downloading release  'bosh-warden-cpi' ...  Finished  ( 00 :00:01 ) \n  Validating releases... Finished  ( 00 :00:03 ) \n  Validating deployment manifest... Finished  ( 00 :00:00 ) \n  Validating cpi release... Finished  ( 00 :00:00 ) \nFinished validating  ( 00 :00:07 ) \n\nStarted installing CPI\n  Compiling package  'golang_1.3/fc3bc1b4431e8913d91362c1183c9852809d35f6' ... Finished  ( 00 :00:10 ) \n  Compiling package  'cpi/6f5b7e1d1050764cd14da9cc8e8683a03a502996' ... Finished  ( 00 :00:04 ) \n  Rendering job templates... Finished  ( 00 :00:00 ) \n  Installing packages... Finished  ( 00 :00:01 ) \n  Installing job  'cpi' ... Finished  ( 00 :00:00 ) \nFinished installing CPI  ( 00 :00:16 ) \n\nStarting registry... Finished  ( 00 :00:00 ) \n\nStarted deleting deployment\n  Waiting  for  the agent on VM  'eadd5540-2816-41c1-5ca3-672818e4f829' ... Finished  ( 00 :00:00 ) \n  Stopping  jobs  on instance  'unknown/0' ... Finished  ( 00 :00:01 ) \n  Unmounting disk  '030015fc-4148-4313-5e17-608dc4b7aa76' ... Finished  ( 00 :00:01 ) \n  Deleting VM  '1987aaea-eb8a-4905-54d3-88202ce550d4' ... Finished  ( 00 :00:00 ) \n  Deleting disk  '030015fc-4148-4313-5e17-608dc4b7aa76' ... Finished  ( 00 :00:00 ) \n  Deleting stemcell  '47017a4e-4a81-41cf-4afc-1121346d46b4' ... Finished  ( 00 :00:01 ) \nFinished deleting deployment  ( 00 :00:04 )",
            "title": "delete-env command "
        },
        {
            "location": "/cli-tunnel/",
            "text": "Note: Applies to CLI v2.\n\n\n\nCLI supports tunneling all of its traffic (HTTP and SSH) through a SOCKS 5 proxy specified via \nBOSH_ALL_PROXY\n environment variable. (Custom environment variable was chosen instead of using \nall_proxy\n environment variable to avoid accidently tunneling non-CLI traffic.)\n\n\nCommon use cases for tunneling through a jumpbox VM include:\n\n\n\n\ndeploying Director VM with \nbosh create-env\n command\n\n\naccessing the Director and UAA APIs\n\n\n\n\n# establish a tunnel and make it available on a local port\n\n$ ssh -4 -D \n12345\n -fNC jumpbox@jumpbox-ip -i jumpbox.key\n\n\n# let CLI know about above tunnel via environment variable\n\n$ \nexport\n \nBOSH_ALL_PROXY\n=\nsocks5://localhost:12345\n\n$ bosh create-env bosh-deployment/bosh.yml ...\n$ bosh alias-env aws -e director-ip --ca-cert ...\n\n\n\n\nSSH options:\n\n\n\n\n-D\n : local SOCKS port; make sure port is not already in use by a different tunnel/process\n\n\n-f\n : forks the process in the background\n\n\n-C\n : compresses data before sending\n\n\n-N\n : tells SSH that no command will be sent once the tunnel is up\n\n\n-4\n : force SSH to use IPv4 to avoid the dreaded \nbind\n:\n \nCannot\n \nassign\n \nrequested\n \naddress\n error",
            "title": "Tunneling"
        },
        {
            "location": "/cli-int/",
            "text": "Note: Applies to CLI v2.\n\n\n\nIt's typically necessary to separate passwords, certificates, S3 bucket names etc. from YAML documents used with CLI commands such as \nbosh create-env\n and \nbosh deploy\n. Even though the structure of a YAML document (manifest) does not change these values are typically different. CLI provides special syntax in YAML documents to annotate such values making plain YAML document into a parameterized template.\n\n\nNote: Changing structure of a YAML document such as adding an S3 access configuration section is a bit more than just YAML document parameterization. Look into [operations files](cli-ops-files.md) for additional details.\n\n\n\n\n\nVariables \n\u00b6\n\n\nVariables provide a way to define parameters for a YAML document. Each variable has a value, one or more reference locations and an optional type and generation options.\n\n\nImplicit declaration \n\u00b6\n\n\nFollowing example shows how to add two variables to a YAML document (\nbase.yml\n):\n\n\ns3_access_key_id\n:\n \n((access_key_id))\n\n\ns3_access_secret_key\n:\n \n((access_secret_key))\n\n\n\n\n\naccess_key_id\n and \naccess_secret_key\n variables are implicitly defined just by being present within double parentheses. By parameterizing above YAML document it can now be used as a template.\n\n\nValue sources \n\u00b6\n\n\nCommands that accept YAML documents such as \nbosh deploy\n and \nbosh update-cloud-config\n typically have a set of flags that can be used to provide variable values. \nbosh interpolate\n command can be used to experiment with such flags as its only job is to print result of variable interpolation.\n\n\nNote that once Director officially supports config server API, it will be recommended to use connected config server to store variable values instead of providing them via CLI flags.\n\n\n\nCLI allows to provide variable values via usage of one or more of the following flags:\n\n\n\n\n\n\n--var=key=val\n (\n-v\n) flag sets single variable value as an argument\n\n\n$ bosh interpolate base.yml -v \naccess_key_id\n=\nsome-key -v \naccess_secret_key\n=\nsome-secret\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret\n\n\n\n\n\n\n\n\n--var-file=key=path\n flag sets single variable value as an entire file\n\n\n$ cat \n1\n.txt\nsome-key\n\n$ cat \n2\n.txt\nsome-secret\n\n$ bosh interpolate base.yml --var-file \naccess_key_id\n=\n1\n.txt --var-file \naccess_secret_key\n=\n2\n.txt\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret\n\n\n\n\n\n\n\n\n--vars-file=path\n (\n-l\n) flag sets file that contains multiple variable values\n\n\n$ cat secrets.yml\naccess_key_id: some-key\naccess_secret_key: some-secret\n\n$ bosh interpolate base.yml -l secrets.yml\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret\n\n\n\n\n\n\n\n\nvia \n--vars-store=path\n flag\n flag sets file that contains multiple variable values (with a possibility that missing variables will be automatically generated)\n\n\n\n\n\n\n--vars-env=prefix\n flag sets variable values found in prefixed environment variables (casing is important)\n\n\n$ \nexport\n \nFOO_access_key_id\n=\nsome-key\n$ \nexport\n \nFOO_access_secret_key\n=\nsome-secret\n\n$ bosh interpolate base.yml --vars-env FOO\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret\n\n\n\n\n\n\n\n\nHere is a more realistic example of using base YAML document (\nbosh.yml\n) from \ncloudfoundry/bosh-deployment repo\n and specifying several variables and operations files to provide necessary missing values:\n\n\n$ bosh create-env ~/workspace/bosh-deployment/bosh.yml \n\\\n\n  --state state.json \n\\\n\n  --vars-store ./creds.yml\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml \n\\\n\n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml \n\\\n\n  -o ~/workspace/bosh-deployment/bosh-lite.yml \n\\\n\n  -o ~/workspace/bosh-deployment/jumpbox-user.yml \n\\\n\n  -v \ndirector_name\n=\nvbox \n\\\n\n  -v \ninternal_ip\n=\n192\n.168.56.6 \n\\\n\n  -v \ninternal_gw\n=\n192\n.168.56.1 \n\\\n\n  -v \ninternal_cidr\n=\n192\n.168.56.0/24 \n\\\n\n  -v \nnetwork_name\n=\nvboxnet0 \n\\\n\n  -v \noutbound_network_name\n=\nNatNetwork\n\n\n\n\nExplicit declaration \n\u00b6\n\n\nIn addition to just implicitly declaring variables it may be useful to explicitly declare them and provide additional details about used variables so that either the CLI or the Director (actually connected config server) can validate, automatically store and generate variable values.\n\n\nDedicated top level \nvariables\n section exists where variable definitions are specified:\n\n\nvariables\n:\n\n\n-\n \nname\n:\n \nadmin_password\n\n  \ntype\n:\n \npassword\n\n\n-\n \nname\n:\n \npostgres_password\n\n  \ntype\n:\n \npassword\n\n\n-\n \nname\n:\n \ndefault_ca\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nis_ca\n:\n \ntrue\n\n    \ncommon_name\n:\n \nbosh-ca\n\n\n-\n \nname\n:\n \ndirector_ssl\n\n  \ntype\n:\n \ncertificate\n\n  \noptions\n:\n\n    \nca\n:\n \ndefault_ca\n\n    \ncommon_name\n:\n \n((internal_ip))\n\n    \nalternative_names\n:\n \n[\n((internal_ip))\n]\n\n\n\n\n\nA variable can define its type and generation options.\n\n\n--vars-store\n flag \n\u00b6\n\n\n--vars-store=path\n flag provides a read write value source unlike all other variables flags that provide read only source. It is able to lazily generate and save (to a given file location) variable values based on their type and options.\n\n\nNote that once Director officially supports config server API, it will be recommended to avoid using `--vars-store` flag for all commands except `bosh create-env`. `bosh create-env` command will not be able to use config server API since it most likely will be deploying a config server alongside the Director.\n\n\n\nCurrently CLI supports \ncertificate\n, \npassword\n, \nrsa\n, and \nssh\n types. The Director (connected to a config server) may support additional types known by the config server.\n\n\nSee \nVariable Types\n for details on variable generation.\n\n\n$ cat base.yml\npass: \n((\nadmin_password\n))\n\nvariables:\n- name: admin_password\n  type: password\n\n$ bosh interpolate base.yml --vars-store\n=\ncreds.yml\npass: vbvdhjbzqelnq7cfyw09\n\n$ cat creds.yml\nadmin_password: vbvdhjbzqelnq7cfyw09\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nOperations Files",
            "title": "Interpolating Variables"
        },
        {
            "location": "/cli-int/#variables",
            "text": "Variables provide a way to define parameters for a YAML document. Each variable has a value, one or more reference locations and an optional type and generation options.",
            "title": "Variables "
        },
        {
            "location": "/cli-int/#implicit-declaration",
            "text": "Following example shows how to add two variables to a YAML document ( base.yml ):  s3_access_key_id :   ((access_key_id))  s3_access_secret_key :   ((access_secret_key))   access_key_id  and  access_secret_key  variables are implicitly defined just by being present within double parentheses. By parameterizing above YAML document it can now be used as a template.",
            "title": "Implicit declaration "
        },
        {
            "location": "/cli-int/#value-sources",
            "text": "Commands that accept YAML documents such as  bosh deploy  and  bosh update-cloud-config  typically have a set of flags that can be used to provide variable values.  bosh interpolate  command can be used to experiment with such flags as its only job is to print result of variable interpolation.  Note that once Director officially supports config server API, it will be recommended to use connected config server to store variable values instead of providing them via CLI flags.  CLI allows to provide variable values via usage of one or more of the following flags:    --var=key=val  ( -v ) flag sets single variable value as an argument  $ bosh interpolate base.yml -v  access_key_id = some-key -v  access_secret_key = some-secret\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret    --var-file=key=path  flag sets single variable value as an entire file  $ cat  1 .txt\nsome-key\n\n$ cat  2 .txt\nsome-secret\n\n$ bosh interpolate base.yml --var-file  access_key_id = 1 .txt --var-file  access_secret_key = 2 .txt\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret    --vars-file=path  ( -l ) flag sets file that contains multiple variable values  $ cat secrets.yml\naccess_key_id: some-key\naccess_secret_key: some-secret\n\n$ bosh interpolate base.yml -l secrets.yml\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret    via  --vars-store=path  flag  flag sets file that contains multiple variable values (with a possibility that missing variables will be automatically generated)    --vars-env=prefix  flag sets variable values found in prefixed environment variables (casing is important)  $  export   FOO_access_key_id = some-key\n$  export   FOO_access_secret_key = some-secret\n\n$ bosh interpolate base.yml --vars-env FOO\ns3_access_key_id: some-key\ns3_access_secret_key: some-secret    Here is a more realistic example of using base YAML document ( bosh.yml ) from  cloudfoundry/bosh-deployment repo  and specifying several variables and operations files to provide necessary missing values:  $ bosh create-env ~/workspace/bosh-deployment/bosh.yml  \\ \n  --state state.json  \\ \n  --vars-store ./creds.yml\n  -o ~/workspace/bosh-deployment/virtualbox/cpi.yml  \\ \n  -o ~/workspace/bosh-deployment/virtualbox/outbound-network.yml  \\ \n  -o ~/workspace/bosh-deployment/bosh-lite.yml  \\ \n  -o ~/workspace/bosh-deployment/jumpbox-user.yml  \\ \n  -v  director_name = vbox  \\ \n  -v  internal_ip = 192 .168.56.6  \\ \n  -v  internal_gw = 192 .168.56.1  \\ \n  -v  internal_cidr = 192 .168.56.0/24  \\ \n  -v  network_name = vboxnet0  \\ \n  -v  outbound_network_name = NatNetwork",
            "title": "Value sources "
        },
        {
            "location": "/cli-int/#explicit-declaration",
            "text": "In addition to just implicitly declaring variables it may be useful to explicitly declare them and provide additional details about used variables so that either the CLI or the Director (actually connected config server) can validate, automatically store and generate variable values.  Dedicated top level  variables  section exists where variable definitions are specified:  variables :  -   name :   admin_password \n   type :   password  -   name :   postgres_password \n   type :   password  -   name :   default_ca \n   type :   certificate \n   options : \n     is_ca :   true \n     common_name :   bosh-ca  -   name :   director_ssl \n   type :   certificate \n   options : \n     ca :   default_ca \n     common_name :   ((internal_ip)) \n     alternative_names :   [ ((internal_ip)) ]   A variable can define its type and generation options.",
            "title": "Explicit declaration "
        },
        {
            "location": "/cli-int/#-vars-store-flag",
            "text": "--vars-store=path  flag provides a read write value source unlike all other variables flags that provide read only source. It is able to lazily generate and save (to a given file location) variable values based on their type and options.  Note that once Director officially supports config server API, it will be recommended to avoid using `--vars-store` flag for all commands except `bosh create-env`. `bosh create-env` command will not be able to use config server API since it most likely will be deploying a config server alongside the Director.  Currently CLI supports  certificate ,  password ,  rsa , and  ssh  types. The Director (connected to a config server) may support additional types known by the config server.  See  Variable Types  for details on variable generation.  $ cat base.yml\npass:  (( admin_password )) \nvariables:\n- name: admin_password\n  type: password\n\n$ bosh interpolate base.yml --vars-store = creds.yml\npass: vbvdhjbzqelnq7cfyw09\n\n$ cat creds.yml\nadmin_password: vbvdhjbzqelnq7cfyw09   Back to Table of Contents  Previous:  Operations Files",
            "title": "--vars-store flag "
        },
        {
            "location": "/cli-ops-files/",
            "text": "Note: Applies to CLI v2.\n\n\n\nIt's usually necessary to apply an opinionated set of structural changes to a YAML document (manifest, cloud config, etc.) before submitting it to the CLI commands (\nbosh create-env\n, \nbosh deploy\n, etc.) for processing. Such changes could be an addition or removal of certain job properties, instance groups, changes to property values.\n\n\nNote: Replacing values such as passwords and certificates is not considered a structural change. Refer to [CLI variable interpolation](cli-int.md) for details.\n\n\n\nTo get a final YAML document one can apply desired changes once and save the result; however, over time it may become harder or just tedious to reapply these changes if base document changes. Additionally if it's necessary to have multiple slightly different changes on top of the base document for different teams existing editing tools may not be enough. To make such workflows easier you can encode a set of changes into one or more operations file.\n\n\nA single operation represents a single change. An operations file is a YAML document that contains multiple operations that are to be applied serially to a different YAML document. Instead of storing all operations in a single file, they can be grouped logically into many operations files.\n\n\nSeveral CLI commands such as \ncreate-env\n, \ndeploy\n and \ninterpolate\n allow to provide operations files via \n--ops-file\n flag to be applied before processing the document.\n\n\n\n\nExample \n\u00b6\n\n\nFollowing is an operations file (\nreplace-name.yml\n) with a single operation that replaces value of top level key \nname\n with a string \nother-cf\n:\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/name\n\n  \nvalue\n:\n \nother-cf\n\n\n\n\n\nGiven base YAML document (\nbase.yml\n):\n\n\nname\n:\n \nmy-cf\n\n\n\n\n\nResult of applying above operations file to the base YAML document would be:\n\n\nname\n:\n \nother-cf\n\n\n\n\n\nThat could be demonstrated with the help of \nbosh interpolate\n command whose purpose is to simply apply operations files to base document and print the result:\n\n\n$ bosh interpolate base.yml --ops-file replace-name.yml\n\nname: other-cf\n\n\n\n\n\n\nPath syntax \n\u00b6\n\n\nEach operation acts on a location within a YAML document. Path represents a location. It's important to note that path (location) does not represent what operation will be performed, just like lat & long do not represent what happens at a physical location.\n\n\nHere are some path examples:\n\n\n\n\n/\n: matches document root\n\n\n/0\n: matches 0\nth\n item in the array at the root\n\n\n/instance_groups/0\n: matches 0\nth\n instance group within \ninstance_groups\n array\n\n\n/instance_groups/name=zookeeper\n: matches instance group (hash) with a \nname\n key that has value \nzookeeper\n\n\n\n\nAll paths follow these rules:\n\n\n\n\n\n\nPaths can have multiple components separated by a \n/\n\n\n\n\n\n\nPaths always start at the root of the document with a \n/\n\n\n\n\n\n\nString components typically refer to hash keys (ex: \n/key1\n)\n\n\n\n\n\n\nStrings ending with \n?\n refer to hash keys that may or may not exist\n\n\n\n\n\"optionality\" carries over to the items to the right\n\n\n\n\n\n\n\n\nInteger components refer to array indices (ex: \n/0\n, \n/-1\n)\n\n\n\n\n\n\nArray index selection could be affected via \n:prev\n and \n:next\n (as of CLI v2.0.40+)\n\n\n\n\n\n\nArray insertion could be affected via \n:before\n and \n:after\n (as of CLI v2.0.40+)\n\n\n\n\n\n\n-\n component refers to an imaginary index after last array index (ex: \n/-\n)\n\n\n\n\n\n\nIf there is an array of length 3 (\n[0,1,2]\n), then \n-\n would refer to 4\nth\n non-existent position\n\n\n\n\n\n\nkey=val\n component matches hashes within an array (ex: \n/key=val\n)\n\n\n\n\nValues ending with \n?\n refer to array items that may or may not exist\n\n\n\n\nPath components without \"optional\" (\n?\n) annotation imply that referenced location must exist within a document. Operation will fail to be performed if that location is not found. \"Optional\" annotation can be used to signify indifference to the presense of referenced location, making it possible for operation either to ignore it (while removal) or create it lazily (while replacement). If a component in a path is annotated as optional, components following it will be considered optional implicitly.\n\n\n\n\nOperations \n\u00b6\n\n\nThere are currently two types of operations: replace and remove.\n\n\nReplace operation can be used to append an item to an array of any length if last component of a path is a \n-\n (see above for details).\n\n\nFollowing base YAML document is used with operations below:\n\n\nkey\n:\n \n1\n\n\n\nkey2\n:\n\n  \nnested\n:\n\n    \nsuper_nested\n:\n \n2\n\n  \nother\n:\n \n3\n\n\n\narray\n:\n \n[\n4\n,\n5\n,\n6\n]\n\n\n\nitems\n:\n\n\n-\n \nname\n:\n \nitem7\n\n\n-\n \nname\n:\n \nitem8\n\n\n-\n \nname\n:\n \nitem8\n\n\n\n\n\n\n\nHash\n\u00b6\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/key\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nsets \nkey\n to \n10\n\n\n\n\n-\n \ntype\n:\n \nremove\n\n  \npath\n:\n \n/key\n\n\n\n\n\n\n\nremoves \nkey\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/key_not_there\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nerrors because \nkey_not_there\n is expected (does not have \n?\n)\n\n\n\n\n-\n \ntype\n:\n \nremove\n\n  \npath\n:\n \n/key_not_there\n\n\n\n\n\n\n\nagain this errors because \nkey_not_there\n is expected (does not have \n?\n)\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/new_key?\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\ncreates \nnew_key\n because it ends with \n?\n and sets it to \n10\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/key2/nested/super_nested\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires that \nkey2\n and \nnested\n hashes exist\n\n\nsets \nsuper_nested\n to \n10\n\n\n\n\n-\n \ntype\n:\n \nremove\n\n  \npath\n:\n \n/key2/nested/super_nested\n\n\n\n\n\n\n\nrequires that \nkey2\n and \nnested\n hashes exist\n\n\nremoves \nsuper_nested\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/key2/nested?/another_nested/super_nested\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires that \nkey2\n hash exists\n\n\nallows \nnested\n, \nanother_nested\n and \nsuper_nested\n not to exist because \n?\n carries over to nested keys\n\n\ncreates \nanother_nested\n and \nsuper_nested\n before setting \nsuper_nested\n to \n10\n, resulting in:\n\n\n\n\n...\n\n\nkey2\n:\n\n  \nnested\n:\n\n    \nanother_nested\n:\n\n      \nsuper_nested\n:\n \n10\n\n    \nsuper_nested\n:\n \n2\n\n  \nother\n:\n \n3\n\n\n\n\n\n\n\nArray\n\u00b6\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/0\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\nreplaces 0\nth\n item in \narray\n array with \n10\n\n\n\n\n-\n \ntype\n:\n \nremove\n\n  \npath\n:\n \n/array/0\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\nremoves 0\nth\n item in \narray\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/-\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\nappends \n10\n to the end of \narray\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array2?/-\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\ncreates \narray2\n array since it does not exist\n\n\nappends \n10\n to the end of \narray2\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/1:prev\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\nreplaces 0\nth\n item in \narray\n array with \n10\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/0:next\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\nreplaces 1\nst\n item (starting at 0) in \narray\n array with \n10\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/0:after\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\ninserts \n10\n after 0\nth\n item in \narray\n array\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/array/0:before\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nrequires \narray\n to exist and be an array\n\n\ninserts \n10\n before 0\nth\n item at the beginning of \narray\n array\n\n\n\n\n\n\nArrays of hashes\n\u00b6\n\n\n-\n \ntype\n:\n \nremove\n\n  \npath\n:\n \n/items/name=item7\n\n\n\n\n\n\n\nfinds and removes array item with matching key \nname\n with value \nitem7\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/items/name=item8/count\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nerrors because there are two values that have \nitem8\n as their \nname\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/items/name=item9?/count\n\n  \nvalue\n:\n \n10\n\n\n\n\n\n\n\nappends array item with matching key \nname\n with value \nitem9\n because values ends with \n?\n and item does not exist\n\n\ncreates \ncount\n and sets it to \n10\n within created array item, resulting in:\n\n\n\n\n...\n\n\nitems\n:\n\n\n-\n \nname\n:\n \nitem7\n\n\n-\n \nname\n:\n \nitem8\n\n\n-\n \nname\n:\n \nitem8\n\n\n-\n \nname\n:\n \nitem9\n\n  \ncount\n:\n \n10\n\n\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/items/name=item7:before\n\n  \nvalue\n:\n\n    \nname\n:\n \nitem6\n\n\n\n\n\n\n\nfinds array item with matching key \nname\n with value \nitem7\n\n\nadds hash \nname\n:\n \nitem6\n before found array item, resulting in:\n\n\n\n\n...\n\n\nitems\n:\n\n\n-\n \nname\n:\n \nitem6\n\n\n-\n \nname\n:\n \nitem7\n\n\n-\n \nname\n:\n \nitem8\n\n\n-\n \nname\n:\n \nitem8\n\n\n\n\n\n\n\nNext: \nCLI Variable Interpolation\n\n\nPrevious: \nCLI Environments",
            "title": "Creating Ops Files"
        },
        {
            "location": "/cli-ops-files/#example",
            "text": "Following is an operations file ( replace-name.yml ) with a single operation that replaces value of top level key  name  with a string  other-cf :  -   type :   replace \n   path :   /name \n   value :   other-cf   Given base YAML document ( base.yml ):  name :   my-cf   Result of applying above operations file to the base YAML document would be:  name :   other-cf   That could be demonstrated with the help of  bosh interpolate  command whose purpose is to simply apply operations files to base document and print the result:  $ bosh interpolate base.yml --ops-file replace-name.yml\n\nname: other-cf",
            "title": "Example "
        },
        {
            "location": "/cli-ops-files/#path-syntax",
            "text": "Each operation acts on a location within a YAML document. Path represents a location. It's important to note that path (location) does not represent what operation will be performed, just like lat & long do not represent what happens at a physical location.  Here are some path examples:   / : matches document root  /0 : matches 0 th  item in the array at the root  /instance_groups/0 : matches 0 th  instance group within  instance_groups  array  /instance_groups/name=zookeeper : matches instance group (hash) with a  name  key that has value  zookeeper   All paths follow these rules:    Paths can have multiple components separated by a  /    Paths always start at the root of the document with a  /    String components typically refer to hash keys (ex:  /key1 )    Strings ending with  ?  refer to hash keys that may or may not exist   \"optionality\" carries over to the items to the right     Integer components refer to array indices (ex:  /0 ,  /-1 )    Array index selection could be affected via  :prev  and  :next  (as of CLI v2.0.40+)    Array insertion could be affected via  :before  and  :after  (as of CLI v2.0.40+)    -  component refers to an imaginary index after last array index (ex:  /- )    If there is an array of length 3 ( [0,1,2] ), then  -  would refer to 4 th  non-existent position    key=val  component matches hashes within an array (ex:  /key=val )   Values ending with  ?  refer to array items that may or may not exist   Path components without \"optional\" ( ? ) annotation imply that referenced location must exist within a document. Operation will fail to be performed if that location is not found. \"Optional\" annotation can be used to signify indifference to the presense of referenced location, making it possible for operation either to ignore it (while removal) or create it lazily (while replacement). If a component in a path is annotated as optional, components following it will be considered optional implicitly.",
            "title": "Path syntax "
        },
        {
            "location": "/cli-ops-files/#operations",
            "text": "There are currently two types of operations: replace and remove.  Replace operation can be used to append an item to an array of any length if last component of a path is a  -  (see above for details).  Following base YAML document is used with operations below:  key :   1  key2 : \n   nested : \n     super_nested :   2 \n   other :   3  array :   [ 4 , 5 , 6 ]  items :  -   name :   item7  -   name :   item8  -   name :   item8",
            "title": "Operations "
        },
        {
            "location": "/cli-ops-files/#hash",
            "text": "-   type :   replace \n   path :   /key \n   value :   10    sets  key  to  10   -   type :   remove \n   path :   /key    removes  key   -   type :   replace \n   path :   /key_not_there \n   value :   10    errors because  key_not_there  is expected (does not have  ? )   -   type :   remove \n   path :   /key_not_there    again this errors because  key_not_there  is expected (does not have  ? )   -   type :   replace \n   path :   /new_key? \n   value :   10    creates  new_key  because it ends with  ?  and sets it to  10   -   type :   replace \n   path :   /key2/nested/super_nested \n   value :   10    requires that  key2  and  nested  hashes exist  sets  super_nested  to  10   -   type :   remove \n   path :   /key2/nested/super_nested    requires that  key2  and  nested  hashes exist  removes  super_nested   -   type :   replace \n   path :   /key2/nested?/another_nested/super_nested \n   value :   10    requires that  key2  hash exists  allows  nested ,  another_nested  and  super_nested  not to exist because  ?  carries over to nested keys  creates  another_nested  and  super_nested  before setting  super_nested  to  10 , resulting in:   ...  key2 : \n   nested : \n     another_nested : \n       super_nested :   10 \n     super_nested :   2 \n   other :   3",
            "title": "Hash"
        },
        {
            "location": "/cli-ops-files/#array",
            "text": "-   type :   replace \n   path :   /array/0 \n   value :   10    requires  array  to exist and be an array  replaces 0 th  item in  array  array with  10   -   type :   remove \n   path :   /array/0    requires  array  to exist and be an array  removes 0 th  item in  array   -   type :   replace \n   path :   /array/- \n   value :   10    requires  array  to exist and be an array  appends  10  to the end of  array   -   type :   replace \n   path :   /array2?/- \n   value :   10    creates  array2  array since it does not exist  appends  10  to the end of  array2   -   type :   replace \n   path :   /array/1:prev \n   value :   10    requires  array  to exist and be an array  replaces 0 th  item in  array  array with  10   -   type :   replace \n   path :   /array/0:next \n   value :   10    requires  array  to exist and be an array  replaces 1 st  item (starting at 0) in  array  array with  10   -   type :   replace \n   path :   /array/0:after \n   value :   10    requires  array  to exist and be an array  inserts  10  after 0 th  item in  array  array   -   type :   replace \n   path :   /array/0:before \n   value :   10    requires  array  to exist and be an array  inserts  10  before 0 th  item at the beginning of  array  array",
            "title": "Array"
        },
        {
            "location": "/cli-ops-files/#arrays-of-hashes",
            "text": "-   type :   remove \n   path :   /items/name=item7    finds and removes array item with matching key  name  with value  item7   -   type :   replace \n   path :   /items/name=item8/count \n   value :   10    errors because there are two values that have  item8  as their  name   -   type :   replace \n   path :   /items/name=item9?/count \n   value :   10    appends array item with matching key  name  with value  item9  because values ends with  ?  and item does not exist  creates  count  and sets it to  10  within created array item, resulting in:   ...  items :  -   name :   item7  -   name :   item8  -   name :   item8  -   name :   item9 \n   count :   10   -   type :   replace \n   path :   /items/name=item7:before \n   value : \n     name :   item6    finds array item with matching key  name  with value  item7  adds hash  name :   item6  before found array item, resulting in:   ...  items :  -   name :   item6  -   name :   item7  -   name :   item8  -   name :   item8    Next:  CLI Variable Interpolation  Previous:  CLI Environments",
            "title": "Arrays of hashes"
        },
        {
            "location": "/cpi-config/",
            "text": "Note: This feature is available with bosh-release v261+.\n\n\n\nIn most cases having single Director use a single CPI and hence a single IaaS section (for example one AWS account, vSphere datacenter, or OpenStack tenant) is enough. But in some cases operator may want to rely on multiple IaaS sections to achieve necessary isolation, availability, capacity, scalability, security, and/or management needs. To address such configuration scenarios  Director can be configured with multiple CPIs at runtime (instead of only allowing single CPI configuration at the deploy time).\n\n\nThe CPI config is a YAML file that defines multiple CPIs and properties necessary for each CPI to communicate with an appropriate IaaS section. Once CPIs are specified, operator can associate particular AZ in their cloud config to a particular CPI.\n\n\n\n\nUpdating and retrieving CPI config \n\u00b6\n\n\nTo update CPI config on the Director use \nbosh update-cpi-config\n CLI command.\n\n\nNote: See \nexample CPI config\n below.\n\n\n\n$ bosh update-cpi-config cpis.yml\n\n$ bosh cpi-config\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\ncpis:\n- name: openstack-1a\n  type: openstack\n  properties:\n    ...\n- name: openstack-1b\n  type: openstack\n  properties:\n    ...\n\n\n\n\nOnce CPI config is updated AZs in the cloud config can reference specific CPI to be used during a deploy. Unlike runtime and cloud configs, CPI config is not tracked directly by the deployments and can be updated separately (useful for updating CPI credentials without forcing redeploy of all the deployments).\n\n\n\n\nCPIs Block \n\u00b6\n\n\ncpis\n [Array, required]: Specifies the CPIs.\n\n\n\n\nname\n [String, required]: Unique name for a CPI. Example: \nopenstack-1a\n.\n\n\ntype\n [String, required]: CPI type. Director will add \n_cpi\n suffix to the end of the type when calling CPI binary. Example: \nopenstack\n, \ngoogle\n.\n\n\nproperties\n [Hash, required]: Set of properties to provide to the CPI for each call so that CPI can authenticate and provision resources in an IaaS.\n\n\n\n\nNote: Properties will vary depending on the CPI you're trying to use. These are the `Global Configuration` of a given CPI. See \na complete list of the CPI properties\n.\n\n\n\nOpenStack example:\n\n\ncpis\n:\n\n\n-\n \nname\n:\n \nopenstack-1a\n\n  \ntype\n:\n \nopenstack\n\n  \nproperties\n:\n\n    \nauth_url\n:\n \n((auth_url))\n\n    \nusername\n:\n \n((openstack_username))\n\n    \napi_key\n:\n \n((openstack_password))\n\n    \ndomain\n:\n \n((openstack_domain))\n\n    \nproject\n:\n \n((openstack_project))\n\n    \nregion\n:\n \n((region))\n\n    \ndefault_key_name\n:\n \n((default_key_name))\n\n    \ndefault_security_groups\n:\n \n((default_security_groups))\n\n    \nhuman_readable_vm_names\n:\n \ntrue\n\n\n\n\n\nvSphere example:\n\n\ncpis\n:\n\n\n-\n \nname\n:\n \n((vcenter_identifier))\n\n  \ntype\n:\n \nvsphere\n\n  \nproperties\n:\n\n    \nhost\n:\n \n((vcenter_ip))\n\n    \nuser\n:\n \n((vcenter_user))\n\n    \npassword\n:\n \n((vcenter_password))\n\n    \ndatacenters\n:\n\n    \n-\n \nclusters\n:\n\n      \n-\n \n{\n \n((vcenter_cluster))\n:\n \n{}}\n\n      \ndatastore_pattern\n:\n \n((vcenter_datastores_pattern))\n\n      \ndisk_path\n:\n \n((folder_to_put_disks_in))\n\n      \nname\n:\n \n((vcenter_datacenter))\n\n      \npersistent_datastore_pattern\n:\n \n((vcenter_persistent_datastores_pattern))\n\n      \ntemplate_folder\n:\n \n((folder_to_put_templates_in))\n\n      \nvm_folder\n:\n \n((folder_to_put_vms_in))\n\n\n\n\n\nFor vSphere, if your datacenter and cluster names have spaces in them, there is no need to put quotes around them when updating your cpi-config.\n\n\n\n\nExample \n\u00b6\n\n\nExample of a CPI config referencing two separate OpenStack installations:\n\n\ncpis\n:\n\n\n-\n \nname\n:\n \nopenstack-1a\n\n  \ntype\n:\n \nopenstack\n\n  \nproperties\n:\n\n    \napi_key\n:\n \n...\n\n    \nauth_url\n:\n \n...\n\n\n-\n \nname\n:\n \nopenstack-1b\n\n  \ntype\n:\n \nopenstack\n\n  \nproperties\n:\n\n    \napi_key\n:\n \n...\n\n    \nauth_url\n:\n \n...\n\n\n\n\n\nAZs in cloud config can reference openstack CPIs by their given names:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncpi\n:\n \nopenstack-1a\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1a\n\n\n-\n \nname\n:\n \nz2\n\n  \ncpi\n:\n \nopenstack-1b\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1b\n\n\n...\n\n\n\n\n\nExample of a CPI config referencing two separate vSphere installations:\n\n\ncpis\n:\n\n\n-\n \nname\n:\n \nvcenter-1a\n\n  \ntype\n:\n \nvsphere\n\n  \nproperties\n:\n\n    \nhost\n:\n \nvcenter-1a-ip\n\n    \nuser\n:\n \nvcenter-1a-user\n\n    \npassword\n:\n \nvcenter-1a-password\n\n    \ndatacenters\n:\n\n    \n-\n \nclusters\n:\n\n      \n-\n \n{\nvcenter-1a-cluster\n:\n \n{}}\n\n      \ndatastore_pattern\n:\n \n^(lun01|lun02|lun03|lun04|lun05)$\n\n      \ndisk_path\n:\n \nvcenter-1a-disks/disks\n\n      \nname\n:\n \nvcenter-1a-datacenter\n\n      \npersistent_datastore_pattern\n:\n \n^(lun01|lun02|lun03|lun04|lun05)$\n\n      \ntemplate_folder\n:\n \nvcenter-1a-disks/templates\n\n      \nvm_folder\n:\n \nvcenter-1a-disks/vms\n\n\n-\n \nname\n:\n \nvcenter-1b\n\n  \ntype\n:\n \nvsphere\n\n  \nproperties\n:\n\n    \nhost\n:\n \nvcenter-1b-ip\n\n    \nuser\n:\n \nvcenter-1b-user\n\n    \npassword\n:\n \nvcenter-1b-password\n\n    \ndatacenters\n:\n\n    \n-\n \nclusters\n:\n\n      \n-\n \n{\nvcenter-1b-cluster\n:\n \n{}}\n\n      \ndatastore_pattern\n:\n \n^(lun01|lun02|lun03|lun04|lun05)$\n\n      \ndisk_path\n:\n \nvcenter-1b-disks/disks\n\n      \nname\n:\n \nvcenter-1b-datacenter\n\n      \npersistent_datastore_pattern\n:\n \n^(lun01|lun02|lun03|lun04|lun05)$\n\n      \ntemplate_folder\n:\n \nvcenter-1b-disks/templates\n\n      \nvm_folder\n:\n \nvcenter-1b-disks/vms\n\n\n\n\n\nAZs in cloud config can reference vSphere CPIs by their given names:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncpi\n:\n \nvcenter-1a\n\n\n-\n \nname\n:\n \nz2\n\n  \ncpi\n:\n \nvcenter-1b\n\n\n...\n\n\n\n\n\n\n\nCPI Specific Stemcells \n\u00b6\n\n\nStemcells need to be assigned to a specific CPI and it occurs on upload. If you've already uploaded an appropriate stemcell you'll need to re-upload with \n--fix\n\n\nbosh upload-stemcell .tgz --fix",
            "title": "Using CPI Config"
        },
        {
            "location": "/cpi-config/#updating-and-retrieving-cpi-config",
            "text": "To update CPI config on the Director use  bosh update-cpi-config  CLI command.  Note: See  example CPI config  below.  $ bosh update-cpi-config cpis.yml\n\n$ bosh cpi-config\nUsing environment  '192.168.56.6'  as client  'admin' \n\ncpis:\n- name: openstack-1a\n  type: openstack\n  properties:\n    ...\n- name: openstack-1b\n  type: openstack\n  properties:\n    ...  Once CPI config is updated AZs in the cloud config can reference specific CPI to be used during a deploy. Unlike runtime and cloud configs, CPI config is not tracked directly by the deployments and can be updated separately (useful for updating CPI credentials without forcing redeploy of all the deployments).",
            "title": "Updating and retrieving CPI config "
        },
        {
            "location": "/cpi-config/#cpis-block",
            "text": "cpis  [Array, required]: Specifies the CPIs.   name  [String, required]: Unique name for a CPI. Example:  openstack-1a .  type  [String, required]: CPI type. Director will add  _cpi  suffix to the end of the type when calling CPI binary. Example:  openstack ,  google .  properties  [Hash, required]: Set of properties to provide to the CPI for each call so that CPI can authenticate and provision resources in an IaaS.   Note: Properties will vary depending on the CPI you're trying to use. These are the `Global Configuration` of a given CPI. See  a complete list of the CPI properties .  OpenStack example:  cpis :  -   name :   openstack-1a \n   type :   openstack \n   properties : \n     auth_url :   ((auth_url)) \n     username :   ((openstack_username)) \n     api_key :   ((openstack_password)) \n     domain :   ((openstack_domain)) \n     project :   ((openstack_project)) \n     region :   ((region)) \n     default_key_name :   ((default_key_name)) \n     default_security_groups :   ((default_security_groups)) \n     human_readable_vm_names :   true   vSphere example:  cpis :  -   name :   ((vcenter_identifier)) \n   type :   vsphere \n   properties : \n     host :   ((vcenter_ip)) \n     user :   ((vcenter_user)) \n     password :   ((vcenter_password)) \n     datacenters : \n     -   clusters : \n       -   {   ((vcenter_cluster)) :   {}} \n       datastore_pattern :   ((vcenter_datastores_pattern)) \n       disk_path :   ((folder_to_put_disks_in)) \n       name :   ((vcenter_datacenter)) \n       persistent_datastore_pattern :   ((vcenter_persistent_datastores_pattern)) \n       template_folder :   ((folder_to_put_templates_in)) \n       vm_folder :   ((folder_to_put_vms_in))   For vSphere, if your datacenter and cluster names have spaces in them, there is no need to put quotes around them when updating your cpi-config.",
            "title": "CPIs Block "
        },
        {
            "location": "/cpi-config/#example",
            "text": "Example of a CPI config referencing two separate OpenStack installations:  cpis :  -   name :   openstack-1a \n   type :   openstack \n   properties : \n     api_key :   ... \n     auth_url :   ...  -   name :   openstack-1b \n   type :   openstack \n   properties : \n     api_key :   ... \n     auth_url :   ...   AZs in cloud config can reference openstack CPIs by their given names:  azs :  -   name :   z1 \n   cpi :   openstack-1a \n   cloud_properties : \n     availability_zone :   us-east-1a  -   name :   z2 \n   cpi :   openstack-1b \n   cloud_properties : \n     availability_zone :   us-east-1b  ...   Example of a CPI config referencing two separate vSphere installations:  cpis :  -   name :   vcenter-1a \n   type :   vsphere \n   properties : \n     host :   vcenter-1a-ip \n     user :   vcenter-1a-user \n     password :   vcenter-1a-password \n     datacenters : \n     -   clusters : \n       -   { vcenter-1a-cluster :   {}} \n       datastore_pattern :   ^(lun01|lun02|lun03|lun04|lun05)$ \n       disk_path :   vcenter-1a-disks/disks \n       name :   vcenter-1a-datacenter \n       persistent_datastore_pattern :   ^(lun01|lun02|lun03|lun04|lun05)$ \n       template_folder :   vcenter-1a-disks/templates \n       vm_folder :   vcenter-1a-disks/vms  -   name :   vcenter-1b \n   type :   vsphere \n   properties : \n     host :   vcenter-1b-ip \n     user :   vcenter-1b-user \n     password :   vcenter-1b-password \n     datacenters : \n     -   clusters : \n       -   { vcenter-1b-cluster :   {}} \n       datastore_pattern :   ^(lun01|lun02|lun03|lun04|lun05)$ \n       disk_path :   vcenter-1b-disks/disks \n       name :   vcenter-1b-datacenter \n       persistent_datastore_pattern :   ^(lun01|lun02|lun03|lun04|lun05)$ \n       template_folder :   vcenter-1b-disks/templates \n       vm_folder :   vcenter-1b-disks/vms   AZs in cloud config can reference vSphere CPIs by their given names:  azs :  -   name :   z1 \n   cpi :   vcenter-1a  -   name :   z2 \n   cpi :   vcenter-1b  ...",
            "title": "Example "
        },
        {
            "location": "/cpi-config/#cpi-specific-stemcells",
            "text": "Stemcells need to be assigned to a specific CPI and it occurs on upload. If you've already uploaded an appropriate stemcell you'll need to re-upload with  --fix  bosh upload-stemcell .tgz --fix",
            "title": "CPI Specific Stemcells "
        },
        {
            "location": "/guide-multi-cpi-aws/",
            "text": "BOSH supports Multi-CPI since version v261+.\n\n\n\nIn this guide we explore how to configure BOSH to deploy VMs from a single deployment across two different regions in two separate AWS Accounts. Communication between regions will be configured via VPC Peering or through a VPN using IPSec.\n\n\nFor simplicity reasons we're going to allow all internal traffic between two VPCs, however this can be configured as desired by the operator.\n\n\n\n\nSet up the IaaS \n\u00b6\n\n\nLet's start by initializing main AZ (\nz1\n) to US East (N. Virginia) by following steps 1 and 2 from \nCreating environment on AWS\n. This will give you a working BOSH Director in a single region. You can perform a deployment to test Director is working fine.\n\n\nTo add a second AZ (\nz2\n) to US West (N. California) you need to perform step 1 from \nCreating environment on AWS\n in another AWS account.\n\n\n\n\nConnecting VPCs \n\u00b6\n\n\nThe VMs in one AZ need to be able to talk to VMs in the other AZ. We're going to describe two ways AZs can be connected. You have two options:\n\n\n\n\n\n\nif VPCs are in the same AWS region you can simply use \nVPC Peering\n as shown below\n\n\n\n\n\n\nif VPCs are in different regions you will need to connect them through a \nOpenVPN\n as shown below\n\n\n\n\n\n\nif VPCs are spread out across multiple regions, you can mix and match two approaches above\n\n\n\n\n\n\n\n\nVPC Peering (only works for VPCs in the same region) \n\u00b6\n\n\nTo connect VPCs in the same region you have to create a VPC Peering Connection between each region. In our case, we have two VPCs so only one connection is required.\n\n\n\n\n\n\nCreate new VPC Peering Connection as shown in the image:\n\n\n\n\n\n\n\n\nFrom the Accepter VPC Account go into the console and click Accept Request. After accepting the request it will recommend you to edit the route tables from each VPC to allow traffic between them through the peering connection.\n\n\n\n\n\n\n\n\nModify VPC Route Table in each VPC and add other VPC's CIDR block with the VPC Peering Connection as the target.\n\n\nFor \nz1\n:\n\n\n\n\nFor \nz2\n:\n\n\n\n\n\n\n\n\nNote: If you want IPv6 traffic to be routed you also need to add the corresponding IPv6 CIDR blocks.\n\n\n\n\n\nOpenVPN using IPSec \n\u00b6\n\n\nHere we are going to use the \nOpenVPN BOSH Release\n to connect both OpenVPN Server and client in each region like shown below:\n\n\n\n\n\n\n\n\nSetup local Multi-CPI directories:\n\n\n$ mkdir -p ~/workspace/multi-cpi-vpn\n$ \ncd\n ~/workspace\n\n# Clone OpenVPN BOSH Release\n\n$ git clone git@github.com:dpb587/openvpn-bosh-release.git\n\n# Clone Multi-CPI Knowledge-Base\n\n$ git clone git@github.com:cdutra/bosh-multi-cpi-kb.git\n$ \ncd\n multi-cpi-vpn\n\n\n\n\n\n\n\n\nAllocate Elastic IPs for each VPN Server in their respective regions.\n\n\n\n\n\n\nCreate following files \n~/workspace/multi-cpi-vpn/creds-az1.yml\n and \n~/workspace/multi-cpi-vpn/creds-az2.yml\n with the following properties. You should have all this information from the above \nSet up the IaaS\n step.\n\n\naccess_key_id\n:\n \n<aws-access-key-id>\n\n\nsecret_access_key\n:\n \n<aws-secret-access-key>\n\n\nregion\n:\n \n<aws-region>\n\n\navailability_zone\n:\n \n<aws-availability-zone>\n\n\nsubnet_id\n:\n \n<subnet-id>\n\n\nwan_ip\n:\n \n<aws-elastic-public-ip>\n \n# Used by OpenVPN Server\n\n\ndefault_security_groups\n:\n \n<security-group-name>\n\n\nbootstrap_ssh_key_name\n:\n \n<ssh-key-name>\n\n\nbootstrap_ssh_key_path\n:\n \n<ssh-private-key>\n\n\nroute_table_id\n:\n \n<aws-route-table-id>\n \n# e.g. rtb-4127673b\n\n\n\n\n\n\n\n\n\nGenerate certificates for each server and client.\n\n\n$ bosh int ~/workspace/bosh-multi-cpi-kb/templates/vpn-ca.yml \n\\\n\n  -l ~/workspace/multi-cpi-vpn/creds-az1.yml \n\\\n\n  --vars-store\n=\n~/workspace/multi-cpi-vpn/certs-vpn-az1.yml\n\n$ bosh int ~/workspace/bosh-multi-cpi-kb/templates/vpn-ca.yml \n\\\n\n  -l ~/workspace/multi-cpi-vpn/creds-az2.yml \n\\\n\n  --vars-store\n=\n~/workspace/multi-cpi-vpn/certs-vpn-az2.yml\n\n\n\n\n\n\n\n\nDeploy OpenVPN Servers in each AZ.\n\n\n# Create VPN server in z1\n\n$ bosh create-env \n\\\n\n  --vars-store ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml \n\\\n\n  --state ./openvpn-az1-state.json \n\\\n\n  -o ~/workspace/openvpn-bosh-release/deployment/init-aws.yml \n\\\n\n  -o ~/workspace/openvpn-bosh-release/deployment/with-pushed-routes.yml \n\\\n\n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-server-ops.yml \n\\\n\n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-client-ops.yml \n\\\n\n  -l ~/workspace/multi-cpi-vpn/creds-az1.yml \n\\\n\n  -v \nserver_key_pair\n=\n$(\n bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml --path /server_key_pair \n)\n \n\\\n\n  -v \npush_routes\n=[\n\"10.0.0.0 255.255.255.0\"\n]\n \n\\\n\n  -v \nlan_gateway\n=\n10\n.0.0.1 \n\\\n\n  -v \nlan_ip\n=\n10\n.0.0.7 \n\\\n\n  -v \nlan_network\n=\n10\n.0.0.0 \n\\\n\n  -v \nlan_network_mask_bits\n=\n24\n \n\\\n\n  -v \nvpn_network\n=\n192\n.168.0.0 \n\\\n\n  -v \nvpn_network_mask\n=\n255\n.255.255.0 \n\\\n\n  -v \nvpn_network_mask_bits\n=\n24\n \n\\\n\n  -v \nremote_network_cidr_block\n=\n10\n.0.1.0/24 \n\\\n\n  -v \nremote_vpn_ip\n=\n<az2-vpn-external-ip> \n\\\n\n  -v \nclient_key_pair\n=\n$(\n bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml --path /client_key_pair \n)\n \n\\\n\n  ~/workspace/openvpn-bosh-release/deployment/openvpn.yml\n\n\n# Create VPN server in z2\n\n$ bosh create-env \n\\\n\n  --vars-store ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml \n\\\n\n  --state ./openvpn-az2-state.json \n\\\n\n  -o ~/workspace/openvpn-bosh-release/deployment/init-aws.yml \n\\\n\n  -o ~/workspace/openvpn-bosh-release/deployment/with-pushed-routes.yml \n\\\n\n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-server-ops.yml \n\\\n\n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-client-ops.yml \n\\\n\n  -l ~/workspace/multi-cpi-vpn/creds-az2.yml \n\\\n\n  -v \nserver_key_pair\n=\n$(\n bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml --path /server_key_pair \n)\n \n\\\n\n  -v \npush_routes\n=[\n\"10.0.1.0 255.255.255.0\"\n]\n \n\\\n\n  -v \nlan_gateway\n=\n10\n.0.1.1 \n\\\n\n  -v \nlan_ip\n=\n10\n.0.1.7 \n\\\n\n  -v \nlan_network\n=\n10\n.0.1.0 \n\\\n\n  -v \nlan_network_mask_bits\n=\n24\n \n\\\n\n  -v \nvpn_network\n=\n192\n.168.1.0 \n\\\n\n  -v \nvpn_network_mask\n=\n255\n.255.255.0 \n\\\n\n  -v \nvpn_network_mask_bits\n=\n24\n \n\\\n\n  -v \nremote_network_cidr_block\n=\n10\n.0.0.0/24 \n\\\n\n  -v \nremote_vpn_ip\n=\n<az1-vpn-external-ip> \n\\\n\n  -v \nclient_key_pair\n=\n$(\n bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml --path /client_key_pair \n)\n \n\\\n\n  ~/workspace/openvpn-bosh-release/deployment/openvpn.yml\n\n\n\n\n\n\n\n\n\n\nConfigure CPI and Cloud configs \n\u00b6\n\n\nNow that the IaaS is configured, update your Director's \nCPI config\n:\n\n\ncpis\n:\n\n\n-\n \nname\n:\n \naws-us-east\n\n  \ntype\n:\n \naws\n\n  \nproperties\n:\n\n    \naccess_key_id\n:\n \n((az1_access_key_id))\n\n    \nsecret_access_key\n:\n \n((az1_secret_access_key))\n\n    \ndefault_key_name\n:\n \naz-1\n\n    \ndefault_security_groups\n:\n\n    \n-\n \n((az1_security_group))\n\n    \nregion\n:\n \nus-east-1\n\n\n-\n \nname\n:\n \naws-us-west\n\n  \ntype\n:\n \naws\n\n  \nproperties\n:\n\n    \naccess_key_id\n:\n \n((az2_access_key_id))\n\n    \nsecret_access_key\n:\n \n((az2_secret_access_key))\n\n    \ndefault_key_name\n:\n \naz-2\n\n    \ndefault_security_groups\n:\n\n    \n-\n \n((az2_security_group))\n\n    \nregion\n:\n \nus-west-1\n\n\n\n\n\n$ bosh update-cpi-config cpi.yml\n\n\n\n\nAnd cloud config:\n\n\nNote: The `azs` section of your `cloud-config` now contains the `cpi` key with available values that are defined in your `cpi-config`.\n\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncpi\n:\n \naws-us-east\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1a\n\n\n-\n \nname\n:\n \nz2\n\n  \ncpi\n:\n \naws-us-west\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-west-1a\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \naz\n:\n \nz1\n\n    \nrange\n:\n \n10.0.0.0/24\n\n    \ngateway\n:\n \n10.0.0.1\n\n    \nreserved\n:\n \n[\n10.0.0.2-10.0.0.9\n]\n\n    \ncloud_properties\n:\n\n      \nsubnet\n:\n \nsubnet-f529c6da\n\n  \n-\n \naz\n:\n \nz2\n\n    \nrange\n:\n \n10.0.1.0/24\n\n    \ngateway\n:\n \n10.0.1.1\n\n    \nreserved\n:\n \n[\n10.0.1.2-10.0.1.9\n]\n\n    \ncloud_properties\n:\n\n      \nsubnet\n:\n \nsubnet-452ec16a\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nt2.medium\n\n\n\ncompilation\n:\n\n  \naz\n:\n \nz1\n\n  \nnetwork\n:\n \nprivate\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \nvm_type\n:\n \ndefault\n\n  \nworkers\n:\n \n1\n\n\n\n\n\n$ bosh update-cloud-config cloud.yml\n\n\n\n\n\n\nDeploy example Zookeeper deployment \n\u00b6\n\n\n...",
            "title": "Using AWS"
        },
        {
            "location": "/guide-multi-cpi-aws/#set-up-the-iaas",
            "text": "Let's start by initializing main AZ ( z1 ) to US East (N. Virginia) by following steps 1 and 2 from  Creating environment on AWS . This will give you a working BOSH Director in a single region. You can perform a deployment to test Director is working fine.  To add a second AZ ( z2 ) to US West (N. California) you need to perform step 1 from  Creating environment on AWS  in another AWS account.",
            "title": "Set up the IaaS "
        },
        {
            "location": "/guide-multi-cpi-aws/#connecting-vpcs",
            "text": "The VMs in one AZ need to be able to talk to VMs in the other AZ. We're going to describe two ways AZs can be connected. You have two options:    if VPCs are in the same AWS region you can simply use  VPC Peering  as shown below    if VPCs are in different regions you will need to connect them through a  OpenVPN  as shown below    if VPCs are spread out across multiple regions, you can mix and match two approaches above",
            "title": "Connecting VPCs "
        },
        {
            "location": "/guide-multi-cpi-aws/#vpc-peering-only-works-for-vpcs-in-the-same-region",
            "text": "To connect VPCs in the same region you have to create a VPC Peering Connection between each region. In our case, we have two VPCs so only one connection is required.    Create new VPC Peering Connection as shown in the image:     From the Accepter VPC Account go into the console and click Accept Request. After accepting the request it will recommend you to edit the route tables from each VPC to allow traffic between them through the peering connection.     Modify VPC Route Table in each VPC and add other VPC's CIDR block with the VPC Peering Connection as the target.  For  z1 :   For  z2 :     Note: If you want IPv6 traffic to be routed you also need to add the corresponding IPv6 CIDR blocks.",
            "title": "VPC Peering (only works for VPCs in the same region) "
        },
        {
            "location": "/guide-multi-cpi-aws/#openvpn-using-ipsec",
            "text": "Here we are going to use the  OpenVPN BOSH Release  to connect both OpenVPN Server and client in each region like shown below:     Setup local Multi-CPI directories:  $ mkdir -p ~/workspace/multi-cpi-vpn\n$  cd  ~/workspace # Clone OpenVPN BOSH Release \n$ git clone git@github.com:dpb587/openvpn-bosh-release.git # Clone Multi-CPI Knowledge-Base \n$ git clone git@github.com:cdutra/bosh-multi-cpi-kb.git\n$  cd  multi-cpi-vpn    Allocate Elastic IPs for each VPN Server in their respective regions.    Create following files  ~/workspace/multi-cpi-vpn/creds-az1.yml  and  ~/workspace/multi-cpi-vpn/creds-az2.yml  with the following properties. You should have all this information from the above  Set up the IaaS  step.  access_key_id :   <aws-access-key-id>  secret_access_key :   <aws-secret-access-key>  region :   <aws-region>  availability_zone :   <aws-availability-zone>  subnet_id :   <subnet-id>  wan_ip :   <aws-elastic-public-ip>   # Used by OpenVPN Server  default_security_groups :   <security-group-name>  bootstrap_ssh_key_name :   <ssh-key-name>  bootstrap_ssh_key_path :   <ssh-private-key>  route_table_id :   <aws-route-table-id>   # e.g. rtb-4127673b     Generate certificates for each server and client.  $ bosh int ~/workspace/bosh-multi-cpi-kb/templates/vpn-ca.yml  \\ \n  -l ~/workspace/multi-cpi-vpn/creds-az1.yml  \\ \n  --vars-store = ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml\n\n$ bosh int ~/workspace/bosh-multi-cpi-kb/templates/vpn-ca.yml  \\ \n  -l ~/workspace/multi-cpi-vpn/creds-az2.yml  \\ \n  --vars-store = ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml    Deploy OpenVPN Servers in each AZ.  # Create VPN server in z1 \n$ bosh create-env  \\ \n  --vars-store ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml  \\ \n  --state ./openvpn-az1-state.json  \\ \n  -o ~/workspace/openvpn-bosh-release/deployment/init-aws.yml  \\ \n  -o ~/workspace/openvpn-bosh-release/deployment/with-pushed-routes.yml  \\ \n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-server-ops.yml  \\ \n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-client-ops.yml  \\ \n  -l ~/workspace/multi-cpi-vpn/creds-az1.yml  \\ \n  -v  server_key_pair = $(  bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml --path /server_key_pair  )   \\ \n  -v  push_routes =[ \"10.0.0.0 255.255.255.0\" ]   \\ \n  -v  lan_gateway = 10 .0.0.1  \\ \n  -v  lan_ip = 10 .0.0.7  \\ \n  -v  lan_network = 10 .0.0.0  \\ \n  -v  lan_network_mask_bits = 24   \\ \n  -v  vpn_network = 192 .168.0.0  \\ \n  -v  vpn_network_mask = 255 .255.255.0  \\ \n  -v  vpn_network_mask_bits = 24   \\ \n  -v  remote_network_cidr_block = 10 .0.1.0/24  \\ \n  -v  remote_vpn_ip = <az2-vpn-external-ip>  \\ \n  -v  client_key_pair = $(  bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml --path /client_key_pair  )   \\ \n  ~/workspace/openvpn-bosh-release/deployment/openvpn.yml # Create VPN server in z2 \n$ bosh create-env  \\ \n  --vars-store ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml  \\ \n  --state ./openvpn-az2-state.json  \\ \n  -o ~/workspace/openvpn-bosh-release/deployment/init-aws.yml  \\ \n  -o ~/workspace/openvpn-bosh-release/deployment/with-pushed-routes.yml  \\ \n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-server-ops.yml  \\ \n  -o ~/workspace/bosh-multi-cpi-kb/templates/vpn-client-ops.yml  \\ \n  -l ~/workspace/multi-cpi-vpn/creds-az2.yml  \\ \n  -v  server_key_pair = $(  bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az2.yml --path /server_key_pair  )   \\ \n  -v  push_routes =[ \"10.0.1.0 255.255.255.0\" ]   \\ \n  -v  lan_gateway = 10 .0.1.1  \\ \n  -v  lan_ip = 10 .0.1.7  \\ \n  -v  lan_network = 10 .0.1.0  \\ \n  -v  lan_network_mask_bits = 24   \\ \n  -v  vpn_network = 192 .168.1.0  \\ \n  -v  vpn_network_mask = 255 .255.255.0  \\ \n  -v  vpn_network_mask_bits = 24   \\ \n  -v  remote_network_cidr_block = 10 .0.0.0/24  \\ \n  -v  remote_vpn_ip = <az1-vpn-external-ip>  \\ \n  -v  client_key_pair = $(  bosh int ~/workspace/multi-cpi-vpn/certs-vpn-az1.yml --path /client_key_pair  )   \\ \n  ~/workspace/openvpn-bosh-release/deployment/openvpn.yml",
            "title": "OpenVPN using IPSec "
        },
        {
            "location": "/guide-multi-cpi-aws/#configure-cpi-and-cloud-configs",
            "text": "Now that the IaaS is configured, update your Director's  CPI config :  cpis :  -   name :   aws-us-east \n   type :   aws \n   properties : \n     access_key_id :   ((az1_access_key_id)) \n     secret_access_key :   ((az1_secret_access_key)) \n     default_key_name :   az-1 \n     default_security_groups : \n     -   ((az1_security_group)) \n     region :   us-east-1  -   name :   aws-us-west \n   type :   aws \n   properties : \n     access_key_id :   ((az2_access_key_id)) \n     secret_access_key :   ((az2_secret_access_key)) \n     default_key_name :   az-2 \n     default_security_groups : \n     -   ((az2_security_group)) \n     region :   us-west-1   $ bosh update-cpi-config cpi.yml  And cloud config:  Note: The `azs` section of your `cloud-config` now contains the `cpi` key with available values that are defined in your `cpi-config`.  azs :  -   name :   z1 \n   cpi :   aws-us-east \n   cloud_properties : \n     availability_zone :   us-east-1a  -   name :   z2 \n   cpi :   aws-us-west \n   cloud_properties : \n     availability_zone :   us-west-1a  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   az :   z1 \n     range :   10.0.0.0/24 \n     gateway :   10.0.0.1 \n     reserved :   [ 10.0.0.2-10.0.0.9 ] \n     cloud_properties : \n       subnet :   subnet-f529c6da \n   -   az :   z2 \n     range :   10.0.1.0/24 \n     gateway :   10.0.1.1 \n     reserved :   [ 10.0.1.2-10.0.1.9 ] \n     cloud_properties : \n       subnet :   subnet-452ec16a  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   t2.medium  compilation : \n   az :   z1 \n   network :   private \n   reuse_compilation_vms :   true \n   vm_type :   default \n   workers :   1   $ bosh update-cloud-config cloud.yml",
            "title": "Configure CPI and Cloud configs "
        },
        {
            "location": "/guide-multi-cpi-aws/#deploy-example-zookeeper-deployment",
            "text": "...",
            "title": "Deploy example Zookeeper deployment "
        },
        {
            "location": "/guide-ipv6-on-vsphere/",
            "text": "BOSH supports IPv6 on vSphere since version bosh-release v264+, stemcell 3468.11+ and CLI v2.0.45+.\n\n\n\nIn this guide we explore how to configure BOSH in an IPv6-enabled environment.\n\n\nTwo possible deployment options:\n\n\n\n\n\n\npure IPv6 configuration: both Director and deployed VMs use IPv6 addresses exclusively (currently being worked on)\n\n\n\n\n\n\nhybrid IPv6 configuration: Director is on IPv4 and deployed VMs use IPv4 and IPv6 addresses\n\n\n\n\n\n\n\n\nHybrid IPv6 configuration \n\u00b6\n\n\nIn this example, we use the BOSH CLI and \nbosh-deployment\n to deploy a Director with an IPv4 address and then deploy VMs with IPv4 and IPv6 addresses.\n\n\nPrerequisites\n\u00b6\n\n\n\n\n\n\nAll IPv6 address \nmust\n be specified in expanded format, leading zeroes, no double-colons. This applies to all variables, deployment manifests, cloud config, etc.\n\n\n\n\n\n\nUse Simple DNS's \ngenerator\n to obtain a \nprivate\n IPv6 address range.\n\n\n\n\n\n\nSteps\n\u00b6\n\n\n\n\n\n\nTo deploy the Director use \nbosh create-env\n command with additional IPv6-specific ops files. See \nCreating environment on vSphere\n for more details on initializing Director on vSphere.\n\n\n# Create directory to keep state\n\n$ mkdir ipv6 \n&&\n \ncd\n ipv6\n\n\n# Clone Director templates\n\n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    --state\n=\nstate.json \n\\\n\n    --vars-store\n=\ncreds.yml \n\\\n\n    -o bosh-deployment/vsphere/cpi.yml \n\\\n\n    -o bosh-deployment/vsphere/resource-pool.yml \n\\\n\n    -o bosh-deployment/jumpbox-user.yml \n\\\n\n    -o bosh-deployment/uaa.yml \n\\\n\n    -o bosh-deployment/credhub.yml \n\\\n\n    -v \ndirector_name\n=\nipv6 \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.9.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.9.1 \n\\\n\n    -v \ninternal_ip\n=\n10\n.0.9.111 \n\\\n\n    -v \nnetwork_name\n=\n\"VM Network\"\n \n\\\n\n    -v \nvcenter_dc\n=\ndc \n\\\n\n    -v \nvcenter_cluster\n=\ncl \n\\\n\n    -v \nvcenter_rp\n=\nIPv6 \n\\\n\n    -v \nvcenter_ds\n=\nSSD-0 \n\\\n\n    -v \nvcenter_ip\n=\n10\n.0.9.105 \n\\\n\n    -v \nvcenter_user\n=\nadministrator@vsphere.local \n\\\n\n    -v \nvcenter_password\n=\nTheClothesMakethTheMan \n\\\n\n    -v \nvcenter_templates\n=\nbosh-ipv6-templates \n\\\n\n    -v \nvcenter_vms\n=\nbosh-ipv6-vms \n\\\n\n    -v \nvcenter_disks\n=\nbosh-ipv6-disks\n\n\n\n\n\n\n\n\nConnect to the Director:\n\n\n$ bosh alias-env ipv6 -e \n10\n.0.9.111 --ca-cert <\n(\nbosh int ./creds.yml --path /director_ssl/ca\n)\n\n$ \nexport\n \nBOSH_CLIENT\n=\nadmin\n$ \nexport\n \nBOSH_CLIENT_SECRET\n=\n`\nbosh int ./creds.yml --path /admin_password\n`\n\n\n\n\n\n\n\n\n\nConfirm that it works:\n\n\n$ bosh -e ipv6 env\nUsing environment \n'10.0.9.111'\n as \n'?'\n\n\nName: ...\nUser: admin\n\nSucceeded\n\n\n\n\n\n\n\n\nDeploy example Zookeeper deployment \n\u00b6\n\n\nFollow steps below or the \ndeploy workflow\n that goes through the same steps but with more explanation.\n\n\n\n\n\n\nUpdate configs\n\n\n# ipv6-net.yml\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nipv6\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nazs\n:\n \n[\nz1\n,\n \nz2\n,\n \nz3\n]\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nVM Network\n\n    \ndns\n:\n\n    \n-\n \n2001:4860:4860:0000:0000:0000:0000:8888\n\n    \n-\n \n2001:4860:4860:0000:0000:0000:0000:8844\n\n    \ngateway\n:\n \n2601:646:100:69f0:20d:b9ff:fe48:9249\n\n    \nrange\n:\n \n2601:0646:0100:69f0:0000:0000:0000:0000/64\n\n    \nreserved\n:\n\n    \n-\n \n2601:0646:0100:69f0:0000:0000:0000:0000-2601:0646:0100:69f0:0000:0000:0000:0020\n\n\n\n\n\n$ bosh -e ipv6 update-cloud-config ~/workspace/bosh-deployment/vsphere/cloud-config.yml \n\\\n\n    -v \nvcenter_cluster\n=\ncl \n\\\n\n    -v \ninternal_cidr\n=\n10\n.0.9.0/24 \n\\\n\n    -v \ninternal_gw\n=\n10\n.0.9.1 \n\\\n\n    -v \nnetwork_name\n=\n\"VM Network\"\n \n\\\n\n\n$ bosh -e ipv6 update-config cloud --name ipv6 ipv6-net.yml\n\n$ bosh -e ipv6 update-runtime-config ~/workspace/bosh-deployment/runtime-configs/dns.yml\n\n\n\n\n\n\n\n\nUpload stemcell\n\n\n$ bosh -e ipv6 upload-stemcell https://bosh.io/d/stemcells/bosh-vsphere-esxi-ubuntu-trusty-go_agent?v\n=\n3468\n.17 \n\\\n\n  --sha1 1691f18b9141ac59aec893a1e8437a7d68a88038\n\n\n\n\nNote that IPv6 is currently only available for Ubuntu Trusty stemcells.\n\n\n\n\n\n\nDeploy example deployment and see IPv6 addresses\n\n\n# ipv6-net-use.yml\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/features?/use_dns_addresses\n\n  \nvalue\n:\n \ntrue\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/instance_groups/name=zookeeper/networks/0/default?\n\n  \nvalue\n:\n \n[\ndns\n,\n \ngateway\n]\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/instance_groups/name=zookeeper/networks/-\n\n  \nvalue\n:\n\n    \nname\n:\n \nipv6\n\n\n\n-\n \ntype\n:\n \nreplace\n\n  \npath\n:\n \n/instance_groups/name=smoke-tests/jobs/name=smoke-tests/consumes?/conn/network\n\n  \nvalue\n:\n \nipv6\n\n\n\n\n\n$ bosh -e ipv6 -d zookeeper deploy <\n(\nwget -O- https://raw.githubusercontent.com/cppforlife/zookeeper-release/master/manifests/zookeeper.yml\n)\n \n\\\n\n  -o ipv6-net-use.yml\n\n$ bosh -e ipv6 -d zookeeper instances\n\n\n\n\n\n\n\n\nRun Zookeeper smoke tests\n\n\n$ bosh -e ipv6 -d zookeeper run-errand smoke-tests",
            "title": "Using IPv6 on vSphere"
        },
        {
            "location": "/guide-ipv6-on-vsphere/#hybrid-ipv6-configuration",
            "text": "In this example, we use the BOSH CLI and  bosh-deployment  to deploy a Director with an IPv4 address and then deploy VMs with IPv4 and IPv6 addresses.",
            "title": "Hybrid IPv6 configuration "
        },
        {
            "location": "/guide-ipv6-on-vsphere/#prerequisites",
            "text": "All IPv6 address  must  be specified in expanded format, leading zeroes, no double-colons. This applies to all variables, deployment manifests, cloud config, etc.    Use Simple DNS's  generator  to obtain a  private  IPv6 address range.",
            "title": "Prerequisites"
        },
        {
            "location": "/guide-ipv6-on-vsphere/#steps",
            "text": "To deploy the Director use  bosh create-env  command with additional IPv6-specific ops files. See  Creating environment on vSphere  for more details on initializing Director on vSphere.  # Create directory to keep state \n$ mkdir ipv6  &&   cd  ipv6 # Clone Director templates \n$ git clone https://github.com/cloudfoundry/bosh-deployment\n\n$ bosh create-env bosh-deployment/bosh.yml  \\ \n    --state = state.json  \\ \n    --vars-store = creds.yml  \\ \n    -o bosh-deployment/vsphere/cpi.yml  \\ \n    -o bosh-deployment/vsphere/resource-pool.yml  \\ \n    -o bosh-deployment/jumpbox-user.yml  \\ \n    -o bosh-deployment/uaa.yml  \\ \n    -o bosh-deployment/credhub.yml  \\ \n    -v  director_name = ipv6  \\ \n    -v  internal_cidr = 10 .0.9.0/24  \\ \n    -v  internal_gw = 10 .0.9.1  \\ \n    -v  internal_ip = 10 .0.9.111  \\ \n    -v  network_name = \"VM Network\"   \\ \n    -v  vcenter_dc = dc  \\ \n    -v  vcenter_cluster = cl  \\ \n    -v  vcenter_rp = IPv6  \\ \n    -v  vcenter_ds = SSD-0  \\ \n    -v  vcenter_ip = 10 .0.9.105  \\ \n    -v  vcenter_user = administrator@vsphere.local  \\ \n    -v  vcenter_password = TheClothesMakethTheMan  \\ \n    -v  vcenter_templates = bosh-ipv6-templates  \\ \n    -v  vcenter_vms = bosh-ipv6-vms  \\ \n    -v  vcenter_disks = bosh-ipv6-disks    Connect to the Director:  $ bosh alias-env ipv6 -e  10 .0.9.111 --ca-cert < ( bosh int ./creds.yml --path /director_ssl/ca ) \n$  export   BOSH_CLIENT = admin\n$  export   BOSH_CLIENT_SECRET = ` bosh int ./creds.yml --path /admin_password `     Confirm that it works:  $ bosh -e ipv6 env\nUsing environment  '10.0.9.111'  as  '?' \n\nName: ...\nUser: admin\n\nSucceeded",
            "title": "Steps"
        },
        {
            "location": "/guide-ipv6-on-vsphere/#deploy-example-zookeeper-deployment",
            "text": "Follow steps below or the  deploy workflow  that goes through the same steps but with more explanation.    Update configs  # ipv6-net.yml  networks :  -   name :   ipv6 \n   type :   manual \n   subnets : \n   -   azs :   [ z1 ,   z2 ,   z3 ] \n     cloud_properties : \n       name :   VM Network \n     dns : \n     -   2001:4860:4860:0000:0000:0000:0000:8888 \n     -   2001:4860:4860:0000:0000:0000:0000:8844 \n     gateway :   2601:646:100:69f0:20d:b9ff:fe48:9249 \n     range :   2601:0646:0100:69f0:0000:0000:0000:0000/64 \n     reserved : \n     -   2601:0646:0100:69f0:0000:0000:0000:0000-2601:0646:0100:69f0:0000:0000:0000:0020   $ bosh -e ipv6 update-cloud-config ~/workspace/bosh-deployment/vsphere/cloud-config.yml  \\ \n    -v  vcenter_cluster = cl  \\ \n    -v  internal_cidr = 10 .0.9.0/24  \\ \n    -v  internal_gw = 10 .0.9.1  \\ \n    -v  network_name = \"VM Network\"   \\ \n\n$ bosh -e ipv6 update-config cloud --name ipv6 ipv6-net.yml\n\n$ bosh -e ipv6 update-runtime-config ~/workspace/bosh-deployment/runtime-configs/dns.yml    Upload stemcell  $ bosh -e ipv6 upload-stemcell https://bosh.io/d/stemcells/bosh-vsphere-esxi-ubuntu-trusty-go_agent?v = 3468 .17  \\ \n  --sha1 1691f18b9141ac59aec893a1e8437a7d68a88038  Note that IPv6 is currently only available for Ubuntu Trusty stemcells.    Deploy example deployment and see IPv6 addresses  # ipv6-net-use.yml  -   type :   replace \n   path :   /features?/use_dns_addresses \n   value :   true  -   type :   replace \n   path :   /instance_groups/name=zookeeper/networks/0/default? \n   value :   [ dns ,   gateway ]  -   type :   replace \n   path :   /instance_groups/name=zookeeper/networks/- \n   value : \n     name :   ipv6  -   type :   replace \n   path :   /instance_groups/name=smoke-tests/jobs/name=smoke-tests/consumes?/conn/network \n   value :   ipv6   $ bosh -e ipv6 -d zookeeper deploy < ( wget -O- https://raw.githubusercontent.com/cppforlife/zookeeper-release/master/manifests/zookeeper.yml )   \\ \n  -o ipv6-net-use.yml\n\n$ bosh -e ipv6 -d zookeeper instances    Run Zookeeper smoke tests  $ bosh -e ipv6 -d zookeeper run-errand smoke-tests",
            "title": "Deploy example Zookeeper deployment "
        },
        {
            "location": "/init-external-ip/",
            "text": "It's strongly recommended to not allow ingress traffic to the Director VM via public IP. One way to achieve that is to use a \njumpbox\n to access internal networks.\n\n\nIf you do have a jumpbox consider using \nCLI tunneling functionality\n instead of running CLI from the jumpbox VM.\n\n\nWhen it's not desirable or possible to have a jumpbox, you can use following steps to assign public IP to the Director VM.\n\n\nFor CPIs that do not use registry (Google, vSphere, vCloud):\n\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    -o ... \n\\\n\n    -o bosh-deployment/external-ip-not-recommended.yml \n\\\n\n    -v ... \n\\\n\n    -v \nexternal_ip\n=\n12\n.34.56.78\n\n\n\n\nOr for CPIs that do use registry (AWS, Azure, and OpenStack):\n\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    -o ... \n\\\n\n    -o bosh-deployment/external-ip-with-registry-not-recommended.yml \n\\\n\n    -v ... \n\\\n\n    -v \nexternal_ip\n=\n12\n.34.56.78\n\n\n\n\nNote that if you have already ran `bosh create-env` command before adding above operations file, you may have to remove generated Director (and other components such as UAA) SSL certificates from the variables store so that SSL certificates can be regenerated with SANs.",
            "title": "Using a Public IP"
        },
        {
            "location": "/jumpbox/",
            "text": "It's recommended:\n\n\n\n\nto maintain a separate jumpbox VM for your environment\n\n\ndo not SSH to the Director\n\n\nuse \nbosh ssh\n to access VMs in your deployments and use jumpbox VM as your SSH gateway\n\n\n\n\nTo obtain SSH access specifically to the Director VM when necessary you can opt into \njumpbox-user.yml\n ops file when running \nbosh create-env\n command\n. It will add a \njumpbox\n user to the VM (by using \nuser_add\n job from \ncloudfoundry/os-conf-release\n).\n\n\n$ bosh int creds.yml --path /jumpbox_ssh/private_key > jumpbox.key\n$ chmod \n600\n jumpbox.key\n$ ssh jumpbox@\n&\nlt\n;\nexternal-or-internal-ip\n&\ngt\n;\n -i jumpbox.key",
            "title": "Enabling SSH Access"
        },
        {
            "location": "/dns/",
            "text": "Note: This feature is still under development; however, there are portions of DNS functionality that are already available in bosh-release v262+. 3468+ Linux stemcells are required.\n\n\n\nUsing DNS instead of plain IPs within deployments:\n\n\n\n\nallows easy use of dynamic networks since IPs change with every redeploy\n\n\nprovides a way to reference deployed VMs more transparently\n\n\nprovides client side load balancing\n\n\nreduces number of configuration changes that need to be propagated when changing cluster layout\n\n\n\n\nHistorically BOSH users did not have an easy highly available solution to enable DNS for their deployments. PowerDNS was a possible choice; however, it required more advanced configuration that we did not feel comfortable recommending to everyone.\n\n\nAddition of native BOSH DNS solves these problems without making it hard to deploy and operate DNS servers.\n\n\n\n\nArchitecture \n\u00b6\n\n\nTo provide native DNS support following changes were made:\n\n\n\n\nDirector keeps track of DNS entries assigned to each instance\n\n\nAgent (on stemcells 3421+) updates DNS records metadata on its VM\n\n\nDNS release (more details below) provides resolution of BOSH specific DNS records\n\n\n\n\nGiven that the Director is the sole orchestrator of the system, it is now responsible for updating DNS records during a deploy. As VMs are created and deleted following DNS related steps happen:\n\n\n\n\nDirector notices that VM, after it's created or deleted, changed its IP\n\n\nDirector creates a new DNS records dataset and saves it to the blobstore\n\n\nDirector issues sync_dns Agent call to \nall\n VMs (in all deployments)\n\n\nEach Agent downloads new DNS records dataset and updates \n/var/vcap/instance/dns/records.json\n\n\nDNS release sees that local \n/var/vcap/instance/dns/records.json\n is updated, hence returns new information in future DNS requests\n\n\n\n\nSee \nDeploying step-by-step\n for full Director deployment flow.\n\n\n\n\nTypes of DNS addresses \n\u00b6\n\n\nThere are two types of DNS addresses that native DNS supports:\n\n\n\n\ninstance specific queries that resolve to a single instance\n\n\nprovided by \nspec.address\n or \nlink(\"...\").instances[...].address\n ERB accessors\n\n\ngroup specific queries that resolve to multiple instances\n\n\nprovided by \nlink(\"...\").address\n ERB accessor\n\n\n\n\nSince BOSH DNS is automatically managed, DNS addresses are not meant to be constructed manually by operators or scripts. To obtain a DNS address you can use upcoming Links API or job template accessors within your jobs.\n\n\n\n\nDNS release \n\u00b6\n\n\nTo take advantage of native DNS functionality, it's expected that \nDNS release\n runs on each VM. We recommend to colocate DNS release by definiting it in an \naddon\n.\n\n\nDNS release provides two jobs: \nbosh-dns\n (for Linux) and \nbosh-dns-windows\n (for Windows) which start a simple DNS server bound to a \nlink local address\n.\n\n\nRecursors \n\u00b6\n\n\nHere is how DNS release chooses recurors before starting its operation:\n\n\n\n\nby default will pick up recursors specified in \n/etc/resolv.conf\n (denoted by \nnameserver\n keyword)\n\n\nalternatively, if \nrecursors\n property is set use specified recursors\n\n\nexclude recursors specified in \nexcluded_recursors\n property\n\n\nrandomly pick one recursor from the list of recursors\n\n\nnote that all recursors in this list will be considered equivalent, ie able to resolve same domains\n\n\nfailover to using another randomly picked recursor, if current recursor exhibits connectivity problems\n\n\nconnectivity problems do not account for resolution problems (NXDOMAIN, or other DNS level errors)\n\n\n\n\nAliases \n\u00b6\n\n\nDNS release allows operators to specify custom names for BOSH generated DNS records to ease migration or work with legacy software that requires very specific DNS record formats (e.g. \nmaster0\n, \nslave0\n, \nslave1\n).\n\n\nThere are two ways to specify aliases:\n\n\n\n\nvia \naliases\n property\n\n\nvia \ndns/aliases.json\n template inside your job\n\n\n\n\nExample usage of \naliases\n property:\n\n\nproperties\n:\n\n  \naliases\n:\n\n    \nbbs.service.cf.internal\n:\n\n    \n-\n \n\"*.database-z1.diego1.cf-cfapps-io2-diego.bosh\"\n\n    \n-\n \n\"*.database-z2.diego2.cf-cfapps-io2-diego.bosh\"\n\n\n\n\n\nAbove will resolve \nbbs.service.cf.internal\n to a all IPs (shuffled) matching following instance patterns: \n*.database-z1.diego1.cf-cfapps-io2-diego.bosh\n or \n*.database-z2.diego2.cf-cfapps-io2-diego.bosh\n.\n\n\nSee \nMigrating from Consul\n for more details.\n\n\nHealthiness\n\u00b6\n\n\nDNS release provides a way to reference all instances (or a subset of instances) in a link via single DNS record. By default only heatlhy instances are returned from a group query when there is at least one healthy insance. When there are no healthy instances, all instances will be returned. The notion of instance healthiness is directly tied to the state of processes running on a VM. DNS release will continiously poll for updated healthiness information (same information is visible via \nbosh instances --ps\n command) on all instances from groups that were resolved at least once.\n\n\nTo enable healthiness, use \nhealth.enabled\n property and specify necessary TLS certificates. Canonical DNS runtime config with healthiness enabled can be found here: \nhttps://github.com/cloudfoundry/bosh-deployment/blob/master/runtime-configs/dns.yml\n.\n\n\nCaching\n\u00b6\n\n\nDNS release provides a way to enable response caching based on response TTLs. Enabling caching typically will alleviate some pressure from your upstream DNS servers and decrease  latency of DNS resolutions.\n\n\nTo enable caching, use \ncache.enabled\n property. Canonical DNS runtime config with caching enabled can be found here: \nhttps://github.com/cloudfoundry/bosh-deployment/blob/master/runtime-configs/dns.yml\n.\n\n\nAdditional Handlers\n\u00b6\n\n\nDNS release provides a way to delegate certain domains via \nhandlers\n property\n to different DNS or HTTP servers. This functionality can be used as an alternative to configuring upstream DNS servers with custom zone configurations.\n\n\n\n\nEnabling DNS \n\u00b6\n\n\nTo enable native BOSH functionality, you must first enable \nlocal_dns.enabled\n property\n in the Director job. See \nbosh-deployment's local-dns.yml\n as an example.\n\n\nEnabling \nlocal_dns.enabled\n configuration will make Director broadcast DNS updates to all VMs. Only VMs based on 3421+ Linux stemcells will accept DNS broadcast message.\n\n\nIf you were relying on instance index based DNS records, you must enable \nlocal_dns.include_index\n property\n in the Director job.\n\n\nAdditionally you should colocate DNS release via an addon in all your deployments. See \nbosh-deployment's runtime-configs/dns.yml\n as an example.\n\n\n\n\nImpact on links \n\u00b6\n\n\nEach link includes some networking information about its provider. Addresses returned by a link may be either IP addresses or DNS addresses.\n\n\nAs of bosh-release v263 opting into DNS addresses in links must be done explicitly. Previous Director versions would opt into this behaviour by default.\n\n\nYou can control type of addresses returned at three different levels:\n\n\n\n\n\n\nfor the entire Director via Director job configuration \ndirector.local_dns.use_dns_addresses\n property\n that if enabled affects all deployments by default. We are planning to eventually change this configuration to true by default.\n\n\n\n\n\n\nfor a specific deployment via \nfeatures.use_dns_addresses\n deployment manifest property\n that if enabled affects links within this deployment\n\n\n\n\n\n\nfor a specific link via its \nip_addresses\n configuration\n\n\nIf for some reason (discouraged) particular job cannot work with links that return DNS addresses, you can ask the Director to return IP addresses on best effort basis. Here is an example how to opt into this behaviour for a single link:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nzookeeper\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nzookeeper\n\n    \nrelease\n:\n \nzookeeper\n\n    \nconsumes\n:\n\n      \npeers\n:\n \n{\nip_addresses\n:\n \ntrue\n}\n\n\n...\n\n\n\n\n\n\n\n\n\nOnce native DNS addresses in links are enabled DNS addresses will be returned instead of IPs. Note that links provided by instance groups placed on dynamic networks will always provide DNS addresses.\n\n\n# before\n\n\nlink\n(\n\"db\"\n)\n.\naddress\n \n=>\n \n\"q-s0.db.default.db.bosh\"\n\n\nlink\n(\n\"db\"\n)\n.\ninstances\n[\n0\n].\naddress\n \n=>\n \n\"172.10.10.0\"\n\n\n\n# after\n\n\nlink\n(\n\"db\"\n)\n.\naddress\n \n=>\n \n\"q-s0.db.default.db.bosh\"\n\n\nlink\n(\n\"db\"\n)\n.\ninstances\n[\n0\n].\naddress\n \n=>\n \n\"ef489dd9-48f6-45f0-b7af-7f3437919b17.db.default.db.bosh\"\n\n\n\n\n\n\n\nImpact on job's address (\nspec.address\n) \n\u00b6\n\n\nSimilar to how \nlinks are affected\n, \nspec.address\n will start returning DNS address once \nuse_dns_addresses\n feature is enabled.\n\n\n\n\nMigrating from PowerDNS \n\u00b6\n\n\nHistorically BOSH users did not have an easy highly available solution to enable DNS for their deployments. PowerDNS was a possible choice; however, it required more advanced configuration that we felt comfortable recommending to everyone. We are planning to deprecate and remove PowerDNS integration. To migrate from PowerDNS to native DNS:\n\n\n\n\ncontinue deploying Director with \npowerdns\n job\n\n\nenable native DNS (follow \nEnabling DNS\n section above) with proper recursors configured\n\n\nredeploy all deployments and make sure that native DNS is in use\n\n\nredeploy Director without \npowerdns\n job\n\n\n\n\n\n\nMigrating from Consul \n\u00b6\n\n\nTo ease migration from Consul DNS entries, DNS release provides \naliases feature\n. It allows operators to define custom DNS entries that can map to BOSH generated DNS entries. To migrate off of Consul to native DNS:\n\n\n\n\nenable native DNS (follow \nEnabling DNS\n section above) with proper recursors configured\n\n\ncontinue deploying \nconsul_agent\n job\n\n\ndefine native DNS aliases that match existing Consul DNS entries\n\n\nredeploy all deployments that use Consul\n\n\nredeploy all deployments without \nconsul_agent\n job",
            "title": "Native DNS Support"
        },
        {
            "location": "/dns/#architecture",
            "text": "To provide native DNS support following changes were made:   Director keeps track of DNS entries assigned to each instance  Agent (on stemcells 3421+) updates DNS records metadata on its VM  DNS release (more details below) provides resolution of BOSH specific DNS records   Given that the Director is the sole orchestrator of the system, it is now responsible for updating DNS records during a deploy. As VMs are created and deleted following DNS related steps happen:   Director notices that VM, after it's created or deleted, changed its IP  Director creates a new DNS records dataset and saves it to the blobstore  Director issues sync_dns Agent call to  all  VMs (in all deployments)  Each Agent downloads new DNS records dataset and updates  /var/vcap/instance/dns/records.json  DNS release sees that local  /var/vcap/instance/dns/records.json  is updated, hence returns new information in future DNS requests   See  Deploying step-by-step  for full Director deployment flow.",
            "title": "Architecture "
        },
        {
            "location": "/dns/#types-of-dns-addresses",
            "text": "There are two types of DNS addresses that native DNS supports:   instance specific queries that resolve to a single instance  provided by  spec.address  or  link(\"...\").instances[...].address  ERB accessors  group specific queries that resolve to multiple instances  provided by  link(\"...\").address  ERB accessor   Since BOSH DNS is automatically managed, DNS addresses are not meant to be constructed manually by operators or scripts. To obtain a DNS address you can use upcoming Links API or job template accessors within your jobs.",
            "title": "Types of DNS addresses "
        },
        {
            "location": "/dns/#dns-release",
            "text": "To take advantage of native DNS functionality, it's expected that  DNS release  runs on each VM. We recommend to colocate DNS release by definiting it in an  addon .  DNS release provides two jobs:  bosh-dns  (for Linux) and  bosh-dns-windows  (for Windows) which start a simple DNS server bound to a  link local address .",
            "title": "DNS release "
        },
        {
            "location": "/dns/#recursors",
            "text": "Here is how DNS release chooses recurors before starting its operation:   by default will pick up recursors specified in  /etc/resolv.conf  (denoted by  nameserver  keyword)  alternatively, if  recursors  property is set use specified recursors  exclude recursors specified in  excluded_recursors  property  randomly pick one recursor from the list of recursors  note that all recursors in this list will be considered equivalent, ie able to resolve same domains  failover to using another randomly picked recursor, if current recursor exhibits connectivity problems  connectivity problems do not account for resolution problems (NXDOMAIN, or other DNS level errors)",
            "title": "Recursors "
        },
        {
            "location": "/dns/#aliases",
            "text": "DNS release allows operators to specify custom names for BOSH generated DNS records to ease migration or work with legacy software that requires very specific DNS record formats (e.g.  master0 ,  slave0 ,  slave1 ).  There are two ways to specify aliases:   via  aliases  property  via  dns/aliases.json  template inside your job   Example usage of  aliases  property:  properties : \n   aliases : \n     bbs.service.cf.internal : \n     -   \"*.database-z1.diego1.cf-cfapps-io2-diego.bosh\" \n     -   \"*.database-z2.diego2.cf-cfapps-io2-diego.bosh\"   Above will resolve  bbs.service.cf.internal  to a all IPs (shuffled) matching following instance patterns:  *.database-z1.diego1.cf-cfapps-io2-diego.bosh  or  *.database-z2.diego2.cf-cfapps-io2-diego.bosh .  See  Migrating from Consul  for more details.",
            "title": "Aliases "
        },
        {
            "location": "/dns/#healthiness",
            "text": "DNS release provides a way to reference all instances (or a subset of instances) in a link via single DNS record. By default only heatlhy instances are returned from a group query when there is at least one healthy insance. When there are no healthy instances, all instances will be returned. The notion of instance healthiness is directly tied to the state of processes running on a VM. DNS release will continiously poll for updated healthiness information (same information is visible via  bosh instances --ps  command) on all instances from groups that were resolved at least once.  To enable healthiness, use  health.enabled  property and specify necessary TLS certificates. Canonical DNS runtime config with healthiness enabled can be found here:  https://github.com/cloudfoundry/bosh-deployment/blob/master/runtime-configs/dns.yml .",
            "title": "Healthiness"
        },
        {
            "location": "/dns/#caching",
            "text": "DNS release provides a way to enable response caching based on response TTLs. Enabling caching typically will alleviate some pressure from your upstream DNS servers and decrease  latency of DNS resolutions.  To enable caching, use  cache.enabled  property. Canonical DNS runtime config with caching enabled can be found here:  https://github.com/cloudfoundry/bosh-deployment/blob/master/runtime-configs/dns.yml .",
            "title": "Caching"
        },
        {
            "location": "/dns/#additional-handlers",
            "text": "DNS release provides a way to delegate certain domains via  handlers  property  to different DNS or HTTP servers. This functionality can be used as an alternative to configuring upstream DNS servers with custom zone configurations.",
            "title": "Additional Handlers"
        },
        {
            "location": "/dns/#enabling-dns",
            "text": "To enable native BOSH functionality, you must first enable  local_dns.enabled  property  in the Director job. See  bosh-deployment's local-dns.yml  as an example.  Enabling  local_dns.enabled  configuration will make Director broadcast DNS updates to all VMs. Only VMs based on 3421+ Linux stemcells will accept DNS broadcast message.  If you were relying on instance index based DNS records, you must enable  local_dns.include_index  property  in the Director job.  Additionally you should colocate DNS release via an addon in all your deployments. See  bosh-deployment's runtime-configs/dns.yml  as an example.",
            "title": "Enabling DNS "
        },
        {
            "location": "/dns/#impact-on-links",
            "text": "Each link includes some networking information about its provider. Addresses returned by a link may be either IP addresses or DNS addresses.  As of bosh-release v263 opting into DNS addresses in links must be done explicitly. Previous Director versions would opt into this behaviour by default.  You can control type of addresses returned at three different levels:    for the entire Director via Director job configuration  director.local_dns.use_dns_addresses  property  that if enabled affects all deployments by default. We are planning to eventually change this configuration to true by default.    for a specific deployment via  features.use_dns_addresses  deployment manifest property  that if enabled affects links within this deployment    for a specific link via its  ip_addresses  configuration  If for some reason (discouraged) particular job cannot work with links that return DNS addresses, you can ask the Director to return IP addresses on best effort basis. Here is an example how to opt into this behaviour for a single link:  instance_groups :  -   name :   zookeeper \n   jobs : \n   -   name :   zookeeper \n     release :   zookeeper \n     consumes : \n       peers :   { ip_addresses :   true }  ...     Once native DNS addresses in links are enabled DNS addresses will be returned instead of IPs. Note that links provided by instance groups placed on dynamic networks will always provide DNS addresses.  # before  link ( \"db\" ) . address   =>   \"q-s0.db.default.db.bosh\"  link ( \"db\" ) . instances [ 0 ]. address   =>   \"172.10.10.0\"  # after  link ( \"db\" ) . address   =>   \"q-s0.db.default.db.bosh\"  link ( \"db\" ) . instances [ 0 ]. address   =>   \"ef489dd9-48f6-45f0-b7af-7f3437919b17.db.default.db.bosh\"",
            "title": "Impact on links "
        },
        {
            "location": "/dns/#impact-on-jobs-address-specaddress",
            "text": "Similar to how  links are affected ,  spec.address  will start returning DNS address once  use_dns_addresses  feature is enabled.",
            "title": "Impact on job's address (spec.address) "
        },
        {
            "location": "/dns/#migrating-from-powerdns",
            "text": "Historically BOSH users did not have an easy highly available solution to enable DNS for their deployments. PowerDNS was a possible choice; however, it required more advanced configuration that we felt comfortable recommending to everyone. We are planning to deprecate and remove PowerDNS integration. To migrate from PowerDNS to native DNS:   continue deploying Director with  powerdns  job  enable native DNS (follow  Enabling DNS  section above) with proper recursors configured  redeploy all deployments and make sure that native DNS is in use  redeploy Director without  powerdns  job",
            "title": "Migrating from PowerDNS "
        },
        {
            "location": "/dns/#migrating-from-consul",
            "text": "To ease migration from Consul DNS entries, DNS release provides  aliases feature . It allows operators to define custom DNS entries that can map to BOSH generated DNS entries. To migrate off of Consul to native DNS:   enable native DNS (follow  Enabling DNS  section above) with proper recursors configured  continue deploying  consul_agent  job  define native DNS aliases that match existing Consul DNS entries  redeploy all deployments that use Consul  redeploy all deployments without  consul_agent  job",
            "title": "Migrating from Consul "
        },
        {
            "location": "/bosh-components/",
            "text": "Before creating a new \nenvironment\n we recommend to learn the names of major components that will be installed, used and configured:\n\n\n\n\n\n\nCommand Line Interface (CLI) \n\u00b6\n\n\nThe Command Line Interface (CLI) is the primary operator interface to BOSH. An operator uses the CLI to interact with the Director and perform actions on the cloud.\n\n\nCLI is typically installed on a machine that can directly communicate with the Director's API, e.g. an operator's laptop, or a jumpbox in the datacenter.\n\n\n\n\nDirector \n\u00b6\n\n\nThe Director is the core orchestrating component in BOSH. The Director controls VM creation and deployment, as well as other software and service lifecycle events.\n\n\nThe Director creates actionable tasks:\n\n\n\n\nBy translating commands sent by an operator through the CLI\n\n\nFrom scheduled processes like backups or snapshots\n\n\nIf needed to reconcile the expected state with the actual state of a VM\n\n\n\n\nOnce created, the Director adds these tasks to the Task Queue. Worker processes take tasks from the Task Queue and act on them.\n\n\nTask Queue \n\u00b6\n\n\nAn asynchronous queue used by the Director and Workers to manage tasks. The Task Queue resides in the Database.\n\n\nWorkers \n\u00b6\n\n\nDirector workers take tasks from the Task Queue and act on them.\n\n\nCloud Provider Interface (CPI) \n\u00b6\n\n\nA Cloud Provider Interface (CPI) is an API that the Director uses to interact with an IaaS to create and manage stemcells, VMs, and disks. A CPI abstracts infrastructure differences from the rest of BOSH.\n\n\n\n\nHealth Monitor \n\u00b6\n\n\nThe Health Monitor uses status and lifecycle events received from Agents to monitor the health of VMs. If the Health Monitor detects a problem with a VM, it can send an alert through notification plugins, or trigger the Resurrector.\n\n\nResurrector \n\u00b6\n\n\nIf enabled, the Resurrector plugin automatically recreates VMs identified by the Health Monitor as missing or unresponsive. It uses same Director API that CLI uses.\n\n\n\n\nDNS Server \n\u00b6\n\n\nBOSH uses PowerDNS to provide DNS resolution between the VMs in a deployment.\n\n\n\n\nComponents used to store Director's persistent data \n\u00b6\n\n\nDatabase \n\u00b6\n\n\nThe Director uses a Postgres database to store information about the desired state of a deployment. This includes information about stemcells, releases, and deployments.\n\n\nBlobstore \n\u00b6\n\n\nThe Blobstore stores the source forms of releases and the compiled images of releases. An operator uploads a release using the CLI, and the Director inserts the release into the Blobstore. When you deploy a release, BOSH orchestrates the compilation of packages and stores the result in the Blobstore.\n\n\n\n\nAgent \n\u00b6\n\n\nBOSH includes an Agent on every VM that it deploys. The Agent listens for instructions from the Director and carries out those instructions. The Agent receives job specifications from the Director and uses them to assign a role, or Job, to the VM.\n\n\nFor example, to assign the job of running MySQL to a VM, the Director sends instructions to the Agent on the VM. These instructions include which packages to install and how to configure those packages. The Agent uses these instructions to install and configure MySQL on the VM.\n\n\n\n\nComponents used for cross-component communication \n\u00b6\n\n\nMessage Bus (NATS) \n\u00b6\n\n\nThe Director and the Agents communicate through a lightweight publish-subscribe messaging system called NATS. These messages have two purposes: to perform provisioning instructions on the VMs, and to inform the Health Monitor about changes in the health of monitored processes.\n\n\nRegistry \n\u00b6\n\n\nWhen the Director creates or updates a VM, it stores configuration information for the VM in the Registry so that it can be used during bootstrapping stage of the VM.\n\n\n\n\nExample component interaction \n\u00b6\n\n\nThis example shows how components interact when creating a new VM.\n\n\nCreating a new VM \n\u00b6\n\n\n\n\n\n\nThrough the CLI, the operator takes an action (e.g. deploy for the first time, scaling up deployment) which requires creating a new VM.\n\n\nThe CLI passes the instruction to the Director.\n\n\nThe Director uses the CPI to tell the IaaS to launch a VM.\n\n\nThe IaaS provides the Director with information (IP addresses and IDs) the Agent on the VM needs to configure the VM.\n\n\nThe Director uses CPI to update the Registry with the configuration information for the VM.\n\n\nThe Agent running on the VM requests the configuration information for the VM from the Registry.\n\n\nThe Registry responds with the IP addresses and IDs.\n\n\nThe Agent uses the IP addresses and IDs to configure the VM.\n\n\n\n\n\n\nNext: \nCreate an environment",
            "title": "Components of BOSH"
        },
        {
            "location": "/bosh-components/#command-line-interface-cli",
            "text": "The Command Line Interface (CLI) is the primary operator interface to BOSH. An operator uses the CLI to interact with the Director and perform actions on the cloud.  CLI is typically installed on a machine that can directly communicate with the Director's API, e.g. an operator's laptop, or a jumpbox in the datacenter.",
            "title": "Command Line Interface (CLI) "
        },
        {
            "location": "/bosh-components/#director",
            "text": "The Director is the core orchestrating component in BOSH. The Director controls VM creation and deployment, as well as other software and service lifecycle events.  The Director creates actionable tasks:   By translating commands sent by an operator through the CLI  From scheduled processes like backups or snapshots  If needed to reconcile the expected state with the actual state of a VM   Once created, the Director adds these tasks to the Task Queue. Worker processes take tasks from the Task Queue and act on them.",
            "title": "Director "
        },
        {
            "location": "/bosh-components/#task-queue",
            "text": "An asynchronous queue used by the Director and Workers to manage tasks. The Task Queue resides in the Database.",
            "title": "Task Queue "
        },
        {
            "location": "/bosh-components/#workers",
            "text": "Director workers take tasks from the Task Queue and act on them.",
            "title": "Workers "
        },
        {
            "location": "/bosh-components/#cloud-provider-interface-cpi",
            "text": "A Cloud Provider Interface (CPI) is an API that the Director uses to interact with an IaaS to create and manage stemcells, VMs, and disks. A CPI abstracts infrastructure differences from the rest of BOSH.",
            "title": "Cloud Provider Interface (CPI) "
        },
        {
            "location": "/bosh-components/#health-monitor",
            "text": "The Health Monitor uses status and lifecycle events received from Agents to monitor the health of VMs. If the Health Monitor detects a problem with a VM, it can send an alert through notification plugins, or trigger the Resurrector.",
            "title": "Health Monitor "
        },
        {
            "location": "/bosh-components/#resurrector",
            "text": "If enabled, the Resurrector plugin automatically recreates VMs identified by the Health Monitor as missing or unresponsive. It uses same Director API that CLI uses.",
            "title": "Resurrector "
        },
        {
            "location": "/bosh-components/#dns-server",
            "text": "BOSH uses PowerDNS to provide DNS resolution between the VMs in a deployment.",
            "title": "DNS Server "
        },
        {
            "location": "/bosh-components/#components-used-to-store-directors-persistent-data",
            "text": "",
            "title": "Components used to store Director's persistent data "
        },
        {
            "location": "/bosh-components/#database",
            "text": "The Director uses a Postgres database to store information about the desired state of a deployment. This includes information about stemcells, releases, and deployments.",
            "title": "Database "
        },
        {
            "location": "/bosh-components/#blobstore",
            "text": "The Blobstore stores the source forms of releases and the compiled images of releases. An operator uploads a release using the CLI, and the Director inserts the release into the Blobstore. When you deploy a release, BOSH orchestrates the compilation of packages and stores the result in the Blobstore.",
            "title": "Blobstore "
        },
        {
            "location": "/bosh-components/#agent",
            "text": "BOSH includes an Agent on every VM that it deploys. The Agent listens for instructions from the Director and carries out those instructions. The Agent receives job specifications from the Director and uses them to assign a role, or Job, to the VM.  For example, to assign the job of running MySQL to a VM, the Director sends instructions to the Agent on the VM. These instructions include which packages to install and how to configure those packages. The Agent uses these instructions to install and configure MySQL on the VM.",
            "title": "Agent "
        },
        {
            "location": "/bosh-components/#components-used-for-cross-component-communication",
            "text": "",
            "title": "Components used for cross-component communication "
        },
        {
            "location": "/bosh-components/#message-bus-nats",
            "text": "The Director and the Agents communicate through a lightweight publish-subscribe messaging system called NATS. These messages have two purposes: to perform provisioning instructions on the VMs, and to inform the Health Monitor about changes in the health of monitored processes.",
            "title": "Message Bus (NATS) "
        },
        {
            "location": "/bosh-components/#registry",
            "text": "When the Director creates or updates a VM, it stores configuration information for the VM in the Registry so that it can be used during bootstrapping stage of the VM.",
            "title": "Registry "
        },
        {
            "location": "/bosh-components/#example-component-interaction",
            "text": "This example shows how components interact when creating a new VM.",
            "title": "Example component interaction "
        },
        {
            "location": "/bosh-components/#creating-a-new-vm",
            "text": "Through the CLI, the operator takes an action (e.g. deploy for the first time, scaling up deployment) which requires creating a new VM.  The CLI passes the instruction to the Director.  The Director uses the CPI to tell the IaaS to launch a VM.  The IaaS provides the Director with information (IP addresses and IDs) the Agent on the VM needs to configure the VM.  The Director uses CPI to update the Registry with the configuration information for the VM.  The Agent running on the VM requests the configuration information for the VM from the Registry.  The Registry responds with the IP addresses and IDs.  The Agent uses the IP addresses and IDs to configure the VM.    Next:  Create an environment",
            "title": "Creating a new VM "
        },
        {
            "location": "/deploying-step-by-step/",
            "text": "The Director will do the following \nsteps\n when \nbosh deploy\n (or its related commands such as start, stop and recreate) command runs:\n\n\n\n\n\n\nCheck if there is a deployment with the name specified by the deployment manifest\n\n\n\n\nif no, create a deployment\n\n\nif yes, lock the deployment so that no other operation can modify it\n\n\n\n\n\n\n\n\nValidate the deployment manifest syntactically and semantically\n\n\n\n\nif invalid, return an error to the user describing the problem(s)\n\n\n\n\n\n\n\n\nContact all existing VMs associated with this deployment to determine their network, configuration, and job configurations\n\n\n\n\nif the Director cannot contact all VMs, return an error to the user. This results in a \"Timed out sending \nget_state\n error\" during the 'Binding existing deployment' stage. At this point the operator is expected to use \nbosh cck\n command to determine why certain VMs are not accessible.\n\n\n\n\n\n\n\n\nDetermine requested networking changes to the existing and new VMs\n\n\n\n\nif network changes cannot be resolved (e.g. currently the Director does not support swapping of static IP reservations), return an error to the user\n\n\nif network changes require more IPs than the deployment's networks allow, return an error to the user\n\n\n\n\n\n\n\n\nDelete instance groups that are no longer specified by the deployment manifest\n\n\n\n\nissue unmount_disk Agent call for attached disks\n\n\nissue delete_vm CPI call for each VM\n\n\norphan persistent disks\n\n\nUpdate and propagate DNS records\n\n\n\n\n\n\n\n\nCreate compilation worker VMs based on as specified by \ncompilation\n section\n\n\n\n\nissue create_vm CPI call\n\n\n\n\n\n\n\n\nDetermine release packages dependency graph and compile each package on compilation worker VMs\n\n\n\n\nissue compile_package Agent call for each package\n\n\n\n\n\n\n\n\nDelete all compilation worker VMs\n\n\n\n\nissue delete_vm CPI call\n\n\n\n\n\n\n\n\nCreate empty VMs for new instance groups\n\n\n\n\nUpdate and propagate DNS records\n\n\n\n\n\n\n\n\nCreate empty VMs for instance groups that increased in instance size\n\n\n\n\nUpdate and propagate DNS records\n\n\n\n\n\n\n\n\nUpdate each one of the instance groups:\n\n\nSubset of instances (within an instance group) is selected to be updated first based on the update options for the instance group or global update options. That group of instances are called canaries.\n\n\nEven if only one job or package changed in the instance group, stopping and starting procedure will apply to all of the jobs on the instances in that group. One of the future enhancements is to make this procedure more surgical and only affect jobs that have changed.\n\n\n\n\n\n\nCheck if the instance previously existed\n\n\n\n\nif no, select a VM and assign it to be this instance\n\n\nif yes, check to see what has changed since last time it was updated\n\n\n\n\n\n\n\n\nDownload updated jobs and packages onto the VM\n\n\n\n\nissue prepare Agent call\n\n\n\n\n\n\n\n\nrun drain and stop scripts to safely stop processes on the VM\n\n\n\n\nissue drain Agent call\n\n\nissue stop Agent call\n\n\n\n\n\n\n\n\nTake persistent disks snapshot associated with the job instance\n\n\n\n\nissue take_snapshot CPI call if Director has snapshotting enabled\n\n\n\n\n\n\n\n\nCheck if the instance group still uses the same stemcell\n\n\n\n\nif no, create a new VM based on a correct stemcell\n\n\nissue delete_vm CPI call\n\n\nissue create_vm CPI call\n\n\nUpdate and propagate DNS records\n\n\n\n\n\n\nif yes, do nothing\n\n\n\n\n\n\n\n\nCheck if the instance group's network configuration changed\n\n\n\n\nif no, do nothing\n\n\nif yes, reconfigure running VM to match new configuration\n\n\nissue delete_vm CPI call\n\n\nissue create_vm CPI call\n\n\nUpdate and propagate DNS records\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate DNS A record for this instance with new IP\n\n\n\n\n\n\nCheck if the instance group's persistent disk changed\n\n\n\n\nif no, do nothing\n\n\nif yes, create a new persistent disk with correct size and type and copy data from the old persistent disk\n\n\nissue create_disk CPI call for the new disk\n\n\nissue attach_disk CPI call on a new disk\n\n\nissue mount_disk Agent call on a new disk\n\n\nissue migrate_disk Agent call on a new disk\n\n\norphan the old disk\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure VM to have new set of jobs\n\n\n\n\nissue apply Agent call\n\n\n\n\n\n\n\n\nstart processes on the VM and wait up to specified amount of time by the \nupdate_watch_time\n or \ncanary_watch_time\n\n\n\n\nissue start Agent call\n\n\nissue get_state Agent call until job state is running or times out\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate and propagate DNS records \n\u00b6\n\n\n\n\nCreate a new DNS records dataset and saves it to the blobstore\n\n\nissues sync_dns Agent call to \nall\n VMs (in all deployments)\n\n\nEach Agent downloads new DNS records dataset and updates \n/var/vcap/instance/dns/records.json",
            "title": "Deploying Step by Step"
        },
        {
            "location": "/deploying-step-by-step/#update-and-propagate-dns-records",
            "text": "Create a new DNS records dataset and saves it to the blobstore  issues sync_dns Agent call to  all  VMs (in all deployments)  Each Agent downloads new DNS records dataset and updates  /var/vcap/instance/dns/records.json",
            "title": "Update and propagate DNS records "
        },
        {
            "location": "/agent-cpi-interactions/",
            "text": "Here is an overview of the interactions between the CPI and the Agent on CloudStack as an example:\n\n\n\n\nThe CPI drives the IaaS and the Agent.\n\n\nThe Agent is a versatile process which configures the OS to leverage IaaS-provisioned resources (network interfaces, disks, etc.), and perform other BOSH tasks (job compilation, job instantiation, etc.)\n\n\nThe CPI asks the IaaS to instantiate VM template, VMs, volumes and possibly other constructs (floating IPs, security groups, connect LBs, etc.)\n\n\nThe Agent is initially driven by the CPI through the bosh-registry, and then by the Director through NATS-based messaging. The registry provides Director-side metadata to the Agent.\n\n\n\n\n\n\n\n\n\nThe following sequence diagram illustrates in more detail the CPI, agent, and registry interactions,\nin the case of the \nCloudStack CPI\n:\n\n\n\n\n\n\n\n\n\nAgent config file formats \n\u00b6\n\n\nThis section details the configuration and protocols supported by the Agent.\n\n\nVM Configuration Locations\n provides a list of Agent configuration files and their roles.\n\n\n\n\nagent.json\n file \n\u00b6\n\n\n/var/vcap/bosh/agent.json\n: Start up settings for the Agent that describe how to find bootstrap settings, and disable certain Agent functionality.\n\n\nThe loading agent.json file is described into \nconfig_test.go\n\n\nThe Platform part of the file is documented into \nLinuxOptions\n\n\nThe Infrastructure part of the file is documented into \nOptions\n and in particular:\n\n\n\n\n\n\nthe sources of configuration with \nSettingsOptions\n:\n\n\n\n\nCDROM\n\n\nConfigDrive\n\n\nHTTP\n\n\n\n\n\n\n\n\nSample \nagent.json\n which configures the agent to read from an HTTP metadata service at a custom URL:\n\n\n{\n\n  \n\"Platform\"\n:\n \n{\n\n    \n\"Linux\"\n:\n \n{\n\n      \n\"CreatePartitionIfNoEphemeralDisk\"\n:\n \ntrue\n,\n\n      \n\"DevicePathResolutionType\"\n:\n \n\"virtio\"\n\n    \n}\n\n  \n},\n\n  \n\"Infrastructure\"\n:\n \n{\n\n    \n\"NetworkingType\"\n:\n \n\"static\"\n,\n\n    \n\"Settings\"\n:\n \n{\n\n      \n\"Sources\"\n:\n \n[\n\n        \n{\n\n          \n\"Type\"\n:\n \n\"HTTP\"\n,\n\n          \n\"URI\"\n:\n \n\"http://10.234.228.142\"\n\n        \n}\n\n      \n],\n\n      \n\"UseServerName\"\n:\n \ntrue\n,\n\n      \n\"UseRegistry\"\n:\n \ntrue\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nSample \nagent.json\n which configures the agent ot read from a config drive. See \nreference sample config-drive json\n. See \nconfig drive integration test\n:\n\n\n{\n\n  \n\"Platform\"\n:\n \n{\n\n    \n\"Linux\"\n:\n \n{\n\n      \n\"UseDefaultTmpDir\"\n:\n \ntrue\n,\n\n      \n\"UsePreformattedPersistentDisk\"\n:\n \ntrue\n,\n\n      \n\"BindMountPersistentDisk\"\n:\n \nfalse\n,\n\n      \n\"DevicePathResolutionType\"\n:\n \n\"virtio\"\n\n    \n}\n\n  \n},\n\n  \n\"Infrastructure\"\n:\n \n{\n\n    \n\"Settings\"\n:\n \n{\n\n      \n\"Sources\"\n:\n \n[\n\n        \n{\n\n          \n\"Type\"\n:\n \n\"ConfigDrive\"\n,\n\n          \n\"DiskPaths\"\n:\n \n[\n\n            \n\"/dev/disk/by-label/CONFIG-2\"\n,\n\n            \n\"/dev/disk/by-label/config-2\"\n\n          \n],\n\n          \n\"MetaDataPath\"\n:\n \n\"ec2/latest/meta-data.json\"\n,\n\n          \n\"UserDataPath\"\n:\n \n\"ec2/latest/user-data.json\"\n\n        \n}\n\n      \n],\n\n\n      \n\"UseServerName\"\n:\n \ntrue\n,\n\n      \n\"UseRegistry\"\n:\n \ntrue\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nMetadata server \n\u00b6\n\n\nThe metadata server initially serves the registry URL and DNS server list.\n\n\nFollowing is a sample content of the user-data part of the HTTP metadata\n\n\n{\n\n  \n\"server\"\n:\n \n{\n\"name\"\n:\n \n\"vm-384sd4-r7re9e\"\n},\n\n  \n\"registry\"\n:\n \n{\n\"endpoint\"\n:\n \n\"http://192.168.0.255:8080/client/api\"\n},\n\n  \n\"dns\"\n:\n \n{\n\"nameserver\"\n:\n \n[\n\"10.234.50.180\"\n]}\n\n\n}\n\n\n\n\n\nThe supported format of the metadata server by the bosh-agent is documented in \nUserDataContentsType\n and \nhttp_metadata_service_test.go\n, along with the expected behavior of the bosh agent when reading this config.\n\n\n\n\nRegistry \n\u00b6\n\n\nThe registry provides bosh-side metadata to the bosh agent.\n\n\nFrom the \nWarden CPI documentation\n:\n\n\n\n\nThe registry is used by the CPI to pass data to the Agent. The registry is started on a server specified by registry properties.\n\n\nIf SSH tunnel options are provided, a reverse ssh tunnel is created from the MicroBOSH VM to the registry, making the registry available to the agent on remote machine.\n\n\n\n\nRegistry HTTP protocol \n\u00b6\n\n\nThe Agent expects to communicate with the bosh registry over a REST API documented in \napi_controller_spec.rb\n\n\nReference registry client and servers implementations are available in:\n\n\n\n\ngo-lang through \nfrodenas/bosh-registry\n\n\njava through \ncloudfoundry-community/bosh-cloudstack-cpi-core\n\n\n\n\nRegistry settings format \n\u00b6\n\n\nThe JSON payload of the settings stored in the registry, and its format supported by the Agent are documented in \nsettings_test.go\n\n\nSample registry content:\n\n\n{\n\n  \n\"agent_id\"\n:\n \n\"agent-xxxxxx\"\n,\n\n  \n\"blobstore\"\n:\n \n{\n\n    \n\"provider\"\n:\n \n\"local\"\n,\n\n    \n\"options\"\n:\n \n{\n\n      \n\"endpoint\"\n:\n \n\"http://xx.xx.xx.xx:25250\"\n,\n\n      \n\"password\"\n:\n \n\"password\"\n,\n\n      \n\"blobstore_path\"\n:\n \n\"/var/vcap/micro_bosh/data/cache\"\n,\n\n      \n\"user\"\n:\n \n\"agent\"\n\n    \n}\n\n  \n},\n\n  \n\"disks\"\n:\n \n{\n\n    \n\"system\"\n:\n \n\"/dev/xvda\"\n,\n\n    \n\"ephemeral\"\n:\n \n\"/dev/sdb\"\n,\n\n    \n\"persistent\"\n:\n \n{}\n\n  \n},\n\n  \n\"env\"\n:\n \n{},\n\n  \n\"networks\"\n:\n \n{\n\n    \n\"default\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"manual\"\n,\n\n      \n\"ip\"\n:\n \n\"10.234.228.158\"\n,\n\n      \n\"netmask\"\n:\n \n\"255.255.255.192\"\n,\n\n      \n\"cloud_properties\"\n:\n \n{\n\"name\"\n:\n \n\"3112 - preprod - back\"\n},\n\n      \n\"dns\"\n:\n \n[\n\n        \n\"10.234.50.180\"\n,\n\n        \n\"10.234.71.124\"\n\n      \n],\n\n      \n\"gateway\"\n:\n \n\"10.234.228.129\"\n,\n\n      \n\"mac\"\n:\n \nnull\n\n    \n}\n\n  \n},\n\n  \n\"ntp\"\n:\n \n[],\n\n  \n\"mbus\"\n:\n \n\"nats://nats:nats-password@yy.yy.yyy:4222\"\n,\n\n  \n\"vm\"\n:\n \n{\n\"name\"\n:\n \n\"vm-yyyy\"\n},\n\n  \n\"trusted_certs\"\n:\n \nnull\n\n\n}\n\n\n\n\n\n\n\nsettings.json file \n\u00b6\n\n\n/var/vcap/bosh/settings.json\n: Local copy of the bootstrap settings used by the Agent to configure network and system properties for the VM. They are refreshed every time Agent is restarted.\n\n\nThe \nsettings.json\n payload format is the same as the settings format initially returned by the registry.\n\n\n\n\nNext: \nBuilding a Stemcell\n\n\nPrevious: \nCPI API v1",
            "title": "Agent Interactions"
        },
        {
            "location": "/agent-cpi-interactions/#agent-config-file-formats",
            "text": "This section details the configuration and protocols supported by the Agent.  VM Configuration Locations  provides a list of Agent configuration files and their roles.",
            "title": "Agent config file formats "
        },
        {
            "location": "/agent-cpi-interactions/#agentjson-file",
            "text": "/var/vcap/bosh/agent.json : Start up settings for the Agent that describe how to find bootstrap settings, and disable certain Agent functionality.  The loading agent.json file is described into  config_test.go  The Platform part of the file is documented into  LinuxOptions  The Infrastructure part of the file is documented into  Options  and in particular:    the sources of configuration with  SettingsOptions :   CDROM  ConfigDrive  HTTP     Sample  agent.json  which configures the agent to read from an HTTP metadata service at a custom URL:  { \n   \"Platform\" :   { \n     \"Linux\" :   { \n       \"CreatePartitionIfNoEphemeralDisk\" :   true , \n       \"DevicePathResolutionType\" :   \"virtio\" \n     } \n   }, \n   \"Infrastructure\" :   { \n     \"NetworkingType\" :   \"static\" , \n     \"Settings\" :   { \n       \"Sources\" :   [ \n         { \n           \"Type\" :   \"HTTP\" , \n           \"URI\" :   \"http://10.234.228.142\" \n         } \n       ], \n       \"UseServerName\" :   true , \n       \"UseRegistry\" :   true \n     } \n   }  }   Sample  agent.json  which configures the agent ot read from a config drive. See  reference sample config-drive json . See  config drive integration test :  { \n   \"Platform\" :   { \n     \"Linux\" :   { \n       \"UseDefaultTmpDir\" :   true , \n       \"UsePreformattedPersistentDisk\" :   true , \n       \"BindMountPersistentDisk\" :   false , \n       \"DevicePathResolutionType\" :   \"virtio\" \n     } \n   }, \n   \"Infrastructure\" :   { \n     \"Settings\" :   { \n       \"Sources\" :   [ \n         { \n           \"Type\" :   \"ConfigDrive\" , \n           \"DiskPaths\" :   [ \n             \"/dev/disk/by-label/CONFIG-2\" , \n             \"/dev/disk/by-label/config-2\" \n           ], \n           \"MetaDataPath\" :   \"ec2/latest/meta-data.json\" , \n           \"UserDataPath\" :   \"ec2/latest/user-data.json\" \n         } \n       ], \n\n       \"UseServerName\" :   true , \n       \"UseRegistry\" :   true \n     } \n   }  }",
            "title": "agent.json file "
        },
        {
            "location": "/agent-cpi-interactions/#metadata-server",
            "text": "The metadata server initially serves the registry URL and DNS server list.  Following is a sample content of the user-data part of the HTTP metadata  { \n   \"server\" :   { \"name\" :   \"vm-384sd4-r7re9e\" }, \n   \"registry\" :   { \"endpoint\" :   \"http://192.168.0.255:8080/client/api\" }, \n   \"dns\" :   { \"nameserver\" :   [ \"10.234.50.180\" ]}  }   The supported format of the metadata server by the bosh-agent is documented in  UserDataContentsType  and  http_metadata_service_test.go , along with the expected behavior of the bosh agent when reading this config.",
            "title": "Metadata server "
        },
        {
            "location": "/agent-cpi-interactions/#registry",
            "text": "The registry provides bosh-side metadata to the bosh agent.  From the  Warden CPI documentation :   The registry is used by the CPI to pass data to the Agent. The registry is started on a server specified by registry properties.  If SSH tunnel options are provided, a reverse ssh tunnel is created from the MicroBOSH VM to the registry, making the registry available to the agent on remote machine.",
            "title": "Registry "
        },
        {
            "location": "/agent-cpi-interactions/#registry-http-protocol",
            "text": "The Agent expects to communicate with the bosh registry over a REST API documented in  api_controller_spec.rb  Reference registry client and servers implementations are available in:   go-lang through  frodenas/bosh-registry  java through  cloudfoundry-community/bosh-cloudstack-cpi-core",
            "title": "Registry HTTP protocol "
        },
        {
            "location": "/agent-cpi-interactions/#registry-settings-format",
            "text": "The JSON payload of the settings stored in the registry, and its format supported by the Agent are documented in  settings_test.go  Sample registry content:  { \n   \"agent_id\" :   \"agent-xxxxxx\" , \n   \"blobstore\" :   { \n     \"provider\" :   \"local\" , \n     \"options\" :   { \n       \"endpoint\" :   \"http://xx.xx.xx.xx:25250\" , \n       \"password\" :   \"password\" , \n       \"blobstore_path\" :   \"/var/vcap/micro_bosh/data/cache\" , \n       \"user\" :   \"agent\" \n     } \n   }, \n   \"disks\" :   { \n     \"system\" :   \"/dev/xvda\" , \n     \"ephemeral\" :   \"/dev/sdb\" , \n     \"persistent\" :   {} \n   }, \n   \"env\" :   {}, \n   \"networks\" :   { \n     \"default\" :   { \n       \"type\" :   \"manual\" , \n       \"ip\" :   \"10.234.228.158\" , \n       \"netmask\" :   \"255.255.255.192\" , \n       \"cloud_properties\" :   { \"name\" :   \"3112 - preprod - back\" }, \n       \"dns\" :   [ \n         \"10.234.50.180\" , \n         \"10.234.71.124\" \n       ], \n       \"gateway\" :   \"10.234.228.129\" , \n       \"mac\" :   null \n     } \n   }, \n   \"ntp\" :   [], \n   \"mbus\" :   \"nats://nats:nats-password@yy.yy.yyy:4222\" , \n   \"vm\" :   { \"name\" :   \"vm-yyyy\" }, \n   \"trusted_certs\" :   null  }",
            "title": "Registry settings format "
        },
        {
            "location": "/agent-cpi-interactions/#settingsjson-file",
            "text": "/var/vcap/bosh/settings.json : Local copy of the bootstrap settings used by the Agent to configure network and system properties for the VM. They are refreshed every time Agent is restarted.  The  settings.json  payload format is the same as the settings format initially returned by the registry.   Next:  Building a Stemcell  Previous:  CPI API v1",
            "title": "settings.json file "
        },
        {
            "location": "/vm-struct/",
            "text": "All managed VMs include:\n\n\n\n\nBOSH Agent\n\n\nMonit daemon\n\n\n/var/vcap/\n directory\n\n\n\n\nEach BOSH managed VM may be assigned a single copy of a deployment job to run. At that point VM is considered to be an instance of that deployment job -- it has a name and an index (e.g.  \nredis-server/0\n).\n\n\nWhen the assignment is made, the Agent will populate \n/var/vcap/\n directory with the release jobs specified in the deployment job definition in the deployment manifest. If selected release jobs depend on release packages those will also be downloaded and placed into the \n/var/vcap\n directory. For example given a following deployment job definition:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nredis-master\n\n  \ntemplates\n:\n\n  \n-\n \n{\nname\n:\n \nredis-server\n,\n \nrelease\n:\n \nredis\n}\n\n  \n-\n \n{\nname\n:\n \nsyslog-forwarder\n,\n \nrelease\n:\n \nsyslog\n}\n\n  \n...\n\n\n\n\n\nthe Agent will download two release jobs (listed in the templates property in the deployment job definition) into the following directories:\n\n\n\n\nredis-server into \n/var/vcap/jobs/redis-server\n\n\nsyslog-forwarder into \n/var/vcap/jobs/syslog-forwarder\n\n\n\n\nAssuming that the redis-server job depends on a redis package and the syslog-forwarder job depends on a syslog-forwarder package, the Agent will download two release packages into the following directories:\n\n\n\n\nredis into \n/var/vcap/packages/redis\n\n\nsyslog-forwarder into \n/var/vcap/packages/syslog-forwarder\n\n\n\n\n\n\nNext: \nVM Configuration Locations",
            "title": "Structure of a VM"
        },
        {
            "location": "/vm-config/",
            "text": "This topic describes important file system locations, configurations and other settings that are true for all VMs managed by BOSH.\n\n\nBOSH tries to encourage release authors to follow conventions listed below, so if you find inconsistencies or improper usage please report such problems to appropriate release authors.\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\n\n\n\n\n/tmp/\n: Global temporary directory is limited to 128MB. A well behaved release job that needs scratch space usually sets up its own temporary directory inside ephemeral data directory (e.g. \n/var/vcap/data/redis-server/tmp\n).\n\n\n\n\n\n\nvcap\n user: Pre-configured user that comes with the stemcells. Release jobs may run processes under that user. Default password is \nc1oudc0w\n.\n\n\n\n\n\n\n/etc/logrotate.d/vcap\n: Logrotate configuration for \n/var/vcap/sys/log/\n sub-directories managed by the Agent.\n\n\n\n\n\n\n\n\nRelease Job and Package Directories \n\u00b6\n\n\n\n\n\n\n/var/vcap/\n: VCAP [1] directory contains majority of the configuration settings and associated assets when VM deployment job instance is assigned to the VM.\n\n\n\n\n\n\n/var/vcap/packages/\n: Contains enabled release packages for the assigned deployment job. The Agent is responsible for managing which packages are enabled or disabled on the VM.\n\n\n\n\n\n\n/var/vcap/jobs/\n: Contains evaluated release jobs for the assigned deployment job. The Agent is responsible for managing which jobs are enabled or disabled on the VM.\n\n\n\n\n\n\n/var/vcap/jobs/<name>/bin/\n: Conventional location for the release job to keep wrapper executables. ctl script(s) invoked by Monit are placed here (e.g. \n/var/vcap/jobs/redis-server/bin/ctl\n).\n\n\n\n\n\n\n/var/vcap/jobs/<name>/config/\n: Conventional location for the release job configuration files (e.g. \n/var/vcap/jobs/redis-server/config/redis.conf\n).\n\n\n\n\n\n\n/var/vcap/jobs/<name>/monit\n: Final monit file for that release job.\n\n\n\n\n\n\n\n\n\n\n\n\nStorage Directories \n\u00b6\n\n\n\n\n\n\n/var/vcap/data/\n: Directory that is used by the release jobs to keep \nephemeral\n data. Each release job usually creates a sub-folder with its name for namespacing (e.g. \nredis-server\n will place data into \n/var/vcap/data/redis-server\n).\n\n\n\n\n\n\n/var/vcap/store/\n: Directory that is used by the release jobs to keep \npersistent\n data. Each release job usually creates a sub-folder with its name for namespacing (e.g. \nredis-server\n will place data into \n/var/vcap/store/redis-server\n).\n\n\n\n\n\n\n/var/vcap/sys/run/\n: Directory that is used by the release jobs to keep miscellaneous ephemeral data about  currently running processes, for example, pid and lock files. Each release job usually creates a sub-folder with its name for namespacing (e.g. \nredis-server\n will place data into \n/var/vcap/sys/run/redis-server\n).\n\n\n\n\n\n\n/var/vcap/sys/log/\n: Directory that is used by the release jobs to keep logs. Each release job usually creates a sub-folder with its name for namespacing (e.g. \nredis-server\n will place data into \n/var/vcap/sys/log/redis-server\n). Files in this directory are log rotated on a specific schedule configure by the Agent.\n\n\n\n\n\n\n\n\nAgent Configuration \n\u00b6\n\n\nIt's discouraged to modify or rely on the contents of this directory.\n\n\n\n\n\n\n/var/vcap/bosh/\n: Directory used by the Agent to keep its internal state.\n\n\n\n\n\n\n/var/vcap/bosh/agent.json\n: Start up settings for the Agent that describe how to find bootstrap settings, and disable certain Agent functionality.\n\n\n\n\n\n\n/var/vcap/bosh/settings.json\n: Local copy of the bootstrap settings used by the Agent to configure network, system properties for the VM. They are refreshed every time Agent is restarted.\n\n\n\n\n\n\n/var/vcap/bosh/spec.json\n: Deployment job settings used by the Agent to configure release jobs and packages for the VM. This file also includes name and index for the deployment job associated with this VM.\n\n\n\n\n\n\n/var/vcap/bosh/log/current\n: Current Agent log. Agent's logs are logrotated and archives are kept in \n/var/vcap/bosh/log/\n directory.\n\n\n\n\n\n\n/var/vcap/bosh/etc/ntpserver\n: File with a list of NTP servers configured by the Agent. This file is used by \n/var/vcap/bosh/bin/ntpdate\n to keep time in sync.\n\n\n\n\n\n\n\n\nMonit Configuration \n\u00b6\n\n\nIt's discouraged to modify or rely on the contents of this directory.\n\n\n\n\n\n\n/var/vcap/monit/job\n: Directory that keeps current Monit configuration files. The Agent is responsible for updating files in this directory when deployment job is updated.\n\n\n\n\n\n\n/var/vcap/monit/monit.log\n: Monit activity log. Includes information about starts, stops, restarts, etc. of release job processes monitored by Monit.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nStructure of a BOSH VM\n\n\n[1] VCAP stands for VMware Cloud Application Platform.",
            "title": "Configuration Locations"
        },
        {
            "location": "/vm-config/#global-configuration",
            "text": "/tmp/ : Global temporary directory is limited to 128MB. A well behaved release job that needs scratch space usually sets up its own temporary directory inside ephemeral data directory (e.g.  /var/vcap/data/redis-server/tmp ).    vcap  user: Pre-configured user that comes with the stemcells. Release jobs may run processes under that user. Default password is  c1oudc0w .    /etc/logrotate.d/vcap : Logrotate configuration for  /var/vcap/sys/log/  sub-directories managed by the Agent.",
            "title": "Global Configuration "
        },
        {
            "location": "/vm-config/#release-job-and-package-directories",
            "text": "/var/vcap/ : VCAP [1] directory contains majority of the configuration settings and associated assets when VM deployment job instance is assigned to the VM.    /var/vcap/packages/ : Contains enabled release packages for the assigned deployment job. The Agent is responsible for managing which packages are enabled or disabled on the VM.    /var/vcap/jobs/ : Contains evaluated release jobs for the assigned deployment job. The Agent is responsible for managing which jobs are enabled or disabled on the VM.    /var/vcap/jobs/<name>/bin/ : Conventional location for the release job to keep wrapper executables. ctl script(s) invoked by Monit are placed here (e.g.  /var/vcap/jobs/redis-server/bin/ctl ).    /var/vcap/jobs/<name>/config/ : Conventional location for the release job configuration files (e.g.  /var/vcap/jobs/redis-server/config/redis.conf ).    /var/vcap/jobs/<name>/monit : Final monit file for that release job.",
            "title": "Release Job and Package Directories "
        },
        {
            "location": "/vm-config/#storage-directories",
            "text": "/var/vcap/data/ : Directory that is used by the release jobs to keep  ephemeral  data. Each release job usually creates a sub-folder with its name for namespacing (e.g.  redis-server  will place data into  /var/vcap/data/redis-server ).    /var/vcap/store/ : Directory that is used by the release jobs to keep  persistent  data. Each release job usually creates a sub-folder with its name for namespacing (e.g.  redis-server  will place data into  /var/vcap/store/redis-server ).    /var/vcap/sys/run/ : Directory that is used by the release jobs to keep miscellaneous ephemeral data about  currently running processes, for example, pid and lock files. Each release job usually creates a sub-folder with its name for namespacing (e.g.  redis-server  will place data into  /var/vcap/sys/run/redis-server ).    /var/vcap/sys/log/ : Directory that is used by the release jobs to keep logs. Each release job usually creates a sub-folder with its name for namespacing (e.g.  redis-server  will place data into  /var/vcap/sys/log/redis-server ). Files in this directory are log rotated on a specific schedule configure by the Agent.",
            "title": "Storage Directories "
        },
        {
            "location": "/vm-config/#agent-configuration",
            "text": "It's discouraged to modify or rely on the contents of this directory.    /var/vcap/bosh/ : Directory used by the Agent to keep its internal state.    /var/vcap/bosh/agent.json : Start up settings for the Agent that describe how to find bootstrap settings, and disable certain Agent functionality.    /var/vcap/bosh/settings.json : Local copy of the bootstrap settings used by the Agent to configure network, system properties for the VM. They are refreshed every time Agent is restarted.    /var/vcap/bosh/spec.json : Deployment job settings used by the Agent to configure release jobs and packages for the VM. This file also includes name and index for the deployment job associated with this VM.    /var/vcap/bosh/log/current : Current Agent log. Agent's logs are logrotated and archives are kept in  /var/vcap/bosh/log/  directory.    /var/vcap/bosh/etc/ntpserver : File with a list of NTP servers configured by the Agent. This file is used by  /var/vcap/bosh/bin/ntpdate  to keep time in sync.",
            "title": "Agent Configuration "
        },
        {
            "location": "/vm-config/#monit-configuration",
            "text": "It's discouraged to modify or rely on the contents of this directory.    /var/vcap/monit/job : Directory that keeps current Monit configuration files. The Agent is responsible for updating files in this directory when deployment job is updated.    /var/vcap/monit/monit.log : Monit activity log. Includes information about starts, stops, restarts, etc. of release job processes monitored by Monit.     Back to Table of Contents  Previous:  Structure of a BOSH VM  [1] VCAP stands for VMware Cloud Application Platform.",
            "title": "Monit Configuration "
        },
        {
            "location": "/job-logs/",
            "text": "This topic describes different types of logs and how to access them.\n\n\nVM logs \n\u00b6\n\n\nYou can access logs from any VM:\n\n\n\n\nvia \nbosh ssh\n command\n to SSH into a VM and look at the log files\n\n\nvia \nbosh logs\n command\n to download logs from the VM\n\n\n\n\nThe following sections describe different types of logs found on each BOSH managed VM.\n\n\n\n\nJob logs \n\u00b6\n\n\nRelease jobs on VMs produce logs throughout different lifecycle events. Release authors are strongly encouraged to place release job logs into \n/var/vcap/sys/log/<release_job_name>/*.log\n, providing a consistent place for the operator to find them.\n\n\nFor example \nredis-server\n release job will create two log files:\n\n\n\n\n/var/vcap/sys/log/redis-server/redis-server.stdout.log\n\n\n/var/vcap/sys/log/redis-server/redis-server.stderr.log\n\n\n\n\nTo download logs from all release jobs on a specific VM, run \nbosh logs <job_name> <index>\n.\n\n\nSee additonal information about following job lifecycle events' logs:\n\n\n\n\npre-start script logs\n\n\ndrain script logs\n\n\n\n\n\n\nErrand logs \n\u00b6\n\n\nUnlike regular job logs BOSH does not automatically redirect errand logs to \n/var/vcap/sys/log/*\n directory, though we are planning to do so in future.\n\n\nErrand's stdout and stderr output will be shown by the CLI when it's smaller than 1MB. If you expect errand to generate output larger than 1MB, currently it needs to be redirected to a file (by convention to \n/var/vcap/sys/log/<job_name>/stdout.log\n) from the errand script and then downloaded, or error will be returned upon errand completion.\n\n\nTo save output from an errand VM:\n\n\n\n\nIn the errand run script, redirect the output to a log.\n\n\n\n\nUsing the CLI, run \nbosh run errand X\n with the \n--download-logs\n option to download the logs.\n\n\nBy default, the CLI downloads the logs to your present working directory. Use the \n--logs-dir destination_directory\n option to change this directory.\n\n\n\n\n\n\n$ bosh run errand smoke-tests --download-logs --logs-dir ~/workspace/smoke-tests-logs\n\n\n\n\nNote: By default upon errand completion errand VM is deleted, so you cannot access logs saved to disk by the errand. You can use \n--keep-alive\n flag when running an errand to keep the VM with its logs.\n\n\n\n\n\nMonit logs \n\u00b6\n\n\nThe Agent uses Monit to start, restart, and stop release job processes as specified by the release jobs. Monit detects errors and outputs often useful information to its log. Use \ntail\n to examine the \nmonit.log\n on a VM:\n\n\n$ sudo tail -f -n \n200\n /var/vcap/monit/monit.log\n\n\n\n\n\n\nAgent logs \n\u00b6\n\n\nAgent logs contain configuration and runtime information from the Agent running on a VM. Review these logs if the Director sees VM as unresponsive or the Director fails to contact it during its creation.\n\n\nThe Agent stores logs in \n/var/vcap/bosh/log/\n and outputs most recent content to \n/var/vcap/bosh/log/current\n.\n\n\n$ sudo tail -f -n \n200\n /var/vcap/bosh/log/current\n\n\n\n\nNote: Agent logs are only accessible to the root user.\n\n\n\n\n\nLog rotation \n\u00b6\n\n\nBOSH log rotates release job logs with the \nLogrotate\n log file management utility. Logrotate is configured by the Agent to act on all \n.log\n files in the \n/var/vcap/sys/log/\n, \n/var/vcap/sys/log/*/\n, and \n/var/vcap/sys/log/*/*/\n directories.\n\n\nFollowing non-configurable settings are used:\n\n\n\n\nmissingok\n: Skip missing log files and do not generate an error message\n\n\nrotate 7\n: Keep seven log files at a time\n\n\ncompress\n: Compress old log files with gzip\n\n\ndelaycompress\n: Postpone compression of log files until the next rotation cycle\n\n\ncopytruncate\n: Copy log files, then truncate in place instead of creating new files\n\n\nsize 50M\n: Rotate log files when they exceed 50 MB in size\n\n\n\n\nCron runs logrotate script every hour.\n\n\n\n\nSyslog configuration \n\u00b6\n\n\nRecommended way to configure syslog forwarding on all or some VMs is to use \nsyslog_forwarder\n job from \nsyslog-release\n as an addon\n.\n\n\n\n\nDirector task logs \n\u00b6\n\n\nWhen you run a \nCLI\n command, the Director stores all activities for the specific command in a task log. Review these logs when you experience an issue with a command.\n\n\nTo access Director task logs:\n\n\n\n\nRun \nbosh tasks recent\n to find the task number of the command.\n\n\nRun \nbosh task <task_number>\n.",
            "title": "Using Logs"
        },
        {
            "location": "/job-logs/#vm-logs",
            "text": "You can access logs from any VM:   via  bosh ssh  command  to SSH into a VM and look at the log files  via  bosh logs  command  to download logs from the VM   The following sections describe different types of logs found on each BOSH managed VM.",
            "title": "VM logs "
        },
        {
            "location": "/job-logs/#job-logs",
            "text": "Release jobs on VMs produce logs throughout different lifecycle events. Release authors are strongly encouraged to place release job logs into  /var/vcap/sys/log/<release_job_name>/*.log , providing a consistent place for the operator to find them.  For example  redis-server  release job will create two log files:   /var/vcap/sys/log/redis-server/redis-server.stdout.log  /var/vcap/sys/log/redis-server/redis-server.stderr.log   To download logs from all release jobs on a specific VM, run  bosh logs <job_name> <index> .  See additonal information about following job lifecycle events' logs:   pre-start script logs  drain script logs",
            "title": "Job logs "
        },
        {
            "location": "/job-logs/#errand-logs",
            "text": "Unlike regular job logs BOSH does not automatically redirect errand logs to  /var/vcap/sys/log/*  directory, though we are planning to do so in future.  Errand's stdout and stderr output will be shown by the CLI when it's smaller than 1MB. If you expect errand to generate output larger than 1MB, currently it needs to be redirected to a file (by convention to  /var/vcap/sys/log/<job_name>/stdout.log ) from the errand script and then downloaded, or error will be returned upon errand completion.  To save output from an errand VM:   In the errand run script, redirect the output to a log.   Using the CLI, run  bosh run errand X  with the  --download-logs  option to download the logs.  By default, the CLI downloads the logs to your present working directory. Use the  --logs-dir destination_directory  option to change this directory.    $ bosh run errand smoke-tests --download-logs --logs-dir ~/workspace/smoke-tests-logs  Note: By default upon errand completion errand VM is deleted, so you cannot access logs saved to disk by the errand. You can use  --keep-alive  flag when running an errand to keep the VM with its logs.",
            "title": "Errand logs "
        },
        {
            "location": "/job-logs/#monit-logs",
            "text": "The Agent uses Monit to start, restart, and stop release job processes as specified by the release jobs. Monit detects errors and outputs often useful information to its log. Use  tail  to examine the  monit.log  on a VM:  $ sudo tail -f -n  200  /var/vcap/monit/monit.log",
            "title": "Monit logs "
        },
        {
            "location": "/job-logs/#agent-logs",
            "text": "Agent logs contain configuration and runtime information from the Agent running on a VM. Review these logs if the Director sees VM as unresponsive or the Director fails to contact it during its creation.  The Agent stores logs in  /var/vcap/bosh/log/  and outputs most recent content to  /var/vcap/bosh/log/current .  $ sudo tail -f -n  200  /var/vcap/bosh/log/current  Note: Agent logs are only accessible to the root user.",
            "title": "Agent logs "
        },
        {
            "location": "/job-logs/#log-rotation",
            "text": "BOSH log rotates release job logs with the  Logrotate  log file management utility. Logrotate is configured by the Agent to act on all  .log  files in the  /var/vcap/sys/log/ ,  /var/vcap/sys/log/*/ , and  /var/vcap/sys/log/*/*/  directories.  Following non-configurable settings are used:   missingok : Skip missing log files and do not generate an error message  rotate 7 : Keep seven log files at a time  compress : Compress old log files with gzip  delaycompress : Postpone compression of log files until the next rotation cycle  copytruncate : Copy log files, then truncate in place instead of creating new files  size 50M : Rotate log files when they exceed 50 MB in size   Cron runs logrotate script every hour.",
            "title": "Log rotation "
        },
        {
            "location": "/job-logs/#syslog-configuration",
            "text": "Recommended way to configure syslog forwarding on all or some VMs is to use  syslog_forwarder  job from  syslog-release  as an addon .",
            "title": "Syslog configuration "
        },
        {
            "location": "/job-logs/#director-task-logs",
            "text": "When you run a  CLI  command, the Director stores all activities for the specific command in a task log. Review these logs when you experience an issue with a command.  To access Director task logs:   Run  bosh tasks recent  to find the task number of the command.  Run  bosh task <task_number> .",
            "title": "Director task logs "
        },
        {
            "location": "/instance-metadata/",
            "text": "Note: This feature is available with bosh-release v255.4+ and on 3213+ stemcell series.\n\n\n\nIt's common for software to need to know where it was deployed so that it can make application level decisions or propagate location information in logs and metrics. Director instance specific information in multiple ways:\n\n\nVia ERB templates \n\u00b6\n\n\nUse \nspec\n variable\n in ERB templates to get access to AZ, deployment name, ID, etc.\n\n\nVia filesystem \n\u00b6\n\n\nAccessing information over filesystem might be useful when building core libraries so that explicit configuration is not required. Each VM has a \n/var/vcap/instance\n directory that contains following files:\n\n\nvcap@7e87e912-35cc-4b43-9645-5a7d7d6f2caa:~$ ls -la /var/vcap/instance/\ntotal \n24\n\ndrwxr-xr-x  \n2\n root root \n4096\n Mar \n17\n \n00\n:06 .\ndrwxr-xr-x \n11\n root root \n4096\n Mar \n17\n \n00\n:16 ..\n-rw-r--r--  \n1\n root root    \n2\n Mar \n17\n \n00\n:07 az\n-rw-r--r--  \n1\n root root   \n10\n Mar \n17\n \n00\n:07 deployment\n-rw-r--r--  \n1\n root root   \n36\n Mar \n17\n \n00\n:07 id\n-rw-r--r--  \n1\n root root    \n3\n Mar \n17\n \n00\n:07 name\n\n\n\n\nExample values:\n\n\n\n\nAZ: \nz1\n\n\nDeployment: \nredis\n\n\nID: \nfdfad8ca-a213-4a1c-b1f6-c53f86bb896a\n\n\nName: \nredis",
            "title": "Instance Metadata"
        },
        {
            "location": "/instance-metadata/#via-erb-templates",
            "text": "Use  spec  variable  in ERB templates to get access to AZ, deployment name, ID, etc.",
            "title": "Via ERB templates "
        },
        {
            "location": "/instance-metadata/#via-filesystem",
            "text": "Accessing information over filesystem might be useful when building core libraries so that explicit configuration is not required. Each VM has a  /var/vcap/instance  directory that contains following files:  vcap@7e87e912-35cc-4b43-9645-5a7d7d6f2caa:~$ ls -la /var/vcap/instance/\ntotal  24 \ndrwxr-xr-x   2  root root  4096  Mar  17   00 :06 .\ndrwxr-xr-x  11  root root  4096  Mar  17   00 :16 ..\n-rw-r--r--   1  root root     2  Mar  17   00 :07 az\n-rw-r--r--   1  root root    10  Mar  17   00 :07 deployment\n-rw-r--r--   1  root root    36  Mar  17   00 :07 id\n-rw-r--r--   1  root root     3  Mar  17   00 :07 name  Example values:   AZ:  z1  Deployment:  redis  ID:  fdfad8ca-a213-4a1c-b1f6-c53f86bb896a  Name:  redis",
            "title": "Via filesystem "
        },
        {
            "location": "/build-cpi/",
            "text": "This topic describes how to build a CPI.\n\n\nDistribution \n\u00b6\n\n\nCPIs are distributed as regular releases, typically with a release job called \ncpi\n and a few packages that provide compilation/runtime environment for that job if necessary (e.g. \nbosh-aws-cpi-release\n includes Ruby and \nbosh-warden-cpi-release\n includes golang). To qualify to be a CPI release, it must include a release job that has \nbin/cpi\n executable.\n\n\nBoth \nbosh create-env\n command and the Director expect to be configured with a CPI release to function properly. In the case of \nbosh create-env\n command, specified CPI release is unpacked and installed on the machine running the command. For the Director, CPI release job is colocated on the same VM, so that the director release job can access it.\n\n\nCPIs written in Ruby:\n\n\n\n\nAWS CPI release\n\n\nAzure CPI release\n\n\nOpenStack CPI release\n\n\nvSphere CPI release\n\n\nvCloud CPI release\n\n\n\n\nCPIs written in golang:\n\n\n\n\nWarden CPI release\n\n\n\n\n\n\nRPC \n\u00b6\n\n\nSince CPI is just an executable, following takes place for each CPI method call:\n\n\n\n\nCallee starts a new CPI executable OS process (shells out)\n\n\nCallee sends a \nsingle JSON request\n over STDIN\n\n\nCPI optionally starts logging debugging information to STDERR\n\n\nCPI responds with a \nsingle JSON response\n over STDOUT\n\n\nCPI executable exits\n\n\nCallee parses and interprets JSON response ignoring process exit code\n\n\n\n\nAs a reference the Director's implementation is in \nbosh_cpi's external_cpi.rb\n and the BOSH CLI implements it in \ncpi_cmd_runner.go\n.\n\n\nRequest \n\u00b6\n\n\n\n\nmethod\n [String]: Name of the CPI method. Example: \ncreate_vm\n.\n\n\narguments\n [Array]: Array of arguments that are specific to the CPI method.\n\n\ncontext\n [Hash]: Additional information provided as a context of this execution. Specified for backwards compatibility and should be ignored.\n\n\n\n\nExample:\n\n\n{\n\n    \n\"method\"\n:\n \n\"delete_disk\"\n,\n\n    \n\"arguments\"\n:\n \n[\n\"vol-1b7fb8fd\"\n],\n\n    \n\"context\"\n:\n \n{\n \n\"director_uuid\"\n:\n\"fefb87c8-38d1-46a5-4552-9749d6b1195c\"\n \n}\n\n\n}\n\n\n\n\n\nResponse \n\u00b6\n\n\n\n\nresult\n [Null or simple values]: Single return value. It must be null if \nerror\n is returned.\n\n\nerror\n [Null or hash]: Occurred error. It must be null if \nresult\n is returned.\n\n\ntype\n [String]: Type of the error.\n\n\nmessage\n [String]: Description of the error.\n\n\nok_to_retry\n [Boolean]: Indicates whether callee should try calling the method again without changing any of the arguments.\n\n\n\n\n\n\nlog\n [String]: Additional information that may be useful for auditing, debugging and understanding what actions CPI took while executing a method. Typically includes info and debug logs, error backtraces.\n\n\n\n\nExample successful response from \ncreate_vm\n CPI call:\n\n\n{\n\n    \n\"result\"\n:\n \n\"i-384959\"\n,\n\n    \n\"error\"\n:\n \nnull\n,\n\n    \n\"log\"\n:\n \n\"\"\n\n\n}\n\n\n\n\n\nExample error response from \ncreate_vm\n CPI call:\n\n\n{\n\n    \n\"result\"\n:\n \nnull\n,\n\n    \n\"error\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"Bosh::Clouds::CloudError\"\n,\n\n        \n\"message\"\n:\n \n\"Flavor\n \n`m1.2xlarge'\n \nnot\n \nfound\"\n,\n\n        \n\"ok_to_retry\"\n:\n \nfalse\n\n    \n},\n\n    \n\"log\"\n:\n \n\"Rescued\n \nerror:\n \n'Flavor\n \n`m1.2xlarge'\n \nnot\n \nfound'.\n \nBacktrace:\n \n~/.bosh_init/ins...\"\n\n\n}\n\n\n\n\n\nBOSH team maintains several CPIs written in Ruby. We currently take advantage of the \nbosh_cpi\n gem which provides \nBosh::Cpi::Cli\n class that deserialized requests and serializes responses. If you are planning to write a CPI in Ruby, we recommend to take advantage of this gem.\n\n\n\n\nTesting \n\u00b6\n\n\nThere are two test suites each CPI is expected to pass before it's considered to be production-ready:\n\n\n\n\nits own \nCPI Lifecycle Tests\n which should provide integration level coverage for each CPI method\n\n\nshared \nBOSH Acceptance Tests (BATS)\n (provided by the BOSH team) which verify high level Director behavior with the CPI activated\n\n\n\n\n\n\nConcurrency \n\u00b6\n\n\nThe CPI is expected to handle multiple method calls concurrently (and in parallel) with a promise that arguments represent different IaaS resources. For example, multiple \ncreate_vm\n CPI method calls may be issued that all use the same stemcell cloud ID; however, \nattach_disk\n CPI method will never be called with the same VM cloud ID concurrently.\n\n\nNote: Since each CPI method call is a separate OS process, simple locking techniques (Ruby's \nMutex.new\n for example) will not work.\n\n\n\n\n\nRate Limiting \n\u00b6\n\n\nMost CPIs have to deal with IaaS APIs that rate limit (e.g. OpenStack, AWS APIs). Currently it's the responsibility of the CPI to handle rate-limiting errors, properly catch them, wait and retry actions that were interrupted. Given that there is no timeout around how long a CPI method can run, it's all right to wait as long as necessary to resume making API calls. Though it's suggested to log such information.\n\n\n\n\nDebugging \n\u00b6\n\n\nIt usually useful to get a detailed log of CPI requests and responses from the callee. To get a full debug log from \nbosh create-env\n command set \nBOSH_LOG_LEVEL=debug\n environment variable.\n\n\nWhen working with the Director you can find similar debug logs via \nbosh task X --debug\n command.\n\n\n\n\nNext: \nCPI API v1",
            "title": "Building a CPI"
        },
        {
            "location": "/build-cpi/#distribution",
            "text": "CPIs are distributed as regular releases, typically with a release job called  cpi  and a few packages that provide compilation/runtime environment for that job if necessary (e.g.  bosh-aws-cpi-release  includes Ruby and  bosh-warden-cpi-release  includes golang). To qualify to be a CPI release, it must include a release job that has  bin/cpi  executable.  Both  bosh create-env  command and the Director expect to be configured with a CPI release to function properly. In the case of  bosh create-env  command, specified CPI release is unpacked and installed on the machine running the command. For the Director, CPI release job is colocated on the same VM, so that the director release job can access it.  CPIs written in Ruby:   AWS CPI release  Azure CPI release  OpenStack CPI release  vSphere CPI release  vCloud CPI release   CPIs written in golang:   Warden CPI release",
            "title": "Distribution "
        },
        {
            "location": "/build-cpi/#rpc",
            "text": "Since CPI is just an executable, following takes place for each CPI method call:   Callee starts a new CPI executable OS process (shells out)  Callee sends a  single JSON request  over STDIN  CPI optionally starts logging debugging information to STDERR  CPI responds with a  single JSON response  over STDOUT  CPI executable exits  Callee parses and interprets JSON response ignoring process exit code   As a reference the Director's implementation is in  bosh_cpi's external_cpi.rb  and the BOSH CLI implements it in  cpi_cmd_runner.go .",
            "title": "RPC "
        },
        {
            "location": "/build-cpi/#request",
            "text": "method  [String]: Name of the CPI method. Example:  create_vm .  arguments  [Array]: Array of arguments that are specific to the CPI method.  context  [Hash]: Additional information provided as a context of this execution. Specified for backwards compatibility and should be ignored.   Example:  { \n     \"method\" :   \"delete_disk\" , \n     \"arguments\" :   [ \"vol-1b7fb8fd\" ], \n     \"context\" :   {   \"director_uuid\" : \"fefb87c8-38d1-46a5-4552-9749d6b1195c\"   }  }",
            "title": "Request "
        },
        {
            "location": "/build-cpi/#response",
            "text": "result  [Null or simple values]: Single return value. It must be null if  error  is returned.  error  [Null or hash]: Occurred error. It must be null if  result  is returned.  type  [String]: Type of the error.  message  [String]: Description of the error.  ok_to_retry  [Boolean]: Indicates whether callee should try calling the method again without changing any of the arguments.    log  [String]: Additional information that may be useful for auditing, debugging and understanding what actions CPI took while executing a method. Typically includes info and debug logs, error backtraces.   Example successful response from  create_vm  CPI call:  { \n     \"result\" :   \"i-384959\" , \n     \"error\" :   null , \n     \"log\" :   \"\"  }   Example error response from  create_vm  CPI call:  { \n     \"result\" :   null , \n     \"error\" :   { \n         \"type\" :   \"Bosh::Clouds::CloudError\" , \n         \"message\" :   \"Flavor   `m1.2xlarge'   not   found\" , \n         \"ok_to_retry\" :   false \n     }, \n     \"log\" :   \"Rescued   error:   'Flavor   `m1.2xlarge'   not   found'.   Backtrace:   ~/.bosh_init/ins...\"  }   BOSH team maintains several CPIs written in Ruby. We currently take advantage of the  bosh_cpi  gem which provides  Bosh::Cpi::Cli  class that deserialized requests and serializes responses. If you are planning to write a CPI in Ruby, we recommend to take advantage of this gem.",
            "title": "Response "
        },
        {
            "location": "/build-cpi/#testing",
            "text": "There are two test suites each CPI is expected to pass before it's considered to be production-ready:   its own  CPI Lifecycle Tests  which should provide integration level coverage for each CPI method  shared  BOSH Acceptance Tests (BATS)  (provided by the BOSH team) which verify high level Director behavior with the CPI activated",
            "title": "Testing "
        },
        {
            "location": "/build-cpi/#concurrency",
            "text": "The CPI is expected to handle multiple method calls concurrently (and in parallel) with a promise that arguments represent different IaaS resources. For example, multiple  create_vm  CPI method calls may be issued that all use the same stemcell cloud ID; however,  attach_disk  CPI method will never be called with the same VM cloud ID concurrently.  Note: Since each CPI method call is a separate OS process, simple locking techniques (Ruby's  Mutex.new  for example) will not work.",
            "title": "Concurrency "
        },
        {
            "location": "/build-cpi/#rate-limiting",
            "text": "Most CPIs have to deal with IaaS APIs that rate limit (e.g. OpenStack, AWS APIs). Currently it's the responsibility of the CPI to handle rate-limiting errors, properly catch them, wait and retry actions that were interrupted. Given that there is no timeout around how long a CPI method can run, it's all right to wait as long as necessary to resume making API calls. Though it's suggested to log such information.",
            "title": "Rate Limiting "
        },
        {
            "location": "/build-cpi/#debugging",
            "text": "It usually useful to get a detailed log of CPI requests and responses from the callee. To get a full debug log from  bosh create-env  command set  BOSH_LOG_LEVEL=debug  environment variable.  When working with the Director you can find similar debug logs via  bosh task X --debug  command.   Next:  CPI API v1",
            "title": "Debugging "
        },
        {
            "location": "/build-stemcell/",
            "text": "(See \nWhat is a Stemcell?\n for an introduction to stemcells.)\n\n\nTo build a stemcell tarball for a supported IaaS-OS combination follow instructions in the \nbosh-linux-stemcell-builder's README\n.\n\n\nStemcell tarballs are currently specific to an IaaS-OS/CPI because they may:\n\n\n\n\ninclude custom Agent configuration (e.g. \nOpenStack's Agent configuration\n)\n\n\ninclude custom OS packages/configuration (e.g. \nOpenStack's OS customizations\n)\n\n\nbe packaged into a custom image format (qcow, vmdk, etc.)\n\n\n\n\nIn the future, BOSH team will investigate how to best consolidate stemcells into a single OS image. In the meantime, if you're developing a CPI for a new IaaS, you may consider reusing one of the officially generated stemcells, or making changes to the following projects:\n\n\n\n\nbosh-linux-stemcell-builder\n\n\nbosh-agent\n\n\n\n\n\n\nTarball Structure \n\u00b6\n\n\nNote that tarball structure is subject to change without notice.\n\n\n\n$ tar tvf light-bosh-stemcell-3033-aws-xen-hvm-ubuntu-trusty-go_agent.tgz\n\n-rw-rw-r--  \n0\n ubuntu ubuntu      \n0\n Aug  \n4\n \n09\n:45 image\n-rw-rw-r--  \n0\n ubuntu ubuntu    \n710\n Aug  \n4\n \n10\n:06 stemcell.MF\n-rw-r--r--  \n0\n ubuntu ubuntu  \n50594\n Aug  \n4\n \n09\n:23 packages.txt\n-rw-r--r--  \n0\n ubuntu ubuntu  \n12543\n Aug  \n4\n \n09\n:22 dev_tools_file_list.txt\n\n\n\n\n\n\nimage\n: OS image in a format (raw, qcow, ova, etc.) understood by the CPI/IaaS.\n\n\nstemcell.MF\n: YAML file with stemcell metadata.\n\n\npackages.txt\n: Text file that includes list of packages installed. (Used to be included as \nstemcell_dpkg_l.txt\n)\n\n\ndev_tools_file_list.txt\n: Text file that includes list of files removed by the agent if Agent's \nremove_dev_tools\n feature is enabled.\n\n\n\n\nMetadata \n\u00b6\n\n\nNote that content of `stemcell.MF` is subject to change without notice.\n\n\n\n\n\nname\n [String, required]: A unique name used to identify stemcell series.\n\n\noperating_system\n [String, required]: Operating system in the stemcell. Example: \nubuntu-trusty\n.\n\n\nversion\n [String, required]: Version of the stemcell. Example: \n3033\n.\n\n\nsha1\n [String, required]: The SHA1 of the image file included in the stemcell tarball.\n\n\nbosh_protocol\n [Integer, optional]: Deprecated.\n\n\ncloud_properties\n [Hash, required]: Describes any IaaS-specific properties needed to import OS image. These properties will be passed in to the \ncreate_stemcell\n CPI call\n.\n\n\n\n\nName, operating system and version values will be visible via \nbosh stemcells\n command once a stemcell is imported into the Director.\n\n\nExample:\n\n\n$ tar -Oxzf light-bosh-stemcell-3033-aws-xen-hvm-ubuntu-trusty-go_agent.tgz stemcell.MF\n\n\n\n\n---\n\n\nname\n:\n \nbosh-aws-xen-hvm-ubuntu-trusty-go_agent\n\n\noperating_system\n:\n \nubuntu-trusty\n\n\nversion\n:\n \n'3033'\n\n\nsha1\n:\n \nc13273b00b762c5aa29240ea62e1b9b5a03ae02c\n\n\nbosh_protocol\n:\n \n1\n\n\ncloud_properties\n:\n\n  \nname\n:\n \nbosh-aws-xen-hvm-ubuntu-trusty-go_agent\n\n  \nversion\n:\n \n'3033'\n\n  \ninfrastructure\n:\n \naws\n\n  \nhypervisor\n:\n \nxen\n\n  \nroot_device_name\n:\n \n/dev/sda1\n\n  \nami\n:\n\n    \nus-east-1\n:\n \nami-3dc56656\n\n    \nus-west-1\n:\n \nami-db9a659f\n\n    \nus-west-2\n:\n \nami-dd5850ed\n\n\n\n\n\n\n\nLight Stemcells \n\u00b6\n\n\nSome IaaSes (or how they are configured) limit how OS images can be imported. Here are couple of examples:\n\n\n\n\nAWS only allows creation of AMIs from running VMs on AWS\n\n\nOpenStack can be configured to disallow Glance image upload\n\n\nan IaaS may take long time to import an image making it beneficial to reuse existing images\n\n\n\n\nIn such cases CPI must use already imported OS image and that's where light stemcells come in. Light stemcell tarballs include additional details about already imported OS images in the \ncloud_properties\n section. For example light stemcells for AWS have \nami\n key in the \ncloud_properties\n section (as shown above), that contains region-to-AMI-ID mappings. When AWS CPI's \ncreate_stemcell\n call\n is made, it will return matching AMI ID without doing any IaaS API calls.\n\n\n\n\nTesting \n\u00b6\n\n\nThere are two test suites each stemcell is expected to pass before it's considered to be production-ready:\n\n\n\n\nshared \nStemcell Tests\n which verify that proper packages and configurations are installed\n\n\nshared \nBOSH Acceptance Tests (BATS)\n (provided by the BOSH team) which verify high level Director behavior with the stemcell being used\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nAgent-CPI interactions",
            "title": "Building a Stemcell"
        },
        {
            "location": "/build-stemcell/#tarball-structure",
            "text": "Note that tarball structure is subject to change without notice.  $ tar tvf light-bosh-stemcell-3033-aws-xen-hvm-ubuntu-trusty-go_agent.tgz\n\n-rw-rw-r--   0  ubuntu ubuntu       0  Aug   4   09 :45 image\n-rw-rw-r--   0  ubuntu ubuntu     710  Aug   4   10 :06 stemcell.MF\n-rw-r--r--   0  ubuntu ubuntu   50594  Aug   4   09 :23 packages.txt\n-rw-r--r--   0  ubuntu ubuntu   12543  Aug   4   09 :22 dev_tools_file_list.txt   image : OS image in a format (raw, qcow, ova, etc.) understood by the CPI/IaaS.  stemcell.MF : YAML file with stemcell metadata.  packages.txt : Text file that includes list of packages installed. (Used to be included as  stemcell_dpkg_l.txt )  dev_tools_file_list.txt : Text file that includes list of files removed by the agent if Agent's  remove_dev_tools  feature is enabled.",
            "title": "Tarball Structure "
        },
        {
            "location": "/build-stemcell/#metadata",
            "text": "Note that content of `stemcell.MF` is subject to change without notice.   name  [String, required]: A unique name used to identify stemcell series.  operating_system  [String, required]: Operating system in the stemcell. Example:  ubuntu-trusty .  version  [String, required]: Version of the stemcell. Example:  3033 .  sha1  [String, required]: The SHA1 of the image file included in the stemcell tarball.  bosh_protocol  [Integer, optional]: Deprecated.  cloud_properties  [Hash, required]: Describes any IaaS-specific properties needed to import OS image. These properties will be passed in to the  create_stemcell  CPI call .   Name, operating system and version values will be visible via  bosh stemcells  command once a stemcell is imported into the Director.  Example:  $ tar -Oxzf light-bosh-stemcell-3033-aws-xen-hvm-ubuntu-trusty-go_agent.tgz stemcell.MF  ---  name :   bosh-aws-xen-hvm-ubuntu-trusty-go_agent  operating_system :   ubuntu-trusty  version :   '3033'  sha1 :   c13273b00b762c5aa29240ea62e1b9b5a03ae02c  bosh_protocol :   1  cloud_properties : \n   name :   bosh-aws-xen-hvm-ubuntu-trusty-go_agent \n   version :   '3033' \n   infrastructure :   aws \n   hypervisor :   xen \n   root_device_name :   /dev/sda1 \n   ami : \n     us-east-1 :   ami-3dc56656 \n     us-west-1 :   ami-db9a659f \n     us-west-2 :   ami-dd5850ed",
            "title": "Metadata "
        },
        {
            "location": "/build-stemcell/#light-stemcells",
            "text": "Some IaaSes (or how they are configured) limit how OS images can be imported. Here are couple of examples:   AWS only allows creation of AMIs from running VMs on AWS  OpenStack can be configured to disallow Glance image upload  an IaaS may take long time to import an image making it beneficial to reuse existing images   In such cases CPI must use already imported OS image and that's where light stemcells come in. Light stemcell tarballs include additional details about already imported OS images in the  cloud_properties  section. For example light stemcells for AWS have  ami  key in the  cloud_properties  section (as shown above), that contains region-to-AMI-ID mappings. When AWS CPI's  create_stemcell  call  is made, it will return matching AMI ID without doing any IaaS API calls.",
            "title": "Light Stemcells "
        },
        {
            "location": "/build-stemcell/#testing",
            "text": "There are two test suites each stemcell is expected to pass before it's considered to be production-ready:   shared  Stemcell Tests  which verify that proper packages and configurations are installed  shared  BOSH Acceptance Tests (BATS)  (provided by the BOSH team) which verify high level Director behavior with the stemcell being used    Back to Table of Contents  Previous:  Agent-CPI interactions",
            "title": "Testing "
        },
        {
            "location": "/repack-stemcell/",
            "text": "Note: Applies to CLI v2 v2.0.12+.\n\n\n\nThe \nCLI v2\n includes a command to repack stemcells; this enables limited customization of a stemcell including the following:\n\n\n\n\nname\n\n\nversion\n\n\ncloud properties\n\n\n\n\n\n\nSyntax \n\u00b6\n\n\n$ bosh repack-stemcell src.tgz dst.tgz \n[\n--name\n=\nnew_name\n]\n \n[\n--version\n=\nnew_version\n]\n \n[\n--cloud-properties\n=\njson-string\n]\n\n\n\n\n\nExamples \n\u00b6\n\n\nIn this example, we first download the stemcell we plan to modify, and then we create a new stemcell that's identical to the one we downloaded with the exception of a new name (\nacme-corporation-stemcell\n):\n\n\n$ curl -OL https://s3.amazonaws.com/bosh-gce-light-stemcells/light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz\n$ bosh repack-stemcell --name\n=\nacme-corporation-stemcell light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz acme-corporation-stemcell.tgz\n\n\n\n\nWe decide to change the stemcell version number to \n100\n as well as the name (note: this does not change the stemcell version in the \n/var/vcap/bosh/etc/stemcell_version\n file in the root filesystem of the stemcell):\n\n\n$ bosh repack-stemcell --name\n=\nacme-corporation-stemcell --version\n=\n100\n light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz acme-corporation-stemcell.tgz\n\n\n\n\nWhen we've uploaded the stemcell and we run \nbosh stemcells\n, we will see our stemcell listed with the new name and new version.\n\n\nCPI-Specific Options \n\u00b6\n\n\nAWS CPI-Specific Options \n\u00b6\n\n\nThe \nrepack-stemcell\n command can be used to enable the encryption of the root filesystem of VMs deployed with the repacked stemcell..\n\n\nTwo arguments enable the encryption of the root filesystem:\n\n\n\n\nencrypted\n [Boolean, optional]: Must be set to \ntrue\n if encryption of the root filesystem\n\n\nkms_key_arn\n [String, optional]: Created in the \nEncryption Keys\n section of the Identity and Access Management (IAM) console. If not specified \nand\n \nencrypted\n is true, the root filesystem will be encrypted with the default key.\n\n\n\n\nWe modify the cloud-properties of an AWS stemcell to encrypt the root filesystem of instances deployed with our repacked stemcell. The cloud-properties must be specified as valid JSON. This only works with heavy stemcells:\n\n\nWe take this opportunity to rename our stemcell so that we don't accidently confuse the unencrypted stemcells with the encrypted stemcells.\n\n\n$ bosh repack-stemcell --name\n=\nacme-ubuntu-encrypted --cloud-properties\n=\n'{\"encrypted\": true, \"kms_key_arn\": \"arn:aws:kms:us-east-1:088444384256:key/4ffbe966-d138-4f4d-a077-4c234d05b3b1\"}'\n bosh-stemcell-3363.9-aws-xen-hvm-ubuntu-trusty-go_agent.tgz acme-encrypted-stemcell.tgz\n\n\n\n\nNote: Available in BOSH AWS CPI v63+.\n\n\n\nThe cloud properties will be merged with the existing cloud properties. It won't delete any properties, but it will overwrite the ones specified. For example, the above command will not delete the stemcell's cloud-property \ninfrastructure\n:\n \naws\n.\n\n\nTechnical Details \n\u00b6\n\n\nThe \nrepack-stemcell\n works by modifying the stemcell manifest file (\nstemcell.MF\n) located within the stemcell tarball. It does not modify any other aspect of the stemcell. For example, it will not make any change to the root partition (it won't add new users or new packages). It does not modify the filesystem image.\n\n\nThe stemcell's manifest may be examined by extracting the \nstemcell.MF\n file from the stemcell tarball:\n\n\n$ curl -L https://bosh.io/d/stemcells/bosh-google-kvm-ubuntu-trusty-go_agent \n|\n tar -Oxvf - -- stemcell.MF\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n100\n   \n137\n  \n100\n   \n137\n    \n0\n     \n0\n    \n268\n      \n0\n --:--:-- --:--:-- --:--:--   \n268\n\n\n100\n \n19230\n  \n100\n \n19230\n    \n0\n     \n0\n  \n18442\n      \n0\n  \n0\n:00:01  \n0\n:00:01 --:--:-- \n18442\n\nx stemcell.MF---\nname: bosh-google-kvm-ubuntu-trusty-go_agent\nversion: \n'3363.9'\n\nbosh_protocol: \n1\n\nsha1: da39a3ee5e6b4b0d3255bfef95601890afd80709\noperating_system: ubuntu-trusty\ncloud_properties:\n  name: bosh-google-kvm-ubuntu-trusty-go_agent\n  version: \n'3363.9'\n\n  infrastructure: google\n  hypervisor: kvm\n  disk: \n3072\n\n  disk_format: rawdisk\n  container_format: bare\n  os_type: linux\n  os_distro: ubuntu\n  architecture: x86_64\n  root_device_name: \n\"/dev/sda1\"\n\n  source_url: https://storage.googleapis.com/bosh-gce-raw-stemcells/bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent-raw.tar.gz\n  raw_disk_sha1: fd8ef3f59b01e5e923c0ff1f70fcfcfdbbd49aeb",
            "title": "Repacking Stemcells"
        },
        {
            "location": "/repack-stemcell/#syntax",
            "text": "$ bosh repack-stemcell src.tgz dst.tgz  [ --name = new_name ]   [ --version = new_version ]   [ --cloud-properties = json-string ]",
            "title": "Syntax "
        },
        {
            "location": "/repack-stemcell/#examples",
            "text": "In this example, we first download the stemcell we plan to modify, and then we create a new stemcell that's identical to the one we downloaded with the exception of a new name ( acme-corporation-stemcell ):  $ curl -OL https://s3.amazonaws.com/bosh-gce-light-stemcells/light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz\n$ bosh repack-stemcell --name = acme-corporation-stemcell light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz acme-corporation-stemcell.tgz  We decide to change the stemcell version number to  100  as well as the name (note: this does not change the stemcell version in the  /var/vcap/bosh/etc/stemcell_version  file in the root filesystem of the stemcell):  $ bosh repack-stemcell --name = acme-corporation-stemcell --version = 100  light-bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent.tgz acme-corporation-stemcell.tgz  When we've uploaded the stemcell and we run  bosh stemcells , we will see our stemcell listed with the new name and new version.",
            "title": "Examples "
        },
        {
            "location": "/repack-stemcell/#cpi-specific-options",
            "text": "",
            "title": "CPI-Specific Options "
        },
        {
            "location": "/repack-stemcell/#aws-cpi-specific-options",
            "text": "The  repack-stemcell  command can be used to enable the encryption of the root filesystem of VMs deployed with the repacked stemcell..  Two arguments enable the encryption of the root filesystem:   encrypted  [Boolean, optional]: Must be set to  true  if encryption of the root filesystem  kms_key_arn  [String, optional]: Created in the  Encryption Keys  section of the Identity and Access Management (IAM) console. If not specified  and   encrypted  is true, the root filesystem will be encrypted with the default key.   We modify the cloud-properties of an AWS stemcell to encrypt the root filesystem of instances deployed with our repacked stemcell. The cloud-properties must be specified as valid JSON. This only works with heavy stemcells:  We take this opportunity to rename our stemcell so that we don't accidently confuse the unencrypted stemcells with the encrypted stemcells.  $ bosh repack-stemcell --name = acme-ubuntu-encrypted --cloud-properties = '{\"encrypted\": true, \"kms_key_arn\": \"arn:aws:kms:us-east-1:088444384256:key/4ffbe966-d138-4f4d-a077-4c234d05b3b1\"}'  bosh-stemcell-3363.9-aws-xen-hvm-ubuntu-trusty-go_agent.tgz acme-encrypted-stemcell.tgz  Note: Available in BOSH AWS CPI v63+.  The cloud properties will be merged with the existing cloud properties. It won't delete any properties, but it will overwrite the ones specified. For example, the above command will not delete the stemcell's cloud-property  infrastructure :   aws .",
            "title": "AWS CPI-Specific Options "
        },
        {
            "location": "/repack-stemcell/#technical-details",
            "text": "The  repack-stemcell  works by modifying the stemcell manifest file ( stemcell.MF ) located within the stemcell tarball. It does not modify any other aspect of the stemcell. For example, it will not make any change to the root partition (it won't add new users or new packages). It does not modify the filesystem image.  The stemcell's manifest may be examined by extracting the  stemcell.MF  file from the stemcell tarball:  $ curl -L https://bosh.io/d/stemcells/bosh-google-kvm-ubuntu-trusty-go_agent  |  tar -Oxvf - -- stemcell.MF\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed 100     137    100     137      0       0      268        0  --:--:-- --:--:-- --:--:--    268  100   19230    100   19230      0       0    18442        0    0 :00:01   0 :00:01 --:--:--  18442 \nx stemcell.MF---\nname: bosh-google-kvm-ubuntu-trusty-go_agent\nversion:  '3363.9' \nbosh_protocol:  1 \nsha1: da39a3ee5e6b4b0d3255bfef95601890afd80709\noperating_system: ubuntu-trusty\ncloud_properties:\n  name: bosh-google-kvm-ubuntu-trusty-go_agent\n  version:  '3363.9' \n  infrastructure: google\n  hypervisor: kvm\n  disk:  3072 \n  disk_format: rawdisk\n  container_format: bare\n  os_type: linux\n  os_distro: ubuntu\n  architecture: x86_64\n  root_device_name:  \"/dev/sda1\" \n  source_url: https://storage.googleapis.com/bosh-gce-raw-stemcells/bosh-stemcell-3363.9-google-kvm-ubuntu-trusty-go_agent-raw.tar.gz\n  raw_disk_sha1: fd8ef3f59b01e5e923c0ff1f70fcfcfdbbd49aeb",
            "title": "Technical Details "
        },
        {
            "location": "/personas/",
            "text": "BOSH is used by several different groups of people with different needs and goals. Here are the primary personas and their responsibilities that we typically consider...\n\n\nRelease Author\n\u00b6\n\n\nA release author is interested in how software can be deployed and run with BOSH. They will typically be familiar with both the underlying software which will be deployed, and BOSH conventions around how BOSH supports configuring and installing the software. They will want to understand...\n\n\n\n\nThe target software...\n\n\nWhat it means to run it at scale and in production.\n\n\nWhat settings users need to configure.\n\n\nWhat conventions the software may have and users expect.\n\n\nWhat dependencies the software may need.\n\n\nWhat upgrade workflows are effective.\n\n\n\n\n\n\nBOSH strategies for managing software...\n\n\nHow to write compilation scripts for the software.\n\n\nHow to write runtime scripts for running the software.\n\n\nHow to manage versions of their release.\n\n\nHow deployment operators expect to deploy their release.\n\n\n\n\n\n\n\n\nSome examples of Release Authors are...\n\n\n\n\nDiego team managing \ncloudfoundry/diego-release\n - this release is dedicated to deploying the Diego component of Cloud Foundry Application Runtime.\n\n\nConcourse team managing \nconcourse/concourse\n - this release integrates several Concourse-specific components into a set of software which makes sense to deploy.\n\n\nDatadog team managing \nDataDog/datadog-agent-boshrelease\n - this release forwards telemetry and other monitoring information to the Datadog hosted services.\n\n\n\n\nDeployment Author\n\u00b6\n\n\nA deployment author is interested in making it easier for others to deploy releases in a pre-configured way. They may not be directly responsible for creating releases and defining how they're configured, nor managing the software running in production. They will want to understand...\n\n\n\n\nHow release authors expose services and configuration of their software.\n\n\nHow deployment operators are interested in running the software or set of services.\n\n\nWhen integrating multiple components...\n\n\nWhich versions of components are safe to integrate with each other.\n\n\nWhich feature sets may need to be optional or configured (likely with \u201cops files\u201d)\n\n\n\n\n\n\n\n\nSome examples of Deployment Authors are...\n\n\n\n\nRelease Integration team managing \ncloudfoundry/cf-deployment\n - this deployment brings together numerous releases to manage a set of versions and features which can be used to deploy Cloud Foundry.\n\n\nTeams developing \nPCF tiles\n (Tile Authors) - a tile integrates some customization options for managing deployments and services with Pivotal's Ops Manager product.\n\n\n\n\nOperator\n\u00b6\n\n\nOperators are responsible for the ongoing stability of BOSH resources. There are usually two layers of BOSH which people take responsibility for, although sometimes the responsibility for both roles may be shared within a single team.\n\n\nDeployment Operator\n\u00b6\n\n\nA deployment operator is interested in telling the director to make sure it deploys and runs software they need on the cloud. They will want to understand...\n\n\n\n\nHow the release software should be configured;\n\n\nWhat other components are involved to successful operate their software;\n\n\nWhat availability zones and networks are available to them for their deployment.\n\n\n\n\nDirector Operator\n\u00b6\n\n\nA director operator is interested in configuring and maintaining the director to ensure deployment operators are able to deploy their services to the cloud. They will want to understand...\n\n\n\n\nWhat the general director architecture and components look like.\n\n\nHow to securely configure director components.\n\n\nWhat clouds are available...\n\n\nHow to connect and authenticate to clouds with CPIs.\n\n\nHow networks are connected.\n\n\n\n\n\n\nAuthorization and authentication control...\n\n\nHow teams and users connect to the director.\n\n\nHow deployments will be accessible to others.\n\n\n\n\n\n\n\n\nCloud Operator\n\u00b6\n\n\nA cloud operator is responsible for planning and managing the underlying infrastructure where resources are managed. They will want to understand...\n\n\n\n\nHow to plan for resource consumption...\n\n\nWhat compute, memory, disk resources are being used and needed.\n\n\nWhat network assignments should be allocated.\n\n\n\n\n\n\nHow to provision access to the environment.\n\n\nHow to monitor the infrastructure to identify issues which may impact availability of higher layers.\n\n\n\n\nSome examples of Cloud Operators are...\n\n\n\n\nOpenStack or vSphere environments are often managed by a dedicated infrastructure team.\n\n\nPublic clouds like Google Cloud Platform or Amazon Web Services may have a team responsible for overseeing general usage and defining policies for teams.\n\n\n\n\nDeveloper\n\u00b6\n\n\nA developer is interested in enhancing functionality of one or more components of BOSH. There are several different projects a developer may be interested in.\n\n\nInternal Developer\n\u00b6\n\n\nAn internal developer is focused on improving or expanding the feature sets of the core BOSH components. There are several components in the core ecosystem, but in general they will want to understand...\n\n\n\n\nScope of features within specific BOSH components.\n\n\nAPI and dependencies between the components of BOSH.\n\n\nTeams currently responsible for components, and repositories where ongoing work can be tracked and discussed.\n\n\n\n\nSome examples of Internal Developers are...\n\n\n\n\nBOSH team - comprised of Cloud Foundry Foundation members, the teams work from around the world on different features.\n\n\nOSS Community members - those who send pull requests and contributions to the various projects.\n\n\n\n\nCPI Developer\n\u00b6\n\n\nA CPI Developer is primarily interested in seeing support for a particular IaaS (like Amazon Web Services and Google Cloud Platform) within BOSH. They will want to understand...\n\n\n\n\nThe API exposed by the director for management of IaaS resources, such as disks and virtual machines.\n\n\nThe APIs exposed by the IaaS for managing resources.\n\n\nConventions encouraged and enforced by the IaaS around...\n\n\nDisaster recovery and failure scenarios.\n\n\nPermissions and access control.\n\n\n\n\n\n\n\n\nSome examples of CPI Developers are...\n\n\n\n\nVMware team managing \ncloudfoundry-incubator/bosh-vsphere-cpi-release\n\n\nSAP team managing \ncloudfoundry-incubator/bosh-openstack-cpi-release",
            "title": "Target Personas"
        },
        {
            "location": "/personas/#release-author",
            "text": "A release author is interested in how software can be deployed and run with BOSH. They will typically be familiar with both the underlying software which will be deployed, and BOSH conventions around how BOSH supports configuring and installing the software. They will want to understand...   The target software...  What it means to run it at scale and in production.  What settings users need to configure.  What conventions the software may have and users expect.  What dependencies the software may need.  What upgrade workflows are effective.    BOSH strategies for managing software...  How to write compilation scripts for the software.  How to write runtime scripts for running the software.  How to manage versions of their release.  How deployment operators expect to deploy their release.     Some examples of Release Authors are...   Diego team managing  cloudfoundry/diego-release  - this release is dedicated to deploying the Diego component of Cloud Foundry Application Runtime.  Concourse team managing  concourse/concourse  - this release integrates several Concourse-specific components into a set of software which makes sense to deploy.  Datadog team managing  DataDog/datadog-agent-boshrelease  - this release forwards telemetry and other monitoring information to the Datadog hosted services.",
            "title": "Release Author"
        },
        {
            "location": "/personas/#deployment-author",
            "text": "A deployment author is interested in making it easier for others to deploy releases in a pre-configured way. They may not be directly responsible for creating releases and defining how they're configured, nor managing the software running in production. They will want to understand...   How release authors expose services and configuration of their software.  How deployment operators are interested in running the software or set of services.  When integrating multiple components...  Which versions of components are safe to integrate with each other.  Which feature sets may need to be optional or configured (likely with \u201cops files\u201d)     Some examples of Deployment Authors are...   Release Integration team managing  cloudfoundry/cf-deployment  - this deployment brings together numerous releases to manage a set of versions and features which can be used to deploy Cloud Foundry.  Teams developing  PCF tiles  (Tile Authors) - a tile integrates some customization options for managing deployments and services with Pivotal's Ops Manager product.",
            "title": "Deployment Author"
        },
        {
            "location": "/personas/#operator",
            "text": "Operators are responsible for the ongoing stability of BOSH resources. There are usually two layers of BOSH which people take responsibility for, although sometimes the responsibility for both roles may be shared within a single team.",
            "title": "Operator"
        },
        {
            "location": "/personas/#deployment-operator",
            "text": "A deployment operator is interested in telling the director to make sure it deploys and runs software they need on the cloud. They will want to understand...   How the release software should be configured;  What other components are involved to successful operate their software;  What availability zones and networks are available to them for their deployment.",
            "title": "Deployment Operator"
        },
        {
            "location": "/personas/#director-operator",
            "text": "A director operator is interested in configuring and maintaining the director to ensure deployment operators are able to deploy their services to the cloud. They will want to understand...   What the general director architecture and components look like.  How to securely configure director components.  What clouds are available...  How to connect and authenticate to clouds with CPIs.  How networks are connected.    Authorization and authentication control...  How teams and users connect to the director.  How deployments will be accessible to others.",
            "title": "Director Operator"
        },
        {
            "location": "/personas/#cloud-operator",
            "text": "A cloud operator is responsible for planning and managing the underlying infrastructure where resources are managed. They will want to understand...   How to plan for resource consumption...  What compute, memory, disk resources are being used and needed.  What network assignments should be allocated.    How to provision access to the environment.  How to monitor the infrastructure to identify issues which may impact availability of higher layers.   Some examples of Cloud Operators are...   OpenStack or vSphere environments are often managed by a dedicated infrastructure team.  Public clouds like Google Cloud Platform or Amazon Web Services may have a team responsible for overseeing general usage and defining policies for teams.",
            "title": "Cloud Operator"
        },
        {
            "location": "/personas/#developer",
            "text": "A developer is interested in enhancing functionality of one or more components of BOSH. There are several different projects a developer may be interested in.",
            "title": "Developer"
        },
        {
            "location": "/personas/#internal-developer",
            "text": "An internal developer is focused on improving or expanding the feature sets of the core BOSH components. There are several components in the core ecosystem, but in general they will want to understand...   Scope of features within specific BOSH components.  API and dependencies between the components of BOSH.  Teams currently responsible for components, and repositories where ongoing work can be tracked and discussed.   Some examples of Internal Developers are...   BOSH team - comprised of Cloud Foundry Foundation members, the teams work from around the world on different features.  OSS Community members - those who send pull requests and contributions to the various projects.",
            "title": "Internal Developer"
        },
        {
            "location": "/personas/#cpi-developer",
            "text": "A CPI Developer is primarily interested in seeing support for a particular IaaS (like Amazon Web Services and Google Cloud Platform) within BOSH. They will want to understand...   The API exposed by the director for management of IaaS resources, such as disks and virtual machines.  The APIs exposed by the IaaS for managing resources.  Conventions encouraged and enforced by the IaaS around...  Disaster recovery and failure scenarios.  Permissions and access control.     Some examples of CPI Developers are...   VMware team managing  cloudfoundry-incubator/bosh-vsphere-cpi-release  SAP team managing  cloudfoundry-incubator/bosh-openstack-cpi-release",
            "title": "CPI Developer"
        },
        {
            "location": "/terminology/",
            "text": "AZ or Availability Zone \n\u00b6\n\n\nAn availability zone represents a separated set of cloud resources (typically compute, networking and storage) such that failures in one AZ cause minimal impact in a different AZ. \nSee usage details\n.\n\n\n\n\nAddon \n\u00b6\n\n\nA release job that is colocated on all VMs managed by the Director. Addons are configured in the runtime config. \nSee usage details\n.\n\n\n\n\nAgent \n\u00b6\n\n\nA process that runs continuously on each VM that BOSH deploys (one Agent process per VM). The Agent executes tasks in response to messages it receives from the Director.\n\n\n\n\nbosh-init \n\u00b6\n\n\nTool that was replace by CLI v2's \ncreate-env\n command used for creating and updating.\n\n\n\n\nBOSH Lite \n\u00b6\n\n\nBOSH Lite is a Director VM that is configured to use Warden CPI, which emulates VMs with containers. It's typically installed locally with VirtualBox; however, it could also be installed onto any cloud BOSH supports. \nSee usage details\n\n\n\n\nCanary (Instance) \n\u00b6\n\n\nCanary instances are first instances updated within an instance group. Any update error in a canary instance causes the deployment to stop. Since only canaries are affected before an update stops, problem jobs and packages are prevented from taking over all instances.\n\n\n\n\nCLI (v1) \n\u00b6\n\n\nThe BOSH Command Line Interface (CLI) is what you use to run BOSH commands. You must \ninstall\n the CLI to use BOSH. Run \nbosh help --all\n to view the help. It is superseded by CLI v2.\n\n\n\n\nCLI v2 \n\u00b6\n\n\nThe BOSH Command Line Interface (CLI) is what you use to run BOSH commands. CLI v2 is a new major version of CLI. It also replaces bosh-init CLI to manage Director VM. It's the recommended way to interact with the Director. \nSee usage details\n.\n\n\n\n\nCloud \n\u00b6\n\n\nSame as Infrastructure as a Service.\n\n\n\n\nCloud ID (CID) \n\u00b6\n\n\nID returned from the Cloud identifying particular resource such as VM or disk.\n\n\n\n\nCloud Config \n\u00b6\n\n\nThe cloud config is a YAML file that defines IaaS specific configuration used by the Director and all deployments. It allows to separate IaaS specific configuration into its own file and keep deployment manifests IaaS agnostic. \nSee usage details\n.\n\n\n\n\nCompiled Release \n\u00b6\n\n\nA compiled release contains jobs and compiled packages. A non-compiled release (or just release) contains jobs and source packages. \nSee usage details\n.\n\n\n\n\nCPI \n\u00b6\n\n\nA Cloud Provider Interface is an abstraction layer between the Director and an IaaS (cloud). CPIs have to implement a small number of methods to perform VM, disk and network operations. CPIs could be written in different languages.\n\n\n\n\nDeploy \n\u00b6\n\n\nBOSH deploys software to the cloud using a deployment manifest, one or more stemcells, and one or more releases.\n\n\n\n\nDeployment \n\u00b6\n\n\nAn encapsulation of software and configuration that BOSH can deploy to the cloud. You can think of a deployment as the state of a collection of VMs: what software is on them, what resources they use, and how these are orchestrated. Even though BOSH creates the deployment using ephemeral resources, the deployment is stable in that BOSH re-creates VMs that fail and otherwise works to keep your software running. BOSH also manages persistent disks so that state (for example, database data files) can survive when VMs are re-created. Combination of a deployment manifest, stemcells, and releases is portable across different clouds with minimal changes to the deployment manifest. See \nWhat is a Deployment?\n.\n\n\n\n\nDirector \n\u00b6\n\n\nThe main BOSH component that coordinates the Agents and responds to user requests and system events. The Director is the orchestrator of deployments.\n\n\n\n\nDirector Blobstore \n\u00b6\n\n\nA repository where BOSH stores release artifacts, logs, stemcells, and other content, at various times during the lifecycle of a BOSH release.\n\n\n\n\nDirector Task \n\u00b6\n\n\nThe basic unit of work performed by the Director. You can get the status and logs for any task. You can monitor the task throughout its lifecycle, which progresses through states like queued, processing, done, and error.\n\n\n\n\nDirector VM (previously known as MicroBOSH) \n \n\u00b6\n\n\nA single VM with the Director and other necessary components.\n\n\n\n\nDisk Type (previously known as Disk Pool) \n \n\u00b6\n\n\nDisk type is a named disk configuration specified in the cloud config. \nSee usage details\n and \nread more about persistent disks\n.\n\n\n\n\nEnvironment \n\u00b6\n\n\nAn environment consists of a Director and deployments that it orchestrates. A good example of two separate environments are staging and production environments.\n\n\n\n\nErrand \n\u00b6\n\n\nAn errand is a short-lived job that can be triggered by an operator any time after the deploy. Examples:\n\n\n\n\nsmoke tests\n\n\ncomprehensive test suites\n\n\nCF service broker binding and unbinding\n\n\n\n\nSee details\n.\n\n\n\n\nEvent \n\u00b6\n\n\nActions taken by the Director (via user or system control) are recorded as events to the Director database. Examples:\n\n\n\n\nVM create/delete\n\n\ncloud config update\n\n\n\n\nSee details\n.\n\n\n\n\nIaaS \n\u00b6\n\n\nShort for Infrastructure as a Service. BOSH enables the Cloud Foundry PaaS and other software deployed with BOSH to support multiple IaaS providers.\n\n\n\n\nInstance \n\u00b6\n\n\nAn instance corresponds to a single VM that performs specific jobs. Each instance is a part of an instance group.\n\n\n\n\nInstance Group (previously known as Deployment Job) \n\u00b6\n\n\nAn instance group is a collection of instances tasked to perform same jobs. Each instance group has an associated VM type, persistent disk type, a stemcell and a set of jobs. Instance groups are configured in the deployment manifest.\n\n\n\n\nInstance Lifecycle \n\u00b6\n\n\nStages that all jobs (and their associated processes) go through during a deployment process on one instance. For example: pre-start, start, drain, etc. \nSee details\n.\n\n\n\n\nJob (aka Release Job) \n\u00b6\n\n\nA job is part of a release. It contains startup, shutdown scripts, and configuration files that tell the Agent how to start, run and monitor software on a VM. Jobs can depend on packages for necessary software. \nSee details\n.\n\n\n\n\nJob Lifecycle \n\u00b6\n\n\nStages that all jobs (and their associated processes) go through during a deployment process on one instance. For example: pre-start, start, drain, etc. \nSee details\n.\n\n\n\n\nJumpbox \n\u00b6\n\n\nA VM that acts as a single access point for the Director and deployed VMs. For resilience, there should be more than one jump box. Allowing access through jump boxes and disabling direct access to the other VMs is a common security measure.\n\n\n\n\nDeployment manifest (or just manifest) \n\u00b6\n\n\nA YAML file that identifies one or more releases, stemcells and specifies how to configure them for a given deployment.\n\n\n\n\nOperator \n\u00b6\n\n\nA user that sets up and/or uses the Director (via BOSH CLI or Director API) to manage cloud resources.\n\n\n\n\nOperations file (ops file) \n\u00b6\n\n\nA YAML file that includes multiple operations to be applied to a different YAML file. Several CLI commands such as \ncreate-env\n and \ninterpolate\n allow to provide multiple operations files via \n--ops-file\n flag. \nSee details\n.\n\n\n\n\nOperation \n\u00b6\n\n\nA single directive in an operations file. An operation describes one change to make to a YAML structure. Currently there are two types of operations: replace and remove. \nSee details\n.\n\n\n\n\nOrphaned (Persistent) Disk \n\u00b6\n\n\nAn orphaned disk is a persistent disk that will be garbage collected after a few days unless it's reattached to an instance.\n\n\n\n\nPackage \n\u00b6\n\n\nA package is part of a release. It contains vendored in software source and scripts to compile it. Packages can depend on other packages.\n\n\n\n\nPersistent Disk \n\u00b6\n\n\nA persistent disk is a disk created in the cloud and associated with a specific \ninstance\n. While instance's associated VM is recreated, same persistent disk will be reattached. \nSee usage details\n.\n\n\n\n\nRelease \n\u00b6\n\n\nA collection of configuration files, source code, jobs, packages and accompanying information needed to make a software component deployable by BOSH. A self-contained release should have no dependencies that need to be fetched from the internet. See \nWhat is a Release?\n.\n\n\n\n\nResource Pool \n\u00b6\n\n\nResource pool is collections of VMs created from the same stemcell, with the same configuration, in a deployment.\n\n\n\n\nRuntime Config \n\u00b6\n\n\nThe runtime config is a YAML file that defines global configuration used by the Director and all deployments. It allows to specify addons. \nSee usage details\n.\n\n\n\n\nStemcell \n\u00b6\n\n\nA generic VM image that BOSH clones and configures during deployment. A stemcell is a template from which BOSH creates whatever VMs are needed for a wide variety of components and products. See \nWhat is a Stemcell?\n.\n\n\n\n\nTeam \n\u00b6\n\n\nEach deployment can be managed by specific teams. A logged in UAA user can belong to one or more teams. \nSee details\n.\n\n\n\n\nVariable (var) \n\u00b6\n\n\nVariable points to a saved value in some store. Variables are typically used in configuration files (manifests) to decouple sensitive (passwords, certificates) or volatile (bucket name, number of instances) data from more static content (general configuration). Variables are denoted with double parens -- \n((namespace/var-name))\n.\n\n\n\n\nVM Extension \n\u00b6\n\n\nVM extension is a named Virtual Machine configuration in the cloud config that allows to specify arbitrary IaaS specific configuration such as associated security groups and load balancers. \nSee usage details\n.\n\n\n\n\nVM Type \n\u00b6\n\n\nVM type is a named Virtual Machine size configuration in the cloud config. \nSee usage details\n.",
            "title": "Terminology"
        },
        {
            "location": "/terminology/#az-or-availability-zone",
            "text": "An availability zone represents a separated set of cloud resources (typically compute, networking and storage) such that failures in one AZ cause minimal impact in a different AZ.  See usage details .",
            "title": "AZ or Availability Zone "
        },
        {
            "location": "/terminology/#addon",
            "text": "A release job that is colocated on all VMs managed by the Director. Addons are configured in the runtime config.  See usage details .",
            "title": "Addon "
        },
        {
            "location": "/terminology/#agent",
            "text": "A process that runs continuously on each VM that BOSH deploys (one Agent process per VM). The Agent executes tasks in response to messages it receives from the Director.",
            "title": "Agent "
        },
        {
            "location": "/terminology/#bosh-init",
            "text": "Tool that was replace by CLI v2's  create-env  command used for creating and updating.",
            "title": "bosh-init "
        },
        {
            "location": "/terminology/#bosh-lite",
            "text": "BOSH Lite is a Director VM that is configured to use Warden CPI, which emulates VMs with containers. It's typically installed locally with VirtualBox; however, it could also be installed onto any cloud BOSH supports.  See usage details",
            "title": "BOSH Lite "
        },
        {
            "location": "/terminology/#canary-instance",
            "text": "Canary instances are first instances updated within an instance group. Any update error in a canary instance causes the deployment to stop. Since only canaries are affected before an update stops, problem jobs and packages are prevented from taking over all instances.",
            "title": "Canary (Instance) "
        },
        {
            "location": "/terminology/#cli-v1",
            "text": "The BOSH Command Line Interface (CLI) is what you use to run BOSH commands. You must  install  the CLI to use BOSH. Run  bosh help --all  to view the help. It is superseded by CLI v2.",
            "title": "CLI (v1) "
        },
        {
            "location": "/terminology/#cli-v2",
            "text": "The BOSH Command Line Interface (CLI) is what you use to run BOSH commands. CLI v2 is a new major version of CLI. It also replaces bosh-init CLI to manage Director VM. It's the recommended way to interact with the Director.  See usage details .",
            "title": "CLI v2 "
        },
        {
            "location": "/terminology/#cloud",
            "text": "Same as Infrastructure as a Service.",
            "title": "Cloud "
        },
        {
            "location": "/terminology/#cloud-id-cid",
            "text": "ID returned from the Cloud identifying particular resource such as VM or disk.",
            "title": "Cloud ID (CID) "
        },
        {
            "location": "/terminology/#cloud-config",
            "text": "The cloud config is a YAML file that defines IaaS specific configuration used by the Director and all deployments. It allows to separate IaaS specific configuration into its own file and keep deployment manifests IaaS agnostic.  See usage details .",
            "title": "Cloud Config "
        },
        {
            "location": "/terminology/#compiled-release",
            "text": "A compiled release contains jobs and compiled packages. A non-compiled release (or just release) contains jobs and source packages.  See usage details .",
            "title": "Compiled Release "
        },
        {
            "location": "/terminology/#cpi",
            "text": "A Cloud Provider Interface is an abstraction layer between the Director and an IaaS (cloud). CPIs have to implement a small number of methods to perform VM, disk and network operations. CPIs could be written in different languages.",
            "title": "CPI "
        },
        {
            "location": "/terminology/#deploy",
            "text": "BOSH deploys software to the cloud using a deployment manifest, one or more stemcells, and one or more releases.",
            "title": "Deploy "
        },
        {
            "location": "/terminology/#deployment",
            "text": "An encapsulation of software and configuration that BOSH can deploy to the cloud. You can think of a deployment as the state of a collection of VMs: what software is on them, what resources they use, and how these are orchestrated. Even though BOSH creates the deployment using ephemeral resources, the deployment is stable in that BOSH re-creates VMs that fail and otherwise works to keep your software running. BOSH also manages persistent disks so that state (for example, database data files) can survive when VMs are re-created. Combination of a deployment manifest, stemcells, and releases is portable across different clouds with minimal changes to the deployment manifest. See  What is a Deployment? .",
            "title": "Deployment "
        },
        {
            "location": "/terminology/#director",
            "text": "The main BOSH component that coordinates the Agents and responds to user requests and system events. The Director is the orchestrator of deployments.",
            "title": "Director "
        },
        {
            "location": "/terminology/#director-blobstore",
            "text": "A repository where BOSH stores release artifacts, logs, stemcells, and other content, at various times during the lifecycle of a BOSH release.",
            "title": "Director Blobstore "
        },
        {
            "location": "/terminology/#director-task",
            "text": "The basic unit of work performed by the Director. You can get the status and logs for any task. You can monitor the task throughout its lifecycle, which progresses through states like queued, processing, done, and error.",
            "title": "Director Task "
        },
        {
            "location": "/terminology/#environment",
            "text": "An environment consists of a Director and deployments that it orchestrates. A good example of two separate environments are staging and production environments.",
            "title": "Environment "
        },
        {
            "location": "/terminology/#errand",
            "text": "An errand is a short-lived job that can be triggered by an operator any time after the deploy. Examples:   smoke tests  comprehensive test suites  CF service broker binding and unbinding   See details .",
            "title": "Errand "
        },
        {
            "location": "/terminology/#event",
            "text": "Actions taken by the Director (via user or system control) are recorded as events to the Director database. Examples:   VM create/delete  cloud config update   See details .",
            "title": "Event "
        },
        {
            "location": "/terminology/#iaas",
            "text": "Short for Infrastructure as a Service. BOSH enables the Cloud Foundry PaaS and other software deployed with BOSH to support multiple IaaS providers.",
            "title": "IaaS "
        },
        {
            "location": "/terminology/#instance",
            "text": "An instance corresponds to a single VM that performs specific jobs. Each instance is a part of an instance group.",
            "title": "Instance "
        },
        {
            "location": "/terminology/#instance-group-previously-known-as-deployment-job",
            "text": "An instance group is a collection of instances tasked to perform same jobs. Each instance group has an associated VM type, persistent disk type, a stemcell and a set of jobs. Instance groups are configured in the deployment manifest.",
            "title": "Instance Group (previously known as Deployment Job) "
        },
        {
            "location": "/terminology/#instance-lifecycle",
            "text": "Stages that all jobs (and their associated processes) go through during a deployment process on one instance. For example: pre-start, start, drain, etc.  See details .",
            "title": "Instance Lifecycle "
        },
        {
            "location": "/terminology/#job-aka-release-job",
            "text": "A job is part of a release. It contains startup, shutdown scripts, and configuration files that tell the Agent how to start, run and monitor software on a VM. Jobs can depend on packages for necessary software.  See details .",
            "title": "Job (aka Release Job) "
        },
        {
            "location": "/terminology/#job-lifecycle",
            "text": "Stages that all jobs (and their associated processes) go through during a deployment process on one instance. For example: pre-start, start, drain, etc.  See details .",
            "title": "Job Lifecycle "
        },
        {
            "location": "/terminology/#jumpbox",
            "text": "A VM that acts as a single access point for the Director and deployed VMs. For resilience, there should be more than one jump box. Allowing access through jump boxes and disabling direct access to the other VMs is a common security measure.",
            "title": "Jumpbox "
        },
        {
            "location": "/terminology/#deployment-manifest-or-just-manifest",
            "text": "A YAML file that identifies one or more releases, stemcells and specifies how to configure them for a given deployment.",
            "title": "Deployment manifest (or just manifest) "
        },
        {
            "location": "/terminology/#operator",
            "text": "A user that sets up and/or uses the Director (via BOSH CLI or Director API) to manage cloud resources.",
            "title": "Operator "
        },
        {
            "location": "/terminology/#operations-file-ops-file",
            "text": "A YAML file that includes multiple operations to be applied to a different YAML file. Several CLI commands such as  create-env  and  interpolate  allow to provide multiple operations files via  --ops-file  flag.  See details .",
            "title": "Operations file (ops file) "
        },
        {
            "location": "/terminology/#operation",
            "text": "A single directive in an operations file. An operation describes one change to make to a YAML structure. Currently there are two types of operations: replace and remove.  See details .",
            "title": "Operation "
        },
        {
            "location": "/terminology/#orphaned-persistent-disk",
            "text": "An orphaned disk is a persistent disk that will be garbage collected after a few days unless it's reattached to an instance.",
            "title": "Orphaned (Persistent) Disk "
        },
        {
            "location": "/terminology/#package",
            "text": "A package is part of a release. It contains vendored in software source and scripts to compile it. Packages can depend on other packages.",
            "title": "Package "
        },
        {
            "location": "/terminology/#persistent-disk",
            "text": "A persistent disk is a disk created in the cloud and associated with a specific  instance . While instance's associated VM is recreated, same persistent disk will be reattached.  See usage details .",
            "title": "Persistent Disk "
        },
        {
            "location": "/terminology/#release",
            "text": "A collection of configuration files, source code, jobs, packages and accompanying information needed to make a software component deployable by BOSH. A self-contained release should have no dependencies that need to be fetched from the internet. See  What is a Release? .",
            "title": "Release "
        },
        {
            "location": "/terminology/#resource-pool",
            "text": "Resource pool is collections of VMs created from the same stemcell, with the same configuration, in a deployment.",
            "title": "Resource Pool "
        },
        {
            "location": "/terminology/#runtime-config",
            "text": "The runtime config is a YAML file that defines global configuration used by the Director and all deployments. It allows to specify addons.  See usage details .",
            "title": "Runtime Config "
        },
        {
            "location": "/terminology/#stemcell",
            "text": "A generic VM image that BOSH clones and configures during deployment. A stemcell is a template from which BOSH creates whatever VMs are needed for a wide variety of components and products. See  What is a Stemcell? .",
            "title": "Stemcell "
        },
        {
            "location": "/terminology/#team",
            "text": "Each deployment can be managed by specific teams. A logged in UAA user can belong to one or more teams.  See details .",
            "title": "Team "
        },
        {
            "location": "/terminology/#variable-var",
            "text": "Variable points to a saved value in some store. Variables are typically used in configuration files (manifests) to decouple sensitive (passwords, certificates) or volatile (bucket name, number of instances) data from more static content (general configuration). Variables are denoted with double parens --  ((namespace/var-name)) .",
            "title": "Variable (var) "
        },
        {
            "location": "/terminology/#vm-extension",
            "text": "VM extension is a named Virtual Machine configuration in the cloud config that allows to specify arbitrary IaaS specific configuration such as associated security groups and load balancers.  See usage details .",
            "title": "VM Extension "
        },
        {
            "location": "/terminology/#vm-type",
            "text": "VM type is a named Virtual Machine size configuration in the cloud config.  See usage details .",
            "title": "VM Type "
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/",
            "text": "Ubuntu Trusty Stemcell Release Notes, 3541.x\n\u00b6\n\n\n3541.10 (2018-03-27)\n\u00b6\n\n\n\n\nPeriodic Ubuntu and CentOS stemcell bump (March 26/27, 2018)\n\n\n\n\n3541.9 (2018-03-12)\n\u00b6\n\n\n\n\nPeriodic Ubuntu and CentOS stemcell bump (March 12, 2018)\n\n\n\n\n3541.8 (2018-03-08)\n\u00b6\n\n\n\n\nBump bosh-agent to 2.67.1\n\n\nAgent will now respect previously set permissions and owner on sys/run, sys/log and data job directories\n\n\nThis should fix stemcell compatibility with Diego/Garden if Agent restarts\n\n\nIf you were using 3541.x stemcell for any of your deployments, it's recommended to update your deployments to this version before updating Director since that would cause Agent restart\n\n\n\n\n\n\n\n\n3541.5 (2018-02-22)\n\u00b6\n\n\n\n\nBump Ubuntu Trusty stemcells for USN-3582-2: Linux kernel (Xenial HWE) vulnerabilities\n\n\n\n\n3541.4 (2018-02-15)\n\u00b6\n\n\n\n\nRolled back custom umask configuration as we found out it was different in some cases (depends on how processes were started)\n\n\nHardening of /var/vcap/jobs/* is still applied by the agent\n\n\n\n\n3541.2 (2018-02-09)\n\u00b6\n\n\n\n\n[breaking] Set default umask to 077 and further harden several /var/vcap/* directories\n\n\nNote that you may have to change your release to adapt to this change\n\n\n[breaking] Renamed /var/vcap/bosh/bin/ntpdate to /var/vcap/bosh/bin/sync-time\n\n\n[breaking] Stop forwarding SSH events to bosh-agent\n\n\nAgent no longer receives and forwards such events to HM. This should remove a lot of noisy generated by releases that expect a lot of SSH sessions (eg Gitlab). This information will continue to be available in logs forwarded to remote destinations (and locally /var/log/auth.log).\n\n\nFixes \nenv.bosh.swap_size: 0\n to work on more clouds (including GCP)\n\n\n\n\nMisc\n\n\n\n\nOrder stemcell tarballs so that upload-stemcell command can execute faster\n\n\nGenerate \npackages.txt\n within stemcell tarball that includes list of installed packages (previously known under different name)",
            "title": "3541.x"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#ubuntu-trusty-stemcell-release-notes-3541x",
            "text": "",
            "title": "Ubuntu Trusty Stemcell Release Notes, 3541.x"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#354110-2018-03-27",
            "text": "Periodic Ubuntu and CentOS stemcell bump (March 26/27, 2018)",
            "title": "3541.10 (2018-03-27)"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#35419-2018-03-12",
            "text": "Periodic Ubuntu and CentOS stemcell bump (March 12, 2018)",
            "title": "3541.9 (2018-03-12)"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#35418-2018-03-08",
            "text": "Bump bosh-agent to 2.67.1  Agent will now respect previously set permissions and owner on sys/run, sys/log and data job directories  This should fix stemcell compatibility with Diego/Garden if Agent restarts  If you were using 3541.x stemcell for any of your deployments, it's recommended to update your deployments to this version before updating Director since that would cause Agent restart",
            "title": "3541.8 (2018-03-08)"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#35415-2018-02-22",
            "text": "Bump Ubuntu Trusty stemcells for USN-3582-2: Linux kernel (Xenial HWE) vulnerabilities",
            "title": "3541.5 (2018-02-22)"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#35414-2018-02-15",
            "text": "Rolled back custom umask configuration as we found out it was different in some cases (depends on how processes were started)  Hardening of /var/vcap/jobs/* is still applied by the agent",
            "title": "3541.4 (2018-02-15)"
        },
        {
            "location": "/bosh-linux-stemcell-builder/release-notes/3541.x/#35412-2018-02-09",
            "text": "[breaking] Set default umask to 077 and further harden several /var/vcap/* directories  Note that you may have to change your release to adapt to this change  [breaking] Renamed /var/vcap/bosh/bin/ntpdate to /var/vcap/bosh/bin/sync-time  [breaking] Stop forwarding SSH events to bosh-agent  Agent no longer receives and forwards such events to HM. This should remove a lot of noisy generated by releases that expect a lot of SSH sessions (eg Gitlab). This information will continue to be available in logs forwarded to remote destinations (and locally /var/log/auth.log).  Fixes  env.bosh.swap_size: 0  to work on more clouds (including GCP)   Misc   Order stemcell tarballs so that upload-stemcell command can execute faster  Generate  packages.txt  within stemcell tarball that includes list of installed packages (previously known under different name)",
            "title": "3541.2 (2018-02-09)"
        },
        {
            "location": "/manifest-v2/",
            "text": "Note: This feature is available with bosh-release v255.4+.\n\n\n\nWarning: If you are using director version between v241 and v256, once you opt into using cloud config all deployments must be converted to use new format. If you want to deploy both v1 and v2 manifests, update to director v257+.\n\n\n\nThe deployment manifest is a YAML file that defines the components and properties of the deployment. When an operator initiates a new deployment using the CLI, the Director receives a manifest and creates or updates a deployment with matching name.\n\n\nAssuming that you are using \ncloud config\n, your deployment manifest is expected to have:\n\n\n\n\nDeployment Identification\n: A name for the deployment and the UUID of the Director managing the deployment\n\n\nFeatures Block\n: Opts into Director features to be used in this deployment\n\n\nReleases Block\n: Name and version of each release in a deployment\n\n\nStemcells Block\n: Name and version of each stemcell in a deployment\n\n\nUpdate Block\n: Defines how BOSH updates instances during deployment\n\n\nInstance Groups Block\n: Configuration and resource information for instance groups\n\n\nAddons\n: Configures deployment specific addons\n\n\nProperties Block\n: Describes global properties and generalized configuration information\n\n\nVariables Block\n: Variables configuration\n\n\nTags Block\n: Sets additional tags for the deployment\n\n\n\n\n\n\nDeployment Identification \n\u00b6\n\n\nname\n [String, required]: The name of the deployment. A single Director can manage multiple deployments and distinguishes them by name.\n\n\ndirector_uuid\n [String, required]: Not required by CLI v2. This string must match the UUID of the currently targeted Director for the CLI to allow any operations on the deployment. Use \nbosh status\n to display the UUID of the currently targeted Director.\n\n\nExample:\n\n\nname\n:\n \nmy-redis\n\n\n\n\n\n\n\nFeatures Block \n\u00b6\n\n\nfeatures\n [Hash, options]: Specifies Director features that should be used within this deployment.\n\n\n\n\nuse_dns_addresses\n [Boolean, optional]: Enables or disables returning of DNS addresses in links. Defaults to global Director \nuse_dns_addresses\n configuration.\n\n\nrandomize_az_placement\n [Boolean, optional]: Randomizes AZs for left over instances that cannot be distributed equally between AZs. For example, given an instance group with 5 instances and only 3 AZs, 1 remaining instance will be placed in randomly chosen AZ out of specified 3 AZs. Available in bosh-release v264+.\n\n\n\n\nExample:\n\n\nfeatures\n:\n\n  \nuse_dns_addresses\n:\n \ntrue\n\n\n\n\n\n\n\nReleases Block \n\u00b6\n\n\nreleases\n [Array, required]: The name and version of each release in the deployment.\n\n\n\n\nname\n [String, required]: Name of a release used in the deployment.\n\n\nversion\n [String, required]: The version of the release to use. Version can be \nlatest\n.\n\n\nurl\n [String, optional]: URL of a release to download. Works with CLI v2. Example: \nhttps://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11\n.\n\n\nsha1\n [String, optional]: SHA1 of asset referenced via URL. Works with CLI v2. Example: \n332ac15609b220a3fdf5efad0e0aa069d8235788\n.\n\n\n\n\nSee \nRelease URLs\n for more details.\n\n\nExample:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nredis\n\n  \nversion\n:\n \n12\n\n\n\n\n\nExample with a URL:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nconcourse\n\n  \nversion\n:\n \n3.3.2\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/concourse/concourse?v=3.3.2\n\n  \nsha1\n:\n \n2c876303dc6866afb845e728eab58abae8ff3be2\n\n\n\n\n\n\n\nStemcells Block \n\u00b6\n\n\nstemcells\n [Array, required]: The name and version of each stemcell in the deployment.\n\n\n\n\nalias\n [String, required]: Name of a stemcell used in the deployment\n\n\nos\n [String, optional]: Operating system of a matching stemcell. Example: \nubuntu-trusty\n.\n\n\nversion\n [String, required]: The version of a matching stemcell. Version can be \nlatest\n.\n\n\nname\n [String, optional]: Full name of a matching stemcell. Either \nname\n or \nos\n keys can be specified.\n\n\n\n\nNote: \nurl\n key is not supported in stemcells block because there is no single stemcell that works on all IaaSes. Since we want to keep deployment manifests decoupled from any cloud specific declarations we do not allow specifying URL.\n\n\nExample:\n\n\nstemcells\n:\n\n\n-\n \nalias\n:\n \ndefault\n\n  \nos\n:\n \nubuntu-trusty\n\n  \nversion\n:\n \n3147\n\n\n-\n \nalias\n:\n \ndefault2\n\n  \nname\n:\n \nbosh-aws-xen-hvm-ubuntu-trusty-go_agent\n\n  \nversion\n:\n \n3149\n\n\n\n\n\n\n\nUpdate Block \n\u00b6\n\n\nupdate\n [Hash, required]: This specifies instance update properties. These properties control how BOSH updates instances during the deployment.\n\n\n\n\ncanaries\n [Integer, required]: The number of \ncanary\n instances.\n\n\nmax_in_flight\n [Integer or Percentage, required]: The maximum number of non-canary instances to update in parallel within an availability zone.\n\n\ncanary_watch_time\n [Integer or Range, required]: Only applies to monit start operation.\n\n\nIf the \ncanary_watch_time\n is an integer, the Director sleeps for that many milliseconds, then checks whether the canary instances are healthy.\n\n\nIf the \ncanary_watch_time\n is a range (low-high), the Director:\n\n\nWaits for \nlow\n milliseconds\n\n\nWaits until instances are healthy or \nhigh\n milliseconds have passed since instances started updating\n\n\n\n\n\n\n\n\n\n\nupdate_watch_time\n [Integer or Range, required]: Only applies to monit start operation.\n\n\nIf the \nupdate_watch_time\n is an integer, the Director sleeps for that many milliseconds, then checks whether the instances are healthy.\n\n\nIf the \nupdate_watch_time\n is a range (low-high), the Director:\n\n\nWaits for \nlow\n milliseconds\n\n\nWaits until instances are healthy or \nhigh\n milliseconds have passed since instances started updating\n\n\n\n\n\n\n\n\n\n\nserial\n [Boolean, optional]: If disabled (set to \nfalse\n), instance groups will be deployed in parallel, otherwise - sequentially. Instances within a group will still follow \ncanary\n and \nmax_in_flight\n configuration. Defaults to \ntrue\n.\n\n\n\n\nSee \njob lifecycle\n for more details on startup/shutdown procedure within each VM.\n\n\nExample:\n\n\nupdate\n:\n\n  \ncanaries\n:\n \n1\n\n  \nmax_in_flight\n:\n \n10\n\n  \ncanary_watch_time\n:\n \n1000-30000\n\n  \nupdate_watch_time\n:\n \n1000-30000\n\n\n\n\n\n\n\nInstance Groups Block \n\u00b6\n\n\ninstance_groups\n [Array, required]: Specifies the mapping between release \njobs\n and instance groups.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference instance group.\n\n\nazs\n [Array, required]: List of AZs associated with this instance group (should only be used when using \nfirst class AZs\n). Example: \n[z1, z2]\n.\n\n\ninstances\n [Integer, required]: The number of instances in this group. Each instance is a VM.\n\n\njobs\n [Array, required]: Specifies the name and release of jobs that will be installed on each instance.\n\n\nname\n [String, required]: The job name\n\n\nrelease\n [String, required]: The release where the job exists\n\n\nconsumes\n [Hash, optional]: Links consumed by the job. \nRead more about link configuration\n\n\nprovides\n [Hash, optional]: Links provided by the job. \nRead more about link configuration\n\n\nproperties\n [Hash, optional]: Specifies job properties. Properties allow BOSH to configure jobs to a specific environment. \nproperties\n defined in a Job block are accessible only to that job. Only properties specified here will be provided to the job.\n\n\nvm_type\n [String, required]: A valid VM type name from the cloud config. Alternatively you can specify \nvm_resources\n key.\n\n\nvm_extensions\n [Array, optional]: A valid list of VM extension names from the cloud config.\n\n\nvm_resources\n [Hash, optional]: Specifies generic VM resources such as CPU, RAM and disk size that are automatically translated into correct VM cloud properties to determine VM size. VM size is determined on best effort basis as some IaaSes may not support exact size configuration. Currently some CPIs (Google) do not support this functionality. Available in bosh-release v264+.\n\n\ncpu\n [Integer, required]: Number of CPUs.\n\n\nram\n [Integer, required]: Amount of RAM in MB.\n\n\nephemeral_disk_size\n [Integer, required]: Ephemeral disk size in MB.\n\n\nstemcell\n [String, required]: A valid stemcell alias from the Stemcells Block.\n\n\npersistent_disk\n [Integer, optional]: Persistent disk size in MB. Alternatively you can specify \npersistent_disk_type\n key. \nRead more about persistent disks\n\n\npersistent_disk_type\n [String, optional]: A valid disk type name from the cloud config. \nRead more about persistent disks\n\n\nnetworks\n [Array, required]: Specifies the networks this instance requires. Each network can have the following properties specified:\n\n\nname\n [String, required]: A valid network name from the cloud config.\n\n\nstatic_ips\n [Array, optional]: Array of IP addresses reserved for the instances on the network.\n\n\ndefault\n [Array, optional]: Specifies which network components (DNS, Gateway) BOSH populates by default from this network. This property is required if more than one network is specified.\n\n\nupdate\n [Hash, optional]: Specific update settings for this instance group. Use this to override \nglobal job update settings\n on a per-instance-group basis.\n\n\nmigrated_from\n [Array, optional]: Specific migration settings for this instance group. Use this to \nrename and/or migrate instance groups\n.\n\n\nlifecycle\n [String, optional]: Specifies the kind of workload the instance group represents. Valid values are \nservice\n and \nerrand\n; defaults to \nservice\n. A \nservice\n runs indefinitely and restarts if it fails. An \nerrand\n starts with a manual trigger and does not restart if it fails.\n\n\nproperties\n [Hash, optional]: Specifies instance group properties. Deprecated in favor of job level properties and links.\n\n\nenv\n [Hash, optional]: Specifies advanced BOSH Agent configuration for each instance in the group.\n\n\nbosh\n [Hash, optional]:\n\n\npassword\n [String, optional]: Crypted password for \nvcap/root\n user (will be placed into /etc/shadow on Linux).\n\n\nkeep_root_password\n [Boolean, optional]: Keep password for \nroot\n and only change password for \nvcap\n. Default: \nfalse\n.\n\n\nremove_dev_tools\n [Boolean, optional]: Remove \ncompilers and dev tools\n on non-compilation VMs. Default: \nfalse\n.\n\n\nremove_static_libraries\n [Boolean, optional]: Remove \nstatic libraries\n on non-compilation VMs. Default: \nfalse\n.\n\n\nswap_size\n [Integer, optional]: Size of swap partition in MB to create. Set this to 0 to avoid having a swap partition created. Default: RAM size of used VM type up to half of the ephemeral disk size.\n\n\nipv6\n [Hash, optional]:\n\n\nenable\n [Boolean, optional]: Force IPv6 enabled in kernel (this configuration is not necessary if one of the VM addresses is IPv6). Default: \nfalse\n.\n\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nredis-master\n\n  \ninstances\n:\n \n1\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nredis-server\n\n    \nrelease\n:\n \nredis\n\n    \nproperties\n:\n\n      \nport\n:\n \n3606\n\n  \nvm_type\n:\n \nmedium\n\n  \nvm_extensions\n:\n \n[\npublic-lbs\n]\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk_type\n:\n \nmedium\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n-\n \nname\n:\n \nredis-slave\n\n  \ninstances\n:\n \n2\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nredis-server\n\n    \nrelease\n:\n \nredis\n\n    \nproperties\n:\n \n{}\n\n  \nvm_type\n:\n \nmedium\n\n  \nstemcell\n:\n \ndefault\n\n  \npersistent_disk_type\n:\n \nmedium\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nAddons Block \n\u00b6\n\n\nNote: This feature is available with bosh-release v262+.\n\n\n\naddons\n [Array, required]: Specifies the \naddons\n to be applied to this deployments.\n\n\nSee \nAddons Block\n for the schema.\n\n\nUnlike addons specified in a runtime config, addons specified in the deployment manifest do not respect inclusion and exclusion rules for \ndeployments\n.\n\n\nExample:\n\n\naddons:\n- name: logging\n  jobs:\n  - name: logging-agent\n    release: logging\n    properties:\n      ...\n\n\n\n\n\n\nProperties Block \n\u00b6\n\n\nproperties\n [Hash, optional]: Describes global properties. Deprecated in favor of job level properties and links.\n\n\n\n\nVariables Block \n\u00b6\n\n\nvariables\n [Array, optional]: Describes variables.\n\n\n\n\nname\n [String, required]: Unique name used to identify a variable. Example: \nadmin_password\n\n\ntype\n [String, required]: Type of a variable. Currently supported variable types are \ncertificate\n, \npassword\n, \nrsa\n, and \nssh\n. Example: \npassword\n.\n\n\noptions\n [Hash, optional]: Specifies generation options used for generating variable value if variable is not found. Example: \n{is_ca: true, common_name: some-ca}\n\n\n\n\nExample:\n\n\nvariables:\n- name: admin_password\n  type: password\n- name: default_ca\n  type: certificate\n  options:\n    is_ca: true\n    common_name: some-ca\n- name: director_ssl\n  type: certificate\n  options:\n    ca: default_ca\n    common_name: cc.cf.internal\n    alternative_names: [cc.cf.internal]\n\n\n\n\nSee \nCLI Variable Interpolation\n for more details about variables.\n\n\n\n\nTags Block \n\u00b6\n\n\ntags\n [Hash, optional]: Specifies key value pairs to be sent to the CPI for VM tagging. Combined with runtime config level tags during the deploy. Available in bosh-release v258+.\n\n\nExample:\n\n\ntags\n:\n\n  \nproject\n:\n \ncf",
            "title": "Deployment Config"
        },
        {
            "location": "/manifest-v2/#deployment-identification",
            "text": "name  [String, required]: The name of the deployment. A single Director can manage multiple deployments and distinguishes them by name.  director_uuid  [String, required]: Not required by CLI v2. This string must match the UUID of the currently targeted Director for the CLI to allow any operations on the deployment. Use  bosh status  to display the UUID of the currently targeted Director.  Example:  name :   my-redis",
            "title": "Deployment Identification "
        },
        {
            "location": "/manifest-v2/#features-block",
            "text": "features  [Hash, options]: Specifies Director features that should be used within this deployment.   use_dns_addresses  [Boolean, optional]: Enables or disables returning of DNS addresses in links. Defaults to global Director  use_dns_addresses  configuration.  randomize_az_placement  [Boolean, optional]: Randomizes AZs for left over instances that cannot be distributed equally between AZs. For example, given an instance group with 5 instances and only 3 AZs, 1 remaining instance will be placed in randomly chosen AZ out of specified 3 AZs. Available in bosh-release v264+.   Example:  features : \n   use_dns_addresses :   true",
            "title": "Features Block "
        },
        {
            "location": "/manifest-v2/#releases-block",
            "text": "releases  [Array, required]: The name and version of each release in the deployment.   name  [String, required]: Name of a release used in the deployment.  version  [String, required]: The version of the release to use. Version can be  latest .  url  [String, optional]: URL of a release to download. Works with CLI v2. Example:  https://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11 .  sha1  [String, optional]: SHA1 of asset referenced via URL. Works with CLI v2. Example:  332ac15609b220a3fdf5efad0e0aa069d8235788 .   See  Release URLs  for more details.  Example:  releases :  -   name :   redis \n   version :   12   Example with a URL:  releases :  -   name :   concourse \n   version :   3.3.2 \n   url :   https://bosh.io/d/github.com/concourse/concourse?v=3.3.2 \n   sha1 :   2c876303dc6866afb845e728eab58abae8ff3be2",
            "title": "Releases Block "
        },
        {
            "location": "/manifest-v2/#stemcells-block",
            "text": "stemcells  [Array, required]: The name and version of each stemcell in the deployment.   alias  [String, required]: Name of a stemcell used in the deployment  os  [String, optional]: Operating system of a matching stemcell. Example:  ubuntu-trusty .  version  [String, required]: The version of a matching stemcell. Version can be  latest .  name  [String, optional]: Full name of a matching stemcell. Either  name  or  os  keys can be specified.   Note:  url  key is not supported in stemcells block because there is no single stemcell that works on all IaaSes. Since we want to keep deployment manifests decoupled from any cloud specific declarations we do not allow specifying URL.  Example:  stemcells :  -   alias :   default \n   os :   ubuntu-trusty \n   version :   3147  -   alias :   default2 \n   name :   bosh-aws-xen-hvm-ubuntu-trusty-go_agent \n   version :   3149",
            "title": "Stemcells Block "
        },
        {
            "location": "/manifest-v2/#update-block",
            "text": "update  [Hash, required]: This specifies instance update properties. These properties control how BOSH updates instances during the deployment.   canaries  [Integer, required]: The number of  canary  instances.  max_in_flight  [Integer or Percentage, required]: The maximum number of non-canary instances to update in parallel within an availability zone.  canary_watch_time  [Integer or Range, required]: Only applies to monit start operation.  If the  canary_watch_time  is an integer, the Director sleeps for that many milliseconds, then checks whether the canary instances are healthy.  If the  canary_watch_time  is a range (low-high), the Director:  Waits for  low  milliseconds  Waits until instances are healthy or  high  milliseconds have passed since instances started updating      update_watch_time  [Integer or Range, required]: Only applies to monit start operation.  If the  update_watch_time  is an integer, the Director sleeps for that many milliseconds, then checks whether the instances are healthy.  If the  update_watch_time  is a range (low-high), the Director:  Waits for  low  milliseconds  Waits until instances are healthy or  high  milliseconds have passed since instances started updating      serial  [Boolean, optional]: If disabled (set to  false ), instance groups will be deployed in parallel, otherwise - sequentially. Instances within a group will still follow  canary  and  max_in_flight  configuration. Defaults to  true .   See  job lifecycle  for more details on startup/shutdown procedure within each VM.  Example:  update : \n   canaries :   1 \n   max_in_flight :   10 \n   canary_watch_time :   1000-30000 \n   update_watch_time :   1000-30000",
            "title": "Update Block "
        },
        {
            "location": "/manifest-v2/#instance-groups-block",
            "text": "instance_groups  [Array, required]: Specifies the mapping between release  jobs  and instance groups.   name  [String, required]: A unique name used to identify and reference instance group.  azs  [Array, required]: List of AZs associated with this instance group (should only be used when using  first class AZs ). Example:  [z1, z2] .  instances  [Integer, required]: The number of instances in this group. Each instance is a VM.  jobs  [Array, required]: Specifies the name and release of jobs that will be installed on each instance.  name  [String, required]: The job name  release  [String, required]: The release where the job exists  consumes  [Hash, optional]: Links consumed by the job.  Read more about link configuration  provides  [Hash, optional]: Links provided by the job.  Read more about link configuration  properties  [Hash, optional]: Specifies job properties. Properties allow BOSH to configure jobs to a specific environment.  properties  defined in a Job block are accessible only to that job. Only properties specified here will be provided to the job.  vm_type  [String, required]: A valid VM type name from the cloud config. Alternatively you can specify  vm_resources  key.  vm_extensions  [Array, optional]: A valid list of VM extension names from the cloud config.  vm_resources  [Hash, optional]: Specifies generic VM resources such as CPU, RAM and disk size that are automatically translated into correct VM cloud properties to determine VM size. VM size is determined on best effort basis as some IaaSes may not support exact size configuration. Currently some CPIs (Google) do not support this functionality. Available in bosh-release v264+.  cpu  [Integer, required]: Number of CPUs.  ram  [Integer, required]: Amount of RAM in MB.  ephemeral_disk_size  [Integer, required]: Ephemeral disk size in MB.  stemcell  [String, required]: A valid stemcell alias from the Stemcells Block.  persistent_disk  [Integer, optional]: Persistent disk size in MB. Alternatively you can specify  persistent_disk_type  key.  Read more about persistent disks  persistent_disk_type  [String, optional]: A valid disk type name from the cloud config.  Read more about persistent disks  networks  [Array, required]: Specifies the networks this instance requires. Each network can have the following properties specified:  name  [String, required]: A valid network name from the cloud config.  static_ips  [Array, optional]: Array of IP addresses reserved for the instances on the network.  default  [Array, optional]: Specifies which network components (DNS, Gateway) BOSH populates by default from this network. This property is required if more than one network is specified.  update  [Hash, optional]: Specific update settings for this instance group. Use this to override  global job update settings  on a per-instance-group basis.  migrated_from  [Array, optional]: Specific migration settings for this instance group. Use this to  rename and/or migrate instance groups .  lifecycle  [String, optional]: Specifies the kind of workload the instance group represents. Valid values are  service  and  errand ; defaults to  service . A  service  runs indefinitely and restarts if it fails. An  errand  starts with a manual trigger and does not restart if it fails.  properties  [Hash, optional]: Specifies instance group properties. Deprecated in favor of job level properties and links.  env  [Hash, optional]: Specifies advanced BOSH Agent configuration for each instance in the group.  bosh  [Hash, optional]:  password  [String, optional]: Crypted password for  vcap/root  user (will be placed into /etc/shadow on Linux).  keep_root_password  [Boolean, optional]: Keep password for  root  and only change password for  vcap . Default:  false .  remove_dev_tools  [Boolean, optional]: Remove  compilers and dev tools  on non-compilation VMs. Default:  false .  remove_static_libraries  [Boolean, optional]: Remove  static libraries  on non-compilation VMs. Default:  false .  swap_size  [Integer, optional]: Size of swap partition in MB to create. Set this to 0 to avoid having a swap partition created. Default: RAM size of used VM type up to half of the ephemeral disk size.  ipv6  [Hash, optional]:  enable  [Boolean, optional]: Force IPv6 enabled in kernel (this configuration is not necessary if one of the VM addresses is IPv6). Default:  false .       Example:  instance_groups :  -   name :   redis-master \n   instances :   1 \n   azs :   [ z1 ,   z2 ] \n   jobs : \n   -   name :   redis-server \n     release :   redis \n     properties : \n       port :   3606 \n   vm_type :   medium \n   vm_extensions :   [ public-lbs ] \n   stemcell :   default \n   persistent_disk_type :   medium \n   networks : \n   -   name :   default  -   name :   redis-slave \n   instances :   2 \n   azs :   [ z1 ,   z2 ] \n   jobs : \n   -   name :   redis-server \n     release :   redis \n     properties :   {} \n   vm_type :   medium \n   stemcell :   default \n   persistent_disk_type :   medium \n   networks : \n   -   name :   default",
            "title": "Instance Groups Block "
        },
        {
            "location": "/manifest-v2/#addons-block",
            "text": "Note: This feature is available with bosh-release v262+.  addons  [Array, required]: Specifies the  addons  to be applied to this deployments.  See  Addons Block  for the schema.  Unlike addons specified in a runtime config, addons specified in the deployment manifest do not respect inclusion and exclusion rules for  deployments .  Example:  addons:\n- name: logging\n  jobs:\n  - name: logging-agent\n    release: logging\n    properties:\n      ...",
            "title": "Addons Block "
        },
        {
            "location": "/manifest-v2/#properties-block",
            "text": "properties  [Hash, optional]: Describes global properties. Deprecated in favor of job level properties and links.",
            "title": "Properties Block "
        },
        {
            "location": "/manifest-v2/#variables-block",
            "text": "variables  [Array, optional]: Describes variables.   name  [String, required]: Unique name used to identify a variable. Example:  admin_password  type  [String, required]: Type of a variable. Currently supported variable types are  certificate ,  password ,  rsa , and  ssh . Example:  password .  options  [Hash, optional]: Specifies generation options used for generating variable value if variable is not found. Example:  {is_ca: true, common_name: some-ca}   Example:  variables:\n- name: admin_password\n  type: password\n- name: default_ca\n  type: certificate\n  options:\n    is_ca: true\n    common_name: some-ca\n- name: director_ssl\n  type: certificate\n  options:\n    ca: default_ca\n    common_name: cc.cf.internal\n    alternative_names: [cc.cf.internal]  See  CLI Variable Interpolation  for more details about variables.",
            "title": "Variables Block "
        },
        {
            "location": "/manifest-v2/#tags-block",
            "text": "tags  [Hash, optional]: Specifies key value pairs to be sent to the CPI for VM tagging. Combined with runtime config level tags during the deploy. Available in bosh-release v258+.  Example:  tags : \n   project :   cf",
            "title": "Tags Block "
        },
        {
            "location": "/cloud-config/",
            "text": "Warning: If you are using Director version between v241 and v256, once you opt into using cloud config all deployments must be converted to use new format. If you want to deploy both v1 and v2 manifests, update to Director v257+.\n\n\n\nPreviously each deployment manifest specified IaaS and IaaS agnostic configuration in a single file. As more deployments are managed by the Director, it becomes inconvenient to keep shared IaaS configuration in sync in all deployment manifests. In addition, multiple deployments typically want to use the same network subnet, hence IP ranges need to be separated and reserved.\n\n\nThe cloud config is a YAML file that defines IaaS specific configuration used by the Director and all deployments. It allows us to separate IaaS specific configuration into its own file and keep deployment manifests IaaS agnostic.\n\n\n\n\nUpdating and retrieving cloud config \n\u00b6\n\n\nTo update cloud config on the Director use \nbosh update-cloud-config\n command\n.\n\n\nNote: See \nexample cloud config\n for AWS below.\n\n\n\n$ bosh -e vbox update-cloud-config cloud.yml\n\n$ bosh -e vbox cloud-config\nActing as user \n'admin'\n on \n'micro'\n\n\nazs:\n- name: z1\n  cloud_properties:\n    availability_zone: us-east-1b\n- name: z2\n  cloud_properties:\n    availability_zone: us-east-1c\n...\n\n\n\n\nOnce cloud config is updated, all existing deployments will be considered outdated, as indicated by \nbosh deployments\n command\n. The Director will apply cloud config changes to each deployment during the next run of \nbosh deploy\n command for that deployment.\n\n\n$ bosh -e vbox deployments\nUsing environment \n'192.168.56.6'\n as \n'?'\n\n\nName       Release\n(\ns\n)\n       Stemcell\n(\ns\n)\n             Team\n(\ns\n)\n  Cloud Config\nzookeeper  zookeeper/0.0.5  bosh-warden-.../3421.4  -        outdated\n\n\n1\n deployment\n\nSucceeded\n\n\n\n\n\n\nExample \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n \n{\navailability_zone\n:\n \nus-east-1a\n}\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n \n{\navailability_zone\n:\n \nus-east-1b\n}\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \nsmall\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nt2.micro\n\n    \nephemeral_disk\n:\n \n{\nsize\n:\n \n3000\n,\n \ntype\n:\n \ngp2\n}\n\n\n-\n \nname\n:\n \nmedium\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm3.medium\n\n    \nephemeral_disk\n:\n \n{\nsize\n:\n \n30000\n,\n \ntype\n:\n \ngp2\n}\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \nsmall\n\n  \ndisk_size\n:\n \n3000\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n-\n \nname\n:\n \nlarge\n\n  \ndisk_size\n:\n \n50_000\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nprivate\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \naz\n:\n \nz1\n\n    \nstatic\n:\n \n[\n10.10.0.62\n]\n\n    \ndns\n:\n \n[\n10.10.0.2\n]\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-f2744a86\n}\n\n  \n-\n \nrange\n:\n \n10.10.64.0/24\n\n    \ngateway\n:\n \n10.10.64.1\n\n    \naz\n:\n \nz2\n\n    \nstatic\n:\n \n[\n10.10.64.121\n,\n \n10.10.64.122\n]\n\n    \ndns\n:\n \n[\n10.10.0.2\n]\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-eb8bd3ad\n}\n\n\n-\n \nname\n:\n \nvip\n\n  \ntype\n:\n \nvip\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \nmedium\n\n  \nnetwork\n:\n \nprivate\n\n\n\n\n\n\n\nSee AWS CPI example\n\n\nSee Azure CPI example\n\n\nSee OpenStack CPI example\n\n\nSee SoftLayer CPI example\n\n\nSee Google Cloud Platform CPI example\n\n\nSee vSphere CPI example\n\n\n\n\n\n\nAZs Block \n\u00b6\n\n\nazs\n [Array, required]: Specifies the AZs available to deployments. At least one should be specified.\n\n\n\n\nname\n [String, required]: Name of an AZ within the Director.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to associated with AZ; for most IaaSes, some data here is actually required. See \nCPI Specific \ncloud_properties\n below. Example: \navailability_zone\n. Default is \n{}\n (empty Hash).\n\n\n\n\nSee \nfirst class AZs\n for more details.\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1c\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1d\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI AZ cloud properties\n\n\nSee Azure CPI AZ cloud properties\n\n\nSee OpenStack CPI AZ cloud properties\n\n\nSee SoftLayer CPI AZ cloud properties\n\n\nSee Google Cloud Platform CPI AZ cloud properties\n\n\nSee vSphere CPI AZ cloud properties\n\n\nSee vCloud CPI AZ cloud properties\n\n\n\n\n\n\nNetworks Block \n\u00b6\n\n\nnetworks\n [Array, required]: Each sub-block listed in the Networks block specifies a network configuration that jobs can reference. There are three different network types: \nmanual\n, \ndynamic\n, and \nvip\n. At least one should be specified.\n\n\nSee \nnetworks\n for more details.\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI network cloud properties\n\n\nSee Azure CPI network cloud properties\n\n\nSee OpenStack CPI network cloud properties\n\n\nSee SoftLayer CPI network cloud properties\n\n\nSee Google Cloud Plaform CPI network cloud properties\n\n\nSee vSphere CPI network cloud properties\n\n\nSee vCloud CPI network cloud properties\n\n\n\n\n\n\nVM Types Block \n\u00b6\n\n\nvm_types\n [Array, required]: Specifies the \nVM types\n available to deployments. At least one should be specified.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the VM type\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create VMs; for most IaaSes, some data here is actually required. See \nCPI Specific \ncloud_properties\n below. Example: \ninstance_type\n:\n \nm3\n.\nmedium\n. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI VM types cloud properties\n\n\nSee Azure CPI VM types cloud properties\n\n\nSee OpenStack CPI VM types cloud properties\n\n\nSee SoftLayer CPI VM types cloud properties\n\n\nSee Google Cloud Platform CPI VM types cloud properties\n\n\nSee vSphere CPI VM types cloud properties\n\n\nSee vCloud CPI VM types cloud properties\n\n\n\n\n\n\nVM Extensions Block \n\u00b6\n\n\nNote: This feature is available with bosh-release v255.4+.\n\n\n\nvm_extensions\n [Array, optional]: Specifies the \nVM extensions\n available to deployments.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the VM extension\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to configure VMs. Example: \nelbs: [...]\n. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nvm_extensions\n:\n\n\n-\n \nname\n:\n \npub-lbs\n\n  \ncloud_properties\n:\n\n    \nelbs\n:\n \n[\nmain\n]\n\n\n\n\n\nAny IaaS specific configuration could be placed into a VM extension's \ncloud_properties\n.\n\n\n\n\nDisk Types Block \n\u00b6\n\n\ndisk_types\n [Array, required]: Specifies the \ndisk types\n available to deployments. At least one should be specified.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the disk type\n\n\ndisk_size\n [Integer, required]: Specifies the disk size. \ndisk_size\n must be a positive integer. BOSH creates a \npersistent disk\n of that size in megabytes and attaches it to each job instance VM.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create disks. Examples: \ntype\n, \niops\n. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n2\n\n  \ncloud_properties\n:\n\n    \ntype\n:\n \ngp2\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI disk type cloud properties\n\n\nSee Azure CPI disk type cloud properties\n\n\nSee OpenStack CPI disk type cloud properties\n\n\nSee SoftLayer CPI disk type cloud properties\n\n\nSee Google Cloud Platform CPI disk type cloud properties\n\n\nSee vSphere CPI disk type cloud properties\n\n\nSee vCloud CPI disk type cloud properties\n\n\n\n\n\n\nCompilation Block \n\u00b6\n\n\nThe Director creates compilation VMs for release compilation. The Director will compile each release on every necessary stemcell used in a deployment. A compilation definition allows to specify VM characteristics.\n\n\ncompilation\n [Hash, required]: Properties of compilation VMs.\n\n\n\n\nworkers\n [Integer, required]: The maximum number of compilation VMs.\n\n\naz\n [String, required]: Name of the AZ defined in AZs section to use for creating compilation VMs.\n\n\nvm_type\n [String, optional]: Name of the VM type defined in VM types section to use for creating compilation VMs. Alternatively, you can specify the \nvm_resources\n, or \ncloud_properties\n key.\n\n\nvm_resources\n [Hash, optional]: Specifies generic VM resources such as CPU, RAM and disk size that are automatically translated into correct VM cloud properties to determine VM size. VM size is determined on best effort basis as some IaaSes may not support exact size configuration. Currently some CPIs (Google and Azure) do not support this functionality. Available in bosh-release v264+.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create VMs. Most IaaSes require this. Examples: \ninstance_type\n, \navailability_zone\n. Default is \n{}\n (empty Hash).\n\n\nnetwork\n [String, required]: References a valid network name defined in the Networks block. BOSH assigns network properties to compilation VMs according to the type and properties of the specified network.\n\n\nreuse_compilation_vms\n [Boolean, optional]: If \nfalse\n, BOSH creates a new compilation VM for each new package compilation and destroys the VM when compilation is complete. If \ntrue\n, compilation VMs are re-used when compiling packages. Defaults to \nfalse\n.\n\n\nenv\n [Hash, optional]: Same as \nenv\n for instance groups\n.\n\n\n\n\nExample:\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n2\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetwork\n:\n \nprivate",
            "title": "Usage"
        },
        {
            "location": "/cloud-config/#updating-and-retrieving-cloud-config",
            "text": "To update cloud config on the Director use  bosh update-cloud-config  command .  Note: See  example cloud config  for AWS below.  $ bosh -e vbox update-cloud-config cloud.yml\n\n$ bosh -e vbox cloud-config\nActing as user  'admin'  on  'micro' \n\nazs:\n- name: z1\n  cloud_properties:\n    availability_zone: us-east-1b\n- name: z2\n  cloud_properties:\n    availability_zone: us-east-1c\n...  Once cloud config is updated, all existing deployments will be considered outdated, as indicated by  bosh deployments  command . The Director will apply cloud config changes to each deployment during the next run of  bosh deploy  command for that deployment.  $ bosh -e vbox deployments\nUsing environment  '192.168.56.6'  as  '?' \n\nName       Release ( s )        Stemcell ( s )              Team ( s )   Cloud Config\nzookeeper  zookeeper/0.0.5  bosh-warden-.../3421.4  -        outdated 1  deployment\n\nSucceeded",
            "title": "Updating and retrieving cloud config "
        },
        {
            "location": "/cloud-config/#example",
            "text": "azs :  -   name :   z1 \n   cloud_properties :   { availability_zone :   us-east-1a }  -   name :   z2 \n   cloud_properties :   { availability_zone :   us-east-1b }  vm_types :  -   name :   small \n   cloud_properties : \n     instance_type :   t2.micro \n     ephemeral_disk :   { size :   3000 ,   type :   gp2 }  -   name :   medium \n   cloud_properties : \n     instance_type :   m3.medium \n     ephemeral_disk :   { size :   30000 ,   type :   gp2 }  disk_types :  -   name :   small \n   disk_size :   3000 \n   cloud_properties :   { type :   gp2 }  -   name :   large \n   disk_size :   50_000 \n   cloud_properties :   { type :   gp2 }  networks :  -   name :   private \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     az :   z1 \n     static :   [ 10.10.0.62 ] \n     dns :   [ 10.10.0.2 ] \n     cloud_properties :   { subnet :   subnet-f2744a86 } \n   -   range :   10.10.64.0/24 \n     gateway :   10.10.64.1 \n     az :   z2 \n     static :   [ 10.10.64.121 ,   10.10.64.122 ] \n     dns :   [ 10.10.0.2 ] \n     cloud_properties :   { subnet :   subnet-eb8bd3ad }  -   name :   vip \n   type :   vip  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   medium \n   network :   private    See AWS CPI example  See Azure CPI example  See OpenStack CPI example  See SoftLayer CPI example  See Google Cloud Platform CPI example  See vSphere CPI example",
            "title": "Example "
        },
        {
            "location": "/cloud-config/#azs-block",
            "text": "azs  [Array, required]: Specifies the AZs available to deployments. At least one should be specified.   name  [String, required]: Name of an AZ within the Director.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to associated with AZ; for most IaaSes, some data here is actually required. See  CPI Specific  cloud_properties  below. Example:  availability_zone . Default is  {}  (empty Hash).   See  first class AZs  for more details.  Example:  azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   us-east-1c  -   name :   z2 \n   cloud_properties : \n     availability_zone :   us-east-1d",
            "title": "AZs Block "
        },
        {
            "location": "/cloud-config/#cpi-specific-cloud_properties",
            "text": "See AWS CPI AZ cloud properties  See Azure CPI AZ cloud properties  See OpenStack CPI AZ cloud properties  See SoftLayer CPI AZ cloud properties  See Google Cloud Platform CPI AZ cloud properties  See vSphere CPI AZ cloud properties  See vCloud CPI AZ cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/cloud-config/#networks-block",
            "text": "networks  [Array, required]: Each sub-block listed in the Networks block specifies a network configuration that jobs can reference. There are three different network types:  manual ,  dynamic , and  vip . At least one should be specified.  See  networks  for more details.",
            "title": "Networks Block "
        },
        {
            "location": "/cloud-config/#cpi-specific-cloud_properties_1",
            "text": "See AWS CPI network cloud properties  See Azure CPI network cloud properties  See OpenStack CPI network cloud properties  See SoftLayer CPI network cloud properties  See Google Cloud Plaform CPI network cloud properties  See vSphere CPI network cloud properties  See vCloud CPI network cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/cloud-config/#vm-types-block",
            "text": "vm_types  [Array, required]: Specifies the  VM types  available to deployments. At least one should be specified.   name  [String, required]: A unique name used to identify and reference the VM type  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to create VMs; for most IaaSes, some data here is actually required. See  CPI Specific  cloud_properties  below. Example:  instance_type :   m3 . medium . Default is  {}  (empty Hash).   Example:  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   m1.small",
            "title": "VM Types Block "
        },
        {
            "location": "/cloud-config/#cpi-specific-cloud_properties_2",
            "text": "See AWS CPI VM types cloud properties  See Azure CPI VM types cloud properties  See OpenStack CPI VM types cloud properties  See SoftLayer CPI VM types cloud properties  See Google Cloud Platform CPI VM types cloud properties  See vSphere CPI VM types cloud properties  See vCloud CPI VM types cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/cloud-config/#vm-extensions-block",
            "text": "Note: This feature is available with bosh-release v255.4+.  vm_extensions  [Array, optional]: Specifies the  VM extensions  available to deployments.   name  [String, required]: A unique name used to identify and reference the VM extension  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to configure VMs. Example:  elbs: [...] . Default is  {}  (empty Hash).   Example:  vm_extensions :  -   name :   pub-lbs \n   cloud_properties : \n     elbs :   [ main ]   Any IaaS specific configuration could be placed into a VM extension's  cloud_properties .",
            "title": "VM Extensions Block "
        },
        {
            "location": "/cloud-config/#disk-types-block",
            "text": "disk_types  [Array, required]: Specifies the  disk types  available to deployments. At least one should be specified.   name  [String, required]: A unique name used to identify and reference the disk type  disk_size  [Integer, required]: Specifies the disk size.  disk_size  must be a positive integer. BOSH creates a  persistent disk  of that size in megabytes and attaches it to each job instance VM.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to create disks. Examples:  type ,  iops . Default is  {}  (empty Hash).   Example:  disk_types :  -   name :   default \n   disk_size :   2 \n   cloud_properties : \n     type :   gp2",
            "title": "Disk Types Block "
        },
        {
            "location": "/cloud-config/#cpi-specific-cloud_properties_3",
            "text": "See AWS CPI disk type cloud properties  See Azure CPI disk type cloud properties  See OpenStack CPI disk type cloud properties  See SoftLayer CPI disk type cloud properties  See Google Cloud Platform CPI disk type cloud properties  See vSphere CPI disk type cloud properties  See vCloud CPI disk type cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/cloud-config/#compilation-block",
            "text": "The Director creates compilation VMs for release compilation. The Director will compile each release on every necessary stemcell used in a deployment. A compilation definition allows to specify VM characteristics.  compilation  [Hash, required]: Properties of compilation VMs.   workers  [Integer, required]: The maximum number of compilation VMs.  az  [String, required]: Name of the AZ defined in AZs section to use for creating compilation VMs.  vm_type  [String, optional]: Name of the VM type defined in VM types section to use for creating compilation VMs. Alternatively, you can specify the  vm_resources , or  cloud_properties  key.  vm_resources  [Hash, optional]: Specifies generic VM resources such as CPU, RAM and disk size that are automatically translated into correct VM cloud properties to determine VM size. VM size is determined on best effort basis as some IaaSes may not support exact size configuration. Currently some CPIs (Google and Azure) do not support this functionality. Available in bosh-release v264+.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to create VMs. Most IaaSes require this. Examples:  instance_type ,  availability_zone . Default is  {}  (empty Hash).  network  [String, required]: References a valid network name defined in the Networks block. BOSH assigns network properties to compilation VMs according to the type and properties of the specified network.  reuse_compilation_vms  [Boolean, optional]: If  false , BOSH creates a new compilation VM for each new package compilation and destroys the VM when compilation is complete. If  true , compilation VMs are re-used when compiling packages. Defaults to  false .  env  [Hash, optional]: Same as  env  for instance groups .   Example:  compilation : \n   workers :   2 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   default \n   network :   private",
            "title": "Compilation Block "
        },
        {
            "location": "/azs/",
            "text": "Note: This feature is available with bosh-release v241+. Once you opt into using cloud config all deployments must be converted to use new format. There is no way to opt out of the cloud config once you opt in.\n\n\n\nPreviously to spread resources over multiple AZs, deployment jobs, resource pools, and networks had to be duplicated and named differently in the deployment manifest. By convention all of these resources were suffixed with \"_z1\" or \"zX\" to indicate which AZ they belonged to.\n\n\nWith first class AZs support in the Director it's no longer necessary to duplicate and rename resources. This allows the Director to eliminate and/or simplify manual configuration for balancing VMs across AZs and IP address management.\n\n\n\n\nDefining AZs \n\u00b6\n\n\nTo use first class AZs, you have to opt into using \ncloud config\n.\n\n\nHere is how AZ configuration looks like for two AZs on AWS.\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1b\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1c\n\n\n...\n\n\n\n\n\nNote that IaaS specific cloud properties related to AZs should now be \nonly\n placed under \nazs\n. Make sure to remove them from \nresource_pools/vm_types\n' cloud properties.\n\n\n\nAZs schema:\n\n\n\n\n\n\nazs\n [Array, required]: List of AZs.\n\n\n\n\n\n\nname\n [String, required]: Name of an AZ within the Director.\n\n\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to associated with AZ; for most IaaSes, some data here is actually required. See \nCPI Specific \ncloud_properties\n below. Example: \navailability_zone\n. Default is \n{}\n (empty Hash).\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI AZ cloud properties\n\n\nSee Azure CPI AZ cloud properties\n\n\nSee OpenStack CPI AZ cloud properties\n\n\nSee SoftLayer CPI AZ cloud properties\n\n\nSee Google Cloud Platform CPI AZ cloud properties\n\n\nSee vSphere CPI AZ cloud properties\n\n\nSee vCloud CPI AZ cloud properties\n\n\n\n\n\n\nAssigning AZs to deployment instance groups \n\u00b6\n\n\nOnce AZs are defined, deployment jobs can be placed into one or more AZs:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nweb\n\n  \ninstances\n:\n \n5\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n  \ntemplates\n:\n\n  \n-\n \nname\n:\n \nweb\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nprivate\n\n\n\n\n\nGiven above configuration, 5 instances will be spread over \"z1\" and \"z2\" AZs, most likely creating 3 instances in \"z1\" and 2 instances in \"z2\". There are several consideration the Director takes into account while determining how instances should be spread:\n\n\n\n\nnew instances will be spread as evenly as possible over specified AZs\n\n\nexisting instances will be preserved if possible but will be rebalanced if necessary to even out distribution\n\n\nexisting instances with persistent disks will not be rebalanced to avoid losing persistent data\n\n\nexisting instances in a removed AZ will be removed and their \npersistent disks will be orphaned\n\n\nif static IPs are specified on one or more networks, AZ selection is focused to satisfy IPs' AZ assignment\n\n\n\n\nWe are planning to eventually introduce \nbosh rebalance\n command to forcefully rebalance instances with persistent disks.\n\n\n---\n## Listing VMs in AZs \n\n\nWhile deploy is in progress or after it finishes, `bosh instances` and `bosh vms` commands can be used to view instances and their associated AZs.\n\n\u0002wzxhzdk:2\u0003",
            "title": "Availability Zones"
        },
        {
            "location": "/azs/#defining-azs",
            "text": "To use first class AZs, you have to opt into using  cloud config .  Here is how AZ configuration looks like for two AZs on AWS.  azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   us-east-1b  -   name :   z2 \n   cloud_properties : \n     availability_zone :   us-east-1c  ...   Note that IaaS specific cloud properties related to AZs should now be  only  placed under  azs . Make sure to remove them from  resource_pools/vm_types ' cloud properties.  AZs schema:    azs  [Array, required]: List of AZs.    name  [String, required]: Name of an AZ within the Director.   cloud_properties  [Hash, optional]: Describes any IaaS-specific properties needed to associated with AZ; for most IaaSes, some data here is actually required. See  CPI Specific  cloud_properties  below. Example:  availability_zone . Default is  {}  (empty Hash).",
            "title": "Defining AZs "
        },
        {
            "location": "/azs/#cpi-specific-cloud_properties",
            "text": "See AWS CPI AZ cloud properties  See Azure CPI AZ cloud properties  See OpenStack CPI AZ cloud properties  See SoftLayer CPI AZ cloud properties  See Google Cloud Platform CPI AZ cloud properties  See vSphere CPI AZ cloud properties  See vCloud CPI AZ cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/azs/#assigning-azs-to-deployment-instance-groups",
            "text": "Once AZs are defined, deployment jobs can be placed into one or more AZs:  jobs :  -   name :   web \n   instances :   5 \n   azs :   [ z1 ,   z2 ] \n   templates : \n   -   name :   web \n   networks : \n   -   name :   private   Given above configuration, 5 instances will be spread over \"z1\" and \"z2\" AZs, most likely creating 3 instances in \"z1\" and 2 instances in \"z2\". There are several consideration the Director takes into account while determining how instances should be spread:   new instances will be spread as evenly as possible over specified AZs  existing instances will be preserved if possible but will be rebalanced if necessary to even out distribution  existing instances with persistent disks will not be rebalanced to avoid losing persistent data  existing instances in a removed AZ will be removed and their  persistent disks will be orphaned  if static IPs are specified on one or more networks, AZ selection is focused to satisfy IPs' AZ assignment   We are planning to eventually introduce  bosh rebalance  command to forcefully rebalance instances with persistent disks. \n\n---\n## Listing VMs in AZs  \n\nWhile deploy is in progress or after it finishes, `bosh instances` and `bosh vms` commands can be used to view instances and their associated AZs.\n\n\u0002wzxhzdk:2\u0003",
            "title": "Assigning AZs to deployment instance groups "
        },
        {
            "location": "/networks/",
            "text": "A BOSH network is an IaaS-agnostic representation of the networking layer. The Director is responsible for configuring each deployment job's networks with the help of the BOSH Agent and the IaaS. Networking configuration is usually assigned at the boot of the VM and/or when network configuration changes in the deployment manifest for already-running deployment jobs.\n\n\nThere are three types of networks that BOSH supports:\n\n\n\n\nmanual\n: The Director decides how to assign IPs to each job instance based on the specified network subnets in the deployment manifest\n\n\ndynamic\n: The Director defers IP selection to the IaaS\n\n\nvip\n: The Director allows one-off IP assignments to specific jobs to enable flexible IP routing (e.g. elastic IP)\n\n\n\n\nEach type of network supports one or both IP reservation types:\n\n\n\n\nstatic\n: IP is explicitly requested by the user in the deployment manifest\n\n\nautomatic\n: IP is selected automatically based on the network type\n\n\n\n\n\n\n\n\n\n\n\n\nmanual network\n\n\ndynamic network\n\n\nvip network\n\n\n\n\n\n\n\n\n\n\nstatic IP assignment\n\n\nSupported\n\n\nNot supported\n\n\nSupported\n\n\n\n\n\n\nautomatic IP assignment\n\n\nSupported, default\n\n\nSupported\n\n\nNot supported\n\n\n\n\n\n\n\n\n\n\nGeneral Structure \n\u00b6\n\n\nNetworking configuration is usually done in three steps:\n\n\n\n\nConfiguring the IaaS: Outside of BOSH's responsibility\n\n\nExample on AWS: User creates a VPC and subnets with routing tables.\n\n\nAdding networks section to the deployment manifest to define networks used in this deployment\n\n\nExample: User adds a manual network with a subnet and adds AWS subnet ID into the subnet's cloud properties.\n\n\nAdding network associations for one or more networks to each deployment job\n\n\n\n\nAll deployment manifests have a similar structure in terms of network definitions and associations:\n\n\n# Network definitions\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network\n\n  \n...\n\n\n\n-\n \nname\n:\n \nmy-other-network\n\n  \ntype\n:\n \n...\n\n\n  \n# IaaS specific attributes\n\n  \ncloud_properties\n:\n \n{\n \n...\n \n}\n\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nmy-job\n\n\n  \n# Network associations for `my-job`\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network\n\n  \n...\n\n\n\n-\n \nname\n:\n \nmy-multi-homed-job\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network\n\n  \n-\n \nname\n:\n \nmy-other-network\n\n  \n...\n\n\n\n-\n \nname\n:\n \nmy-static-job\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network\n\n    \n# Static IP reservations for `my-job`\n\n    \nstatic_ips\n:\n \n[\nIP1\n]\n\n  \n...\n\n\n\n\n\nSee how to define each network type below.\n\n\n\n\nManual Networks \n\u00b6\n\n\nManual networking allows you to specify one or more subnets and let the Director choose available IPs from one of the subnet ranges. A subnet definition specifies the CIDR range and, optionally, the gateway and DNS servers. In addition, certain IPs can be blacklisted (the Director will not use these IPs) via the \nreserved\n property.\n\n\nEach manual network attached to a job instance is typically represented as its own NIC in the IaaS layer.\n\n\nSchema for manual network definition:\n\n\n\n\nname\n [String, required]: Name used to reference this network configuration\n\n\ntype\n [String, required]: Value should be \nmanual\n\n\nsubnets\n [Array, required]: Lists subnets in this network\n\n\nrange\n [String, required]: Subnet IP range that includes all IPs from this subnet\n\n\ngateway\n [String, required]: Subnet gateway IP\n\n\ndns\n [Array, optional]: DNS IP addresses for this subnet\n\n\nreserved\n [Array, optional]: Array of reserved IPs and/or IP ranges. BOSH does not assign IPs from this range to any VM\n\n\nstatic\n [Array, optional]: Array of static IPs and/or IP ranges. BOSH assigns IPs from this range to jobs requesting static IPs. Only IPs specified here can be used for static IP reservations.\n\n\naz\n [String, optional]: AZ associated with this subnet (should only be used when using \nfirst class AZs\n). Example: \nz1\n. Available in v241+.\n\n\nazs\n [Array, optional]: List of AZs associated with this subnet (should only be used when using \nfirst class AZs\n). Example: \n[z1, z2]\n. Available in v241+.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties for the subnet. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network\n\n  \ntype\n:\n \nmanual\n\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n    \n10.10.0.0/24\n\n    \ngateway\n:\n  \n10.10.0.1\n\n    \ndns\n:\n      \n[\n10.10.0.2\n]\n\n\n    \n# IPs that will not be used for anything\n\n    \nreserved\n:\n \n[\n10.10.0.2-10.10.0.10\n]\n\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-9be6c3f7\n}\n\n\n  \n-\n \nrange\n:\n   \n10.10.1.0/24\n\n    \ngateway\n:\n \n10.10.1.1\n\n    \ndns\n:\n     \n[\n10.10.1.2\n]\n\n\n    \n# IPs that can only be used for static IP reservations within this subnet\n\n    \nstatic\n:\n \n[\n10.10.1.11-10.10.1.20\n]\n\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-9be6c6gh\n}\n\n\n\n\n\nManual networks use automatic IP reservation by default. They also support static IP reservation. To assign specific IPs to instances of the deployment job, they must be specified in deployment job's \nnetworks\n section, in the \nstatic_ips\n property for the associated network. That network's subnet definition must also specify them in its \nstatic\n property:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nmy-job-with-static-ip\n\n  \ninstances\n:\n \n2\n\n  \n...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network\n\n\n    \n# IPs associated with 2 instances of `my-job-with-static-ip` job\n\n    \nstatic_ips\n:\n \n[\n10.10.1.11\n,\n \n10.10.1.12\n]\n\n\n\n\n\nNote\n: If a deployment job uses static IP reservation, all instances must be given static IPs.\n\n\n\nA common problem that you may run into is configuring multiple deployments to use overlapping IP ranges. The Director does not consider an IP to be \"used\" even if the Director used that IP in a different deployment. There are two possible solutions for this problem: reconfigure one of the deployments to use a different IP range, or use the same IP range but configure each deployment such that reserved IPs exclude the deployment from each other.\n\n\nNote: While the Director usually selects the next available IP address, this behavior is not guaranteed.\n\n\n\n\n\nDynamic Networks \n\u00b6\n\n\nDynamic networking defers IP selection to the IaaS. For example, AWS assigns a private IP to each instance in the VPC by default. By associating a deployment job to a dynamic network, BOSH will pick up AWS-assigned private IP addresses.\n\n\nEach dynamic network attached to a job instance is typically represented as its own NIC in the IaaS layer.\n\n\nDynamic networking only supports automatic IP reservations.\n\n\nSchema for dynamic network definition:\n\n\n\n\nname\n [String, required]: Name used to reference this network configuration\n\n\ntype\n [String, required]: Value should be \ndynamic\n\n\ndns\n [Array, optional]: DNS IP addresses for this network\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties for the network. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network\n\n  \ntype\n:\n \ndynamic\n\n  \ndns\n:\n  \n[\n10.10.0.2\n]\n\n  \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-9be6c3f7\n}\n\n\n\n\n\nSchema for dynamic network definition with multiple subnets (available in v241+):\n\n\n\n\nname\n [String, required]: Name used to reference this network configuration\n\n\ntype\n [String, required]: Value should be \ndynamic\n\n\nsubnets\n [Array, required]: Lists subnets in this network.\n\n\ndns\n [Array, optional]: DNS IP addresses for this subnet\n\n\naz\n [String, optional]: AZ associated with this subnet (should only be used when using \nfirst class AZs\n). Example: \nz1\n.\n\n\nazs\n [Array, optional]: List of AZs associated with this subnet (should only be used when using \nfirst class AZs\n). Example: \n[z1, z2]\n.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties for the subnet. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network\n\n  \ntype\n:\n \ndynamic\n\n  \nsubnets\n:\n\n  \n-\n \n{\naz\n:\n \nz1\n,\n \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-9be6c3f7\n}}\n\n  \n-\n \n{\naz\n:\n \nz2\n,\n \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-9be6c384\n}}\n\n\n\n\n\n\n\nVIP (Virtual IP) Networks \n\u00b6\n\n\nVIP networking enables the association of an IP address that is not backed by any particular NIC. This flexibility enables users to remap a virtual IP to a different instance in cases of a failure.\n\n\nVIP network attachment is not represented as a NIC in the IaaS layer. In the AWS CPI, it is implemented with \nelastic IPs\n. In OpenStack CPI, it is implemented with \nfloating IPs\n.\n\n\nVIP networking only supports static IP reservations.\n\n\nSchema for VIP network definition:\n\n\n\n\nname\n [String, required]: Name used to reference this network configuration\n\n\ntype\n [String, required]: Value should be \nvip\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties for the network. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network\n\n  \ntype\n:\n \nvip\n\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nmy-job\n\n  \n...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network\n\n    \nstatic_ips\n:\n \n[\n54.47.189.8\n]\n\n\n\n\n\nUnlike the manual networking setup, static IPs for VIP networks are only specified on the deployment job.\n\n\n\n\nMulti-homed VMs \n\u00b6\n\n\nA deployment job can be configured to have multiple IP addresses (multiple NICs) by being on multiple networks. Given that there are multiple network settings available for a deployment job, the Agent needs to decide which network's DNS settings to use and which network's gateway should be the default gateway on the VM. Agent performs such selection based on the network's \ndefault\n property specified in the deployment job.\n\n\nSchema for \ndefault\n property:\n\n\n\n\ndefault\n [Array, optional]: Configures this network to provide its settings for specific category as a default. Possible values are: \ndns\n, \ngateway\n and since bosh-release v258 \naddressable\n. All values can be specified together. \naddressable\n can be used to specify which IP address other jobs see.\n\n\n\n\nExample:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmy-network-1\n\n  \ntype\n:\n \ndynamic\n\n  \ndns\n:\n \n[\n8.8.8.8\n]\n\n\n\n-\n \nname\n:\n \nmy-network-2\n\n  \ntype\n:\n \ndynamic\n\n  \ndns\n:\n \n[\n4.4.4.4\n]\n\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nmy-multi-homed-job\n\n  \n...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network-1\n\n    \ndefault\n:\n \n[\ndns\n,\n \ngateway\n]\n\n  \n-\n \nname\n:\n \nmy-network-2\n\n\n\n-\n \nname\n:\n \nmy-other-multi-homed-job\n\n  \n...\n\n  \nnetworks\n:\n\n  \n-\n \nname\n:\n \nmy-network-1\n\n    \ndefault\n:\n \n[\ndns\n]\n\n  \n-\n \nname\n:\n \nmy-network-2\n\n    \ndefault\n:\n \n[\ngateway\n]\n\n\n\n\n\nIn the above example, VM allocated to \nmy-multi-homed-job\n deployment job will have \n8.8.8.8\n as its primary DNS server and the default gateway will be set to \nmy-network-1\n's gateway. VM allocated to \nmy-other-multi-homed-job\n deployment job will also have \n8.8.8.8\n as its primary DNS server but the default gateway will be set to \nmy-network-2\n's gateway.\n\n\nNote: See \nCPI limitations\n to find which CPIs support this feature.\n\n\n\nNote: See \nrakutentech/bosh-routing-release\n if you are looking for even more specific routing configuration.\n\n\n\n\n\nCPI Limitations \n\u00b6\n\n\nThe Director does not enforce how many networks can be assigned to each job instance; however, each CPI might impose custom requirements either due to the IaaS limitations or simply because support was not yet implemented.\n\n\n\n\n\n\n\n\n\n\nmanual network\n\n\ndynamic network\n\n\nvip network\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nSingle per job instance\n\n\nSingle per job instance\n\n\nSingle, corresponds to an elastic IP\n\n\n\n\n\n\nAzure\n\n\nMultiple per job instance\n\n\nMultiple per job instance\n\n\nSingle, corresponds to a reserved IP\n\n\n\n\n\n\nOpenStack\n\n\nMultiple per job instance\n\n\nSingle per job instance\n\n\nSingle, corresponds to a floating IP\n\n\n\n\n\n\nvSphere\n\n\nMultiple per job instance\n\n\nNot supported\n\n\nNot supported\n\n\n\n\n\n\nvCloud\n\n\nMultiple per job instance\n\n\nNot supported\n\n\nNot supported\n\n\n\n\n\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI network cloud properties\n\n\nSee Azure CPI network cloud properties\n\n\nSee OpenStack CPI network cloud properties\n\n\nSee SoftLayer CPI network cloud properties\n\n\nSee Google Cloud Platform CPI network cloud properties\n\n\nSee vSphere CPI network cloud properties\n\n\nSee vCloud CPI network cloud properties",
            "title": "Networks"
        },
        {
            "location": "/networks/#general-structure",
            "text": "Networking configuration is usually done in three steps:   Configuring the IaaS: Outside of BOSH's responsibility  Example on AWS: User creates a VPC and subnets with routing tables.  Adding networks section to the deployment manifest to define networks used in this deployment  Example: User adds a manual network with a subnet and adds AWS subnet ID into the subnet's cloud properties.  Adding network associations for one or more networks to each deployment job   All deployment manifests have a similar structure in terms of network definitions and associations:  # Network definitions  networks :  -   name :   my-network \n   ...  -   name :   my-other-network \n   type :   ... \n\n   # IaaS specific attributes \n   cloud_properties :   {   ...   }  jobs :  -   name :   my-job \n\n   # Network associations for `my-job` \n   networks : \n   -   name :   my-network \n   ...  -   name :   my-multi-homed-job \n   networks : \n   -   name :   my-network \n   -   name :   my-other-network \n   ...  -   name :   my-static-job \n   networks : \n   -   name :   my-network \n     # Static IP reservations for `my-job` \n     static_ips :   [ IP1 ] \n   ...   See how to define each network type below.",
            "title": "General Structure "
        },
        {
            "location": "/networks/#manual-networks",
            "text": "Manual networking allows you to specify one or more subnets and let the Director choose available IPs from one of the subnet ranges. A subnet definition specifies the CIDR range and, optionally, the gateway and DNS servers. In addition, certain IPs can be blacklisted (the Director will not use these IPs) via the  reserved  property.  Each manual network attached to a job instance is typically represented as its own NIC in the IaaS layer.  Schema for manual network definition:   name  [String, required]: Name used to reference this network configuration  type  [String, required]: Value should be  manual  subnets  [Array, required]: Lists subnets in this network  range  [String, required]: Subnet IP range that includes all IPs from this subnet  gateway  [String, required]: Subnet gateway IP  dns  [Array, optional]: DNS IP addresses for this subnet  reserved  [Array, optional]: Array of reserved IPs and/or IP ranges. BOSH does not assign IPs from this range to any VM  static  [Array, optional]: Array of static IPs and/or IP ranges. BOSH assigns IPs from this range to jobs requesting static IPs. Only IPs specified here can be used for static IP reservations.  az  [String, optional]: AZ associated with this subnet (should only be used when using  first class AZs ). Example:  z1 . Available in v241+.  azs  [Array, optional]: List of AZs associated with this subnet (should only be used when using  first class AZs ). Example:  [z1, z2] . Available in v241+.  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties for the subnet. Default is  {}  (empty Hash).   Example:  networks :  -   name :   my-network \n   type :   manual \n\n   subnets : \n   -   range :      10.10.0.0/24 \n     gateway :    10.10.0.1 \n     dns :        [ 10.10.0.2 ] \n\n     # IPs that will not be used for anything \n     reserved :   [ 10.10.0.2-10.10.0.10 ] \n\n     cloud_properties :   { subnet :   subnet-9be6c3f7 } \n\n   -   range :     10.10.1.0/24 \n     gateway :   10.10.1.1 \n     dns :       [ 10.10.1.2 ] \n\n     # IPs that can only be used for static IP reservations within this subnet \n     static :   [ 10.10.1.11-10.10.1.20 ] \n\n     cloud_properties :   { subnet :   subnet-9be6c6gh }   Manual networks use automatic IP reservation by default. They also support static IP reservation. To assign specific IPs to instances of the deployment job, they must be specified in deployment job's  networks  section, in the  static_ips  property for the associated network. That network's subnet definition must also specify them in its  static  property:  jobs :  -   name :   my-job-with-static-ip \n   instances :   2 \n   ... \n   networks : \n   -   name :   my-network \n\n     # IPs associated with 2 instances of `my-job-with-static-ip` job \n     static_ips :   [ 10.10.1.11 ,   10.10.1.12 ]   Note : If a deployment job uses static IP reservation, all instances must be given static IPs.  A common problem that you may run into is configuring multiple deployments to use overlapping IP ranges. The Director does not consider an IP to be \"used\" even if the Director used that IP in a different deployment. There are two possible solutions for this problem: reconfigure one of the deployments to use a different IP range, or use the same IP range but configure each deployment such that reserved IPs exclude the deployment from each other.  Note: While the Director usually selects the next available IP address, this behavior is not guaranteed.",
            "title": "Manual Networks "
        },
        {
            "location": "/networks/#dynamic-networks",
            "text": "Dynamic networking defers IP selection to the IaaS. For example, AWS assigns a private IP to each instance in the VPC by default. By associating a deployment job to a dynamic network, BOSH will pick up AWS-assigned private IP addresses.  Each dynamic network attached to a job instance is typically represented as its own NIC in the IaaS layer.  Dynamic networking only supports automatic IP reservations.  Schema for dynamic network definition:   name  [String, required]: Name used to reference this network configuration  type  [String, required]: Value should be  dynamic  dns  [Array, optional]: DNS IP addresses for this network  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties for the network. Default is  {}  (empty Hash).   Example:  networks :  -   name :   my-network \n   type :   dynamic \n   dns :    [ 10.10.0.2 ] \n   cloud_properties :   { subnet :   subnet-9be6c3f7 }   Schema for dynamic network definition with multiple subnets (available in v241+):   name  [String, required]: Name used to reference this network configuration  type  [String, required]: Value should be  dynamic  subnets  [Array, required]: Lists subnets in this network.  dns  [Array, optional]: DNS IP addresses for this subnet  az  [String, optional]: AZ associated with this subnet (should only be used when using  first class AZs ). Example:  z1 .  azs  [Array, optional]: List of AZs associated with this subnet (should only be used when using  first class AZs ). Example:  [z1, z2] .  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties for the subnet. Default is  {}  (empty Hash).   Example:  networks :  -   name :   my-network \n   type :   dynamic \n   subnets : \n   -   { az :   z1 ,   cloud_properties :   { subnet :   subnet-9be6c3f7 }} \n   -   { az :   z2 ,   cloud_properties :   { subnet :   subnet-9be6c384 }}",
            "title": "Dynamic Networks "
        },
        {
            "location": "/networks/#vip-virtual-ip-networks",
            "text": "VIP networking enables the association of an IP address that is not backed by any particular NIC. This flexibility enables users to remap a virtual IP to a different instance in cases of a failure.  VIP network attachment is not represented as a NIC in the IaaS layer. In the AWS CPI, it is implemented with  elastic IPs . In OpenStack CPI, it is implemented with  floating IPs .  VIP networking only supports static IP reservations.  Schema for VIP network definition:   name  [String, required]: Name used to reference this network configuration  type  [String, required]: Value should be  vip  cloud_properties  [Hash, optional]: Describes any IaaS-specific properties for the network. Default is  {}  (empty Hash).   Example:  networks :  -   name :   my-network \n   type :   vip  jobs :  -   name :   my-job \n   ... \n   networks : \n   -   name :   my-network \n     static_ips :   [ 54.47.189.8 ]   Unlike the manual networking setup, static IPs for VIP networks are only specified on the deployment job.",
            "title": "VIP (Virtual IP) Networks "
        },
        {
            "location": "/networks/#multi-homed-vms",
            "text": "A deployment job can be configured to have multiple IP addresses (multiple NICs) by being on multiple networks. Given that there are multiple network settings available for a deployment job, the Agent needs to decide which network's DNS settings to use and which network's gateway should be the default gateway on the VM. Agent performs such selection based on the network's  default  property specified in the deployment job.  Schema for  default  property:   default  [Array, optional]: Configures this network to provide its settings for specific category as a default. Possible values are:  dns ,  gateway  and since bosh-release v258  addressable . All values can be specified together.  addressable  can be used to specify which IP address other jobs see.   Example:  networks :  -   name :   my-network-1 \n   type :   dynamic \n   dns :   [ 8.8.8.8 ]  -   name :   my-network-2 \n   type :   dynamic \n   dns :   [ 4.4.4.4 ]  jobs :  -   name :   my-multi-homed-job \n   ... \n   networks : \n   -   name :   my-network-1 \n     default :   [ dns ,   gateway ] \n   -   name :   my-network-2  -   name :   my-other-multi-homed-job \n   ... \n   networks : \n   -   name :   my-network-1 \n     default :   [ dns ] \n   -   name :   my-network-2 \n     default :   [ gateway ]   In the above example, VM allocated to  my-multi-homed-job  deployment job will have  8.8.8.8  as its primary DNS server and the default gateway will be set to  my-network-1 's gateway. VM allocated to  my-other-multi-homed-job  deployment job will also have  8.8.8.8  as its primary DNS server but the default gateway will be set to  my-network-2 's gateway.  Note: See  CPI limitations  to find which CPIs support this feature.  Note: See  rakutentech/bosh-routing-release  if you are looking for even more specific routing configuration.",
            "title": "Multi-homed VMs "
        },
        {
            "location": "/networks/#cpi-limitations",
            "text": "The Director does not enforce how many networks can be assigned to each job instance; however, each CPI might impose custom requirements either due to the IaaS limitations or simply because support was not yet implemented.      manual network  dynamic network  vip network      AWS  Single per job instance  Single per job instance  Single, corresponds to an elastic IP    Azure  Multiple per job instance  Multiple per job instance  Single, corresponds to a reserved IP    OpenStack  Multiple per job instance  Single per job instance  Single, corresponds to a floating IP    vSphere  Multiple per job instance  Not supported  Not supported    vCloud  Multiple per job instance  Not supported  Not supported",
            "title": "CPI Limitations "
        },
        {
            "location": "/networks/#cpi-specific-cloud_properties",
            "text": "See AWS CPI network cloud properties  See Azure CPI network cloud properties  See OpenStack CPI network cloud properties  See SoftLayer CPI network cloud properties  See Google Cloud Platform CPI network cloud properties  See vSphere CPI network cloud properties  See vCloud CPI network cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/director-api-v1/",
            "text": "Note: Before using the Director API directly, we strongly encourage to consider using the CLI for automation such as performing a scheduled deploy from a CI. We hope that you will open a \nGitHub issue\n to share your use cases so that we can suggest or possibly make additions to the CLI.\n\n\n\nThis document lists common API endpoints provided by the Director.\n\n\n\n\nOverview \n\u00b6\n\n\nSecurity \n\u00b6\n\n\nAll API access should be done over verified HTTPS.\n\n\nThe Director can be configured in two authentication modes: \nbasic auth\n and \nUAA\n. \nInfo endpoint\n does not require any authentication and can be used to determine which authentication mechanism to use. All other endpoints require authentication.\n\n\n401 Unauthorized\n will be returned for requests that contain an invalid basic auth credentials or an invalid/expired UAA access token.\n\n\nHTTP verbs \n\u00b6\n\n\nStandard HTTP verb semantics are followed:\n\n\n\n\n\n\n\n\nVerb\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\nUsed for retrieving resources.\n\n\n\n\n\n\nPOST/PUT\n\n\nUsed for creating/updating resources.\n\n\n\n\n\n\nDELETE\n\n\nUsed for deleting resources.\n\n\n\n\n\n\n\n\nHTTP redirects \n\u00b6\n\n\nAny request may result in a redirection. Receiving an HTTP redirection is not an error and clients should follow that redirect. Redirect responses will have a Location header field. Clients should use same authentication method when following a redirect.\n\n\nRate limiting \n\u00b6\n\n\nCurrently no rate limiting is performed.\n\n\nPagination \n\u00b6\n\n\nCurrently none of the resources are paginated.\n\n\nLong running operations (aka Director tasks) \n\u00b6\n\n\nCertain requests result in complex and potentially long running operations against the IaaS, blobstore, or other resources. \nPOST /deployments\n is a good example. Such requests start a \nDirector task\n and continue running on the Director after response is returned. Response to such request will be a \n302 Moved Temporarily\n redirect to a created task resource.\n\n\nOnce a Director task is created, clients can follow its progress by polling \nGET /tasks/{id}\n to find out its state. While waiting for the task to finish, different types of logs (\nevent\n, \nresult\n, \ndebug\n information, etc.) can be followed to gain insight into what the task is doing.\n\n\n\n\nGeneral \n\u00b6\n\n\nGET /info\n: Info \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: Director details.\n\n\n\n\nname\n [String]: Name of the Director.\n\n\nuuid\n [String]: Unique ID of the Director.\n\n\nversion\n [String]: Version of the Director software.\n\n\nuser\n [String or null]: Logged in user's user name if authentication is provided, otherwise null.\n\n\ncpi\n [String]: Name of the CPI the Director will use.\n\n\nuser_authentication\n [Hash]:\n\n\ntype\n [String]: Type of the authentication the Director is configured to expect.\n\n\noptions\n [Hash]: Additional information provided to how authentication should be performed.\n\n\n\n\n\n\nfeatures\n [Hash]:\n\n\nconfig_server\n [Hash]:\n\n\nstatus\n [Boolean]: Default false.\n\n\nextras\n [Hash]:\n\n\nurls\n [Array]: List of URLs for the Config Server.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\u00b6\n\n\n\n\nThis is the only endpoint that does not require authentication.\n\n\nIn future \nversion\n will contain version of the deployed BOSH release.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -s -k https://192.168.50.4:25555/info \n|\n jq .\n\n\n\n\n{\n\n  \n\"name\"\n:\n \n\"Bosh\n \nLite\n \nDirector\"\n,\n\n  \n\"uuid\"\n:\n \n\"2daf673a-9755-4b4f-aa6d-3632fbed8012\"\n,\n\n  \n\"version\"\n:\n \n\"1.3126.0\n \n(00000000)\"\n,\n\n  \n\"user\"\n:\n \nnull\n,\n\n  \n\"cpi\"\n:\n \n\"warden_cpi\"\n,\n\n  \n\"user_authentication\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"basic\"\n,\n\n    \n\"options\"\n:\n \n{}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nConfigs \n\u00b6\n\n\nGET /configs\n: List configs \n\u00b6\n\n\nRequest query\n\u00b6\n\n\n\n\nlatest\n [Boolean, required]: Returns latest configs when set to \ntrue\n. Otherwise return all configs when set to \nfalse\n. Possible values: \ntrue\n or \nfalse\n. There is no default.\n\n\ntype\n [String, optional]: Filters list of configs by the given type.\n\n\nname\n [String, optional]: Filters list of configs by the given name.\n\n\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of configs.\n\n\n\n\nid\n [String]: ID of the config.\n\n\nname\n [String]: Name of the config.\n\n\ntype\n [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.\n\n\ncontent\n [String]: YAML containing the config manifest.\n\n\ncreated_at\n [Time]: Creation time of the config.\n\n\ndeleted\n [Boolean]: Soft delete flag.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -s -k https://192.168.50.4:25555/configs?latest\n=\ntrue\n \n|\n jq .\n\n\n\n\n[\n\n   \n{\n\n    \n\"content\"\n:\n \n\"azs:\\n-\n \nname:\n \nz1\\n...\"\n,\n\n    \n\"id\"\n:\n \n\"1\"\n,\n\n    \n\"type\"\n:\n \n\"cloud\"\n,\n\n    \n\"name\"\n:\n \n\"default\"\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nPOST /configs\n: Create config. \n\u00b6\n\n\nRequest headers\n\u00b6\n\n\n\n\nContent-Type\n must be \ntext/json\n.\n\n\n\n\nRequest body\n\u00b6\n\n\nThe request body consists of a single JSON hash with the following key, value pairs:\n\n\n\n\nname\n [String]: Name of the config.\n\n\ntype\n [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.\n\n\ncontent\n [String]: YAML containing the config manifest.\n\n\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: The newly created config.\n\n\n\n\nname\n [String]: Name of the config.\n\n\ntype\n [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.\n\n\ncontent\n [String]: YAML containing the config manifest.\n\n\ncreated_at\n [Time]: Creation time of the config.\n\n\ndeleted\n [Boolean]: Soft delete flag.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -s -k -H \n'Content-Type: application/json'\n -d \n'{\"name\": \"test\", \"type\": \"cloud\", \"content\": \"--- {}\"}'\n https://192.168.50.4:25555/configs \n|\n jq .\n\n\n\n\n{\n\n  \n\"content\"\n:\n \n\"---\n \n{}\"\n,\n\n  \n\"id\"\n:\n \n\"3\"\n,\n\n  \n\"type\"\n:\n \n\"cloud\"\n,\n\n  \n\"name\"\n:\n \n\"test\"\n\n\n}\n\n\n\n\n\n\n\nPOST /configs/diff\n: Diff config. \n\u00b6\n\n\nRequest headers\n\u00b6\n\n\n\n\nContent-Type\n must be \ntext/json\n.\n\n\n\n\nRequest body\n\u00b6\n\n\nThe request body consists of a single JSON hash with the following key, value pairs:\n\n\n\n\nname\n [String]: Name of the config.\n\n\ntype\n [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.\n\n\ncontent\n [String]: YAML containing the config manifest.\n\n\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: The changes.\n\n\n\n\ndiff\n [Array]: List of differences.\n\n\nerror\n [String]: Error description if an error happened.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -s -k -H \n'Content-Type: application/json'\n -d \n'{\"name\": \"default\", \"type\": \"cloud\", \"content\": \"--- {}\"}'\n https://192.168.50.4:25555/configs/diff \n|\n jq .\n\n\n\n\n{\n\n  \n\"diff\"\n:\n \n[\n\n    \n[\n\n      \n\"az:\n \nz2\"\n,\n\n      \n\"added\"\n\n    \n]\n\n  \n]\n\n\n}\n\n\n\n\n\n\n\nDELETE /configs\n: Marks configs as deleted. \n\u00b6\n\n\nRequest Query\n\u00b6\n\n\n\n\nname\n [String]: Name of the config.\n\n\ntype\n [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -s -k -X DELETE https://192.168.50.4:25555/configs?type\n=\ncloud\n&\nname\n=\ntest\n\n\n\n\n\n\n\nTasks \n\u00b6\n\n\nSee \nDirector tasks\n for related info.\n\n\nGET /tasks\n: List all tasks \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of tasks.\n\n\n\n\nid\n [Integer]: Numeric ID of the task.\n\n\nstate\n [String]: Current state of the task. Possible values are: \nqueued\n, \nprocessing\n, \ncancelled\n, \ncancelling\n, \ndone\n, \nerror\n, \ntimeout\n.\n\n\ndescription\n [String]: Description of the task's purpose.\n\n\ntimestamp\n [Integer]: todo.\n\n\nresult\n [String or null]: Description of the task's result. Will not be populated (string) unless tasks finishes.\n\n\nuser\n [String]: User which started the task.\n\n\ncontext_id\n [String]: Context ID of the task, if provided when task was created, otherwise empty string.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks?verbose=2&limit=3'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"id\"\n:\n \n1180\n,\n\n    \n\"state\"\n:\n \n\"processing\"\n,\n\n    \n\"description\"\n:\n \n\"run\n \nerrand\n \nacceptance_tests\n \nfrom\n \ndeployment\n \ncf-warden\"\n,\n\n    \n\"timestamp\"\n:\n \n1447033291\n,\n\n    \n\"result\"\n:\n \nnull\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"\"\n\n  \n},\n\n  \n{\n\n    \n\"id\"\n:\n \n1179\n,\n\n    \n\"state\"\n:\n \n\"done\"\n,\n\n    \n\"description\"\n:\n \n\"scan\n \nand\n \nfix\"\n,\n\n    \n\"timestamp\"\n:\n \n1447031334\n,\n\n    \n\"result\"\n:\n \n\"scan\n \nand\n \nfix\n \ncomplete\"\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"\"\n\n  \n},\n\n  \n{\n\n    \n\"id\"\n:\n \n1178\n,\n\n    \n\"state\"\n:\n \n\"done\"\n,\n\n    \n\"description\"\n:\n \n\"scan\n \nand\n \nfix\"\n,\n\n    \n\"timestamp\"\n:\n \n1447031334\n,\n\n    \n\"result\"\n:\n \n\"scan\n \nand\n \nfix\n \ncomplete\"\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"\"\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /tasks?state=...\n: List currently running tasks \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\nSee schema \nabove\n.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks?state=queued,processing,cancelling&verbose=2'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"id\"\n:\n \n1180\n,\n\n    \n\"state\"\n:\n \n\"processing\"\n,\n\n    \n\"description\"\n:\n \n\"run\n \nerrand\n \nacceptance_tests\n \nfrom\n \ndeployment\n \ncf-warden\"\n,\n\n    \n\"timestamp\"\n:\n \n1447033291\n,\n\n    \n\"result\"\n:\n \nnull\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"\"\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /tasks?deployment=...\n: List tasks associated with a deployment \n\u00b6\n\n\nOther tasks query params can be applied.\n\n\nResponse body schema\n\u00b6\n\n\nSee schema \nabove\n.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks?deploymet=cf-warden'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"id\"\n:\n \n1180\n,\n\n    \n\"state\"\n:\n \n\"processing\"\n,\n\n    \n\"description\"\n:\n \n\"run\n \nerrand\n \nacceptance_tests\n \nfrom\n \ndeployment\n \ncf-warden\"\n,\n\n    \n\"timestamp\"\n:\n \n1447033291\n,\n\n    \n\"result\"\n:\n \nnull\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"\"\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /tasks?context_id=...\n: List tasks associated with a context ID \n\u00b6\n\n\nOther tasks query params can be applied.\n\n\nResponse body schema\n\u00b6\n\n\nSee schema \nabove\n.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks?context_id=4528'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"id\"\n:\n \n1180\n,\n\n    \n\"state\"\n:\n \n\"processing\"\n,\n\n    \n\"description\"\n:\n \n\"run\n \nerrand\n \nacceptance_tests\n \nfrom\n \ndeployment\n \ncf-warden\"\n,\n\n    \n\"timestamp\"\n:\n \n1447033291\n,\n\n    \n\"result\"\n:\n \nnull\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"context_id\"\n:\n \n\"4528\"\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /tasks/{id}\n: Retrieve single task \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: Task details.\n\n\nSee additional schema details \nabove\n.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1180'\n \n|\n jq .\n\n\n\n\n{\n\n  \n\"id\"\n:\n \n1180\n,\n\n  \n\"state\"\n:\n \n\"processing\"\n,\n\n  \n\"description\"\n:\n \n\"run\n \nerrand\n \nacceptance_tests\n \nfrom\n \ndeployment\n \ncf-warden\"\n,\n\n  \n\"timestamp\"\n:\n \n1447033291\n,\n\n  \n\"result\"\n:\n \nnull\n,\n\n  \n\"user\"\n:\n \n\"admin\"\n,\n\n  \n\"context_id\"\n:\n \n\"\"\n\n\n}\n\n\n\n\n\n\n\nGET /tasks/{id}/output?type=debug\n: Retrieve task's debug log \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [String]: Debug output for the chosen task.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1180/output?type=debug'\n\n\n\n\n\n...\nD, [2015-11-09 02:19:36 #32545] [] DEBUG -- DirectorJobRunner: RECEIVED: director.37d8c089-853e-458c-8535-195085b4b7ed.459b05ae-8b69-4679-b2d5-b34e5fef2dcc {\"value\":{\"agent_task_id\":\"c9f5b328-0656-41f1-631c-e17151be1e18\",\"state\":\"running\"}}\nD, [2015-11-09 02:19:36 #32545] [task:1180] DEBUG -- DirectorJobRunner: (0.000441s) SELECT NULL\nD, [2015-11-09 02:19:36 #32545] [task:1180] DEBUG -- DirectorJobRunner: (0.000317s) SELECT * FROM \"tasks\" WHERE \"id\" = 1180\n\n\n\n\n\n\nGET /tasks/{id}/output?type=event\n: Retrieve task's event log \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [String]: Result output for the chosen task. Newlines separate valid event JSON records.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=event'\n\n\n\n\n\n...\n{\n  \"time\": 1446959491,\n  \"stage\": \"Deleting errand instances\",\n  \"tags\": [ \"smoke_tests\" ],\n  \"total\": 1,\n  \"task\": \"59d5b228-a732-4c68-6017-31fe5bc9d8c5\",\n  \"index\": 1,\n  \"state\": \"started\",\n  \"progress\": 0\n}\n{\n  \"time\": 1446959496,\n  \"stage\": \"Deleting errand instances\",\n  \"tags\": [ \"smoke_tests\" ],\n  \"total\": 1,\n  \"task\": \"59d5b228-a732-4c68-6017-31fe5bc9d8c5\",\n  \"index\": 1,\n  \"state\": \"finished\",\n  \"progress\": 100\n}\n\n\n\n\n\n\nGET /tasks/{id}/output?type=result\n: Retrieve task's result \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [String]: Result output for the chosen task. Contents depend on a type of task.\n\n\nExample of VM details task\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=result'\n\n\n\n\n\n...\n{\"vm_cid\":\"ec974048-3352-4ba4-669d-beab87b16bcb\",\"disk_cid\":null,\"ips\":[\"10.244.0.142\"],\"dns\":[],\"agent_id\":\"c5e7c705-459e-41c0-b640-db32d8dc6e71\",\"job_name\":\"doppler_z1\",\"index\":0,\"job_state\":\"running\",\"resource_pool\":\"medium_z1\",\"vitals\":{\"cpu\":{\"sys\":\"9.1\",\"user\":\"2.1\",\"wait\":\"1.7\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"11\",\"percent\":\"36\"},\"system\":{\"inode_percent\":\"11\",\"percent\":\"36\"}},\"load\":[\"0.61\",\"0.74\",\"1.10\"],\"mem\":{\"kb\":\"2520960\",\"percent\":\"41\"},\"swap\":{\"kb\":\"102200\",\"percent\":\"10\"}},\"processes\":[{\"name\":\"doppler\",\"state\":\"running\"},{\"name\":\"syslog_drain_binder\",\"state\":\"running\"},{\"name\":\"metron_agent\",\"state\":\"running\"}],\"resurrection_paused\":false}\n\n\n\n\n\n\nStemcells \n\u00b6\n\n\nGET /stemcells\n: List all uploaded stemcells \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of stemcells.\n\n\n\n\nname\n [String]: Name of the stemcell.\n\n\nversion\n [String]: Version of the stemcell.\n\n\noperating_system\n [String]: Operating system identifier. Example: \nubuntu-trusty\n and \ncentos-7\n.\n\n\ncid\n [String]: Cloud ID of the stemcell.\n\n\ndeployments\n [Array]: List of deployments currently using this stemcell version.\n\n\nname\n [String]: Deployment name.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k https://admin:admin@192.168.50.4:25555/stemcells \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"name\"\n:\n \n\"bosh-warden-boshlite-ubuntu-trusty-go_agent\"\n,\n\n    \n\"operating_system\"\n:\n \n\"ubuntu-trusty\"\n,\n\n    \n\"version\"\n:\n \n\"3126\"\n,\n\n    \n\"cid\"\n:\n \n\"c3705a0d-0dd3-4b67-52b5-50533a432244\"\n,\n\n    \n\"deployments\"\n:\n \n[\n\n      \n{\n \n\"name\"\n:\n \n\"cf-warden\"\n \n}\n\n    \n]\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nReleases \n\u00b6\n\n\nGET /releases\n: List all uploaded releases \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of releases.\n\n\n\n\nname\n [String]: Name of the release.\n\n\nrelease_versions\n [Array]: List of versions available.\n\n\nversion\n [String]: Version of the release version.\n\n\ncommit_hash\n [String]: Identifier in the SCM repository for the release version source code.\n\n\nuncommitted_changes\n [Boolean]: Whether or not the release version was created from a SCM repository with unsaved changes.\n\n\ncurrently_deployed\n [Boolean]: Whether or not the release version is used by any deployments.\n\n\njob_names\n [Array of strings]: List of job names associated with the release version.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k https://admin:admin@192.168.50.4:25555/releases \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"name\"\n:\n \n\"bosh-warden-cpi\"\n,\n\n    \n\"release_versions\"\n:\n \n[\n\n      \n{\n\n        \n\"version\"\n:\n \n\"28\"\n,\n\n        \n\"commit_hash\"\n:\n \n\"4c36884a\"\n,\n\n        \n\"uncommitted_changes\"\n:\n \nfalse\n,\n\n        \n\"currently_deployed\"\n:\n \nfalse\n,\n\n        \n\"job_names\"\n:\n \n[\n \n\"warden_cpi\"\n \n]\n\n      \n}\n\n    \n]\n\n  \n},\n\n  \n{\n\n    \n\"name\"\n:\n \n\"test\"\n,\n\n    \n\"release_versions\"\n:\n \n[\n\n      \n{\n\n        \n\"version\"\n:\n \n\"0+dev.16\"\n,\n\n        \n\"commit_hash\"\n:\n \n\"31ef3167\"\n,\n\n        \n\"uncommitted_changes\"\n:\n \ntrue\n,\n\n        \n\"currently_deployed\"\n:\n \nfalse\n,\n\n        \n\"job_names\"\n:\n \n[\n \n\"http_server\"\n,\n \n\"service\"\n \n]\n\n      \n},\n\n      \n{\n\n        \n\"version\"\n:\n \n\"0+dev.17\"\n,\n\n        \n\"commit_hash\"\n:\n \n\"e5416248\"\n,\n\n        \n\"uncommitted_changes\"\n:\n \ntrue\n,\n\n        \n\"currently_deployed\"\n:\n \ntrue\n,\n\n        \n\"job_names\"\n:\n \n[\n \n\"drain\"\n,\n \n\"errand\"\n,\n \n\"http_server\"\n,\n \n\"pre_start\"\n,\n \n\"service\"\n \n]\n\n      \n},\n\n    \n]\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nDeployments \n\u00b6\n\n\nGET /deployments\n: List all deployments \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of deployments.\n\n\n\n\nname\n [String]: Name of the deployment.\n\n\ncloud_config\n [String]: Indicator whether latest cloud config is used for this deployment. Possible values: \nnone\n, \noutdated\n, \nlatest\n.\n\n\nreleases\n [Array]: List of releases used by the deployment.\n\n\nname\n [String]: Name of the release.\n\n\nversion\n [String]: Version of the release.\n\n\n\n\n\n\nstemcells\n [Array]: List of stemcells used by the deploymemt.\n\n\nname\n [String]: Name of the stemcell.\n\n\nversion\n [String]: Version of the stemcell.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k https://admin:admin@192.168.50.4:25555/deployments \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"name\"\n:\n \n\"cf-warden\"\n,\n\n    \n\"cloud_config\"\n:\n \n\"none\"\n,\n\n    \n\"releases\"\n:\n \n[\n\n      \n{\n\n        \n\"name\"\n:\n \n\"cf\"\n,\n\n        \n\"version\"\n:\n \n\"222\"\n\n      \n},\n\n      \n{\n\n        \n\"name\"\n:\n \n\"cf\"\n,\n\n        \n\"version\"\n:\n \n\"223\"\n\n      \n}\n\n    \n],\n\n    \n\"stemcells\"\n:\n \n[\n\n      \n{\n\n        \n\"name\"\n:\n \n\"bosh-warden-boshlite-ubuntu-trusty-go_agent\"\n,\n\n        \n\"version\"\n:\n \n\"2776\"\n\n      \n},\n\n      \n{\n\n        \n\"name\"\n:\n \n\"bosh-warden-boshlite-ubuntu-trusty-go_agent\"\n,\n\n        \n\"version\"\n:\n \n\"3126\"\n\n      \n}\n\n    \n]\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nPOST /deployments\n: Create/update single deployment \n\u00b6\n\n\nRequest query\n\u00b6\n\n\n\n\nrecreate\n [Boolean]: Whether or not to ignore deletion errors. Possible values: \ntrue\n or not present. Default is not present.\n\n\nskip_drain\n [String]: Comma separated list of job names that should not run drain scripts during the update. Possible values: \n*\n to represent all jobs, \n<job1>,<job2>\n to list job names, or not present. Default is not present.\n\n\n\n\nRequest headers\n\u00b6\n\n\n\n\nContent-Type\n must be \ntext/yaml\n.\n\n\nX-Bosh-Context-Id\n can be optionally configured with a Context ID that can be used to link related BOSH requests\n\n\n\n\nRequest body scheme\n\u00b6\n\n\n[root]\n [String]: Manifest string. Note that non-exact version values (\nlatest\n value) for releases and stemcells must be resolved before making a request.\n\n\nResponse\n\u00b6\n\n\nCreating/updating a deployment is performed in a Director task. Response will be a redirect to a task resource.\n\n\n\n\nGET /deployments/{name}\n: Retrieve single deployment \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: Single deployment.\n\n\n\n\nmanifest\n [String]: Last successfully deployed manifest string.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k https://admin:admin@192.168.50.4:25555/deployments/cf-warden \n|\n jq .\n\n\n\n\n{\n\n  \n\"manifest\"\n:\n \n\"---\\nname:\n \ncf-warden\\n...\"\n,\n\n\n}\n\n\n\n\n\n\n\nDELETE /deployments/{name}\n: Delete single deployment \n\u00b6\n\n\nRequest query\n\u00b6\n\n\n\n\nforce\n [Boolean]: Whether or not to ignore deletion errors. Dangerous! Possible values: \ntrue\n or not present. Default is not present.\n\n\n\n\nRequest body\n\u00b6\n\n\nEmpty.\n\n\nResponse\n\u00b6\n\n\nDeleting a deployment is performed in a Director task. Response will be a redirect to a task resource.\n\n\n\n\nInstances in a deployment \n\u00b6\n\n\nNote: This feature is available with bosh-release v256+.\n\n\n\nInstances\n represent the expected state of the VMs of a deployment. The actual state of the VMs can be retrieved with the \nvms\n endpoints\n. \ninstances\n is similar to \nvms\n, but also contains instances that do not have a VM.\n\n\nGET /deployments/{name}/instances\n: List all instances \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of instances.\n\n\n\n\nagent_id\n [String]: Unique ID of the Agent associated with the VM. Could be \nnil\n if there is no VM for this instance.\n\n\ncid\n [String]: Cloud ID of the VM. Could be \nnil\n if there is no VM for this instance.\n\n\njob\n [String]: Name of the job.\n\n\nindex\n [Integer]: Numeric job index.\n\n\nid\n [String]: ID of the instance.\n\n\nexpects_vm\n [Boolean]: \ntrue\n if a VM should exist for this instance.\n\n\n\n\nNotes\n\u00b6\n\n\n\n\nThis endpoint does not query Agents on the VMs, hence is returned immediately.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/deployments/example/instances'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"agent_id\"\n:\n \n\"c5e7c705-459e-41c0-b640-db32d8dc6e71\"\n,\n\n    \n\"cid\"\n:\n \n\"ec974048-3352-4ba4-669d-beab87b16bcb\"\n,\n\n    \n\"job\"\n:\n \n\"example_service\"\n,\n\n    \n\"index\"\n:\n \n0\n,\n\n    \n\"id\"\n:\n \n\"209b96c8-e482-43c7-9f3e-04de9f93c535\"\n,\n\n    \n\"expects_vm\"\n:\n \ntrue\n\n  \n},\n\n  \n{\n\n    \n\"agent_id\"\n:\n \nnil\n,\n\n    \n\"cid\"\n:\n \nnil\n,\n\n    \n\"job\"\n:\n \n\"example_errand\"\n,\n\n    \n\"index\"\n:\n \n0\n,\n\n    \n\"id\"\n:\n \n\"548d6aa0-eb8f-4890-bd3a-e6b526f3aeea\"\n,\n\n    \n\"expects_vm\"\n:\n \nfalse\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /deployments/{name}/instances?format=full\n: List details of instances \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [String]: For each instance there is one line of JSON. The JSON contains the following details.\n\n\n\n\nagent_id\n [String]: Unique ID of the Agent associated with the VM.\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\nresource_pool\n [String]: Name of the resource pool used for the VM.\n\n\ndisk_cid\n [String or null]: Cloud ID of the associated persistent disk if one is attached.\n\n\njob_name\n [String]: Name of the job.\n\n\nindex\n [Integer]: Numeric job index.\n\n\nresurrection_paused\n [Boolean]: Whether or not resurrector will try to bring back the VM is it goes missing.\n\n\njob_state\n [String]: Aggregate state of job. Possible values: \nrunning\n and other values that represent unhealthy state.\n\n\nips\n [Array of strings]: List of IPs.\n\n\ndns\n [Array of strings]: List of DNS records.\n\n\nvitals\n [Hash]: VM vitals.\n\n\nprocesses\n [Array of hashes]: List of processes running as part of the job.\n\n\nstate\n [String]: State of instance\n\n\nvm_type\n [String]: Name of \nVM type\n\n\naz\n [String]: Name of \navailability zone\n\n\nid\n [String]: ID of instance\n\n\nbootstrap\n [Boolean]: bootstrap property of \ninstance specific configuration\n\n\nignore\n [Boolean]: Ignore this instance if set to \ntrue\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/deployments/example/instances?format=full'\n\n< HTTP/1.1 \n302\n Moved Temporarily\n< Location: https://192.168.50.4:25555/tasks/1287\n...\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1287'\n \n|\n jq .\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1287/output?type=result'\n\n\n\n\n\n...\n{\"vm_cid\":\"3938cc70-8f5e-4318-ad05-24d991e0e66e\",\"disk_cid\":null,\"ips\":[\"10.0.1.3\"],\"dns\":[],\"agent_id\":\"d927e75b-2a2d-4015-b5cc-306a067e94e9\",\"job_name\":\"example_service\",\"index\":1,\"job_state\":\"running\",\"state\":\"started\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"0.3\",\"user\":\"0.1\",\"wait\":\"0.0\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"5\",\"percent\":\"32\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"66\"}},\"load\":[\"0.00\",\"0.01\",\"0.10\"],\"mem\":{\"kb\":\"605008\",\"percent\":\"7\"},\"swap\":{\"kb\":\"75436\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"beacon\",\"state\":\"running\",\"uptime\":{\"secs\":1212184},\"mem\":{\"kb\":776,\"percent\":0},\"cpu\":{\"total\":0}},{\"name\":\"baggageclaim\",\"state\":\"running\",\"uptime\":{\"secs\":1212152},\"mem\":{\"kb\":8920,\"percent\":0.1},\"cpu\":{\"total\":0}},{\"name\":\"garden\",\"state\":\"running\",\"uptime\":{\"secs\":1212153},\"mem\":{\"kb\":235004,\"percent\":2.8},\"cpu\":{\"total\":0.2}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"abe6a4e9-cfca-490b-8515-2893f9e54d20\",\"bootstrap\":false,\"ignore\":false}\n{\"vm_cid\":\"86eb5e7e-a1c8-4f7b-a20c-cd696bf80938\",\"disk_cid\":\"70b3c01c-729e-4335-9630-1f1985a40c99\",\"ips\":[\"10.0.1.5\"],\"dns\":[],\"agent_id\":\"7a54d3bb-f77b-412f-b662-dbff7733a823\",\"job_name\":\"example_errand\",\"index\":0,\"job_state\":\"stopped\",\"state\":\"stopped\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"1.3\",\"user\":\"4.9\",\"wait\":\"0.1\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"67\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"48\"}},\"load\":[\"0.00\",\"0.03\",\"0.05\"],\"mem\":{\"kb\":\"227028\",\"percent\":\"6\"},\"swap\":{\"kb\":\"25972\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"postgresql\",\"state\":\"running\",\"uptime\":{\"secs\":1212309},\"mem\":{\"kb\":489836,\"percent\":12.1},\"cpu\":{\"total\":0}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"548d7aa0-eb8f-4890-bd3a-e9b526f3aeeb\",\"bootstrap\":false,\"ignore\":false}\n\n\n\n\nFormatted example of details of a single instance\n\u00b6\n\n\n{\n\n  \n\"vm_cid\"\n:\n \n\"86eb5e8e-a8c8-4f7b-a20c-cd696bf80938\"\n,\n\n  \n\"disk_cid\"\n:\n \n\"70a3c01c-728e-4335-9630-1f1985a40c99\"\n,\n\n  \n\"ips\"\n:\n \n[\n\n    \n\"10.0.1.5\"\n\n  \n],\n\n  \n\"dns\"\n:\n \n[],\n\n  \n\"agent_id\"\n:\n \n\"0a54d3bb-f78b-412f-b662-dbff7733a823\"\n,\n\n  \n\"job_name\"\n:\n \n\"example_service\"\n,\n\n  \n\"index\"\n:\n \n0\n,\n\n  \n\"job_state\"\n:\n \n\"running\"\n,\n\n  \n\"state\"\n:\n \n\"started\"\n,\n\n  \n\"resource_pool\"\n:\n \n\"resource_pool_1\"\n,\n\n  \n\"vm_type\"\n:\n \n\"resource_pool_1\"\n,\n\n  \n\"vitals\"\n:\n \n{\n\n    \n\"cpu\"\n:\n \n{\n\n      \n\"sys\"\n:\n \n\"1.3\"\n,\n\n      \n\"user\"\n:\n \n\"4.9\"\n,\n\n      \n\"wait\"\n:\n \n\"0.1\"\n\n    \n},\n\n    \n\"disk\"\n:\n \n{\n\n      \n\"ephemeral\"\n:\n \n{\n\n        \n\"inode_percent\"\n:\n \n\"0\"\n,\n\n        \n\"percent\"\n:\n \n\"0\"\n\n      \n},\n\n      \n\"persistent\"\n:\n \n{\n\n        \n\"inode_percent\"\n:\n \n\"0\"\n,\n\n        \n\"percent\"\n:\n \n\"67\"\n\n      \n},\n\n      \n\"system\"\n:\n \n{\n\n        \n\"inode_percent\"\n:\n \n\"34\"\n,\n\n        \n\"percent\"\n:\n \n\"48\"\n\n      \n}\n\n    \n},\n\n    \n\"load\"\n:\n \n[\n\n      \n\"0.00\"\n,\n\n      \n\"0.03\"\n,\n\n      \n\"0.05\"\n\n    \n],\n\n    \n\"mem\"\n:\n \n{\n\n      \n\"kb\"\n:\n \n\"227028\"\n,\n\n      \n\"percent\"\n:\n \n\"6\"\n\n    \n},\n\n    \n\"swap\"\n:\n \n{\n\n      \n\"kb\"\n:\n \n\"25972\"\n,\n\n      \n\"percent\"\n:\n \n\"1\"\n\n    \n}\n\n  \n},\n\n  \n\"processes\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"postgresql\"\n,\n\n      \n\"state\"\n:\n \n\"running\"\n,\n\n      \n\"uptime\"\n:\n \n{\n\n        \n\"secs\"\n:\n \n1212309\n\n      \n},\n\n      \n\"mem\"\n:\n \n{\n\n        \n\"kb\"\n:\n \n489836\n,\n\n        \n\"percent\"\n:\n \n12.1\n\n      \n},\n\n      \n\"cpu\"\n:\n \n{\n\n        \n\"total\"\n:\n \n0\n\n      \n}\n\n    \n}\n\n  \n],\n\n  \n\"resurrection_paused\"\n:\n \ntrue\n,\n\n  \n\"az\"\n:\n \nnull\n,\n\n  \n\"id\"\n:\n \n\"548d6aa0-eb8f-4890-bd3a-e9b526f3aeeb\"\n,\n\n  \n\"bootstrap\"\n:\n \nfalse\n,\n\n  \n\"ignore\"\n:\n \nfalse\n\n\n}\n\n\n\n\n\n\n\nVMs in a deployment \n\u00b6\n\n\nGET /deployments/{name}/vms\n: List all VMs \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List of VMs.\n\n\n\n\nagent_id\n [String]: Unique ID of the Agent associated with the VM.\n\n\ncid\n [String]: Cloud ID of the VM.\n\n\njob\n [String]: Name of the job.\n\n\nindex\n [Integer]: Numeric job index.\n\n\n\n\nNotes\n\u00b6\n\n\n\n\nThis endpoint does not query Agents on the VMs, hence is returned immediately.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/deployments/cf-warden/vms'\n \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"agent_id\"\n:\n \n\"c5e7c705-459e-41c0-b640-db32d8dc6e71\"\n,\n\n    \n\"cid\"\n:\n \n\"ec974048-3352-4ba4-669d-beab87b16bcb\"\n,\n\n    \n\"job\"\n:\n \n\"doppler_z1\"\n,\n\n    \n\"index\"\n:\n \n0\n\n  \n},\n\n  \n{\n\n    \n\"agent_id\"\n:\n \n\"81f7b585-f3d3-4dbc-8d7c-f76dbe861bdc\"\n,\n\n    \n\"cid\"\n:\n \n\"427c1995-2d06-42b2-4218-418150bc31c9\"\n,\n\n    \n\"job\"\n:\n \n\"api_z1\"\n,\n\n    \n\"index\"\n:\n \n0\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /deployments/{name}/vms?format=full\n: List VM details \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [String]: Each VM's details are separated by a newline.\n\n\n\n\nagent_id\n [String]: Unique ID of the Agent associated with the VM.\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\nresource_pool\n [String]: Name of the resource pool used for the VM.\n\n\ndisk_cid\n [String or null]: Cloud ID of the associated persistent disk if one is attached.\n\n\njob_name\n [String]: Name of the job.\n\n\nindex\n [Integer]: Numeric job index.\n\n\nresurrection_paused\n [Boolean]: Whether or not resurrector will try to bring back the VM is it goes missing.\n\n\njob_state\n [String]: Aggregate state of job. Possible values: \nrunning\n and other values that represent unhealthy state.\n\n\nips\n [Array of strings]: List of IPs.\n\n\ndns\n [Array of strings]: List of DNS records.\n\n\nvitals\n [Hash]: VM vitals.\n\n\nprocesses\n [Array of hashes]: List of processes running as part of the job.\n\n\nstate\n [String]: State of the VM\n\n\nvm_type\n [String]: Name of \nVM type\n\n\naz\n [String]: Name of \navailability zone\n\n\nid\n [String]: ID of the VM\n\n\nbootstrap\n [Boolean]: bootstrap property of \nVM specific configuration\n\n\nignore\n [Boolean]: Ignore this VM if set to \ntrue\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/deployments/cf-warden/vms?format=full'\n\n< HTTP/1.1 \n302\n Moved Temporarily\n< Location: https://192.168.50.4:25555/tasks/1181\n...\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1181'\n \n|\n jq .\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=result'\n\n\n\n\n\n...\n{\"vm_cid\":\"3938cc70-8f5e-4318-ad05-24d991e0e66e\",\"disk_cid\":null,\"ips\":[\"10.0.1.3\"],\"dns\":[],\"agent_id\":\"d927e75b-2a2d-4015-b5cc-306a067e94e9\",\"job_name\":\"example_service\",\"index\":0,\"job_state\":\"running\",\"state\":\"started\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"0.3\",\"user\":\"0.1\",\"wait\":\"0.0\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"5\",\"percent\":\"32\"},\"persistent\":{\"inode_percent\":\"3\",\"percent\":\"67\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"66\"}},\"load\":[\"0.00\",\"0.01\",\"0.10\"],\"mem\":{\"kb\":\"605008\",\"percent\":\"7\"},\"swap\":{\"kb\":\"75436\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"beacon\",\"state\":\"running\",\"uptime\":{\"secs\":1212184},\"mem\":{\"kb\":776,\"percent\":0},\"cpu\":{\"total\":0}},{\"name\":\"baggageclaim\",\"state\":\"running\",\"uptime\":{\"secs\":1212152},\"mem\":{\"kb\":8920,\"percent\":0.1},\"cpu\":{\"total\":0}},{\"name\":\"garden\",\"state\":\"running\",\"uptime\":{\"secs\":1212153},\"mem\":{\"kb\":235004,\"percent\":2.8},\"cpu\":{\"total\":0.2}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"abe6a4e9-cfca-490b-8515-2893f9e54d20\",\"bootstrap\":false,\"ignore\":false}\n\n\n\n\nExample of a single VM details formatted\n\u00b6\n\n\n{\n\n  \n\"agent_id\"\n:\n \n\"c5e7c705-459e-41c0-b640-db32d8dc6e71\"\n,\n\n\n  \n\"vm_cid\"\n:\n \n\"ec974048-3352-4ba4-669d-beab87b16bcb\"\n,\n\n  \n\"resource_pool\"\n:\n \n\"medium_z1\"\n,\n\n  \n\"disk_cid\"\n:\n \nnull\n,\n\n\n  \n\"job_name\"\n:\n \n\"doppler_z1\"\n,\n\n  \n\"index\"\n:\n \n0\n,\n\n  \n\"resurrection_paused\"\n:\n \nfalse\n,\n\n\n  \n\"job_state\"\n:\n \n\"running\"\n,\n\n  \n\"ips\"\n:\n \n[\n \n\"10.244.0.142\"\n \n],\n\n  \n\"dns\"\n:\n \n[],\n\n\n  \n\"vitals\"\n:\n \n{\n\n    \n\"cpu\"\n:\n \n{\n\n      \n\"sys\"\n:\n \n\"9.1\"\n,\n\n      \n\"user\"\n:\n \n\"2.1\"\n,\n\n      \n\"wait\"\n:\n \n\"1.7\"\n\n    \n},\n\n    \n\"disk\"\n:\n \n{\n\n      \n\"ephemeral\"\n:\n \n{\n\n        \n\"inode_percent\"\n:\n \n\"11\"\n,\n\n        \n\"percent\"\n:\n \n\"36\"\n\n      \n},\n\n      \n\"system\"\n:\n \n{\n\n        \n\"inode_percent\"\n:\n \n\"11\"\n,\n\n        \n\"percent\"\n:\n \n\"36\"\n\n      \n}\n\n    \n},\n\n    \n\"load\"\n:\n \n[\n \n\"0.61\"\n,\n \n\"0.74\"\n,\n \n\"1.10\"\n \n],\n\n    \n\"mem\"\n:\n \n{\n\n      \n\"kb\"\n:\n \n\"2520960\"\n,\n\n      \n\"percent\"\n:\n \n\"41\"\n\n    \n},\n\n    \n\"swap\"\n:\n \n{\n\n      \n\"kb\"\n:\n \n\"102200\"\n,\n\n      \n\"percent\"\n:\n \n\"10\"\n\n    \n}\n\n  \n},\n\n\n  \n\"processes\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"doppler\"\n,\n\n      \n\"state\"\n:\n \n\"running\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"syslog_drain_binder\"\n,\n\n      \n\"state\"\n:\n \n\"running\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"metron_agent\"\n,\n\n      \n\"state\"\n:\n \n\"running\"\n\n    \n}\n\n  \n]\n\n\n}\n\n\n\n\n\n\n\nEvents \n\u00b6\n\n\nSee \nEvents\n for info.\n\n\nGET /events\n: List events \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Array]: List 200 events matching particular criteria. See query params below for filtering options.\n\n\n\n\nid\n [String]: Event ID.\n\n\nparent_id\n [String]: Associated start event ID if this event represents an end of some action.\n\n\ntimestamp\n [Integer]: Time at which event was recorded.\n\n\nuser\n [String]: Associated user name. Also can be \n_director\n for system initiated events. Example: \nadmin\n.\n\n\naction\n [String]: Action performed against an object. Example: \ncreate\n, \ndelete\n, \nupdate\n.\n\n\nobject_type\n [String]: Type of an affected object. Example: \ndeployment\n, \ninstance\n.\n\n\nobject_name\n [String]: Identifier of an affected object. Example: \nbosh\n (deployment), \nbosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\n (instance).\n\n\ntask\n [String]: Associated task ID. Example: \n293543\n.\n\n\ndeployment\n [String]: Name of the deployment.\n\n\nerror\n [String]: Error description if an error happened.\n\n\ncontext\n [Hash]: Additional data specific to this event. For example for update deployment ending event context includes list of releases and stemcells before and after the deployment.\n\n\n\n\nQuery filters\n\u00b6\n\n\n\n\nbefore_id`:\n [String]: Event ID.\n\n\nbefore_time\n [String]: Ruby parseable time. Example: \nThu May 4 17:06:40 UTC 2017\n.\n\n\nafter_time\n [String]: Ruby parseable time. Example: \nThu May 4 17:06:40 UTC 2017\n\n\ntask\n [String]: Task ID.\n\n\ndeployment\n [String]: Deployment name.\n\n\ninstance\n [String]: Instance name.\n\n\nuser\n [String]: User name.\n\n\naction\n [String]: Action.\n\n\nobject_type\n [String]: Object type.\n\n\nobject_name\n [String]: Object name.\n\n\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k https://admin:admin@192.168.50.4:25555/events \n|\n jq .\n\n\n\n\n[\n\n  \n{\n\n    \n\"id\"\n:\n \n\"3134\"\n,\n\n    \n\"parent_id\"\n:\n \n\"3123\"\n,\n\n    \n\"timestamp\"\n:\n \n1493917600\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"action\"\n:\n \n\"update\"\n,\n\n    \n\"object_type\"\n:\n \n\"deployment\"\n,\n\n    \n\"object_name\"\n:\n \n\"bosh\"\n,\n\n    \n\"task\"\n:\n \n\"37037\"\n,\n\n    \n\"deployment\"\n:\n \n\"bosh\"\n,\n\n    \n\"context\"\n:\n \n{\n\n      \n\"before\"\n:\n \n{\n\n        \n\"releases\"\n:\n \n[\n\n          \n\"uaa/27\"\n,\n\n          \n\"bosh/261.4+dev.1493403626\"\n,\n\n          \n\"bosh-aws-cpi/62+dev.1\"\n,\n\n        \n],\n\n        \n\"stemcells\"\n:\n \n[\n\n          \n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3363.9\"\n\n        \n]\n\n      \n},\n\n      \n\"after\"\n:\n \n{\n\n        \n\"releases\"\n:\n \n[\n\n          \n\"uaa/27\"\n,\n\n          \n\"bosh/261.4+dev.1493916984\"\n,\n\n          \n\"bosh-aws-cpi/62+dev.1\"\n,\n\n        \n],\n\n        \n\"stemcells\"\n:\n \n[\n\n          \n\"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3363.9\"\n\n        \n]\n\n      \n}\n\n    \n}\n\n  \n},\n\n  \n{\n\n    \n\"id\"\n:\n \n\"3133\"\n,\n\n    \n\"parent_id\"\n:\n \n\"3132\"\n,\n\n    \n\"timestamp\"\n:\n \n1493917600\n,\n\n    \n\"user\"\n:\n \n\"admin\"\n,\n\n    \n\"action\"\n:\n \n\"update\"\n,\n\n    \n\"object_type\"\n:\n \n\"instance\"\n,\n\n    \n\"object_name\"\n:\n \n\"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\"\n,\n\n    \n\"task\"\n:\n \n\"37037\"\n,\n\n    \n\"deployment\"\n:\n \n\"bosh\"\n,\n\n    \n\"instance\"\n:\n \n\"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\"\n,\n\n    \n\"context\"\n:\n \n{}\n\n  \n}\n\n\n]\n\n\n\n\n\n\n\nGET /events/{id}\n: Retrieve single event \n\u00b6\n\n\nResponse body schema\n\u00b6\n\n\n[root]\n [Hash]: Event details.\n\n\nSee additional schema details \nabove\n.\n\n\nExample\n\u00b6\n\n\n$ curl -v -s -k \n'https://admin:admin@192.168.50.4:25555/events/3133'\n \n|\n jq .\n\n\n\n\n{\n\n  \n\"id\"\n:\n \n\"3133\"\n,\n\n  \n\"parent_id\"\n:\n \n\"3132\"\n,\n\n  \n\"timestamp\"\n:\n \n1493917600\n,\n\n  \n\"user\"\n:\n \n\"admin\"\n,\n\n  \n\"action\"\n:\n \n\"update\"\n,\n\n  \n\"object_type\"\n:\n \n\"instance\"\n,\n\n  \n\"object_name\"\n:\n \n\"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\"\n,\n\n  \n\"task\"\n:\n \n\"37037\"\n,\n\n  \n\"deployment\"\n:\n \n\"bosh\"\n,\n\n  \n\"instance\"\n:\n \n\"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\"\n,\n\n  \n\"context\"\n:\n \n{}\n\n\n}\n\n\n\n\n\n\n\nPOST /events\n: Create single event \n\u00b6\n\n\nRequest body schema\n\u00b6\n\n\n[root]\n [Hash]: Event details.\n\n\n\n\ntimestamp\n [String, optional]: Optionally provide a timestamp when event occurred.\n\n\naction\n [String, required]\n\n\nobject_type\n [String, required]\n\n\nobject_name\n [String, required]\n\n\ndeployment\n [String, optional]: Deployment name.\n\n\ninstance\n [String, optional]: Instance name.\n\n\nerror\n [String, optional]: Error description.\n\n\ncontext\n [Hash, optional]",
            "title": "Director HTTP API"
        },
        {
            "location": "/director-api-v1/#overview",
            "text": "",
            "title": "Overview "
        },
        {
            "location": "/director-api-v1/#security",
            "text": "All API access should be done over verified HTTPS.  The Director can be configured in two authentication modes:  basic auth  and  UAA .  Info endpoint  does not require any authentication and can be used to determine which authentication mechanism to use. All other endpoints require authentication.  401 Unauthorized  will be returned for requests that contain an invalid basic auth credentials or an invalid/expired UAA access token.",
            "title": "Security "
        },
        {
            "location": "/director-api-v1/#http-verbs",
            "text": "Standard HTTP verb semantics are followed:     Verb  Description      GET  Used for retrieving resources.    POST/PUT  Used for creating/updating resources.    DELETE  Used for deleting resources.",
            "title": "HTTP verbs "
        },
        {
            "location": "/director-api-v1/#http-redirects",
            "text": "Any request may result in a redirection. Receiving an HTTP redirection is not an error and clients should follow that redirect. Redirect responses will have a Location header field. Clients should use same authentication method when following a redirect.",
            "title": "HTTP redirects "
        },
        {
            "location": "/director-api-v1/#rate-limiting",
            "text": "Currently no rate limiting is performed.",
            "title": "Rate limiting "
        },
        {
            "location": "/director-api-v1/#pagination",
            "text": "Currently none of the resources are paginated.",
            "title": "Pagination "
        },
        {
            "location": "/director-api-v1/#long-running-operations-aka-director-tasks",
            "text": "Certain requests result in complex and potentially long running operations against the IaaS, blobstore, or other resources.  POST /deployments  is a good example. Such requests start a  Director task  and continue running on the Director after response is returned. Response to such request will be a  302 Moved Temporarily  redirect to a created task resource.  Once a Director task is created, clients can follow its progress by polling  GET /tasks/{id}  to find out its state. While waiting for the task to finish, different types of logs ( event ,  result ,  debug  information, etc.) can be followed to gain insight into what the task is doing.",
            "title": "Long running operations (aka Director tasks) "
        },
        {
            "location": "/director-api-v1/#general",
            "text": "",
            "title": "General "
        },
        {
            "location": "/director-api-v1/#get-info-info",
            "text": "",
            "title": "GET /info: Info "
        },
        {
            "location": "/director-api-v1/#response-body-schema",
            "text": "[root]  [Hash]: Director details.   name  [String]: Name of the Director.  uuid  [String]: Unique ID of the Director.  version  [String]: Version of the Director software.  user  [String or null]: Logged in user's user name if authentication is provided, otherwise null.  cpi  [String]: Name of the CPI the Director will use.  user_authentication  [Hash]:  type  [String]: Type of the authentication the Director is configured to expect.  options  [Hash]: Additional information provided to how authentication should be performed.    features  [Hash]:  config_server  [Hash]:  status  [Boolean]: Default false.  extras  [Hash]:  urls  [Array]: List of URLs for the Config Server.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#notes",
            "text": "This is the only endpoint that does not require authentication.  In future  version  will contain version of the deployed BOSH release.",
            "title": "Notes"
        },
        {
            "location": "/director-api-v1/#example",
            "text": "$ curl -s -k https://192.168.50.4:25555/info  |  jq .  { \n   \"name\" :   \"Bosh   Lite   Director\" , \n   \"uuid\" :   \"2daf673a-9755-4b4f-aa6d-3632fbed8012\" , \n   \"version\" :   \"1.3126.0   (00000000)\" , \n   \"user\" :   null , \n   \"cpi\" :   \"warden_cpi\" , \n   \"user_authentication\" :   { \n     \"type\" :   \"basic\" , \n     \"options\" :   {} \n   }  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#configs",
            "text": "",
            "title": "Configs "
        },
        {
            "location": "/director-api-v1/#get-configs-list-configs",
            "text": "",
            "title": "GET /configs: List configs "
        },
        {
            "location": "/director-api-v1/#request-query",
            "text": "latest  [Boolean, required]: Returns latest configs when set to  true . Otherwise return all configs when set to  false . Possible values:  true  or  false . There is no default.  type  [String, optional]: Filters list of configs by the given type.  name  [String, optional]: Filters list of configs by the given name.",
            "title": "Request query"
        },
        {
            "location": "/director-api-v1/#response-body-schema_1",
            "text": "[root]  [Array]: List of configs.   id  [String]: ID of the config.  name  [String]: Name of the config.  type  [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.  content  [String]: YAML containing the config manifest.  created_at  [Time]: Creation time of the config.  deleted  [Boolean]: Soft delete flag.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_1",
            "text": "$ curl -s -k https://192.168.50.4:25555/configs?latest = true   |  jq .  [ \n    { \n     \"content\" :   \"azs:\\n-   name:   z1\\n...\" , \n     \"id\" :   \"1\" , \n     \"type\" :   \"cloud\" , \n     \"name\" :   \"default\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#post-configs-create-config",
            "text": "",
            "title": "POST /configs: Create config. "
        },
        {
            "location": "/director-api-v1/#request-headers",
            "text": "Content-Type  must be  text/json .",
            "title": "Request headers"
        },
        {
            "location": "/director-api-v1/#request-body",
            "text": "The request body consists of a single JSON hash with the following key, value pairs:   name  [String]: Name of the config.  type  [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.  content  [String]: YAML containing the config manifest.",
            "title": "Request body"
        },
        {
            "location": "/director-api-v1/#response-body-schema_2",
            "text": "[root]  [Hash]: The newly created config.   name  [String]: Name of the config.  type  [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.  content  [String]: YAML containing the config manifest.  created_at  [Time]: Creation time of the config.  deleted  [Boolean]: Soft delete flag.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_2",
            "text": "$ curl -s -k -H  'Content-Type: application/json'  -d  '{\"name\": \"test\", \"type\": \"cloud\", \"content\": \"--- {}\"}'  https://192.168.50.4:25555/configs  |  jq .  { \n   \"content\" :   \"---   {}\" , \n   \"id\" :   \"3\" , \n   \"type\" :   \"cloud\" , \n   \"name\" :   \"test\"  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#post-configsdiff-diff-config",
            "text": "",
            "title": "POST /configs/diff: Diff config. "
        },
        {
            "location": "/director-api-v1/#request-headers_1",
            "text": "Content-Type  must be  text/json .",
            "title": "Request headers"
        },
        {
            "location": "/director-api-v1/#request-body_1",
            "text": "The request body consists of a single JSON hash with the following key, value pairs:   name  [String]: Name of the config.  type  [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.  content  [String]: YAML containing the config manifest.",
            "title": "Request body"
        },
        {
            "location": "/director-api-v1/#response-body-schema_3",
            "text": "[root]  [Hash]: The changes.   diff  [Array]: List of differences.  error  [String]: Error description if an error happened.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_3",
            "text": "$ curl -s -k -H  'Content-Type: application/json'  -d  '{\"name\": \"default\", \"type\": \"cloud\", \"content\": \"--- {}\"}'  https://192.168.50.4:25555/configs/diff  |  jq .  { \n   \"diff\" :   [ \n     [ \n       \"az:   z2\" , \n       \"added\" \n     ] \n   ]  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#delete-configs-marks-configs-as-deleted",
            "text": "",
            "title": "DELETE /configs: Marks configs as deleted. "
        },
        {
            "location": "/director-api-v1/#request-query_1",
            "text": "name  [String]: Name of the config.  type  [String]: Type of the config, i.e. 'cloud', 'cpi', 'runtime'.",
            "title": "Request Query"
        },
        {
            "location": "/director-api-v1/#example_4",
            "text": "$ curl -s -k -X DELETE https://192.168.50.4:25555/configs?type = cloud & name = test",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#tasks",
            "text": "See  Director tasks  for related info.",
            "title": "Tasks "
        },
        {
            "location": "/director-api-v1/#get-tasks-list-all-tasks",
            "text": "",
            "title": "GET /tasks: List all tasks "
        },
        {
            "location": "/director-api-v1/#response-body-schema_4",
            "text": "[root]  [Array]: List of tasks.   id  [Integer]: Numeric ID of the task.  state  [String]: Current state of the task. Possible values are:  queued ,  processing ,  cancelled ,  cancelling ,  done ,  error ,  timeout .  description  [String]: Description of the task's purpose.  timestamp  [Integer]: todo.  result  [String or null]: Description of the task's result. Will not be populated (string) unless tasks finishes.  user  [String]: User which started the task.  context_id  [String]: Context ID of the task, if provided when task was created, otherwise empty string.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_5",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks?verbose=2&limit=3'   |  jq .  [ \n   { \n     \"id\" :   1180 , \n     \"state\" :   \"processing\" , \n     \"description\" :   \"run   errand   acceptance_tests   from   deployment   cf-warden\" , \n     \"timestamp\" :   1447033291 , \n     \"result\" :   null , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"\" \n   }, \n   { \n     \"id\" :   1179 , \n     \"state\" :   \"done\" , \n     \"description\" :   \"scan   and   fix\" , \n     \"timestamp\" :   1447031334 , \n     \"result\" :   \"scan   and   fix   complete\" , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"\" \n   }, \n   { \n     \"id\" :   1178 , \n     \"state\" :   \"done\" , \n     \"description\" :   \"scan   and   fix\" , \n     \"timestamp\" :   1447031334 , \n     \"result\" :   \"scan   and   fix   complete\" , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksstate-list-currently-running-tasks",
            "text": "",
            "title": "GET /tasks?state=...: List currently running tasks "
        },
        {
            "location": "/director-api-v1/#response-body-schema_5",
            "text": "See schema  above .",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_6",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks?state=queued,processing,cancelling&verbose=2'   |  jq .  [ \n   { \n     \"id\" :   1180 , \n     \"state\" :   \"processing\" , \n     \"description\" :   \"run   errand   acceptance_tests   from   deployment   cf-warden\" , \n     \"timestamp\" :   1447033291 , \n     \"result\" :   null , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksdeployment-list-tasks-associated-with-a-deployment",
            "text": "Other tasks query params can be applied.",
            "title": "GET /tasks?deployment=...: List tasks associated with a deployment "
        },
        {
            "location": "/director-api-v1/#response-body-schema_6",
            "text": "See schema  above .",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_7",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks?deploymet=cf-warden'   |  jq .  [ \n   { \n     \"id\" :   1180 , \n     \"state\" :   \"processing\" , \n     \"description\" :   \"run   errand   acceptance_tests   from   deployment   cf-warden\" , \n     \"timestamp\" :   1447033291 , \n     \"result\" :   null , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-taskscontext_id-list-tasks-associated-with-a-context-id",
            "text": "Other tasks query params can be applied.",
            "title": "GET /tasks?context_id=...: List tasks associated with a context ID "
        },
        {
            "location": "/director-api-v1/#response-body-schema_7",
            "text": "See schema  above .",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_8",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks?context_id=4528'   |  jq .  [ \n   { \n     \"id\" :   1180 , \n     \"state\" :   \"processing\" , \n     \"description\" :   \"run   errand   acceptance_tests   from   deployment   cf-warden\" , \n     \"timestamp\" :   1447033291 , \n     \"result\" :   null , \n     \"user\" :   \"admin\" , \n     \"context_id\" :   \"4528\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksid-retrieve-single-task",
            "text": "",
            "title": "GET /tasks/{id}: Retrieve single task "
        },
        {
            "location": "/director-api-v1/#response-body-schema_8",
            "text": "[root]  [Hash]: Task details.  See additional schema details  above .",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_9",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1180'   |  jq .  { \n   \"id\" :   1180 , \n   \"state\" :   \"processing\" , \n   \"description\" :   \"run   errand   acceptance_tests   from   deployment   cf-warden\" , \n   \"timestamp\" :   1447033291 , \n   \"result\" :   null , \n   \"user\" :   \"admin\" , \n   \"context_id\" :   \"\"  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksidoutputtypedebug-retrieve-tasks-debug-log",
            "text": "",
            "title": "GET /tasks/{id}/output?type=debug: Retrieve task's debug log "
        },
        {
            "location": "/director-api-v1/#response-body-schema_9",
            "text": "[root]  [String]: Debug output for the chosen task.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_10",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1180/output?type=debug'   ...\nD, [2015-11-09 02:19:36 #32545] [] DEBUG -- DirectorJobRunner: RECEIVED: director.37d8c089-853e-458c-8535-195085b4b7ed.459b05ae-8b69-4679-b2d5-b34e5fef2dcc {\"value\":{\"agent_task_id\":\"c9f5b328-0656-41f1-631c-e17151be1e18\",\"state\":\"running\"}}\nD, [2015-11-09 02:19:36 #32545] [task:1180] DEBUG -- DirectorJobRunner: (0.000441s) SELECT NULL\nD, [2015-11-09 02:19:36 #32545] [task:1180] DEBUG -- DirectorJobRunner: (0.000317s) SELECT * FROM \"tasks\" WHERE \"id\" = 1180",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksidoutputtypeevent-retrieve-tasks-event-log",
            "text": "",
            "title": "GET /tasks/{id}/output?type=event: Retrieve task's event log "
        },
        {
            "location": "/director-api-v1/#response-body-schema_10",
            "text": "[root]  [String]: Result output for the chosen task. Newlines separate valid event JSON records.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_11",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=event'   ...\n{\n  \"time\": 1446959491,\n  \"stage\": \"Deleting errand instances\",\n  \"tags\": [ \"smoke_tests\" ],\n  \"total\": 1,\n  \"task\": \"59d5b228-a732-4c68-6017-31fe5bc9d8c5\",\n  \"index\": 1,\n  \"state\": \"started\",\n  \"progress\": 0\n}\n{\n  \"time\": 1446959496,\n  \"stage\": \"Deleting errand instances\",\n  \"tags\": [ \"smoke_tests\" ],\n  \"total\": 1,\n  \"task\": \"59d5b228-a732-4c68-6017-31fe5bc9d8c5\",\n  \"index\": 1,\n  \"state\": \"finished\",\n  \"progress\": 100\n}",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-tasksidoutputtyperesult-retrieve-tasks-result",
            "text": "",
            "title": "GET /tasks/{id}/output?type=result: Retrieve task's result "
        },
        {
            "location": "/director-api-v1/#response-body-schema_11",
            "text": "[root]  [String]: Result output for the chosen task. Contents depend on a type of task.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example-of-vm-details-task",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=result'   ...\n{\"vm_cid\":\"ec974048-3352-4ba4-669d-beab87b16bcb\",\"disk_cid\":null,\"ips\":[\"10.244.0.142\"],\"dns\":[],\"agent_id\":\"c5e7c705-459e-41c0-b640-db32d8dc6e71\",\"job_name\":\"doppler_z1\",\"index\":0,\"job_state\":\"running\",\"resource_pool\":\"medium_z1\",\"vitals\":{\"cpu\":{\"sys\":\"9.1\",\"user\":\"2.1\",\"wait\":\"1.7\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"11\",\"percent\":\"36\"},\"system\":{\"inode_percent\":\"11\",\"percent\":\"36\"}},\"load\":[\"0.61\",\"0.74\",\"1.10\"],\"mem\":{\"kb\":\"2520960\",\"percent\":\"41\"},\"swap\":{\"kb\":\"102200\",\"percent\":\"10\"}},\"processes\":[{\"name\":\"doppler\",\"state\":\"running\"},{\"name\":\"syslog_drain_binder\",\"state\":\"running\"},{\"name\":\"metron_agent\",\"state\":\"running\"}],\"resurrection_paused\":false}",
            "title": "Example of VM details task"
        },
        {
            "location": "/director-api-v1/#stemcells",
            "text": "",
            "title": "Stemcells "
        },
        {
            "location": "/director-api-v1/#get-stemcells-list-all-uploaded-stemcells",
            "text": "",
            "title": "GET /stemcells: List all uploaded stemcells "
        },
        {
            "location": "/director-api-v1/#response-body-schema_12",
            "text": "[root]  [Array]: List of stemcells.   name  [String]: Name of the stemcell.  version  [String]: Version of the stemcell.  operating_system  [String]: Operating system identifier. Example:  ubuntu-trusty  and  centos-7 .  cid  [String]: Cloud ID of the stemcell.  deployments  [Array]: List of deployments currently using this stemcell version.  name  [String]: Deployment name.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_12",
            "text": "$ curl -v -s -k https://admin:admin@192.168.50.4:25555/stemcells  |  jq .  [ \n   { \n     \"name\" :   \"bosh-warden-boshlite-ubuntu-trusty-go_agent\" , \n     \"operating_system\" :   \"ubuntu-trusty\" , \n     \"version\" :   \"3126\" , \n     \"cid\" :   \"c3705a0d-0dd3-4b67-52b5-50533a432244\" , \n     \"deployments\" :   [ \n       {   \"name\" :   \"cf-warden\"   } \n     ] \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#releases",
            "text": "",
            "title": "Releases "
        },
        {
            "location": "/director-api-v1/#get-releases-list-all-uploaded-releases",
            "text": "",
            "title": "GET /releases: List all uploaded releases "
        },
        {
            "location": "/director-api-v1/#response-body-schema_13",
            "text": "[root]  [Array]: List of releases.   name  [String]: Name of the release.  release_versions  [Array]: List of versions available.  version  [String]: Version of the release version.  commit_hash  [String]: Identifier in the SCM repository for the release version source code.  uncommitted_changes  [Boolean]: Whether or not the release version was created from a SCM repository with unsaved changes.  currently_deployed  [Boolean]: Whether or not the release version is used by any deployments.  job_names  [Array of strings]: List of job names associated with the release version.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_13",
            "text": "$ curl -v -s -k https://admin:admin@192.168.50.4:25555/releases  |  jq .  [ \n   { \n     \"name\" :   \"bosh-warden-cpi\" , \n     \"release_versions\" :   [ \n       { \n         \"version\" :   \"28\" , \n         \"commit_hash\" :   \"4c36884a\" , \n         \"uncommitted_changes\" :   false , \n         \"currently_deployed\" :   false , \n         \"job_names\" :   [   \"warden_cpi\"   ] \n       } \n     ] \n   }, \n   { \n     \"name\" :   \"test\" , \n     \"release_versions\" :   [ \n       { \n         \"version\" :   \"0+dev.16\" , \n         \"commit_hash\" :   \"31ef3167\" , \n         \"uncommitted_changes\" :   true , \n         \"currently_deployed\" :   false , \n         \"job_names\" :   [   \"http_server\" ,   \"service\"   ] \n       }, \n       { \n         \"version\" :   \"0+dev.17\" , \n         \"commit_hash\" :   \"e5416248\" , \n         \"uncommitted_changes\" :   true , \n         \"currently_deployed\" :   true , \n         \"job_names\" :   [   \"drain\" ,   \"errand\" ,   \"http_server\" ,   \"pre_start\" ,   \"service\"   ] \n       }, \n     ] \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#deployments",
            "text": "",
            "title": "Deployments "
        },
        {
            "location": "/director-api-v1/#get-deployments-list-all-deployments",
            "text": "",
            "title": "GET /deployments: List all deployments "
        },
        {
            "location": "/director-api-v1/#response-body-schema_14",
            "text": "[root]  [Array]: List of deployments.   name  [String]: Name of the deployment.  cloud_config  [String]: Indicator whether latest cloud config is used for this deployment. Possible values:  none ,  outdated ,  latest .  releases  [Array]: List of releases used by the deployment.  name  [String]: Name of the release.  version  [String]: Version of the release.    stemcells  [Array]: List of stemcells used by the deploymemt.  name  [String]: Name of the stemcell.  version  [String]: Version of the stemcell.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_14",
            "text": "$ curl -v -s -k https://admin:admin@192.168.50.4:25555/deployments  |  jq .  [ \n   { \n     \"name\" :   \"cf-warden\" , \n     \"cloud_config\" :   \"none\" , \n     \"releases\" :   [ \n       { \n         \"name\" :   \"cf\" , \n         \"version\" :   \"222\" \n       }, \n       { \n         \"name\" :   \"cf\" , \n         \"version\" :   \"223\" \n       } \n     ], \n     \"stemcells\" :   [ \n       { \n         \"name\" :   \"bosh-warden-boshlite-ubuntu-trusty-go_agent\" , \n         \"version\" :   \"2776\" \n       }, \n       { \n         \"name\" :   \"bosh-warden-boshlite-ubuntu-trusty-go_agent\" , \n         \"version\" :   \"3126\" \n       } \n     ] \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#post-deployments-createupdate-single-deployment",
            "text": "",
            "title": "POST /deployments: Create/update single deployment "
        },
        {
            "location": "/director-api-v1/#request-query_2",
            "text": "recreate  [Boolean]: Whether or not to ignore deletion errors. Possible values:  true  or not present. Default is not present.  skip_drain  [String]: Comma separated list of job names that should not run drain scripts during the update. Possible values:  *  to represent all jobs,  <job1>,<job2>  to list job names, or not present. Default is not present.",
            "title": "Request query"
        },
        {
            "location": "/director-api-v1/#request-headers_2",
            "text": "Content-Type  must be  text/yaml .  X-Bosh-Context-Id  can be optionally configured with a Context ID that can be used to link related BOSH requests",
            "title": "Request headers"
        },
        {
            "location": "/director-api-v1/#request-body-scheme",
            "text": "[root]  [String]: Manifest string. Note that non-exact version values ( latest  value) for releases and stemcells must be resolved before making a request.",
            "title": "Request body scheme"
        },
        {
            "location": "/director-api-v1/#response",
            "text": "Creating/updating a deployment is performed in a Director task. Response will be a redirect to a task resource.",
            "title": "Response"
        },
        {
            "location": "/director-api-v1/#get-deploymentsname-retrieve-single-deployment",
            "text": "",
            "title": "GET /deployments/{name}: Retrieve single deployment "
        },
        {
            "location": "/director-api-v1/#response-body-schema_15",
            "text": "[root]  [Hash]: Single deployment.   manifest  [String]: Last successfully deployed manifest string.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_15",
            "text": "$ curl -v -s -k https://admin:admin@192.168.50.4:25555/deployments/cf-warden  |  jq .  { \n   \"manifest\" :   \"---\\nname:   cf-warden\\n...\" ,  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#delete-deploymentsname-delete-single-deployment",
            "text": "",
            "title": "DELETE /deployments/{name}: Delete single deployment "
        },
        {
            "location": "/director-api-v1/#request-query_3",
            "text": "force  [Boolean]: Whether or not to ignore deletion errors. Dangerous! Possible values:  true  or not present. Default is not present.",
            "title": "Request query"
        },
        {
            "location": "/director-api-v1/#request-body_2",
            "text": "Empty.",
            "title": "Request body"
        },
        {
            "location": "/director-api-v1/#response_1",
            "text": "Deleting a deployment is performed in a Director task. Response will be a redirect to a task resource.",
            "title": "Response"
        },
        {
            "location": "/director-api-v1/#instances-in-a-deployment",
            "text": "Note: This feature is available with bosh-release v256+.  Instances  represent the expected state of the VMs of a deployment. The actual state of the VMs can be retrieved with the  vms  endpoints .  instances  is similar to  vms , but also contains instances that do not have a VM.",
            "title": "Instances in a deployment "
        },
        {
            "location": "/director-api-v1/#get-deploymentsnameinstances-list-all-instances",
            "text": "",
            "title": "GET /deployments/{name}/instances: List all instances "
        },
        {
            "location": "/director-api-v1/#response-body-schema_16",
            "text": "[root]  [Array]: List of instances.   agent_id  [String]: Unique ID of the Agent associated with the VM. Could be  nil  if there is no VM for this instance.  cid  [String]: Cloud ID of the VM. Could be  nil  if there is no VM for this instance.  job  [String]: Name of the job.  index  [Integer]: Numeric job index.  id  [String]: ID of the instance.  expects_vm  [Boolean]:  true  if a VM should exist for this instance.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#notes_1",
            "text": "This endpoint does not query Agents on the VMs, hence is returned immediately.",
            "title": "Notes"
        },
        {
            "location": "/director-api-v1/#example_16",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/deployments/example/instances'   |  jq .  [ \n   { \n     \"agent_id\" :   \"c5e7c705-459e-41c0-b640-db32d8dc6e71\" , \n     \"cid\" :   \"ec974048-3352-4ba4-669d-beab87b16bcb\" , \n     \"job\" :   \"example_service\" , \n     \"index\" :   0 , \n     \"id\" :   \"209b96c8-e482-43c7-9f3e-04de9f93c535\" , \n     \"expects_vm\" :   true \n   }, \n   { \n     \"agent_id\" :   nil , \n     \"cid\" :   nil , \n     \"job\" :   \"example_errand\" , \n     \"index\" :   0 , \n     \"id\" :   \"548d6aa0-eb8f-4890-bd3a-e6b526f3aeea\" , \n     \"expects_vm\" :   false \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-deploymentsnameinstancesformatfull-list-details-of-instances",
            "text": "",
            "title": "GET /deployments/{name}/instances?format=full: List details of instances "
        },
        {
            "location": "/director-api-v1/#response-body-schema_17",
            "text": "[root]  [String]: For each instance there is one line of JSON. The JSON contains the following details.   agent_id  [String]: Unique ID of the Agent associated with the VM.  vm_cid  [String]: Cloud ID of the VM.  resource_pool  [String]: Name of the resource pool used for the VM.  disk_cid  [String or null]: Cloud ID of the associated persistent disk if one is attached.  job_name  [String]: Name of the job.  index  [Integer]: Numeric job index.  resurrection_paused  [Boolean]: Whether or not resurrector will try to bring back the VM is it goes missing.  job_state  [String]: Aggregate state of job. Possible values:  running  and other values that represent unhealthy state.  ips  [Array of strings]: List of IPs.  dns  [Array of strings]: List of DNS records.  vitals  [Hash]: VM vitals.  processes  [Array of hashes]: List of processes running as part of the job.  state  [String]: State of instance  vm_type  [String]: Name of  VM type  az  [String]: Name of  availability zone  id  [String]: ID of instance  bootstrap  [Boolean]: bootstrap property of  instance specific configuration  ignore  [Boolean]: Ignore this instance if set to  true",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_17",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/deployments/example/instances?format=full' \n< HTTP/1.1  302  Moved Temporarily\n< Location: https://192.168.50.4:25555/tasks/1287\n...\n\n$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1287'   |  jq .\n\n$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1287/output?type=result'   ...\n{\"vm_cid\":\"3938cc70-8f5e-4318-ad05-24d991e0e66e\",\"disk_cid\":null,\"ips\":[\"10.0.1.3\"],\"dns\":[],\"agent_id\":\"d927e75b-2a2d-4015-b5cc-306a067e94e9\",\"job_name\":\"example_service\",\"index\":1,\"job_state\":\"running\",\"state\":\"started\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"0.3\",\"user\":\"0.1\",\"wait\":\"0.0\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"5\",\"percent\":\"32\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"66\"}},\"load\":[\"0.00\",\"0.01\",\"0.10\"],\"mem\":{\"kb\":\"605008\",\"percent\":\"7\"},\"swap\":{\"kb\":\"75436\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"beacon\",\"state\":\"running\",\"uptime\":{\"secs\":1212184},\"mem\":{\"kb\":776,\"percent\":0},\"cpu\":{\"total\":0}},{\"name\":\"baggageclaim\",\"state\":\"running\",\"uptime\":{\"secs\":1212152},\"mem\":{\"kb\":8920,\"percent\":0.1},\"cpu\":{\"total\":0}},{\"name\":\"garden\",\"state\":\"running\",\"uptime\":{\"secs\":1212153},\"mem\":{\"kb\":235004,\"percent\":2.8},\"cpu\":{\"total\":0.2}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"abe6a4e9-cfca-490b-8515-2893f9e54d20\",\"bootstrap\":false,\"ignore\":false}\n{\"vm_cid\":\"86eb5e7e-a1c8-4f7b-a20c-cd696bf80938\",\"disk_cid\":\"70b3c01c-729e-4335-9630-1f1985a40c99\",\"ips\":[\"10.0.1.5\"],\"dns\":[],\"agent_id\":\"7a54d3bb-f77b-412f-b662-dbff7733a823\",\"job_name\":\"example_errand\",\"index\":0,\"job_state\":\"stopped\",\"state\":\"stopped\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"1.3\",\"user\":\"4.9\",\"wait\":\"0.1\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"67\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"48\"}},\"load\":[\"0.00\",\"0.03\",\"0.05\"],\"mem\":{\"kb\":\"227028\",\"percent\":\"6\"},\"swap\":{\"kb\":\"25972\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"postgresql\",\"state\":\"running\",\"uptime\":{\"secs\":1212309},\"mem\":{\"kb\":489836,\"percent\":12.1},\"cpu\":{\"total\":0}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"548d7aa0-eb8f-4890-bd3a-e9b526f3aeeb\",\"bootstrap\":false,\"ignore\":false}",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#formatted-example-of-details-of-a-single-instance",
            "text": "{ \n   \"vm_cid\" :   \"86eb5e8e-a8c8-4f7b-a20c-cd696bf80938\" , \n   \"disk_cid\" :   \"70a3c01c-728e-4335-9630-1f1985a40c99\" , \n   \"ips\" :   [ \n     \"10.0.1.5\" \n   ], \n   \"dns\" :   [], \n   \"agent_id\" :   \"0a54d3bb-f78b-412f-b662-dbff7733a823\" , \n   \"job_name\" :   \"example_service\" , \n   \"index\" :   0 , \n   \"job_state\" :   \"running\" , \n   \"state\" :   \"started\" , \n   \"resource_pool\" :   \"resource_pool_1\" , \n   \"vm_type\" :   \"resource_pool_1\" , \n   \"vitals\" :   { \n     \"cpu\" :   { \n       \"sys\" :   \"1.3\" , \n       \"user\" :   \"4.9\" , \n       \"wait\" :   \"0.1\" \n     }, \n     \"disk\" :   { \n       \"ephemeral\" :   { \n         \"inode_percent\" :   \"0\" , \n         \"percent\" :   \"0\" \n       }, \n       \"persistent\" :   { \n         \"inode_percent\" :   \"0\" , \n         \"percent\" :   \"67\" \n       }, \n       \"system\" :   { \n         \"inode_percent\" :   \"34\" , \n         \"percent\" :   \"48\" \n       } \n     }, \n     \"load\" :   [ \n       \"0.00\" , \n       \"0.03\" , \n       \"0.05\" \n     ], \n     \"mem\" :   { \n       \"kb\" :   \"227028\" , \n       \"percent\" :   \"6\" \n     }, \n     \"swap\" :   { \n       \"kb\" :   \"25972\" , \n       \"percent\" :   \"1\" \n     } \n   }, \n   \"processes\" :   [ \n     { \n       \"name\" :   \"postgresql\" , \n       \"state\" :   \"running\" , \n       \"uptime\" :   { \n         \"secs\" :   1212309 \n       }, \n       \"mem\" :   { \n         \"kb\" :   489836 , \n         \"percent\" :   12.1 \n       }, \n       \"cpu\" :   { \n         \"total\" :   0 \n       } \n     } \n   ], \n   \"resurrection_paused\" :   true , \n   \"az\" :   null , \n   \"id\" :   \"548d6aa0-eb8f-4890-bd3a-e9b526f3aeeb\" , \n   \"bootstrap\" :   false , \n   \"ignore\" :   false  }",
            "title": "Formatted example of details of a single instance"
        },
        {
            "location": "/director-api-v1/#vms-in-a-deployment",
            "text": "",
            "title": "VMs in a deployment "
        },
        {
            "location": "/director-api-v1/#get-deploymentsnamevms-list-all-vms",
            "text": "",
            "title": "GET /deployments/{name}/vms: List all VMs "
        },
        {
            "location": "/director-api-v1/#response-body-schema_18",
            "text": "[root]  [Array]: List of VMs.   agent_id  [String]: Unique ID of the Agent associated with the VM.  cid  [String]: Cloud ID of the VM.  job  [String]: Name of the job.  index  [Integer]: Numeric job index.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#notes_2",
            "text": "This endpoint does not query Agents on the VMs, hence is returned immediately.",
            "title": "Notes"
        },
        {
            "location": "/director-api-v1/#example_18",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/deployments/cf-warden/vms'   |  jq .  [ \n   { \n     \"agent_id\" :   \"c5e7c705-459e-41c0-b640-db32d8dc6e71\" , \n     \"cid\" :   \"ec974048-3352-4ba4-669d-beab87b16bcb\" , \n     \"job\" :   \"doppler_z1\" , \n     \"index\" :   0 \n   }, \n   { \n     \"agent_id\" :   \"81f7b585-f3d3-4dbc-8d7c-f76dbe861bdc\" , \n     \"cid\" :   \"427c1995-2d06-42b2-4218-418150bc31c9\" , \n     \"job\" :   \"api_z1\" , \n     \"index\" :   0 \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-deploymentsnamevmsformatfull-list-vm-details",
            "text": "",
            "title": "GET /deployments/{name}/vms?format=full: List VM details "
        },
        {
            "location": "/director-api-v1/#response-body-schema_19",
            "text": "[root]  [String]: Each VM's details are separated by a newline.   agent_id  [String]: Unique ID of the Agent associated with the VM.  vm_cid  [String]: Cloud ID of the VM.  resource_pool  [String]: Name of the resource pool used for the VM.  disk_cid  [String or null]: Cloud ID of the associated persistent disk if one is attached.  job_name  [String]: Name of the job.  index  [Integer]: Numeric job index.  resurrection_paused  [Boolean]: Whether or not resurrector will try to bring back the VM is it goes missing.  job_state  [String]: Aggregate state of job. Possible values:  running  and other values that represent unhealthy state.  ips  [Array of strings]: List of IPs.  dns  [Array of strings]: List of DNS records.  vitals  [Hash]: VM vitals.  processes  [Array of hashes]: List of processes running as part of the job.  state  [String]: State of the VM  vm_type  [String]: Name of  VM type  az  [String]: Name of  availability zone  id  [String]: ID of the VM  bootstrap  [Boolean]: bootstrap property of  VM specific configuration  ignore  [Boolean]: Ignore this VM if set to  true",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_19",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/deployments/cf-warden/vms?format=full' \n< HTTP/1.1  302  Moved Temporarily\n< Location: https://192.168.50.4:25555/tasks/1181\n...\n\n$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1181'   |  jq .\n\n$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/tasks/1181/output?type=result'   ...\n{\"vm_cid\":\"3938cc70-8f5e-4318-ad05-24d991e0e66e\",\"disk_cid\":null,\"ips\":[\"10.0.1.3\"],\"dns\":[],\"agent_id\":\"d927e75b-2a2d-4015-b5cc-306a067e94e9\",\"job_name\":\"example_service\",\"index\":0,\"job_state\":\"running\",\"state\":\"started\",\"resource_pool\":\"resource_pool_1\",\"vm_type\":\"resource_pool_1\",\"vitals\":{\"cpu\":{\"sys\":\"0.3\",\"user\":\"0.1\",\"wait\":\"0.0\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"5\",\"percent\":\"32\"},\"persistent\":{\"inode_percent\":\"3\",\"percent\":\"67\"},\"system\":{\"inode_percent\":\"34\",\"percent\":\"66\"}},\"load\":[\"0.00\",\"0.01\",\"0.10\"],\"mem\":{\"kb\":\"605008\",\"percent\":\"7\"},\"swap\":{\"kb\":\"75436\",\"percent\":\"1\"}},\"processes\":[{\"name\":\"beacon\",\"state\":\"running\",\"uptime\":{\"secs\":1212184},\"mem\":{\"kb\":776,\"percent\":0},\"cpu\":{\"total\":0}},{\"name\":\"baggageclaim\",\"state\":\"running\",\"uptime\":{\"secs\":1212152},\"mem\":{\"kb\":8920,\"percent\":0.1},\"cpu\":{\"total\":0}},{\"name\":\"garden\",\"state\":\"running\",\"uptime\":{\"secs\":1212153},\"mem\":{\"kb\":235004,\"percent\":2.8},\"cpu\":{\"total\":0.2}}],\"resurrection_paused\":true,\"az\":null,\"id\":\"abe6a4e9-cfca-490b-8515-2893f9e54d20\",\"bootstrap\":false,\"ignore\":false}",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#example-of-a-single-vm-details-formatted",
            "text": "{ \n   \"agent_id\" :   \"c5e7c705-459e-41c0-b640-db32d8dc6e71\" , \n\n   \"vm_cid\" :   \"ec974048-3352-4ba4-669d-beab87b16bcb\" , \n   \"resource_pool\" :   \"medium_z1\" , \n   \"disk_cid\" :   null , \n\n   \"job_name\" :   \"doppler_z1\" , \n   \"index\" :   0 , \n   \"resurrection_paused\" :   false , \n\n   \"job_state\" :   \"running\" , \n   \"ips\" :   [   \"10.244.0.142\"   ], \n   \"dns\" :   [], \n\n   \"vitals\" :   { \n     \"cpu\" :   { \n       \"sys\" :   \"9.1\" , \n       \"user\" :   \"2.1\" , \n       \"wait\" :   \"1.7\" \n     }, \n     \"disk\" :   { \n       \"ephemeral\" :   { \n         \"inode_percent\" :   \"11\" , \n         \"percent\" :   \"36\" \n       }, \n       \"system\" :   { \n         \"inode_percent\" :   \"11\" , \n         \"percent\" :   \"36\" \n       } \n     }, \n     \"load\" :   [   \"0.61\" ,   \"0.74\" ,   \"1.10\"   ], \n     \"mem\" :   { \n       \"kb\" :   \"2520960\" , \n       \"percent\" :   \"41\" \n     }, \n     \"swap\" :   { \n       \"kb\" :   \"102200\" , \n       \"percent\" :   \"10\" \n     } \n   }, \n\n   \"processes\" :   [ \n     { \n       \"name\" :   \"doppler\" , \n       \"state\" :   \"running\" \n     }, \n     { \n       \"name\" :   \"syslog_drain_binder\" , \n       \"state\" :   \"running\" \n     }, \n     { \n       \"name\" :   \"metron_agent\" , \n       \"state\" :   \"running\" \n     } \n   ]  }",
            "title": "Example of a single VM details formatted"
        },
        {
            "location": "/director-api-v1/#events",
            "text": "See  Events  for info.",
            "title": "Events "
        },
        {
            "location": "/director-api-v1/#get-events-list-events",
            "text": "",
            "title": "GET /events: List events "
        },
        {
            "location": "/director-api-v1/#response-body-schema_20",
            "text": "[root]  [Array]: List 200 events matching particular criteria. See query params below for filtering options.   id  [String]: Event ID.  parent_id  [String]: Associated start event ID if this event represents an end of some action.  timestamp  [Integer]: Time at which event was recorded.  user  [String]: Associated user name. Also can be  _director  for system initiated events. Example:  admin .  action  [String]: Action performed against an object. Example:  create ,  delete ,  update .  object_type  [String]: Type of an affected object. Example:  deployment ,  instance .  object_name  [String]: Identifier of an affected object. Example:  bosh  (deployment),  bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62  (instance).  task  [String]: Associated task ID. Example:  293543 .  deployment  [String]: Name of the deployment.  error  [String]: Error description if an error happened.  context  [Hash]: Additional data specific to this event. For example for update deployment ending event context includes list of releases and stemcells before and after the deployment.",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#query-filters",
            "text": "before_id`:  [String]: Event ID.  before_time  [String]: Ruby parseable time. Example:  Thu May 4 17:06:40 UTC 2017 .  after_time  [String]: Ruby parseable time. Example:  Thu May 4 17:06:40 UTC 2017  task  [String]: Task ID.  deployment  [String]: Deployment name.  instance  [String]: Instance name.  user  [String]: User name.  action  [String]: Action.  object_type  [String]: Object type.  object_name  [String]: Object name.",
            "title": "Query filters"
        },
        {
            "location": "/director-api-v1/#example_20",
            "text": "$ curl -v -s -k https://admin:admin@192.168.50.4:25555/events  |  jq .  [ \n   { \n     \"id\" :   \"3134\" , \n     \"parent_id\" :   \"3123\" , \n     \"timestamp\" :   1493917600 , \n     \"user\" :   \"admin\" , \n     \"action\" :   \"update\" , \n     \"object_type\" :   \"deployment\" , \n     \"object_name\" :   \"bosh\" , \n     \"task\" :   \"37037\" , \n     \"deployment\" :   \"bosh\" , \n     \"context\" :   { \n       \"before\" :   { \n         \"releases\" :   [ \n           \"uaa/27\" , \n           \"bosh/261.4+dev.1493403626\" , \n           \"bosh-aws-cpi/62+dev.1\" , \n         ], \n         \"stemcells\" :   [ \n           \"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3363.9\" \n         ] \n       }, \n       \"after\" :   { \n         \"releases\" :   [ \n           \"uaa/27\" , \n           \"bosh/261.4+dev.1493916984\" , \n           \"bosh-aws-cpi/62+dev.1\" , \n         ], \n         \"stemcells\" :   [ \n           \"bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3363.9\" \n         ] \n       } \n     } \n   }, \n   { \n     \"id\" :   \"3133\" , \n     \"parent_id\" :   \"3132\" , \n     \"timestamp\" :   1493917600 , \n     \"user\" :   \"admin\" , \n     \"action\" :   \"update\" , \n     \"object_type\" :   \"instance\" , \n     \"object_name\" :   \"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\" , \n     \"task\" :   \"37037\" , \n     \"deployment\" :   \"bosh\" , \n     \"instance\" :   \"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\" , \n     \"context\" :   {} \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#get-eventsid-retrieve-single-event",
            "text": "",
            "title": "GET /events/{id}: Retrieve single event "
        },
        {
            "location": "/director-api-v1/#response-body-schema_21",
            "text": "[root]  [Hash]: Event details.  See additional schema details  above .",
            "title": "Response body schema"
        },
        {
            "location": "/director-api-v1/#example_21",
            "text": "$ curl -v -s -k  'https://admin:admin@192.168.50.4:25555/events/3133'   |  jq .  { \n   \"id\" :   \"3133\" , \n   \"parent_id\" :   \"3132\" , \n   \"timestamp\" :   1493917600 , \n   \"user\" :   \"admin\" , \n   \"action\" :   \"update\" , \n   \"object_type\" :   \"instance\" , \n   \"object_name\" :   \"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\" , \n   \"task\" :   \"37037\" , \n   \"deployment\" :   \"bosh\" , \n   \"instance\" :   \"bosh/db7658b6-de2f-4c94-a261-acaf4c4b7f62\" , \n   \"context\" :   {}  }",
            "title": "Example"
        },
        {
            "location": "/director-api-v1/#post-events-create-single-event",
            "text": "",
            "title": "POST /events: Create single event "
        },
        {
            "location": "/director-api-v1/#request-body-schema",
            "text": "[root]  [Hash]: Event details.   timestamp  [String, optional]: Optionally provide a timestamp when event occurred.  action  [String, required]  object_type  [String, required]  object_name  [String, required]  deployment  [String, optional]: Deployment name.  instance  [String, optional]: Instance name.  error  [String, optional]: Error description.  context  [Hash, optional]",
            "title": "Request body schema"
        },
        {
            "location": "/runtime-config/",
            "text": "Note: This feature is available with bosh-release v255.4+.\n\n\n\nThe Director has a way to specify global configuration for all VMs in all deployments. The runtime config is a YAML file that defines IaaS agnostic configuration that applies to all deployments.\n\n\n\n\nUpdating and retrieving runtime config \n\u00b6\n\n\nTo update runtime config on the Director use \nbosh update runtime-config\n CLI command.\n\n\nNote: See \nexample runtime config\n below.\n\n\n\n$ bosh update-runtime-config runtime.yml\n\n$ bosh runtime-config\nActing as user \n'admin'\n on \n'micro'\n\n\nreleases:\n- name: strongswan\n  version: \n6\n.0.0\n\naddons:\n- name: security\n  jobs:\n  - name: strongswan\n    release: strongswan\n...\n\n\n\n\nOnce runtime config is updated all deployments will be considered outdated. \nbosh deployments\n does not currently show that but we have plans to show that information. The Director will apply runtime config changes to each deployment during the next \nbosh deploy\n for that deployment.\n\n\n\n\nExample \n\u00b6\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nstrongswan\n\n  \nversion\n:\n \n6.0.0\n\n\n\naddons\n:\n\n\n-\n \nname\n:\n \nsecurity\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nstrongswan\n\n    \nrelease\n:\n \nstrongswan\n\n    \nproperties\n:\n\n      \nstrongswan\n:\n\n        \nca_cert\n:\n \n...\n\n\n\n\n\nNote: To remove all addons, specify empty arrays as follows:\n\n\n\nreleases\n:\n \n[]\n\n\naddons\n:\n \n[]\n\n\n\n\n\n\n\nReleases Block \n\u00b6\n\n\nreleases\n [Array, required]: Specifies the releases used by the addons.\n\n\n\n\nname\n [String, required]: Name of a release name used by an addon.\n\n\nversion\n [String, required]: The version of the release to use. Version \ncannot\n be \nlatest\n; it must be specified explicitly.\n\n\nurl\n [String, optional]: URL of a release to download. Works with CLI v2. Example: \nhttps://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11\n.\n\n\nsha1\n [String, optional]: SHA1 of asset referenced via URL. Works with CLI v2. Example: \n332ac15609b220a3fdf5efad0e0aa069d8235788\n.\n\n\n\n\nSee \nRelease URLs\n for more details.\n\n\nExample:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nstrongswan\n\n  \nversion\n:\n \n6.0.0\n\n\n\n\n\nExample with a URL:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nconcourse\n\n  \nversion\n:\n \n3.3.2\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/concourse/concourse?v=3.3.2\n\n  \nsha1\n:\n \n2c876303dc6866afb845e728eab58abae8ff3be2\n\n\n\n\n\n\n\nAddons Block \n\u00b6\n\n\nOperators typically want to ensure that certain software runs on all VMs managed by the Director. Examples of such software are:\n\n\n\n\nsecurity agents like Tripwire, IPsec, etc.\n\n\nanti-viruses like McAfee\n\n\ncustom health monitoring agents like Datadog\n\n\nlogging agents like Loggregator's Metron\n\n\n\n\nAn addon is a release job that is colocated on all VMs managed by the Director.\n\n\naddons\n [Array, optional]: Specifies the \naddons\n to be applied to all deployments.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the addon.\n\n\njobs\n [Array of hashes, requires]: Specifies the name and release of release jobs to be colocated.\n\n\nname\n [String, required]: The job name.\n\n\nrelease\n [String, required]: The release where the job exists.\n\n\nproperties\n [Hash, optional]: Specifies job properties. Properties allow the Director to configure jobs to a specific environment.\n\n\ninclude\n [Hash, optional]: Specifies inclusion \nplacement rules\n Available in bosh-release v260+.\n\n\nexclude\n [Hash, optional]: Specifies exclusion \nplacement rules\n. Available in bosh-release v260+.\n\n\n\n\nPlacement Rules for \ninclude\n and \nexclude\n Directives \n\u00b6\n\n\nAvailable rules:\n\n\n\n\nstemcell\n [Array of hashes, optional]\n\n\nos\n [String, required]: Matches stemcell's operating system. Example: \nubuntu-trusty\n\n\ndeployments\n [Array of strings, optional]: Matches based on deployment names.\n\n\njobs\n [Array of hashes, optional]\n\n\nname\n [String, required]: Matching job name.\n\n\nrelease\n [String, required]: Matching release name.\n\n\nnetworks\n [Array of strings, optional]: Matches based on network names. Available in bosh-release v262+.\n\n\n\n\nAll arrays within inclusion/exclusion rules use \nor\n operator.\n\n\nExample:\n\n\naddons\n:\n\n\n-\n \nname\n:\n \nsecurity\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nstrongswan\n\n    \nrelease\n:\n \nstrongswan\n\n    \nproperties\n:\n\n      \nstrongswan\n:\n\n        \nca_cert\n:\n \n...\n\n  \n-\n \nname\n:\n \nsyslog_drain\n\n    \nrelease\n:\n \nsyslog\n\n    \nproperties\n:\n\n      \nsyslog_drain_ips\n:\n \n[\n10.10.0.20\n]\n\n  \ninclude\n:\n\n    \ndeployments\n:\n \n[\ndep1\n,\n \ndep2\n]\n\n\n\n\n\nExample with \ninclude\n rules:\n\n\ninclude\n:\n\n  \ndeployments\n:\n \n[\ndep1\n,\n \ndep2\n]\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nredis\n\n    \nrelease\n:\n \nredis-release\n\n  \nstemcell\n:\n\n  \n-\n \nos\n:\n \nubuntu-trusty\n\n\n\n\n\nSee \ncommon addons list\n for several examples.\n\n\n\n\nTags Block \n\u00b6\n\n\ntags\n [Hash, optional]: Specifies key value pairs to be sent to the CPI for VM tagging. Combined with deployment level tags during the deploy. Available in bosh-release v260+.\n\n\nExample:\n\n\ntags\n:\n\n  \nbusiness_unit\n:\n \nmarketing\n\n  \nemail_contact\n:\n \nops@marketing.co.com",
            "title": "Usage"
        },
        {
            "location": "/runtime-config/#updating-and-retrieving-runtime-config",
            "text": "To update runtime config on the Director use  bosh update runtime-config  CLI command.  Note: See  example runtime config  below.  $ bosh update-runtime-config runtime.yml\n\n$ bosh runtime-config\nActing as user  'admin'  on  'micro' \n\nreleases:\n- name: strongswan\n  version:  6 .0.0\n\naddons:\n- name: security\n  jobs:\n  - name: strongswan\n    release: strongswan\n...  Once runtime config is updated all deployments will be considered outdated.  bosh deployments  does not currently show that but we have plans to show that information. The Director will apply runtime config changes to each deployment during the next  bosh deploy  for that deployment.",
            "title": "Updating and retrieving runtime config "
        },
        {
            "location": "/runtime-config/#example",
            "text": "releases :  -   name :   strongswan \n   version :   6.0.0  addons :  -   name :   security \n   jobs : \n   -   name :   strongswan \n     release :   strongswan \n     properties : \n       strongswan : \n         ca_cert :   ...   Note: To remove all addons, specify empty arrays as follows:  releases :   []  addons :   []",
            "title": "Example "
        },
        {
            "location": "/runtime-config/#releases-block",
            "text": "releases  [Array, required]: Specifies the releases used by the addons.   name  [String, required]: Name of a release name used by an addon.  version  [String, required]: The version of the release to use. Version  cannot  be  latest ; it must be specified explicitly.  url  [String, optional]: URL of a release to download. Works with CLI v2. Example:  https://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11 .  sha1  [String, optional]: SHA1 of asset referenced via URL. Works with CLI v2. Example:  332ac15609b220a3fdf5efad0e0aa069d8235788 .   See  Release URLs  for more details.  Example:  releases :  -   name :   strongswan \n   version :   6.0.0   Example with a URL:  releases :  -   name :   concourse \n   version :   3.3.2 \n   url :   https://bosh.io/d/github.com/concourse/concourse?v=3.3.2 \n   sha1 :   2c876303dc6866afb845e728eab58abae8ff3be2",
            "title": "Releases Block "
        },
        {
            "location": "/runtime-config/#addons-block",
            "text": "Operators typically want to ensure that certain software runs on all VMs managed by the Director. Examples of such software are:   security agents like Tripwire, IPsec, etc.  anti-viruses like McAfee  custom health monitoring agents like Datadog  logging agents like Loggregator's Metron   An addon is a release job that is colocated on all VMs managed by the Director.  addons  [Array, optional]: Specifies the  addons  to be applied to all deployments.   name  [String, required]: A unique name used to identify and reference the addon.  jobs  [Array of hashes, requires]: Specifies the name and release of release jobs to be colocated.  name  [String, required]: The job name.  release  [String, required]: The release where the job exists.  properties  [Hash, optional]: Specifies job properties. Properties allow the Director to configure jobs to a specific environment.  include  [Hash, optional]: Specifies inclusion  placement rules  Available in bosh-release v260+.  exclude  [Hash, optional]: Specifies exclusion  placement rules . Available in bosh-release v260+.",
            "title": "Addons Block "
        },
        {
            "location": "/runtime-config/#placement-rules-for-include-and-exclude-directives",
            "text": "Available rules:   stemcell  [Array of hashes, optional]  os  [String, required]: Matches stemcell's operating system. Example:  ubuntu-trusty  deployments  [Array of strings, optional]: Matches based on deployment names.  jobs  [Array of hashes, optional]  name  [String, required]: Matching job name.  release  [String, required]: Matching release name.  networks  [Array of strings, optional]: Matches based on network names. Available in bosh-release v262+.   All arrays within inclusion/exclusion rules use  or  operator.  Example:  addons :  -   name :   security \n   jobs : \n   -   name :   strongswan \n     release :   strongswan \n     properties : \n       strongswan : \n         ca_cert :   ... \n   -   name :   syslog_drain \n     release :   syslog \n     properties : \n       syslog_drain_ips :   [ 10.10.0.20 ] \n   include : \n     deployments :   [ dep1 ,   dep2 ]   Example with  include  rules:  include : \n   deployments :   [ dep1 ,   dep2 ] \n   jobs : \n   -   name :   redis \n     release :   redis-release \n   stemcell : \n   -   os :   ubuntu-trusty   See  common addons list  for several examples.",
            "title": "Placement Rules for include and exclude Directives "
        },
        {
            "location": "/runtime-config/#tags-block",
            "text": "tags  [Hash, optional]: Specifies key value pairs to be sent to the CPI for VM tagging. Combined with deployment level tags during the deploy. Available in bosh-release v260+.  Example:  tags : \n   business_unit :   marketing \n   email_contact :   ops@marketing.co.com",
            "title": "Tags Block "
        },
        {
            "location": "/configs/",
            "text": "Note: Generic `configs` functionality is available with bosh-release v264+.\n\n\n\nSeveral configuration files such as cloud config must be specified for the Director to successfully complete a deploy. Even though only cloud config is required, there are other configs like runtime config and CPI config you may want to set. Given that functionality of saving, retrieving, viewing, diffing, and listing for different configs is very similar, Director provides a consolidated CLI and API functionality to support all these actions.\n\n\nAdditionally, in some cases it may be useful to split cloud config and/or other configurations into multiple named files so that they can be managed and evolved separately. For example one team can be setting runtime config with IPSec addon and another team separately can manage Syslog forwarding addon. To achieve separation you can give different names (e.g. \nipsec\n and \nsyslog\n) to configs of the same type (e.g. \nruntime\n).\n\n\n\n\nDirector Types \n\u00b6\n\n\nThere are three built-in types: \ncloud\n, \nruntime\n and \ncpi\n. You can interact with the Director config types just as you have been doing so far via the \nupdate-cloud-config\n, \nupdate-runtime-config\n and \nupdate-cpi-config\n CLI commands respectively. By using these commands you will only be able to interact with the \ndefault\n named config of the given type. This will be good enough in most cases but like in our example before if you need to create separate configs with different names, you need to use the \nupdate-config\n command. Keep in mind that if you use the \nconfig commands\n to interact with the built-in types, you still need to comply with the structure of the YAML file for each type.\n\n\n\n\nUser defined Types \n\u00b6\n\n\nIn addition to the Director types an operator can set config of any other type using the \nupdate-config\n CLI command. The config file can be any file containing valid YAML. Root of the file must be a hash.\n\n\nOne of the use cases for providing such open ended functionality is to provide shared configuration API for supporting BOSH services instead of reimplementing something similar in each service. An upcoming example that will use this feature will be introduction of the \nressurection\n config type that will allow operators to define custom resurrection rules, later read and interpreted by the Health Monitor.\n\n\n\n\nUpdating and retrieving a config \n\u00b6\n\n\nTo add or update a config on the Director use the \nbosh update-config\n CLI command. If you do not provide a name using the \n--name\n option, \ndefault\n will be used.\n\n\n$ bosh update-config my-type configs.yml\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\n+ configs:\n+   - name: team-a-config\n+     properties:\n+       ...\n+   - name: team-b-config\n+     properties:\n+       ...\n\nContinue? \n[\nyN\n]\n: y\n\nSucceeded\n\n$ bosh config my-type\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nconfigs:\n  - name: team-a-config\n    properties:\n      ...\n  - name: team-b-config\n    properties:\n      ...\n\n\n\n\nOr you could split them into different \"branches\" using the \n--name\n option.\n\n\n$ bosh update-config my-type --name\n=\nteam-a config-a.yml\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\n+ configs:\n+   - name: team-a-config\n+     properties:\n+       ...\n\nContinue? \n[\nyN\n]\n: y\n\nSucceeded\n\n$ bosh config my-type --name\n=\nteam-a\nUsing environment \n'192.168.56.6'\n as client \n'admin'\n\n\nconfigs:\n  - name: team-a-config\n    properties:\n      ...\n\nSucceeded\n\n$ bosh update-config my-type --name\n=\nteam-b config-b.yml\n...\n\n$ bosh config my-type --name\n=\nteam-b\n...\n\n\n\n\n\n\nListing configs \n\u00b6\n\n\nTo list all configs use the \nbosh configs\n CLI command.\n\n\n$ bosh configs my-type\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nType     Name\nmy-type  default\n~        team-a\n~        team-b\n\n\n3\n configs\n\nSucceeded\n\n\n\n\nYou can also filter configs by \ntype\n and/or \nname\n:\n\n\n$ bosh configs --type\n=\nmy-type --name\n=\nteam-a\nUsing environment \n'192.168.50.6'\n as client \n'admin'\n\n\nType     Name\nmy-type  team-a\n\n\n1\n configs\n\n\n\n\n\n\nDeleting configs \n\u00b6\n\n\nTo delete configs use the \nbosh delete-config\n CLI command. If you do not provide a name using the \n--name=\n option, \ndefault\n will be used.\n\n\n$ bosh delete-config my-type",
            "title": "Generic Configs"
        },
        {
            "location": "/configs/#director-types",
            "text": "There are three built-in types:  cloud ,  runtime  and  cpi . You can interact with the Director config types just as you have been doing so far via the  update-cloud-config ,  update-runtime-config  and  update-cpi-config  CLI commands respectively. By using these commands you will only be able to interact with the  default  named config of the given type. This will be good enough in most cases but like in our example before if you need to create separate configs with different names, you need to use the  update-config  command. Keep in mind that if you use the  config commands  to interact with the built-in types, you still need to comply with the structure of the YAML file for each type.",
            "title": "Director Types "
        },
        {
            "location": "/configs/#user-defined-types",
            "text": "In addition to the Director types an operator can set config of any other type using the  update-config  CLI command. The config file can be any file containing valid YAML. Root of the file must be a hash.  One of the use cases for providing such open ended functionality is to provide shared configuration API for supporting BOSH services instead of reimplementing something similar in each service. An upcoming example that will use this feature will be introduction of the  ressurection  config type that will allow operators to define custom resurrection rules, later read and interpreted by the Health Monitor.",
            "title": "User defined Types "
        },
        {
            "location": "/configs/#updating-and-retrieving-a-config",
            "text": "To add or update a config on the Director use the  bosh update-config  CLI command. If you do not provide a name using the  --name  option,  default  will be used.  $ bosh update-config my-type configs.yml\nUsing environment  '192.168.50.6'  as client  'admin' \n\n+ configs:\n+   - name: team-a-config\n+     properties:\n+       ...\n+   - name: team-b-config\n+     properties:\n+       ...\n\nContinue?  [ yN ] : y\n\nSucceeded\n\n$ bosh config my-type\nUsing environment  '192.168.56.6'  as client  'admin' \n\nconfigs:\n  - name: team-a-config\n    properties:\n      ...\n  - name: team-b-config\n    properties:\n      ...  Or you could split them into different \"branches\" using the  --name  option.  $ bosh update-config my-type --name = team-a config-a.yml\nUsing environment  '192.168.50.6'  as client  'admin' \n\n+ configs:\n+   - name: team-a-config\n+     properties:\n+       ...\n\nContinue?  [ yN ] : y\n\nSucceeded\n\n$ bosh config my-type --name = team-a\nUsing environment  '192.168.56.6'  as client  'admin' \n\nconfigs:\n  - name: team-a-config\n    properties:\n      ...\n\nSucceeded\n\n$ bosh update-config my-type --name = team-b config-b.yml\n...\n\n$ bosh config my-type --name = team-b\n...",
            "title": "Updating and retrieving a config "
        },
        {
            "location": "/configs/#listing-configs",
            "text": "To list all configs use the  bosh configs  CLI command.  $ bosh configs my-type\nUsing environment  '192.168.50.6'  as client  'admin' \n\nType     Name\nmy-type  default\n~        team-a\n~        team-b 3  configs\n\nSucceeded  You can also filter configs by  type  and/or  name :  $ bosh configs --type = my-type --name = team-a\nUsing environment  '192.168.50.6'  as client  'admin' \n\nType     Name\nmy-type  team-a 1  configs",
            "title": "Listing configs "
        },
        {
            "location": "/configs/#deleting-configs",
            "text": "To delete configs use the  bosh delete-config  CLI command. If you do not provide a name using the  --name=  option,  default  will be used.  $ bosh delete-config my-type",
            "title": "Deleting configs "
        },
        {
            "location": "/cpi-api-v1/",
            "text": "For an overview of the sequence of CPI calls, the following resources are helpful:\n\n\n\n\nBOSH components\n and its example component interaction diagram\n\n\nCLI v2 architecture doc\n and \nbosh create-env\n flow\n where calls to the CPI are marked as \ncloud\n.\n\n\n\n\nExamples of API request and response:\n\n\n\n\nBuilding a CPI: RPC - Request\n\n\nBuilding a CPI: RPC - Response\n\n\n\n\n\n\nGlossary \n\u00b6\n\n\n\n\n\n\ncloud ID\n is an ID (string) that the Director uses to reference any created infrastructure resource; typically CPI methods return cloud IDs and later receive them. For example AWS CPI's \ncreate_vm\n method would return \ni-f789df\n and \nattach_disk\n would take it.\n\n\n\n\n\n\ncloud_properties\n is a hash that can be specified for several objects (resource pool, disk pool, stemcell, network) to provide infrastructure specific settings to the CPI for that object. Only CPIs know the meaning of its contents. For example resource pool's \ncloud_properties\n for AWS can specify \ninstance_type\n:\n\n\n\n\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nlarge_machines\n\n  \ncloud_properties\n:\n \n{\ninstance_type\n:\n \nr3.8xlarge\n}\n\n\n\n\n\n\n\nCPI Info \n\u00b6\n\n\ninfo\n \n\u00b6\n\n\nReturns information about the CPI to help the Director to make decisions on which CPI to call for certain operations in a multi CPI scenario.\n\n\nArguments\n\u00b6\n\n\nNo arguments\n\n\nReturned\n\u00b6\n\n\n\n\nstemcell_formats\n [Array of strings]: Stemcell formats supported by the CPI. Currently used in combination with \ncreate_stemcell\n by the Director to determine which CPI to call when uploading a stemcell.\n\n\n\n\n\n\nStemcell management \n\u00b6\n\n\ncreate_stemcell\n \n\u00b6\n\n\nCreates a reusable VM image in the IaaS from the \nstemcell\n image. It's used later for creating VMs. For example AWS CPI creates an AMI and returns AMI ID.\n\n\nSee \nStemcell Building\n for more details.\n\n\nArguments\n\u00b6\n\n\n\n\nimage_path\n [String]: Path to the stemcell image extracted from the stemcell tarball on a local filesystem.\n\n\ncloud_properties\n [Hash]: Cloud properties hash extracted from the stemcell tarball.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n    \n\"/tmp/extracted-stemcell-348754vdsn87fr/image\"\n,\n\n    \n{\n\n        \n\"name\"\n:\n \n\"bosh-openstack-esxi-ubuntu-trusty-go_agent\"\n,\n\n        \n\"version\"\n:\n \n\"2972\"\n,\n\n        \n\"infrastructure\"\n:\n \n\"openstack\"\n,\n\n        \n\"hypervisor\"\n:\n \n\"esxi\"\n,\n\n        \n\"disk\"\n:\n \n3072\n,\n\n        \n\"disk_format\"\n:\n \n\"ovf\"\n,\n\n        \n\"container_format\"\n:\n \n\"bare\"\n,\n\n        \n\"os_type\"\n:\n \n\"linux\"\n,\n\n        \n\"os_distro\"\n:\n \n\"ubuntu\"\n,\n\n        \n\"architecture\"\n:\n \n\"x86_64\"\n,\n\n        \n\"auto_disk_config\"\n:\n \ntrue\n\n    \n}\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\n\n\nstemcell_cid\n [String]: Cloud ID of the created stemcell (e.g. stemcells in AWS CPI are made into AMIs so cid .would be \nami-83fdflf\n)\n\n\n\n\nExample create_stemcell.go\n\n\n\n\ndelete_stemcell\n \n\u00b6\n\n\nDeletes previously created stemcell. Assume that none of the VMs require presence of the stemcell.\n\n\nArguments\n\u00b6\n\n\n\n\nstemcell_cid\n [String]: Cloud ID of the stemcell to delete; returned from \ncreate_stemcell\n.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample delete_stemcell.go\n\n\n\n\nVM management \n\u00b6\n\n\ncreate_vm\n \n\u00b6\n\n\nCreates a new VM based on the stemcell. Created VM must be powered on and accessible on the provided networks.\n\n\nWaiting for the VM to finish booting is not required because the Director waits until the Agent on the VM responds back.\n\n\nMake sure to properly delete created resources if VM cannot be successfully created.\n\n\nArguments\n\u00b6\n\n\n\n\nagent_id\n [String]: ID selected by the Director for the VM's agent.\n\n\nstemcell_cid\n [String]: Cloud ID of the stemcell to use as a base image for new VM.\n\n\ncloud_properties\n [Hash]: Cloud properties hash specified in the deployment manifest under VM's resource pool.\n\n\nnetworks\n [Hash]: Networks hash that specifies which VM networks must be configured.\n\n\ndisk_cids\n [Array of strings] Array of disk cloud IDs for each disk that created VM will most \nlikely\n be attached; they could be used to optimize VM placement so that disks are located nearby.\n\n\nenvironment\n [Hash]: Resource pool's env hash specified in deployment manifest including initial properties added by the BOSH director as shown below.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n    \n\"4149ba0f-38d9-4485-476f-1581be36f290\"\n,\n\n    \n\"ami-478585\"\n,\n\n    \n{\n \n\"instance_type\"\n:\n \n\"m1.small\"\n \n},\n\n    \n{\n\n        \n\"private\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"manual\"\n,\n\n            \n\"netmask\"\n:\n \n\"255.255.255.0\"\n,\n\n            \n\"gateway\"\n:\n \n\"10.230.13.1\"\n,\n\n            \n\"ip\"\n:\n \n\"10.230.13.6\"\n,\n\n            \n\"default\"\n:\n \n[\n \n\"dns\"\n,\n \n\"gateway\"\n \n],\n\n            \n\"cloud_properties\"\n:\n \n{\n \n\"net_id\"\n:\n \n\"subnet-48rt54\"\n \n}\n\n        \n},\n\n        \n\"private2\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"dynamic\"\n,\n\n            \n\"cloud_properties\"\n:\n \n{\n \n\"net_id\"\n:\n \n\"subnet-e12364\"\n \n}\n\n        \n},\n\n        \n\"public\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"vip\"\n,\n\n            \n\"ip\"\n:\n \n\"173.101.112.104\"\n,\n\n            \n\"cloud_properties\"\n:\n \n{}\n\n        \n}\n\n    \n},\n\n    \n[\n \n\"vol-3475945\"\n \n],\n\n    \n{\n\n        \n\"bosh\"\n:\n \n{\n\n            \n\"group\"\n:\n \n\"my-group\"\n,\n\n            \n\"groups\"\n:\n \n[\n\n                \n\"my-second-group\"\n,\n\n                \n\"another-group\"\n\n            \n]\n\n        \n}\n\n    \n}\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the created VM.\n\n\n\n\nAgent settings\n\u00b6\n\n\nFor the Agent to successfully start on the created VM, several bootstrapping settings must be exposed which include network configuration, message bus location (NATS/HTTPS), agent id, etc. Each infrastructure might have a different way of providing such settings to the Agent. For example AWS CPI uses instance user metadata and BOSH Registry. vSphere CPI uses CDROM drive. Most CPIs choose to communicate with default Agent hence communication settings follow certain format:\n\n\n{\n\n    \n\"agent_id\"\n:\n \n\"4149ba0f-38d9-4485-476f-1581be36f290\"\n,\n\n\n    \n\"vm\"\n:\n \n{\n \n\"name\"\n:\n \n\"i-347844\"\n \n},\n\n\n    \n\"networks\"\n:\n \n{\n\n        \n\"private\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"manual\"\n,\n\n            \n\"netmask\"\n:\n \n\"255.255.255.0\"\n,\n\n            \n\"gateway\"\n:\n \n\"10.230.13.1\"\n,\n\n            \n\"ip\"\n:\n \n\"10.230.13.6\"\n,\n\n            \n\"default\"\n:\n \n[\n \n\"dns\"\n,\n \n\"gateway\"\n \n],\n\n            \n\"cloud_properties\"\n:\n \n{\n \n\"net_id\"\n:\n \n\"d29fdb0d-44d8-4e04-818d-5b03888f8eaa\"\n \n}\n\n        \n},\n\n        \n\"public\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"vip\"\n,\n\n            \n\"ip\"\n:\n \n\"173.101.112.104\"\n,\n\n            \n\"cloud_properties\"\n:\n \n{}\n\n        \n}\n\n    \n},\n\n\n    \n\"disks\"\n:\n \n{\n\n        \n\"system\"\n:\n \n\"/dev/sda\"\n,\n\n        \n\"ephemeral\"\n:\n \n\"/dev/sdb\"\n,\n\n        \n\"persistent\"\n:\n \n{}\n\n    \n},\n\n\n    \n\"mbus\"\n:\n \n\"https://mbus:mbus-password@0.0.0.0:6868\"\n\n\n    \n\"ntp\"\n:\n \n[\n \n\"0.pool.ntp.org\"\n,\n \n\"1.pool.ntp.org\"\n \n],\n\n\n    \n\"blobstore\"\n:\n \n{\n\n        \n\"provider\"\n:\n \n\"local\"\n,\n\n        \n\"options\"\n:\n \n{\n \n\"blobstore_path\"\n:\n \n\"/var/vcap/micro_bosh/data/cache\"\n \n}\n\n    \n},\n\n\n    \n\"env\"\n:\n \n{},\n\n\n}\n\n\n\n\n\nSee \nAgent Configuration\n for an overview of the Agent configuration file locations.\n\n\nExample create_vm.go\n\n\n\n\ndelete_vm\n \n\u00b6\n\n\nDeletes the VM.\n\n\nThis method will be called while the VM still has persistent disks attached. It's important to make sure that IaaS behaves appropriately in this case and properly disassociates persistent disks from the VM.\n\n\nTo avoid losing track of VMs, make sure to raise an error if VM deletion is not absolutely certain.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM to delete; returned from \ncreate_vm\n.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample delete_vm.go\n\n\n\n\nhas_vm\n \n\u00b6\n\n\nChecks for VM presence in the IaaS.\n\n\nThis method is mostly used by the consistency check tool (cloudcheck) to determine if the VM still exists.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM to check; returned from \ncreate_vm\n.\n\n\n\n\nReturned\n\u00b6\n\n\n\n\nexists\n [Boolean]: True if VM is present.\n\n\n\n\nExample has_vm.go\n\n\n\n\nreboot_vm\n \n\u00b6\n\n\nReboots the VM. Assume that VM can be either be powered on or off at the time of the call.\n\n\nWaiting for the VM to finish rebooting is not required because the Director waits until the Agent on the VM responds back.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM to reboot; returned from \ncreate_vm\n.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample #reboot_vm\n\n\n\n\nset_vm_metadata\n \n\u00b6\n\n\nSets VM's metadata to make it easier for operators to categorize VMs when looking at the IaaS management console. For example AWS CPI uses tags to store metadata for operators to see in the AWS Console.\n\n\nWe recommend to set VM name based on \nsometimes\n provided \nname\n key.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM to modify; returned from \ncreate_vm\n.\n\n\nmetadata\n [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n    \n\"i-387459\"\n,\n\n    \n{\n\n        \n\"director\"\n:\n \n\"director-784430\"\n,\n\n        \n\"deployment\"\n:\n \n\"redis\"\n,\n\n        \n\"name\"\n:\n \n\"redis/ce7d2040-212e-4d5a-a62d-952a12c50741\"\n,\n\n        \n\"job\"\n:\n \n\"redis\"\n,\n\n        \n\"id\"\n:\n \n\"ce7d2040-212e-4d5a-a62d-952a12c50741\"\n,\n\n        \n\"index\"\n:\n \n\"1\"\n\n    \n}\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample #set_vm_metadata\n\n\n\n\nconfigure_networks\n \n\u00b6\n\n\nThe recommended implementation is to raise \nBosh::Clouds::NotSupported\n error. This method will be deprecated in API v2.\n\n\nAfter the Director received NotSupported error, it will delete the VM (via \ndelete_vm\n) and create a new VM with desired network configuration (via \ncreate_vm\n).\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM to modify; returned from \ncreate_vm\n.\n\n\nnetworks\n [Hash]: Network hashes that specify networks VM must be configured.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n    \n\"i-238445\"\n,\n\n    \n{\n\n        \n\"private\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"manual\"\n,\n\n            \n\"netmask\"\n:\n \n\"255.255.255.0\"\n,\n\n            \n\"gateway\"\n:\n \n\"10.230.13.1\"\n,\n\n            \n\"ip\"\n:\n \n\"10.230.13.6\"\n,\n\n            \n\"default\"\n:\n \n[\n \n\"dns\"\n,\n \n\"gateway\"\n \n],\n\n            \n\"cloud_properties\"\n:\n \n{\n \n\"net_id\"\n:\n \n\"subnet-48rt54\"\n \n}\n\n        \n},\n\n        \n\"private2\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"dynamic\"\n,\n\n            \n\"cloud_properties\"\n:\n \n{\n \n\"net_id\"\n:\n \n\"subnet-e12364\"\n \n}\n\n        \n}\n\n        \n\"public\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"vip\"\n,\n\n            \n\"ip\"\n:\n \n\"173.247.112.104\"\n,\n\n            \n\"cloud_properties\"\n:\n \n{}\n\n        \n}\n\n    \n}\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\n\n\ncalculate_vm_cloud_properties\n (Experimental) \n\u00b6\n\n\nNote: This method is not called by BOSH yet.\n\n\nReturns a hash that can be used as VM \ncloud_properties\n when calling \ncreate_vm\n; it describes the IaaS instance type closest to the arguments passed.\n\n\nThe \ncloud_properties\n returned are IaaS-specific. For example, when querying the AWS CPI for a VM with the parameters \n{ \"cpu\": 1, \"ram\": 512, \"ephemeral_disk_size\": 1024 }\n, it will return the following, which includes a \nt2.nano\n instance type which has 1 CPU and 512MB RAM:\n\n\n{\n\n  \n\"instance_type\"\n:\n \n\"t2.nano\"\n,\n\n  \n\"ephemeral_disk\"\n:\n \n{\n \n\"size\"\n:\n \n1024\n \n}\n\n\n}\n\n\n\n\n\ncalculate_vm_cloud_properties\n returns the minimum resources that satisfy the parameters, which may result in a larger machine than expected. For example, when querying the AWS CPI for a VM with the parameters \n{ \"cpu\": 1, \"ram\": 8192, \"ephemeral_disk_size\": 4096}\n, it will return an \nm4.large\n instance type (which has 2 CPUs) because it is the smallest instance type which has at least 8 GiB RAM.\n\n\nIf a parameter is set to a value greater than what is available (e.g. 1024 CPUs), an error is raised.\n\n\nArguments\n\u00b6\n\n\n\n\ndesired_instance_size\n [Hash]: Parameters of the desired size of the VM consisting of the following keys:\n\n\ncpu\n [Integer]: Number of virtual cores desired\n\n\nram\n [Integer]: Amount of RAM, in MiB (i.e. \n4096\n for 4 GiB)\n\n\nephemeral_disk_size\n [Integer]: Size of ephemeral disk, in MB\n\n\n\n\nExample\n\u00b6\n\n\n{\n\n  \n\"ram\"\n:\n \n1024\n,\n\n  \n\"cpu\"\n:\n \n2\n,\n\n  \n\"ephemeral_disk_size\"\n:\n \n2048\n\n\n}\n\n\n\n\n\nReturned\n\u00b6\n\n\n\n\ncloud_properties\n [Hash]: an IaaS-specific set of cloud properties that define the size of the VM.\n\n\n\n\n\n\nDisk management \n\u00b6\n\n\ncreate_disk\n \n\u00b6\n\n\nCreates disk with specific size. Disk does not belong to any given VM.\n\n\nArguments\n\u00b6\n\n\n\n\nsize\n [Integer]: Size of the disk in MiB.\n\n\ncloud_properties\n [Hash]: Cloud properties hash specified in the deployment manifest under the disk pool.\n\n\nvm_cid\n [String]: Cloud ID of the VM created disk will most \nlikely\n be attached; it could be used to .optimize disk placement so that disk is located near the VM.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n    \n25000\n,\n\n    \n{\n\n        \n\"type\"\n:\n \n\"gp2\"\n,\n\n        \n\"encrypted\"\n:\n \ntrue\n\n    \n},\n\n    \n\"i-2387475\"\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the created disk.\n\n\n\n\nExample create_disk.go\n\n\n\n\ndelete_disk\n \n\u00b6\n\n\nDeletes disk. Assume that disk was detached from all VMs.\n\n\nTo avoid losing track of disks, make sure to raise an error if disk deletion is not absolutely certain.\n\n\nArguments\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the disk to delete; returned from \ncreate_disk\n.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample delete_disk.go\n\n\n\n\nresize_disk\n \n\u00b6\n\n\nResizes disk with IaaS-native methods. Assume that disk was detached from all VMs. Set property \ndirector.enable_cpi_resize_disk\n to \ntrue\n to have the Director call this method.\n\n\nDepending on the capabilities of the underlying infrastructure, this method may raise an \nBosh::Clouds::NotSupported\n error when the \nnew_size\n is smaller than the current disk size. The same error is raised when the method is not implemented.\n\n\nIf \nBosh::Clouds::NotSupported\n is raised, the Director falls back to creating a new disk and copying data.\n\n\nArguments\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the disk to resize; returned from \ncreate_disk\n.\n\n\nnew_size\n [Integer]: New disk size in MiB.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample #resize_disk\n\n\n\n\nhas_disk\n \n\u00b6\n\n\nChecks for disk presence in the IaaS.\n\n\nThis method is mostly used by the consistency check tool (cloudcheck) to determine if the disk still exists.\n\n\nArguments\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the disk to check; returned from \ncreate_disk\n.\n\n\n\n\nReturned\n\u00b6\n\n\n\n\nexists\n [Boolean]: True if disk is present.\n\n\n\n\nExample #has_disk\n\n\n\n\nattach_disk\n \n\u00b6\n\n\nAttaches disk to the VM.\n\n\nTypically each VM will have one disk attached at a time to store persistent data; however, there are important cases when multiple disks may be attached to a VM. Most common scenario involves persistent data migration from a smaller to a larger disk. Given a VM with a smaller disk attached, the operator decides to increase the disk size for that VM, so new larger disk is created, it is then attached to the VM. The Agent then copies over the data from one disk to another, and smaller disk subsequently is detached and deleted.\n\n\nAgent settings should have been updated with necessary information about given disk.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\ndisk_cid\n [String]: Cloud ID of the disk.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nAgent settings\n\u00b6\n\n\nFor the Agent to eventually format, partition and mount attached disk, it needs to identify the disk attachment from inside the OS. The Agent can currently identify attached disk based on either device path, disk's ID, or SCSI volume ID. For example settings below show that CPI attached a disk \nvol-7447851\n at \n/dev/sdd\n:\n\n\n{\n\n    \n\"agent_id\"\n:\n \n\"4149ba0f-38d9-4485-476f-1581be36f290\"\n,\n\n\n    \n\"vm\"\n:\n \n{\n \n\"name\"\n:\n \n\"i-347844\"\n \n},\n\n\n    \n\"networks\"\n:\n \n{\n \n...\n \n},\n\n\n    \n\"disks\"\n:\n \n{\n\n        \n\"system\"\n:\n \n\"/dev/sda\"\n,\n\n        \n\"ephemeral\"\n:\n \n\"/dev/sdb\"\n,\n\n        \n\"persistent\"\n:\n \n{\n\n            \n\"vol-3475945\"\n:\n \n{\n \n\"volume_id\"\n:\n \n\"3\"\n \n},\n\n            \n\"vol-7447851\"\n:\n \n{\n \n\"path\"\n:\n \n\"/dev/sdd\"\n \n},\n\n        \n}\n\n    \n},\n\n\n    \n\"mbus\"\n:\n \n\"https://mbus:mbus-password@0.0.0.0:6868\"\n\n\n    \n\"ntp\"\n:\n \n[\n \n...\n \n],\n\n\n    \n\"blobstore\"\n:\n \n{\n \n...\n \n},\n\n\n    \n\"env\"\n:\n \n{},\n\n\n}\n\n\n\n\n\nExample attach_disk.go\n\n\n\n\ndetach_disk\n \n\u00b6\n\n\nDetaches disk from the VM.\n\n\nIf the persistent disk is attached to a VM that will be deleted, it's more likely \ndelete_vm\n CPI method will be called without a call to \ndetach_disk\n with an expectation that \ndelete_vm\n will make sure disks are disassociated from the VM upon its deletion.\n\n\nAgent settings should have been updated to remove information about given disk.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\ndisk_cid\n [String]: Cloud ID of the disk.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample detach_disk.go\n\n\n\n\nset_disk_metadata\n \n\u00b6\n\n\nNote: This method is called by BOSH v262+.\n\n\n\nSets disk's metadata to make it easier for operators to categorize disks when looking at the IaaS management console. For example AWS CPI uses tags to store metadata for operators to see in the AWS Console.\n\n\nDisk metadata is written when the disk is attached to a VM. Metadata is not removed when disk is detached or VM is deleted.\n\n\nArguments\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the disk to modify; returned from \ncreate_disk\n.\n\n\nmetadata\n [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.\n\n\n\n\nExample\n\u00b6\n\n\n[\n\n  \n\"vol-3475945\"\n,\n\n  \n{\n\n    \n\"director\"\n:\n \n\"director-784430\"\n,\n\n    \n\"deployment\"\n:\n \n\"redis\"\n,\n\n    \n\"instance_id\"\n:\n \n\"ce7d2040-212e-4d5a-a62d-952a12c50741\"\n,\n\n    \n\"job\"\n:\n \n\"redis\"\n,\n\n    \n\"instance_index\"\n:\n \n\"1\"\n,\n\n    \n\"instance_name\"\n:\n \n\"redis/ce7d2040-212e-4d5a-a62d-952a12c50741\"\n,\n\n    \n\"attached_at\"\n:\n \n\"2017-08-10T12:03:32Z\"\n\n  \n}\n\n\n]\n\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\nExample #set_disk_metadata\n\n\n\n\nget_disks\n \n\u00b6\n\n\nReturns list of disks \ncurrently\n attached to the VM.\n\n\nThis method is mostly used by the consistency check tool (cloudcheck) to determine if the VM has required disks attached.\n\n\nArguments\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\n\n\nReturned\n\u00b6\n\n\n\n\ndisk_cids\n [Array of strings]: Array of \ndisk_cid\ns that are currently attached to the VM.\n\n\n\n\n\n\nDisk snapshots \n\u00b6\n\n\nsnapshot_disk\n \n\u00b6\n\n\nTakes a snapshot of the disk.\n\n\nArguments\n\u00b6\n\n\n\n\ndisk_cid\n [String]: Cloud ID of the disk.\n\n\nmetadata\n [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.\n\n\n\n\nReturned\n\u00b6\n\n\n\n\nsnapshot_cid\n [String]: Cloud ID of the disk snapshot.\n\n\n\n\n\n\ndelete_snapshot\n \n\u00b6\n\n\nDeletes the disk snapshot.\n\n\nArguments\n\u00b6\n\n\n\n\nsnapshot_cid\n [String]: Cloud ID of the disk snapshot.\n\n\n\n\nReturned\n\u00b6\n\n\nNo return value\n\n\n\n\ncurrent_vm_id\n \n\u00b6\n\n\nDetermines cloud ID of the VM executing the CPI code. Currently used in combination with \nget_disks\n by the Director to determine which disks to self-snapshot.\n\n\nNote: Do not implement; this method will be deprecated and removed.\n\n\n\nArguments\n\u00b6\n\n\nNo arguments\n\n\nReturned\n\u00b6\n\n\n\n\nvm_cid\n [String]: Cloud ID of the VM.\n\n\n\n\n\n\nNext: \nAgent-CPI interactions\n\n\nPrevious: \nBuilding a CPI",
            "title": "Cloud Provider Interface"
        },
        {
            "location": "/cpi-api-v1/#glossary",
            "text": "cloud ID  is an ID (string) that the Director uses to reference any created infrastructure resource; typically CPI methods return cloud IDs and later receive them. For example AWS CPI's  create_vm  method would return  i-f789df  and  attach_disk  would take it.    cloud_properties  is a hash that can be specified for several objects (resource pool, disk pool, stemcell, network) to provide infrastructure specific settings to the CPI for that object. Only CPIs know the meaning of its contents. For example resource pool's  cloud_properties  for AWS can specify  instance_type :    resource_pools :  -   name :   large_machines \n   cloud_properties :   { instance_type :   r3.8xlarge }",
            "title": "Glossary "
        },
        {
            "location": "/cpi-api-v1/#cpi-info",
            "text": "",
            "title": "CPI Info "
        },
        {
            "location": "/cpi-api-v1/#info",
            "text": "Returns information about the CPI to help the Director to make decisions on which CPI to call for certain operations in a multi CPI scenario.",
            "title": "info "
        },
        {
            "location": "/cpi-api-v1/#arguments",
            "text": "No arguments",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned",
            "text": "stemcell_formats  [Array of strings]: Stemcell formats supported by the CPI. Currently used in combination with  create_stemcell  by the Director to determine which CPI to call when uploading a stemcell.",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#stemcell-management",
            "text": "",
            "title": "Stemcell management "
        },
        {
            "location": "/cpi-api-v1/#create_stemcell",
            "text": "Creates a reusable VM image in the IaaS from the  stemcell  image. It's used later for creating VMs. For example AWS CPI creates an AMI and returns AMI ID.  See  Stemcell Building  for more details.",
            "title": "create_stemcell "
        },
        {
            "location": "/cpi-api-v1/#arguments_1",
            "text": "image_path  [String]: Path to the stemcell image extracted from the stemcell tarball on a local filesystem.  cloud_properties  [Hash]: Cloud properties hash extracted from the stemcell tarball.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example",
            "text": "[ \n     \"/tmp/extracted-stemcell-348754vdsn87fr/image\" , \n     { \n         \"name\" :   \"bosh-openstack-esxi-ubuntu-trusty-go_agent\" , \n         \"version\" :   \"2972\" , \n         \"infrastructure\" :   \"openstack\" , \n         \"hypervisor\" :   \"esxi\" , \n         \"disk\" :   3072 , \n         \"disk_format\" :   \"ovf\" , \n         \"container_format\" :   \"bare\" , \n         \"os_type\" :   \"linux\" , \n         \"os_distro\" :   \"ubuntu\" , \n         \"architecture\" :   \"x86_64\" , \n         \"auto_disk_config\" :   true \n     }  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_1",
            "text": "stemcell_cid  [String]: Cloud ID of the created stemcell (e.g. stemcells in AWS CPI are made into AMIs so cid .would be  ami-83fdflf )   Example create_stemcell.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#delete_stemcell",
            "text": "Deletes previously created stemcell. Assume that none of the VMs require presence of the stemcell.",
            "title": "delete_stemcell "
        },
        {
            "location": "/cpi-api-v1/#arguments_2",
            "text": "stemcell_cid  [String]: Cloud ID of the stemcell to delete; returned from  create_stemcell .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_2",
            "text": "No return value  Example delete_stemcell.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#vm-management",
            "text": "",
            "title": "VM management "
        },
        {
            "location": "/cpi-api-v1/#create_vm",
            "text": "Creates a new VM based on the stemcell. Created VM must be powered on and accessible on the provided networks.  Waiting for the VM to finish booting is not required because the Director waits until the Agent on the VM responds back.  Make sure to properly delete created resources if VM cannot be successfully created.",
            "title": "create_vm "
        },
        {
            "location": "/cpi-api-v1/#arguments_3",
            "text": "agent_id  [String]: ID selected by the Director for the VM's agent.  stemcell_cid  [String]: Cloud ID of the stemcell to use as a base image for new VM.  cloud_properties  [Hash]: Cloud properties hash specified in the deployment manifest under VM's resource pool.  networks  [Hash]: Networks hash that specifies which VM networks must be configured.  disk_cids  [Array of strings] Array of disk cloud IDs for each disk that created VM will most  likely  be attached; they could be used to optimize VM placement so that disks are located nearby.  environment  [Hash]: Resource pool's env hash specified in deployment manifest including initial properties added by the BOSH director as shown below.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_1",
            "text": "[ \n     \"4149ba0f-38d9-4485-476f-1581be36f290\" , \n     \"ami-478585\" , \n     {   \"instance_type\" :   \"m1.small\"   }, \n     { \n         \"private\" :   { \n             \"type\" :   \"manual\" , \n             \"netmask\" :   \"255.255.255.0\" , \n             \"gateway\" :   \"10.230.13.1\" , \n             \"ip\" :   \"10.230.13.6\" , \n             \"default\" :   [   \"dns\" ,   \"gateway\"   ], \n             \"cloud_properties\" :   {   \"net_id\" :   \"subnet-48rt54\"   } \n         }, \n         \"private2\" :   { \n             \"type\" :   \"dynamic\" , \n             \"cloud_properties\" :   {   \"net_id\" :   \"subnet-e12364\"   } \n         }, \n         \"public\" :   { \n             \"type\" :   \"vip\" , \n             \"ip\" :   \"173.101.112.104\" , \n             \"cloud_properties\" :   {} \n         } \n     }, \n     [   \"vol-3475945\"   ], \n     { \n         \"bosh\" :   { \n             \"group\" :   \"my-group\" , \n             \"groups\" :   [ \n                 \"my-second-group\" , \n                 \"another-group\" \n             ] \n         } \n     }  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_3",
            "text": "vm_cid  [String]: Cloud ID of the created VM.",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#agent-settings",
            "text": "For the Agent to successfully start on the created VM, several bootstrapping settings must be exposed which include network configuration, message bus location (NATS/HTTPS), agent id, etc. Each infrastructure might have a different way of providing such settings to the Agent. For example AWS CPI uses instance user metadata and BOSH Registry. vSphere CPI uses CDROM drive. Most CPIs choose to communicate with default Agent hence communication settings follow certain format:  { \n     \"agent_id\" :   \"4149ba0f-38d9-4485-476f-1581be36f290\" , \n\n     \"vm\" :   {   \"name\" :   \"i-347844\"   }, \n\n     \"networks\" :   { \n         \"private\" :   { \n             \"type\" :   \"manual\" , \n             \"netmask\" :   \"255.255.255.0\" , \n             \"gateway\" :   \"10.230.13.1\" , \n             \"ip\" :   \"10.230.13.6\" , \n             \"default\" :   [   \"dns\" ,   \"gateway\"   ], \n             \"cloud_properties\" :   {   \"net_id\" :   \"d29fdb0d-44d8-4e04-818d-5b03888f8eaa\"   } \n         }, \n         \"public\" :   { \n             \"type\" :   \"vip\" , \n             \"ip\" :   \"173.101.112.104\" , \n             \"cloud_properties\" :   {} \n         } \n     }, \n\n     \"disks\" :   { \n         \"system\" :   \"/dev/sda\" , \n         \"ephemeral\" :   \"/dev/sdb\" , \n         \"persistent\" :   {} \n     }, \n\n     \"mbus\" :   \"https://mbus:mbus-password@0.0.0.0:6868\" \n\n     \"ntp\" :   [   \"0.pool.ntp.org\" ,   \"1.pool.ntp.org\"   ], \n\n     \"blobstore\" :   { \n         \"provider\" :   \"local\" , \n         \"options\" :   {   \"blobstore_path\" :   \"/var/vcap/micro_bosh/data/cache\"   } \n     }, \n\n     \"env\" :   {},  }   See  Agent Configuration  for an overview of the Agent configuration file locations.  Example create_vm.go",
            "title": "Agent settings"
        },
        {
            "location": "/cpi-api-v1/#delete_vm",
            "text": "Deletes the VM.  This method will be called while the VM still has persistent disks attached. It's important to make sure that IaaS behaves appropriately in this case and properly disassociates persistent disks from the VM.  To avoid losing track of VMs, make sure to raise an error if VM deletion is not absolutely certain.",
            "title": "delete_vm "
        },
        {
            "location": "/cpi-api-v1/#arguments_4",
            "text": "vm_cid  [String]: Cloud ID of the VM to delete; returned from  create_vm .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_4",
            "text": "No return value  Example delete_vm.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#has_vm",
            "text": "Checks for VM presence in the IaaS.  This method is mostly used by the consistency check tool (cloudcheck) to determine if the VM still exists.",
            "title": "has_vm "
        },
        {
            "location": "/cpi-api-v1/#arguments_5",
            "text": "vm_cid  [String]: Cloud ID of the VM to check; returned from  create_vm .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_5",
            "text": "exists  [Boolean]: True if VM is present.   Example has_vm.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#reboot_vm",
            "text": "Reboots the VM. Assume that VM can be either be powered on or off at the time of the call.  Waiting for the VM to finish rebooting is not required because the Director waits until the Agent on the VM responds back.",
            "title": "reboot_vm "
        },
        {
            "location": "/cpi-api-v1/#arguments_6",
            "text": "vm_cid  [String]: Cloud ID of the VM to reboot; returned from  create_vm .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_6",
            "text": "No return value  Example #reboot_vm",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#set_vm_metadata",
            "text": "Sets VM's metadata to make it easier for operators to categorize VMs when looking at the IaaS management console. For example AWS CPI uses tags to store metadata for operators to see in the AWS Console.  We recommend to set VM name based on  sometimes  provided  name  key.",
            "title": "set_vm_metadata "
        },
        {
            "location": "/cpi-api-v1/#arguments_7",
            "text": "vm_cid  [String]: Cloud ID of the VM to modify; returned from  create_vm .  metadata  [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_2",
            "text": "[ \n     \"i-387459\" , \n     { \n         \"director\" :   \"director-784430\" , \n         \"deployment\" :   \"redis\" , \n         \"name\" :   \"redis/ce7d2040-212e-4d5a-a62d-952a12c50741\" , \n         \"job\" :   \"redis\" , \n         \"id\" :   \"ce7d2040-212e-4d5a-a62d-952a12c50741\" , \n         \"index\" :   \"1\" \n     }  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_7",
            "text": "No return value  Example #set_vm_metadata",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#configure_networks",
            "text": "The recommended implementation is to raise  Bosh::Clouds::NotSupported  error. This method will be deprecated in API v2.  After the Director received NotSupported error, it will delete the VM (via  delete_vm ) and create a new VM with desired network configuration (via  create_vm ).",
            "title": "configure_networks "
        },
        {
            "location": "/cpi-api-v1/#arguments_8",
            "text": "vm_cid  [String]: Cloud ID of the VM to modify; returned from  create_vm .  networks  [Hash]: Network hashes that specify networks VM must be configured.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_3",
            "text": "[ \n     \"i-238445\" , \n     { \n         \"private\" :   { \n             \"type\" :   \"manual\" , \n             \"netmask\" :   \"255.255.255.0\" , \n             \"gateway\" :   \"10.230.13.1\" , \n             \"ip\" :   \"10.230.13.6\" , \n             \"default\" :   [   \"dns\" ,   \"gateway\"   ], \n             \"cloud_properties\" :   {   \"net_id\" :   \"subnet-48rt54\"   } \n         }, \n         \"private2\" :   { \n             \"type\" :   \"dynamic\" , \n             \"cloud_properties\" :   {   \"net_id\" :   \"subnet-e12364\"   } \n         } \n         \"public\" :   { \n             \"type\" :   \"vip\" , \n             \"ip\" :   \"173.247.112.104\" , \n             \"cloud_properties\" :   {} \n         } \n     }  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_8",
            "text": "No return value",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#calculate_vm_cloud_properties-experimental",
            "text": "Note: This method is not called by BOSH yet.  Returns a hash that can be used as VM  cloud_properties  when calling  create_vm ; it describes the IaaS instance type closest to the arguments passed.  The  cloud_properties  returned are IaaS-specific. For example, when querying the AWS CPI for a VM with the parameters  { \"cpu\": 1, \"ram\": 512, \"ephemeral_disk_size\": 1024 } , it will return the following, which includes a  t2.nano  instance type which has 1 CPU and 512MB RAM:  { \n   \"instance_type\" :   \"t2.nano\" , \n   \"ephemeral_disk\" :   {   \"size\" :   1024   }  }   calculate_vm_cloud_properties  returns the minimum resources that satisfy the parameters, which may result in a larger machine than expected. For example, when querying the AWS CPI for a VM with the parameters  { \"cpu\": 1, \"ram\": 8192, \"ephemeral_disk_size\": 4096} , it will return an  m4.large  instance type (which has 2 CPUs) because it is the smallest instance type which has at least 8 GiB RAM.  If a parameter is set to a value greater than what is available (e.g. 1024 CPUs), an error is raised.",
            "title": "calculate_vm_cloud_properties (Experimental) "
        },
        {
            "location": "/cpi-api-v1/#arguments_9",
            "text": "desired_instance_size  [Hash]: Parameters of the desired size of the VM consisting of the following keys:  cpu  [Integer]: Number of virtual cores desired  ram  [Integer]: Amount of RAM, in MiB (i.e.  4096  for 4 GiB)  ephemeral_disk_size  [Integer]: Size of ephemeral disk, in MB",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_4",
            "text": "{ \n   \"ram\" :   1024 , \n   \"cpu\" :   2 , \n   \"ephemeral_disk_size\" :   2048  }",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_9",
            "text": "cloud_properties  [Hash]: an IaaS-specific set of cloud properties that define the size of the VM.",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#disk-management",
            "text": "",
            "title": "Disk management "
        },
        {
            "location": "/cpi-api-v1/#create_disk",
            "text": "Creates disk with specific size. Disk does not belong to any given VM.",
            "title": "create_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_10",
            "text": "size  [Integer]: Size of the disk in MiB.  cloud_properties  [Hash]: Cloud properties hash specified in the deployment manifest under the disk pool.  vm_cid  [String]: Cloud ID of the VM created disk will most  likely  be attached; it could be used to .optimize disk placement so that disk is located near the VM.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_5",
            "text": "[ \n     25000 , \n     { \n         \"type\" :   \"gp2\" , \n         \"encrypted\" :   true \n     }, \n     \"i-2387475\"  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_10",
            "text": "disk_cid  [String]: Cloud ID of the created disk.   Example create_disk.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#delete_disk",
            "text": "Deletes disk. Assume that disk was detached from all VMs.  To avoid losing track of disks, make sure to raise an error if disk deletion is not absolutely certain.",
            "title": "delete_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_11",
            "text": "disk_cid  [String]: Cloud ID of the disk to delete; returned from  create_disk .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_11",
            "text": "No return value  Example delete_disk.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#resize_disk",
            "text": "Resizes disk with IaaS-native methods. Assume that disk was detached from all VMs. Set property  director.enable_cpi_resize_disk  to  true  to have the Director call this method.  Depending on the capabilities of the underlying infrastructure, this method may raise an  Bosh::Clouds::NotSupported  error when the  new_size  is smaller than the current disk size. The same error is raised when the method is not implemented.  If  Bosh::Clouds::NotSupported  is raised, the Director falls back to creating a new disk and copying data.",
            "title": "resize_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_12",
            "text": "disk_cid  [String]: Cloud ID of the disk to resize; returned from  create_disk .  new_size  [Integer]: New disk size in MiB.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_12",
            "text": "No return value  Example #resize_disk",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#has_disk",
            "text": "Checks for disk presence in the IaaS.  This method is mostly used by the consistency check tool (cloudcheck) to determine if the disk still exists.",
            "title": "has_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_13",
            "text": "disk_cid  [String]: Cloud ID of the disk to check; returned from  create_disk .",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_13",
            "text": "exists  [Boolean]: True if disk is present.   Example #has_disk",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#attach_disk",
            "text": "Attaches disk to the VM.  Typically each VM will have one disk attached at a time to store persistent data; however, there are important cases when multiple disks may be attached to a VM. Most common scenario involves persistent data migration from a smaller to a larger disk. Given a VM with a smaller disk attached, the operator decides to increase the disk size for that VM, so new larger disk is created, it is then attached to the VM. The Agent then copies over the data from one disk to another, and smaller disk subsequently is detached and deleted.  Agent settings should have been updated with necessary information about given disk.",
            "title": "attach_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_14",
            "text": "vm_cid  [String]: Cloud ID of the VM.  disk_cid  [String]: Cloud ID of the disk.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_14",
            "text": "No return value",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#agent-settings_1",
            "text": "For the Agent to eventually format, partition and mount attached disk, it needs to identify the disk attachment from inside the OS. The Agent can currently identify attached disk based on either device path, disk's ID, or SCSI volume ID. For example settings below show that CPI attached a disk  vol-7447851  at  /dev/sdd :  { \n     \"agent_id\" :   \"4149ba0f-38d9-4485-476f-1581be36f290\" , \n\n     \"vm\" :   {   \"name\" :   \"i-347844\"   }, \n\n     \"networks\" :   {   ...   }, \n\n     \"disks\" :   { \n         \"system\" :   \"/dev/sda\" , \n         \"ephemeral\" :   \"/dev/sdb\" , \n         \"persistent\" :   { \n             \"vol-3475945\" :   {   \"volume_id\" :   \"3\"   }, \n             \"vol-7447851\" :   {   \"path\" :   \"/dev/sdd\"   }, \n         } \n     }, \n\n     \"mbus\" :   \"https://mbus:mbus-password@0.0.0.0:6868\" \n\n     \"ntp\" :   [   ...   ], \n\n     \"blobstore\" :   {   ...   }, \n\n     \"env\" :   {},  }   Example attach_disk.go",
            "title": "Agent settings"
        },
        {
            "location": "/cpi-api-v1/#detach_disk",
            "text": "Detaches disk from the VM.  If the persistent disk is attached to a VM that will be deleted, it's more likely  delete_vm  CPI method will be called without a call to  detach_disk  with an expectation that  delete_vm  will make sure disks are disassociated from the VM upon its deletion.  Agent settings should have been updated to remove information about given disk.",
            "title": "detach_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_15",
            "text": "vm_cid  [String]: Cloud ID of the VM.  disk_cid  [String]: Cloud ID of the disk.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_15",
            "text": "No return value  Example detach_disk.go",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#set_disk_metadata",
            "text": "Note: This method is called by BOSH v262+.  Sets disk's metadata to make it easier for operators to categorize disks when looking at the IaaS management console. For example AWS CPI uses tags to store metadata for operators to see in the AWS Console.  Disk metadata is written when the disk is attached to a VM. Metadata is not removed when disk is detached or VM is deleted.",
            "title": "set_disk_metadata "
        },
        {
            "location": "/cpi-api-v1/#arguments_16",
            "text": "disk_cid  [String]: Cloud ID of the disk to modify; returned from  create_disk .  metadata  [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#example_6",
            "text": "[ \n   \"vol-3475945\" , \n   { \n     \"director\" :   \"director-784430\" , \n     \"deployment\" :   \"redis\" , \n     \"instance_id\" :   \"ce7d2040-212e-4d5a-a62d-952a12c50741\" , \n     \"job\" :   \"redis\" , \n     \"instance_index\" :   \"1\" , \n     \"instance_name\" :   \"redis/ce7d2040-212e-4d5a-a62d-952a12c50741\" , \n     \"attached_at\" :   \"2017-08-10T12:03:32Z\" \n   }  ]",
            "title": "Example"
        },
        {
            "location": "/cpi-api-v1/#returned_16",
            "text": "No return value  Example #set_disk_metadata",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#get_disks",
            "text": "Returns list of disks  currently  attached to the VM.  This method is mostly used by the consistency check tool (cloudcheck) to determine if the VM has required disks attached.",
            "title": "get_disks "
        },
        {
            "location": "/cpi-api-v1/#arguments_17",
            "text": "vm_cid  [String]: Cloud ID of the VM.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_17",
            "text": "disk_cids  [Array of strings]: Array of  disk_cid s that are currently attached to the VM.",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#disk-snapshots",
            "text": "",
            "title": "Disk snapshots "
        },
        {
            "location": "/cpi-api-v1/#snapshot_disk",
            "text": "Takes a snapshot of the disk.",
            "title": "snapshot_disk "
        },
        {
            "location": "/cpi-api-v1/#arguments_18",
            "text": "disk_cid  [String]: Cloud ID of the disk.  metadata  [Hash]: Collection of key-value pairs. CPI should not rely on presence of specific keys.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_18",
            "text": "snapshot_cid  [String]: Cloud ID of the disk snapshot.",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#delete_snapshot",
            "text": "Deletes the disk snapshot.",
            "title": "delete_snapshot "
        },
        {
            "location": "/cpi-api-v1/#arguments_19",
            "text": "snapshot_cid  [String]: Cloud ID of the disk snapshot.",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_19",
            "text": "No return value",
            "title": "Returned"
        },
        {
            "location": "/cpi-api-v1/#current_vm_id",
            "text": "Determines cloud ID of the VM executing the CPI code. Currently used in combination with  get_disks  by the Director to determine which disks to self-snapshot.  Note: Do not implement; this method will be deprecated and removed.",
            "title": "current_vm_id "
        },
        {
            "location": "/cpi-api-v1/#arguments_20",
            "text": "No arguments",
            "title": "Arguments"
        },
        {
            "location": "/cpi-api-v1/#returned_20",
            "text": "vm_cid  [String]: Cloud ID of the VM.    Next:  Agent-CPI interactions  Previous:  Building a CPI",
            "title": "Returned"
        },
        {
            "location": "/release/",
            "text": "A release is a versioned collection of configuration properties, configuration templates, start up scripts, source code, binary artifacts, and anything else required to build and deploy software in a reproducible way.\n\n\nA release is the layer placed on top of a \nstemcell\n. They are self-contained and provide very specific software for the purpose of that release. For example, a Redis release might include start-up and shutdown scripts for \nredis-server\n, a tarball with Redis source code obtained from the Redis official website, and a few configuration properties allowing cluster operators to alter that Redis configuration.\n\n\nBy allowing layering of stemcells and releases, BOSH is able to solve problems such as \"how does one make sure that the compiled version of the software is reliably available throughout the deploy\", or \"how to version and roll out updated software to the whole cluster, VM-by-VM\", that other orchestration software is not able to solve.\n\n\nThere are two common formats in which releases are distributed: artifacts checked into a git repository and as a single tarball.\n\n\nBy introducing the concept of a release, the following concerns are addressed:\n\n\n\n\nCapturing all needed configuration options and scripts for deployment of the software\n\n\nRecording and keeping track of all dependencies for the software\n\n\nVersioning and keeping track of software releases\n\n\nCreating releases that can be IaaS agnostic\n\n\nCreating releases that are self-contained and do not require internet access for deployment\n\n\n\n\n\n\nNext: \nWhat is a Deployment?\n\n\nPrevious: \nWhat is a Stemcell?",
            "title": "Overview"
        },
        {
            "location": "/release-urls/",
            "text": "This topic describes allowed types of URLs for downloading releases (typically found in deployment manifests).\n\n\nLocal URLs \n\u00b6\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nsyslog\n\n  \nversion\n:\n \n11\n\n  \nurl\n:\n \nfile://syslog-11.tgz\n\n  \nsha1\n:\n \n332ac15609b220a3fdf5efad0e0aa069d8235788\n\n\n\n\n\nsha1\n key is not required but can be specified.\n\n\nCLI v2 will look for file locally and provide it to the \nupload-release\n command.\n\n\nAbove declaration is equivalent to \nbosh upload-release syslog-11.tgz --sha1 332ac15609b220a3fdf5efad0e0aa069d8235788\n.\n\n\n\n\nHTTP/HTTPs URLs \n\u00b6\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nsyslog\n\n  \nversion\n:\n \n11\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11\n\n  \nsha1\n:\n \n332ac15609b220a3fdf5efad0e0aa069d8235788\n\n\n\n\n\nCLI v2 will delegate download of the release to the Director, hence Director must have connectivity to specified resource.\n\n\nAbove declaration is equivalent to \nbosh upload-release https://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11 --sha1 332ac15609b220a3fdf5efad0e0aa069d8235788\n.\n\n\n\n\nGit over HTTP/HTTPs URLs \n\u00b6\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nsyslog\n\n  \nversion\n:\n \n11\n\n  \nurl\n:\n \ngit+https://github.com/cloudfoundry/syslog-release\n\n\n\n\n\nsha1\n key is not required as we rely on Git's HTTP transport mechanism to pull down expected content.\n\n\nCLI v2 will perform a shallow clone locally and run \nupload-release\n command from within cloned repository.\n\n\nAbove declaration is equivalent to \nbosh upload-release git+https://github.com/cloudfoundry/syslog-release --name syslog --version 11\n.",
            "title": "Release URLs"
        },
        {
            "location": "/release-urls/#local-urls",
            "text": "releases :  -   name :   syslog \n   version :   11 \n   url :   file://syslog-11.tgz \n   sha1 :   332ac15609b220a3fdf5efad0e0aa069d8235788   sha1  key is not required but can be specified.  CLI v2 will look for file locally and provide it to the  upload-release  command.  Above declaration is equivalent to  bosh upload-release syslog-11.tgz --sha1 332ac15609b220a3fdf5efad0e0aa069d8235788 .",
            "title": "Local URLs "
        },
        {
            "location": "/release-urls/#httphttps-urls",
            "text": "releases :  -   name :   syslog \n   version :   11 \n   url :   https://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11 \n   sha1 :   332ac15609b220a3fdf5efad0e0aa069d8235788   CLI v2 will delegate download of the release to the Director, hence Director must have connectivity to specified resource.  Above declaration is equivalent to  bosh upload-release https://bosh.io/d/github.com/cloudfoundry/syslog-release?v=11 --sha1 332ac15609b220a3fdf5efad0e0aa069d8235788 .",
            "title": "HTTP/HTTPs URLs "
        },
        {
            "location": "/release-urls/#git-over-httphttps-urls",
            "text": "releases :  -   name :   syslog \n   version :   11 \n   url :   git+https://github.com/cloudfoundry/syslog-release   sha1  key is not required as we rely on Git's HTTP transport mechanism to pull down expected content.  CLI v2 will perform a shallow clone locally and run  upload-release  command from within cloned repository.  Above declaration is equivalent to  bosh upload-release git+https://github.com/cloudfoundry/syslog-release --name syslog --version 11 .",
            "title": "Git over HTTP/HTTPs URLs "
        },
        {
            "location": "/jobs/",
            "text": "Each release job represents a specific chunk of work that the release performs. For example a DHCP release may have a \"dhcp-server\" job, and a Postgres release may have \"postgres\" and \"periodic-backup\" jobs. A release can define one or more jobs.\n\n\nA job typically includes:\n\n\n\n\nmetadata that specifies available configuration options\n\n\nERB configuration files\n\n\na Monit file that describes how to start, stop and monitor processes\n\n\nstart and stop scripts for each process\n\n\nadditional hook scripts\n\n\n\n\nJobs are typically OS specific (Windows vs Linux); however, structure of a job remains same.\n\n\n\n\nSpec file (metadata) \n\u00b6\n\n\nSpec file defines job metadata. It will be interpreted by the Director when the release is uploaded and when it's deployed.\n\n\n---\n\n\nname\n:\n \nhttp-server\n\n\n\ndescription\n:\n \nThis job runs a simple HTTP server.\n\n\n\ntemplates\n:\n\n  \nctl.sh\n:\n \nbin/ctl\n\n  \nconfig.json\n:\n \nconfig/config.json\n\n\n\npackages\n:\n\n\n-\n \nhttp-server\n\n\n\nproperties\n:\n\n  \nlisten_port\n:\n\n    \ndescription\n:\n \n\"Port\n \nto\n \nlisten\n \non\"\n\n    \ndefault\n:\n \n8080\n\n\n\n\n\nSchema:\n\n\n\n\nname\n [String, required]: Name of the job.\n\n\ndescription\n [String, optional]: Describes purpose of the job.\n\n\ntemplates\n [Hash, optional]: \nTemplate files\n found in the \ntemplates\n directory of the job and their final destinations relative to the job directory on the deployed VMs. By convention executable files should be placed into \nbin/\n directory so that the Agent can mark them as executable, and configuration files should be placed into \nconfig/\n directory.\n\n\npackages\n [Array, optional]: Package dependencies required by the job at runtime.\n\n\nproperties\n [Hash, optional]: Configuration options supported by the job.\n\n\n\\<name>\n [String, required]: Property key in dot notation. Typical properties include account names, passwords, shared secrets, hostnames, IP addresses, port numbers, and descriptions.\n\n\ndescription\n [String, required]: Describes purpose of the property.\n\n\nexample\n [Any, optional]: Example value. Default is \nnil\n.\n\n\ndefault\n [Any, optional]: Default value. Default is \nnil\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemplates (ERB configuration files) \n\u00b6\n\n\nRelease author can define zero or more templates for each job, but typically you need at least a template for a control script.\n\n\nMonit \n\u00b6\n\n\nExample \nmonit\n file for configuring single process that can start, monitor and stop a Postgres process:\n\n\ncheck process postgres\n  with pidfile /var/vcap/sys/run/postgres/pid\n  start program \"/var/vcap/jobs/postgres/bin/ctl start\"\n  stop program \"/var/vcap/jobs/postgres/bin/ctl stop\"\n\n\n\n\nControl script (\n*_ctl\n script) \n\u00b6\n\n\nIn a typical setup, control script is called by the Monit when it tries to start, and stop processes.\n\n\nMonit expects that executing \"start program\" directive will get a process running and output its PID into \"pidfile\" location. Once process is started, Monit will monitor that process is running and if it exits, it will try to restart it.\n\n\nMonit also expects that executing \"stop program\" directive will stop running process when the Director is restarting or shutting down the VM.\n\n\nHook scripts \n\u00b6\n\n\nThere are several job lifecycle events that a job can react to: pre-start, post-start, post-deploy, and drain. See \nJob lifecycle\n for the execution order.\n\n\nUse of Properties \n\u00b6\n\n\nEach template file is evaluated with \nERB\n before being sent to each instance.\n\n\nBasic ERB syntax includes:\n\n\n\n\n<%=\n \n\"value\"\n \n%>\n: Inserts string \"value\".\n\n\n<%\n \nexpression\n \n%>\n: Evaluates \nexpression\n but does not insert content into the template. Useful for \nif/else/end\n statements.\n\n\n\n\nTemplates have access to merged job property values, built by merging default property values and operator specified property values in the deployment manifest. To access properties \np\n and \nif_p\n ERB helpers are available:\n\n\n\n\n<%=\n \np\n(\n\"some.property\"\n)\n \n%>\n: Insert the property \nsome.property\n value, else a default value from the job spec file. If \nsome.property\n does not have a default in the spec file, error will be raised to the user specifying that property value is missing.\nAdvanced usage:\n\n\nOperator \np\n can take optional parameter as a default value, e.g. \n<%=\n \np\n(\n\"some.property\"\n,\n \nsome_value\n)\n \n%>\n. This value is used as a last resort.\n\n\nThe first parameter can be an array, e.g. \n<%=\n \np\n(\n[\n\"some.property1\"\n,\n \n\"some.property2\"\n]\n,\n \nsome_value\n)\n \n%>\n. Value of the first property which is set will be returned.\n\n\n\n\n\n\n<%\n \nif_p\n(\n\"some.property\"\n)\n \ndo\n \n|\nprop\n|\n \n%>\n...\n<%\n \nend\n \n%>\n - Evaluates the block only if \nsome.property\n property has been provided. The property value is available in the variable \nprop\n. Multiple properties can be specified: \n<%\n \nif_p\n(\n\"prop1\"\n,\n \n\"prop2\"\n)\n \ndo\n \n|\nprop1\n,\n \nprop2\n|\n \n%>\n.\n\n\n\n\n\n\nEach template can also access the special \nspec\n object for instance-specific configuration:\n\n\n\n\nspec.address\n: Default network address (IPv4, IPv6 or DNS record) for the instance. Available in bosh-release v255.4+.\n\n\nspec.az\n: Availability zone of the instance.\n\n\nspec.bootstrap\n: True if this instance is the first instance of its group.\n\n\nspec.deployment\n: Name of the BOSH deployment containing this instance.\n\n\nspec.id\n: ID of the instance.\n\n\nspec.index\n: Instance index. Use \nspec.bootstrap\n to determine the first instead of checking whether the index is 0. Additionally, there is no guarantee that instances will be numbered consecutively, so that there are no gaps between different indices.\n\n\nspec.ip\n: IP address of the instance. In case multiple IP addresses are available, the IP of the \naddressable or default network\n is used. Available in bosh-release v258+.\n\n\nspec.name\n: Name of the instance.\n\n\nspec.networks\n: Entire set of network information for the instance.\n\n\n\n\n\n\nNext: \nErrands",
            "title": "Usage"
        },
        {
            "location": "/jobs/#spec-file-metadata",
            "text": "Spec file defines job metadata. It will be interpreted by the Director when the release is uploaded and when it's deployed.  ---  name :   http-server  description :   This job runs a simple HTTP server.  templates : \n   ctl.sh :   bin/ctl \n   config.json :   config/config.json  packages :  -   http-server  properties : \n   listen_port : \n     description :   \"Port   to   listen   on\" \n     default :   8080   Schema:   name  [String, required]: Name of the job.  description  [String, optional]: Describes purpose of the job.  templates  [Hash, optional]:  Template files  found in the  templates  directory of the job and their final destinations relative to the job directory on the deployed VMs. By convention executable files should be placed into  bin/  directory so that the Agent can mark them as executable, and configuration files should be placed into  config/  directory.  packages  [Array, optional]: Package dependencies required by the job at runtime.  properties  [Hash, optional]: Configuration options supported by the job.  \\<name>  [String, required]: Property key in dot notation. Typical properties include account names, passwords, shared secrets, hostnames, IP addresses, port numbers, and descriptions.  description  [String, required]: Describes purpose of the property.  example  [Any, optional]: Example value. Default is  nil .  default  [Any, optional]: Default value. Default is  nil .",
            "title": "Spec file (metadata) "
        },
        {
            "location": "/jobs/#templates-erb-configuration-files",
            "text": "Release author can define zero or more templates for each job, but typically you need at least a template for a control script.",
            "title": "Templates (ERB configuration files) "
        },
        {
            "location": "/jobs/#monit",
            "text": "Example  monit  file for configuring single process that can start, monitor and stop a Postgres process:  check process postgres\n  with pidfile /var/vcap/sys/run/postgres/pid\n  start program \"/var/vcap/jobs/postgres/bin/ctl start\"\n  stop program \"/var/vcap/jobs/postgres/bin/ctl stop\"",
            "title": "Monit "
        },
        {
            "location": "/jobs/#control-script-_ctl-script",
            "text": "In a typical setup, control script is called by the Monit when it tries to start, and stop processes.  Monit expects that executing \"start program\" directive will get a process running and output its PID into \"pidfile\" location. Once process is started, Monit will monitor that process is running and if it exits, it will try to restart it.  Monit also expects that executing \"stop program\" directive will stop running process when the Director is restarting or shutting down the VM.",
            "title": "Control script (*_ctl script) "
        },
        {
            "location": "/jobs/#hook-scripts",
            "text": "There are several job lifecycle events that a job can react to: pre-start, post-start, post-deploy, and drain. See  Job lifecycle  for the execution order.",
            "title": "Hook scripts "
        },
        {
            "location": "/jobs/#use-of-properties",
            "text": "Each template file is evaluated with  ERB  before being sent to each instance.  Basic ERB syntax includes:   <%=   \"value\"   %> : Inserts string \"value\".  <%   expression   %> : Evaluates  expression  but does not insert content into the template. Useful for  if/else/end  statements.   Templates have access to merged job property values, built by merging default property values and operator specified property values in the deployment manifest. To access properties  p  and  if_p  ERB helpers are available:   <%=   p ( \"some.property\" )   %> : Insert the property  some.property  value, else a default value from the job spec file. If  some.property  does not have a default in the spec file, error will be raised to the user specifying that property value is missing.\nAdvanced usage:  Operator  p  can take optional parameter as a default value, e.g.  <%=   p ( \"some.property\" ,   some_value )   %> . This value is used as a last resort.  The first parameter can be an array, e.g.  <%=   p ( [ \"some.property1\" ,   \"some.property2\" ] ,   some_value )   %> . Value of the first property which is set will be returned.    <%   if_p ( \"some.property\" )   do   | prop |   %> ... <%   end   %>  - Evaluates the block only if  some.property  property has been provided. The property value is available in the variable  prop . Multiple properties can be specified:  <%   if_p ( \"prop1\" ,   \"prop2\" )   do   | prop1 ,   prop2 |   %> .    Each template can also access the special  spec  object for instance-specific configuration:   spec.address : Default network address (IPv4, IPv6 or DNS record) for the instance. Available in bosh-release v255.4+.  spec.az : Availability zone of the instance.  spec.bootstrap : True if this instance is the first instance of its group.  spec.deployment : Name of the BOSH deployment containing this instance.  spec.id : ID of the instance.  spec.index : Instance index. Use  spec.bootstrap  to determine the first instead of checking whether the index is 0. Additionally, there is no guarantee that instances will be numbered consecutively, so that there are no gaps between different indices.  spec.ip : IP address of the instance. In case multiple IP addresses are available, the IP of the  addressable or default network  is used. Available in bosh-release v258+.  spec.name : Name of the instance.  spec.networks : Entire set of network information for the instance.    Next:  Errands",
            "title": "Use of Properties "
        },
        {
            "location": "/job-lifecycle/",
            "text": "There are several stages that all jobs (and their associated processes) on each VM go through during a deployment process:\n\n\nWhen start is issued \n\u00b6\n\n\n\n\n\n\nPersistent disks are mounted on the VM if configured, and not already mounted\n\n\n\n\n\n\nAll jobs and their dependent packages are downloaded and placed onto a machine\n\n\n\n\n\n\npre-start scripts\n run for all jobs on the VM in parallel\n\n\n\n\n(waits for all pre-start scripts to finish)\n\n\ndoes not time out\n\n\n\n\n\n\n\n\nmonit start\n is called for each process in no particular order\n\n\n\n\neach job can specify zero or more processes\n\n\n\n\ntimes out based on \ncanary_watch_time\n/\nupdate_watch_time\n settings\n\n\n\n\n\n\npost-start scripts\n run for all jobs on the VM in parallel\n\n\n\n\n(waits for all post-start scripts to finish)\n\n\ndoes not time out\n\n\n\n\n\n\n\n\npost-deploy scripts\n run for all jobs on \nall\n VMs in parallel\n\n\n\n\n(waits for all post-deploy scripts to finish)\n\n\ndoes not time out\n\n\n\n\n\n\n\n\nNote that scripts should not rely on the order they are run. Agent may decide to run them serially or in parallel.\n\n\n\n\nWhen processes are running \n\u00b6\n\n\n\n\nMonit will automatically restart processes that failed their associated checks\n\n\na common pattern used is a PID check\n\n\n\n\n\n\nWhen stop is issued (or before update and subsequent start happens) \n\u00b6\n\n\n\n\n\n\nmonit unmonitor\n is called for each process\n\n\n\n\n\n\ndrain scripts\n run for all jobs on the VM in parallel\n\n\n\n\n(waits for all drain scripts to finish)\n\n\ndoes not time out\n\n\n\n\n\n\n\n\nmonit stop\n is called for each process\n\n\n\n\n\n\ntimes out after 5 minutes as of bosh v258+ on 3302+ stemcells\n\n\n\n\n\n\nPersistent disks are unmounted on the VM if configured\n\n\n\n\n\n\n\n\nNext: \nPre-start script",
            "title": "Update Lifecycle"
        },
        {
            "location": "/job-lifecycle/#when-start-is-issued",
            "text": "Persistent disks are mounted on the VM if configured, and not already mounted    All jobs and their dependent packages are downloaded and placed onto a machine    pre-start scripts  run for all jobs on the VM in parallel   (waits for all pre-start scripts to finish)  does not time out     monit start  is called for each process in no particular order   each job can specify zero or more processes   times out based on  canary_watch_time / update_watch_time  settings    post-start scripts  run for all jobs on the VM in parallel   (waits for all post-start scripts to finish)  does not time out     post-deploy scripts  run for all jobs on  all  VMs in parallel   (waits for all post-deploy scripts to finish)  does not time out     Note that scripts should not rely on the order they are run. Agent may decide to run them serially or in parallel.",
            "title": "When start is issued "
        },
        {
            "location": "/job-lifecycle/#when-processes-are-running",
            "text": "Monit will automatically restart processes that failed their associated checks  a common pattern used is a PID check",
            "title": "When processes are running "
        },
        {
            "location": "/job-lifecycle/#when-stop-is-issued-or-before-update-and-subsequent-start-happens",
            "text": "monit unmonitor  is called for each process    drain scripts  run for all jobs on the VM in parallel   (waits for all drain scripts to finish)  does not time out     monit stop  is called for each process    times out after 5 minutes as of bosh v258+ on 3302+ stemcells    Persistent disks are unmounted on the VM if configured     Next:  Pre-start script",
            "title": "When stop is issued (or before update and subsequent start happens) "
        },
        {
            "location": "/drain/",
            "text": "(See \nJob Lifecycle\n for an explanation of when drain scripts run.)\n\n\nRelease job can have a drain script that will run when the job is restarted or stopped. This script allows the job to clean up and get into a state where it can be safely stopped. For example, when writing a release for a load balancer, each node can safely stop accepting new connections and drain existing connections before fully stopping.\n\n\n\n\nJob Configuration \n\u00b6\n\n\nTo add a drain script to a release job:\n\n\n\n\nCreate a script with any name in the templates directory of a release job.\n\n\nIn the \ntemplates\n section of the release job spec file, add the script name and the \nbin/drain\n directory as a key value pair.\n\n\n\n\nExample:\n\n\n---\n\n\nname\n:\n \nnginx\n\n\ntemplates\n:\n\n  \ndrain-web-requests.erb\n:\n \nbin/drain\n\n\n\n\n\nNote: Drain script from each release job will run if they are deployed on 3093+ stemcells. Before only the first release job's drain script ran.\n\n\n\n\n\nScript Implementation \n\u00b6\n\n\nDrain script is usually just a regular shell script. Since drain script is executed in a similar way as other release job scripts (start, stop, pre-start scripts) you can use job's package dependencies.\n\n\nDrain script should be idempotent. It may be called multiple times before or after process is stopped.\n\n\nYou must ensure that your drain script exits in one of following ways:\n\n\n\n\n\n\nexit with a non-\n0\n exit code to indicate drain script failed\n\n\n\n\n\n\nexit with \n0\n exit code and also print an integer followed by a newline to \nstdout\n (nothing else must be printed to \nstdout\n):\n\n\nstatic draining\n: If the drain script prints a zero or a positive integer, BOSH sleeps for that many seconds before continuing.\n\n\ndynamic draining\n: If the drain script prints a negative integer, BOSH sleeps for that many seconds, then calls the drain script again.\n\n\nNote: BOSH re-runs a script indefinitely as long as the script exits with a exit code \n0\n and outputs a negative integer.\n\n\nNote: It's recommended to only use static draining as dynamic draining will be eventually deprecated.\n\n\n\n\n\n\n\n\nEnvironment Variables \n\u00b6\n\n\nDrain script can access the following environment variables:\n\n\n\n\nBOSH_JOB_STATE\n: JSON description of the current job state\n\n\nBOSH_JOB_NEXT_STATE\n: JSON description of the new job state that is being applied\n\n\n\n\nFor example drain script can use this feature to determine if the size of the persistent disk changes and take a specified action.\n\n\n\n\nLogs \n\u00b6\n\n\nCurrently logs from the drain script are not saved on disk by default, though release author may choose to do so explicitly. We are planning to eventually make it more consistent with \npre-start script logging\n.\n\n\n\n\nExample \n\u00b6\n\n\n#!/bin/bash\n\n\n\npid_path\n=\n/var/vcap/sys/run/worker/worker.pid\n\n\nif\n \n[\n -f \n$pid_path\n \n]\n;\n \nthen\n\n  \npid\n=\n$(\ncat \n$pid_path\n)\n\n  \nkill\n \n$pid\n        \n# process is running; kill it softly\n\n  sleep \n10\n         \n# wait a bit\n\n  \nkill\n -9 \n$pid\n     \n# kill it hard\n\n  rm -rf \n$pid_path\n \n# remove pid file\n\n\nfi\n\n\n\necho\n \n0\n \n# ok to exit; do not wait for anything\n\n\n\nexit\n \n0\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nPost-deploy script",
            "title": "Drain"
        },
        {
            "location": "/drain/#job-configuration",
            "text": "To add a drain script to a release job:   Create a script with any name in the templates directory of a release job.  In the  templates  section of the release job spec file, add the script name and the  bin/drain  directory as a key value pair.   Example:  ---  name :   nginx  templates : \n   drain-web-requests.erb :   bin/drain   Note: Drain script from each release job will run if they are deployed on 3093+ stemcells. Before only the first release job's drain script ran.",
            "title": "Job Configuration "
        },
        {
            "location": "/drain/#script-implementation",
            "text": "Drain script is usually just a regular shell script. Since drain script is executed in a similar way as other release job scripts (start, stop, pre-start scripts) you can use job's package dependencies.  Drain script should be idempotent. It may be called multiple times before or after process is stopped.  You must ensure that your drain script exits in one of following ways:    exit with a non- 0  exit code to indicate drain script failed    exit with  0  exit code and also print an integer followed by a newline to  stdout  (nothing else must be printed to  stdout ):  static draining : If the drain script prints a zero or a positive integer, BOSH sleeps for that many seconds before continuing.  dynamic draining : If the drain script prints a negative integer, BOSH sleeps for that many seconds, then calls the drain script again.  Note: BOSH re-runs a script indefinitely as long as the script exits with a exit code  0  and outputs a negative integer.  Note: It's recommended to only use static draining as dynamic draining will be eventually deprecated.",
            "title": "Script Implementation "
        },
        {
            "location": "/drain/#environment-variables",
            "text": "Drain script can access the following environment variables:   BOSH_JOB_STATE : JSON description of the current job state  BOSH_JOB_NEXT_STATE : JSON description of the new job state that is being applied   For example drain script can use this feature to determine if the size of the persistent disk changes and take a specified action.",
            "title": "Environment Variables "
        },
        {
            "location": "/drain/#logs",
            "text": "Currently logs from the drain script are not saved on disk by default, though release author may choose to do so explicitly. We are planning to eventually make it more consistent with  pre-start script logging .",
            "title": "Logs "
        },
        {
            "location": "/drain/#example",
            "text": "#!/bin/bash  pid_path = /var/vcap/sys/run/worker/worker.pid if   [  -f  $pid_path   ] ;   then \n   pid = $( cat  $pid_path ) \n   kill   $pid          # process is running; kill it softly \n  sleep  10           # wait a bit \n   kill  -9  $pid       # kill it hard \n  rm -rf  $pid_path   # remove pid file  fi  echo   0   # ok to exit; do not wait for anything  exit   0    Back to Table of Contents  Previous:  Post-deploy script",
            "title": "Example "
        },
        {
            "location": "/pre-start/",
            "text": "(See \nJob Lifecycle\n for an explanation of when pre-start scripts run.)\n\n\nNote: This feature is available with bosh-release v206+ (1.3072.0) and only for releases deployed with 3125+ stemcells.\n\n\n\nNote: Releases that make use of pre-start scripts and are deployed on older stemcells or with an older Director may potentially deploy; however, pre-start script will not be called.\n\n\n\nRelease job can have a pre-start script that will run before the job is started. This script allows the job to prepare machine and/or persistent data before starting its operation. For example, when writing a release for Cassandra, each node will need to migrate format of SSTables. That procedure may be lengthy and should happen before the node can successfully start.\n\n\n\n\nJob Configuration \n\u00b6\n\n\nTo add a pre-start script to a release job:\n\n\n\n\nCreate a script with any name in the templates directory of a release job.\n\n\nIn the \ntemplates\n section of the release job spec file, add the script name and the \nbin/pre-start\n directory as a key value pair.\n\n\n\n\nExample:\n\n\n---\n\n\nname\n:\n \ncassandra_node\n\n\ntemplates\n:\n\n  \npre-start.erb\n:\n \nbin/pre-start\n\n\n\n\n\n\n\nScript Implementation \n\u00b6\n\n\nPre-start script is usually just a regular shell script. ERB tags may be used for templating. Since pre-start script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.\n\n\nAfter templating, the pre-start script must have its shebang on the first line.\n\n\n\nPre-start script should be idempotent. It may be called multiple times before process is successfully started.\n\n\nUnlike a drain script, a pre-start script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).\n\n\nPre-start script is called every time before job is started (ctl script is called) by the Director, which means that pre-start script should perform its operations in an idempotent way.\n\n\nNote: Running `monit start` directly on a VM will not trigger pre-start scripts.\n\n\n\nPre-start scripts in a single deployment job (typically is composed of multiple release jobs) are executed in parallel.\n\n\n\n\nLogs \n\u00b6\n\n\nYou can find logs for each release job's pre-start script in the following locations:\n\n\n\n\nstdout in \n/var/vcap/sys/log/<job-name>/pre-start.stdout.log\n\n\nstderr in \n/var/vcap/sys/log/<job-name>/pre-start.stderr.log\n\n\n\n\nSince pre-start script will be called multiple times, new output will be appended to the files above. Standard \nlog rotation policy\n applies.\n\n\n\n\nNext: \nPost-start script\n\n\nPrevious: \nJob lifecycle",
            "title": "Pre-start"
        },
        {
            "location": "/pre-start/#job-configuration",
            "text": "To add a pre-start script to a release job:   Create a script with any name in the templates directory of a release job.  In the  templates  section of the release job spec file, add the script name and the  bin/pre-start  directory as a key value pair.   Example:  ---  name :   cassandra_node  templates : \n   pre-start.erb :   bin/pre-start",
            "title": "Job Configuration "
        },
        {
            "location": "/pre-start/#script-implementation",
            "text": "Pre-start script is usually just a regular shell script. ERB tags may be used for templating. Since pre-start script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.  After templating, the pre-start script must have its shebang on the first line.  Pre-start script should be idempotent. It may be called multiple times before process is successfully started.  Unlike a drain script, a pre-start script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).  Pre-start script is called every time before job is started (ctl script is called) by the Director, which means that pre-start script should perform its operations in an idempotent way.  Note: Running `monit start` directly on a VM will not trigger pre-start scripts.  Pre-start scripts in a single deployment job (typically is composed of multiple release jobs) are executed in parallel.",
            "title": "Script Implementation "
        },
        {
            "location": "/pre-start/#logs",
            "text": "You can find logs for each release job's pre-start script in the following locations:   stdout in  /var/vcap/sys/log/<job-name>/pre-start.stdout.log  stderr in  /var/vcap/sys/log/<job-name>/pre-start.stderr.log   Since pre-start script will be called multiple times, new output will be appended to the files above. Standard  log rotation policy  applies.   Next:  Post-start script  Previous:  Job lifecycle",
            "title": "Logs "
        },
        {
            "location": "/post-start/",
            "text": "(See \nJob Lifecycle\n for an explanation of when post-start scripts run.)\n\n\nNote: This feature is available with bosh-release v255.4+ and only for releases deployed with 3125+ stemcells.\n\n\n\nNote: Releases that make use of post-start scripts and are deployed on older stemcells or with an older Director may potentially deploy; however, post-start script will not be called.\n\n\n\nRelease job can have a post-start script that will run after the job is started (specifically after monit successfully starts a process). This script allows the job to execute any additional commands against a machine and/or persistent data before considering release job as successfully started.\n\n\n\n\nJob Configuration \n\u00b6\n\n\nTo add a post-start script to a release job:\n\n\n\n\nCreate a script with any name in the templates directory of a release job.\n\n\nIn the \ntemplates\n section of the release job spec file, add the script name and the \nbin/post-start\n directory as a key value pair.\n\n\n\n\nExample:\n\n\n---\n\n\nname\n:\n \ncassandra_node\n\n\ntemplates\n:\n\n  \npost-start.erb\n:\n \nbin/post-start\n\n\n\n\n\n\n\nScript Implementation \n\u00b6\n\n\nPost-start script is usually just a regular shell script. Since post-start script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.\n\n\nPost-start script should be idempotent. It may be called multiple times after process is successfully started.\n\n\nUnlike a drain script, a post-start script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).\n\n\nPost-start script is called every time after job is started (ctl script is called) by the Director, which means that post-start script should perform its operations in an idempotent way.\n\n\nNote: Running `monit start` directly on a VM will not trigger post-start scripts.\n\n\n\nPost-start scripts in a single deployment job (typically is composed of multiple release jobs) are executed in parallel.\n\n\n\n\nLogs \n\u00b6\n\n\nYou can find logs for each release job's post-start script in the following locations:\n\n\n\n\nstdout in \n/var/vcap/sys/log/<job-name>/post-start.stdout.log\n\n\nstderr in \n/var/vcap/sys/log/<job-name>/post-start.stderr.log\n\n\n\n\nSince post-start script will be called multiple times, new output will be appended to the files above. Standard \nlog rotation policy\n applies.\n\n\n\n\nNext: \nPost-deploy script\n\n\nPrevious: \nPre-start script",
            "title": "Post-start"
        },
        {
            "location": "/post-start/#job-configuration",
            "text": "To add a post-start script to a release job:   Create a script with any name in the templates directory of a release job.  In the  templates  section of the release job spec file, add the script name and the  bin/post-start  directory as a key value pair.   Example:  ---  name :   cassandra_node  templates : \n   post-start.erb :   bin/post-start",
            "title": "Job Configuration "
        },
        {
            "location": "/post-start/#script-implementation",
            "text": "Post-start script is usually just a regular shell script. Since post-start script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.  Post-start script should be idempotent. It may be called multiple times after process is successfully started.  Unlike a drain script, a post-start script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).  Post-start script is called every time after job is started (ctl script is called) by the Director, which means that post-start script should perform its operations in an idempotent way.  Note: Running `monit start` directly on a VM will not trigger post-start scripts.  Post-start scripts in a single deployment job (typically is composed of multiple release jobs) are executed in parallel.",
            "title": "Script Implementation "
        },
        {
            "location": "/post-start/#logs",
            "text": "You can find logs for each release job's post-start script in the following locations:   stdout in  /var/vcap/sys/log/<job-name>/post-start.stdout.log  stderr in  /var/vcap/sys/log/<job-name>/post-start.stderr.log   Since post-start script will be called multiple times, new output will be appended to the files above. Standard  log rotation policy  applies.   Next:  Post-deploy script  Previous:  Pre-start script",
            "title": "Logs "
        },
        {
            "location": "/post-deploy/",
            "text": "(See \nJob Lifecycle\n for an explanation of when post-deploy scripts run.)\n\n\nNote: This feature is available with bosh-release v255.4+ and only for releases deployed with 3125+ stemcells.\n\n\n\nNote: Releases that make use of post-deploy scripts and are deployed on older stemcells or with an older Director may potentially deploy; however, post-deploy script will not be called.\n\n\n\nRelease job can have a post-deploy script that will run after all jobs in the deployments successfully started (and ran post-start scripts). This script allows the job to execute any additional commands against a whole deployment before considering deploy finished.\n\n\n\n\nDirector Configuration \n\u00b6\n\n\nCurrently the Director does not run post deploy scripts by default. Use \ndirector.enable_post_deploy\n property\n to enable running scripts.\n\n\n\n\nJob Configuration \n\u00b6\n\n\nTo add a post-deploy script to a release job:\n\n\n\n\nCreate a script with any name in the templates directory of a release job.\n\n\nIn the \ntemplates\n section of the release job spec file, add the script name and the \nbin/post-deploy\n directory as a key value pair.\n\n\n\n\nExample:\n\n\n---\n\n\nname\n:\n \ncassandra_node\n\n\ntemplates\n:\n\n  \npost-deploy.erb\n:\n \nbin/post-deploy\n\n\n\n\n\n\n\nScript Implementation \n\u00b6\n\n\nPost-deploy script is usually just a regular shell script. Since post-deploy script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.\n\n\nPost-deploy script should be idempotent. It may be called multiple times after process is successfully started.\n\n\nUnlike a drain script, a post-deploy script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).\n\n\nPost-deploy script is called every time after job is started (ctl script is called) by the Director, which means that post-deploy script should perform its operations in an idempotent way.\n\n\nNote: Running `monit start` directly on a VM will not trigger post-deploy scripts.\n\n\n\nPost-deploy scripts in a deployment are executed in parallel.\n\n\n\n\nLogs \n\u00b6\n\n\nYou can find logs for each release job's post-deploy script in the following locations:\n\n\n\n\nstdout in \n/var/vcap/sys/log/<job-name>/post-deploy.stdout.log\n\n\nstderr in \n/var/vcap/sys/log/<job-name>/post-deploy.stderr.log\n\n\n\n\nSince post-deploy script will be called multiple times, new output will be appended to the files above. Standard \nlog rotation policy\n applies.\n\n\n\n\nNext: \nDrain script\n\n\nPrevious: \nPost-start script",
            "title": "Post-deploy"
        },
        {
            "location": "/post-deploy/#director-configuration",
            "text": "Currently the Director does not run post deploy scripts by default. Use  director.enable_post_deploy  property  to enable running scripts.",
            "title": "Director Configuration "
        },
        {
            "location": "/post-deploy/#job-configuration",
            "text": "To add a post-deploy script to a release job:   Create a script with any name in the templates directory of a release job.  In the  templates  section of the release job spec file, add the script name and the  bin/post-deploy  directory as a key value pair.   Example:  ---  name :   cassandra_node  templates : \n   post-deploy.erb :   bin/post-deploy",
            "title": "Job Configuration "
        },
        {
            "location": "/post-deploy/#script-implementation",
            "text": "Post-deploy script is usually just a regular shell script. Since post-deploy script is executed in a similar way as other release job scripts (start, stop, drain scripts) you can use job's package dependencies.  Post-deploy script should be idempotent. It may be called multiple times after process is successfully started.  Unlike a drain script, a post-deploy script uses an exit code to indicate its success (exit code 0) or failure (any other exit code).  Post-deploy script is called every time after job is started (ctl script is called) by the Director, which means that post-deploy script should perform its operations in an idempotent way.  Note: Running `monit start` directly on a VM will not trigger post-deploy scripts.  Post-deploy scripts in a deployment are executed in parallel.",
            "title": "Script Implementation "
        },
        {
            "location": "/post-deploy/#logs",
            "text": "You can find logs for each release job's post-deploy script in the following locations:   stdout in  /var/vcap/sys/log/<job-name>/post-deploy.stdout.log  stderr in  /var/vcap/sys/log/<job-name>/post-deploy.stderr.log   Since post-deploy script will be called multiple times, new output will be appended to the files above. Standard  log rotation policy  applies.   Next:  Drain script  Previous:  Post-start script",
            "title": "Logs "
        },
        {
            "location": "/links-common-types/",
            "text": "Common suggested link types that release authors should conform to if type is used.\n\n\n\n\ndatabase\n type \n\u00b6\n\n\nlink('...').address\n returns database address.\n\n\n\n\nadapter\n [String]: Name of the adapter.\n\n\nport\n [Integer]: Port to connect on.\n\n\ntls\n [Hash]: See \nTLS properties\n\n\nusername\n [String]: Database username to use. Example: \nu387455\n.\n\n\npassword\n [String]: Database password to use.\n\n\ndatabase\n [String]: Database name. Example: \napp-server\n\n\noptions\n [Hash, optional]: Opaque set of options for the adapter. Example: \n{max_connections: 32, pool_timeout: 10}\n.\n\n\n\n\n\n\nblobstore\n type \n\u00b6\n\n\nlink('...').address\n returns blobstore address.\n\n\n\n\nadapter\n [String]: Name of the adapter (s3, gcp, etc.).\n\n\ntls\n [Hash]: See \nTLS properties\n\n\noptions\n [Hash, optional]: Opaque set of options. Example: \n{aws_access_key: ..., secret_access_key: ...}\n.",
            "title": "Common Types"
        },
        {
            "location": "/links-common-types/#database-type",
            "text": "link('...').address  returns database address.   adapter  [String]: Name of the adapter.  port  [Integer]: Port to connect on.  tls  [Hash]: See  TLS properties  username  [String]: Database username to use. Example:  u387455 .  password  [String]: Database password to use.  database  [String]: Database name. Example:  app-server  options  [Hash, optional]: Opaque set of options for the adapter. Example:  {max_connections: 32, pool_timeout: 10} .",
            "title": "database type "
        },
        {
            "location": "/links-common-types/#blobstore-type",
            "text": "link('...').address  returns blobstore address.   adapter  [String]: Name of the adapter (s3, gcp, etc.).  tls  [Hash]: See  TLS properties  options  [Hash, optional]: Opaque set of options. Example:  {aws_access_key: ..., secret_access_key: ...} .",
            "title": "blobstore type "
        },
        {
            "location": "/aws-cpi/",
            "text": "This topic describes cloud properties for different resources created by the AWS CPI.\n\n\nAZs \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\navailability_zone\n [String, required]: Availability zone to use for creating instances. Example: \nus-east-1a\n.\n\n\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \nus-east-1a\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by dynamic network or manual network subnet:\n\n\n\n\nsubnet\n [String, required]: Subnet ID in which the instance will be created. Example: \nsubnet-9be6c3f7\n.\n\n\nsecurity_groups\n [Array, optional]: Array of \nSecurity Groups\n, by name or ID, to apply to all VMs placed on this network. Security groups can be specified as follows, ordered by greatest precedence: \nvm_types\n, followed by \nnetworks\n, followed by \ndefault_security_groups\n.\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ncloud_properties\n:\n\n      \nsubnet\n:\n \nsubnet-9be6c3f7\n\n      \nsecurity_groups\n:\n \n[\nbosh\n]\n\n\n\n\n\nExample of dynamic network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \ncloud_properties\n:\n\n    \nsubnet\n:\n \nsubnet-9be6c6gh\n\n\n\n\n\nExample of vip network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nvip\n\n\n\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ninstance_type\n [String, required]: Type of the \ninstance\n. Example: \nm3.medium\n.\n\n\navailability_zone\n [String, required]: Availability zone to use for creating instances. Example: \nus-east-1a\n.\n\n\nsecurity_groups\n [Array, optional]: See description under \nnetworks\n. Available in v46+.\n\n\nkey_name\n [String, optional]: Key pair name. Defaults to key pair name specified by \ndefault_key_name\n in global CPI settings. Example: \nbosh\n.\n\n\nspot_bid_price\n [Float, optional]: Bid price in dollars for \nAWS spot instance\n. Using this option will slow down VM creation. Example: \n0.03\n.\n\n\nspot_ondemand_fallback\n [Boolean, optional]: Set to \ntrue\n to use an on demand instance if a spot instance is not available during VM creation. Defaults to \nfalse\n. Available in v36.\n\n\nelbs\n [Array, optional]: Array of ELB names that should be attached to created VMs. Example: \n[prod-elb]\n. Default is \n[]\n.\n\n\nlb_target_groups\n [Array, optional]: Array of Load Balancer Target Groups to which created VMs should be attached. Example: \n[prod-group1, prod-group2]\n. Default is \n[]\n. Available in v63 or newer.\n\n\niam_instance_profile\n [String, optional]: Name of an \nIAM instance profile\n. Example: \ndirector\n.\n\n\nplacement_group\n [String, optional]: Name of a \nplacement group\n. Example: \nmy-group\n.\n\n\ntenancy\n [String, optional]: VM \ntenancy\n configuration. Example: \ndedicated\n. Default is \ndefault\n.\n\n\nauto_assign_public_ip\n [Boolean, optional]: Assigns a public IP address to the created VM. This IP is ephemeral and may change; use an \nElastic IP\n instead for a persistent address. Defaults to \nfalse\n. Available in v55+.\n\n\nadvertised_routes\n [Array, optional]: Creates routes in an \nAWS Route Table\n with the created BOSH VM as the target. Requires IAM action \nec2\n:\nCreateRoute\n, \nec2\n:\nDescribeRouteTables\n, \nec2\n:\nReplaceRoute\n.\n\n\ntable_id\n [String, required]: ID of the route table in which to create the route (e.g. \nrt-abcdef123\n).\n\n\ndestination\n [String, required]: Destination CIDR for the route. All traffic with a destination within this CIDR will be routed through the created BOSH VM.\n\n\nraw_instance_storage\n [Boolean, optional]: Exposes all available \ninstance storage via labeled disks\n. Defaults to \nfalse\n.\n\n\nsource_dest_check\n [Boolean, optional]: Specifies whether the instance must be the source or destination of any traffic it sends or receives. If set to \nfalse\n, the instance does \nnot\n need to be the source or destination. Used for network address translation (NAT) boxes, frequently to communicate between VPCs. Defaults to \ntrue\n. Requires IAM action \nec2\n:\nModifyInstanceAttribute\n. Available in v59+.\n\n\nephemeral_disk\n [Hash, optional]: EBS backed ephemeral disk of custom size. Default disk size is either the size of first instance storage disk, if the instance_type offers it, or 10GB. Before v53: Used EBS only if instance storage is not large enough or not available for selected instance type.\n\n\nsize\n [Integer, required]: Specifies the disk size in megabytes.\n\n\ntype\n [String, optional]: Type of the \ndisk\n: \nstandard\n, \ngp2\n. Defaults to \ngp2\n.\n\n\nstandard\n stands for EBS magnetic drives\n\n\ngp2\n stands for EBS general purpose drives (SSD)\n\n\nio1\n stands for EBS provisioned IOPS drives (SSD)\n\n\n\n\n\n\niops\n [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.\n\n\nOnly valid for \nio1\n type drive.\n\n\nRequired when \nio1\n type drive is specified.\n\n\n\n\n\n\nencrypted\n [Boolean, optional] Enables encryption for the EBS backed ephemeral disk. An error is raised, if the \ninstance_type\n does not support it. Since v53. Defaults to \nfalse\n. Overrides the global \nencrypted\n property.\n\n\nkms_key_arn\n [String, optional] The ARN of an Amazon KMS key to use when encrypting the disk.\n\n\nuse_instance_storage\n [Boolean, optional] Forces the usage of instance storage as ephemeral disk backing. Will raise an error, if the used \ninstance_type\n does not have instance storage. Cannot be combined with any other option under \nephemeral_disk\n or with \nraw_instance_storage\n. Since v53. Defaults to \nfalse\n.\n\n\n\n\n\n\nroot_disk\n [Hash, optional]: EBS backed root disk of custom size.\n\n\nsize\n [Integer, required]: Specifies the disk size in megabytes.\n\n\ntype\n [String, optional]: Type of the \ndisk\n: \nstandard\n, \ngp2\n. Defaults to \ngp2\n.\n\n\nstandard\n stands for EBS magnetic drives\n\n\ngp2\n stands for EBS general purpose drives (SSD)\n\n\n\n\n\n\niops\n [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.\n\n\nOnly valid for \nio1\n type drive.\n\n\nRequired when \nio1\n type drive is specified.\n\n\n\n\n\n\n\n\n\n\n\n\nExample of an \nm3.medium\n instance:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-aws-xen-hvm-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm3.medium\n\n    \navailability_zone\n:\n \nus-east-1a\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ntype\n [String, optional]: Type of the \ndisk\n: \nstandard\n, \ngp2\n. Defaults to \ngp2\n.\n\n\nstandard\n stands for EBS magnetic drives\n\n\ngp2\n stands for EBS general purpose drives (SSD)\n\n\nio1\n stands for EBS provisioned IOPS drives (SSD)\n\n\niops\n [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.\n\n\nOnly valid for \nio1\n type drive.\n\n\nRequired when \nio1\n type drive is specified.\n\n\nencrypted\n [Boolean, optional]: Turns on \nEBS volume encryption\n for this persistent disk. VM root and ephemeral disk are not encrypted. Defaults to \nfalse\n. Overrides the global \nencrypted\n property.\n\n\nkms_key_arn\n [String, optional]: Encrypts the disk using an encryption key stored in the \nAWS Key Management Service (KMS)\n. The format of the ID is \nXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n. Be sure to use the Key ID, not the Alias. If omitted the disk will be encrypted using the global \nkms_key_arn\n property. If, no global \nkms_key_arn\n is set will use your account's default \naws/ebs\n encryption key.\n\n\n\n\nEBS volumes are created in the availability zone of an instance that volume will be attached.\n\n\nExample of 10GB disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n  \ncloud_properties\n:\n\n    \ntype\n:\n \ngp2\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nThe CPI can only talk to a single AWS region.\n\n\nSchema:\n\n\n\n\ncredentials_source\n [String, optional]: Selects credentials source between credentials provided in this configuration, or from an \nIAM instance profile\n. Default: \nstatic\n.\n\n\naccess_key_id\n [String, optional]: Accesss Key ID. Example: \nAKI...\n.\n\n\nsecret_access_key\n [String, optional]: Secret Access Key. Example: \n0kwh...\n.\n\n\ndefault_key_name\n [String, required]: Name of the \nKey Pair\n that will be applied to all created VMs. Example: \nbosh\n\n\ndefault_security_groups\n [Array, required]: See description under \nnetworks\n.\n\n\ndefault_iam_instance_profile\n [String, optional]: Name of the \nIAM instance profile\n that will be applied to all created VMs. Example: \ndirector\n.\n\n\nregion\n [String, required]: AWS region name. Example: \nus-east-1\n\n\nmax_retries\n [Integer, optional]: The maximum number of times AWS service errors (500) and throttling errors (\nAWS::EC2::Errors::RequestLimitExceeded\n) should be retried. There is an exponential backoff in between retries, so the more retries the longer it can take to fail. Defaults to 2.\n\n\nencrypted\n [Boolean, optional]: Turns on \nEBS volume encryption\n for all VM's root (system), ephemeral and persistent disks. Defaults to \nfalse\n. Available in v67+.\n\n\nkms_key_arn\n [String, optional]: Encrypts the disks using an encryption key stored in the \nAWS Key Management Service (KMS)\n. The format of the ID is \nXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n. Be sure to use the Key ID, not the Alias. If this property is omitted and \nencrypted\n is \ntrue\n, the disks will be encrypted using your account's default \naws/ebs\n encryption key. Available in v67+.\n\n\n\n\nSee \nall configuration options\n.\n\n\nExample with hard-coded credentials:\n\n\nproperties\n:\n\n  \naws\n:\n\n    \naccess_key_id\n:\n \nACCESS-KEY-ID\n\n    \nsecret_access_key\n:\n \nSECRET-ACCESS-KEY\n\n    \ndefault_key_name\n:\n \nbosh\n\n    \ndefault_security_groups\n:\n \n[\nbosh\n]\n\n    \nregion\n:\n \nus-east-1\n\n\n\n\n\nExample when \nIAM instance profiles\n are used:\n\n\nproperties\n:\n\n  \naws\n:\n\n    \ncredentials_source\n:\n \nenv_or_profile\n\n    \ndefault_key_name\n:\n \nbosh\n\n    \ndefault_security_groups\n:\n \n[\nbosh\n]\n\n    \ndefault_iam_instance_profile\n:\n \ndeployed-vm\n\n    \nregion\n:\n \nus-east-1\n\n\n\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n \n{\navailability_zone\n:\n \nus-east-1a\n}\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n \n{\navailability_zone\n:\n \nus-east-1b\n}\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nt2.micro\n\n    \nephemeral_disk\n:\n \n{\nsize\n:\n \n3000\n,\n \ntype\n:\n \ngp2\n}\n\n\n-\n \nname\n:\n \nlarge\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm3.large\n\n    \nephemeral_disk\n:\n \n{\nsize\n:\n \n30000\n,\n \ntype\n:\n \ngp2\n}\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n3000\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n-\n \nname\n:\n \nlarge\n\n  \ndisk_size\n:\n \n50_000\n\n  \ncloud_properties\n:\n \n{\ntype\n:\n \ngp2\n}\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \naz\n:\n \nz1\n\n    \nstatic\n:\n \n[\n10.10.0.62\n]\n\n    \ndns\n:\n \n[\n10.10.0.2\n]\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-f2744a86\n}\n\n  \n-\n \nrange\n:\n \n10.10.64.0/24\n\n    \ngateway\n:\n \n10.10.64.1\n\n    \naz\n:\n \nz2\n\n    \nstatic\n:\n \n[\n10.10.64.121\n,\n \n10.10.64.122\n]\n\n    \ndns\n:\n \n[\n10.10.0.2\n]\n\n    \ncloud_properties\n:\n \n{\nsubnet\n:\n \nsubnet-eb8bd3ad\n}\n\n\n-\n \nname\n:\n \nvip\n\n  \ntype\n:\n \nvip\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \nlarge\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nErrors \n\u00b6\n\n\nStemcell does not contain an AMI for this region (us-west-2c)\n\n\n\n\nMake sure that \nregion\n specified in global CPI configuration is one of the \nofficial AWS regions\n. AWS regions typically end with a number, so in the example above region is erroneously specified (it's set to an AZ since each region is divided into multiple AZ which end with a letter.)\n\n\nNetwork vpc-a09e18c5 is not attached to any internet gateway\n\n\n\n\nYou need to create and attach an internet gateway to your VPC so that VMs can connect to the Internet.\n\n\nThe subnet ID 'subnet-c3051fad' does not exist\n\n\n\n\nMake sure that the \nregion\n specified in the global CPI configuration matches the region where your specified subnet resides.\n\n\nSignature expired: 20141106T010406Z is now earlier than 20141106T011252Z (20141106T011752Z - 5 min.)\n\n\n\n\nThis error is usually caused by out-of-sync system time. Use \nntpdate\n to sync the clock on the machine where BOSH CLI is run: \nsudo ntpdate pool.ntp.org\n. Alternatively make sure that \nntpd\n is correctly configured and running.\n\n\nresource eipalloc-6a45950f is already associated with associate-id eipassoc-427beb26\n\n\n\n\nThis error indicates that elastic IP specified in the manifest to be associated to the VM is in use by another VM. Check AWS console and decide whether other VM should be deleted to make elastic IP available for use.\n\n\nSpecifying an IP address is only valid for VPC instances and thus requires a subnet in which to launch\n\n\n\n\nMake sure that each manual network subnet has \ncloud_properties\n key and its contents include \nsubnet\n key with the AWS Subnet ID. (You may have accidentally specified \ncloud_properties\n on the network itself.)\n\n\nArguments are not correct\n\n\n\n\nThis error may be raised when:\n- \ninstance_type\n is missing from the compilation or one of the resource pools' \ncloud_properties\n section\n- the deployment job instance is not assigned a static IP\n\n\nAddress 10.10.16.251 is in use.\n\n\n\n\nThis error indicates that unknown VM took up the IP that the Director is trying to assign to a new VM. Either let the Director know to not use this IP by including it in the reserved section of a subnet in your manual network, or make that IP available by terminating the unknown VM.\n\n\nWhen specifying a security group you must specify a group id for each item.\n\n\n\n\nMake sure all security groups in the CPI configuration and networks' \ncloud_properties\n sections are specified in the same format, as IDs (e.g. \nsg-384fher\n) or names (e.g. \ncf-public\n).\n\n\nYou are not authorized to perform this operation. Encoded authorization failure message: vHU-KncL6Yo4pG5J9p...\n\n\n\n\nSee \nIAM instance profiles errors\n.\n\n\nNon-Windows instances with a virtualization type of 'hvm' are currently not supported for this instance type.\n\n\n\n\nYou cannot use HVM stemcells with certain instance types. Review which instance type is specified in a referenced resource pool.\n\n\nAWS::EC2::Errors::RequestLimitExceeded Request limit exceeded.\n\n\n\n\nAWS API is throttling the number of request in your account. You can reduce the number of threads running in BOSH, or increase the value of \naws.max_retries\n to let the AWS client library perform retries in a exponential backoff. Note that the more retries, the longer will take to fail.\n\n\n\n\nNext: \nUsing IAM instance profiles",
            "title": "Usage"
        },
        {
            "location": "/aws-cpi/#azs",
            "text": "Schema for  cloud_properties  section:   availability_zone  [String, required]: Availability zone to use for creating instances. Example:  us-east-1a .   Example:  azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   us-east-1a",
            "title": "AZs "
        },
        {
            "location": "/aws-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by dynamic network or manual network subnet:   subnet  [String, required]: Subnet ID in which the instance will be created. Example:  subnet-9be6c3f7 .  security_groups  [Array, optional]: Array of  Security Groups , by name or ID, to apply to all VMs placed on this network. Security groups can be specified as follows, ordered by greatest precedence:  vm_types , followed by  networks , followed by  default_security_groups .   Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     cloud_properties : \n       subnet :   subnet-9be6c3f7 \n       security_groups :   [ bosh ]   Example of dynamic network:  networks :  -   name :   default \n   type :   dynamic \n   cloud_properties : \n     subnet :   subnet-9be6c6gh   Example of vip network:  networks :  -   name :   default \n   type :   vip",
            "title": "Networks "
        },
        {
            "location": "/aws-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   instance_type  [String, required]: Type of the  instance . Example:  m3.medium .  availability_zone  [String, required]: Availability zone to use for creating instances. Example:  us-east-1a .  security_groups  [Array, optional]: See description under  networks . Available in v46+.  key_name  [String, optional]: Key pair name. Defaults to key pair name specified by  default_key_name  in global CPI settings. Example:  bosh .  spot_bid_price  [Float, optional]: Bid price in dollars for  AWS spot instance . Using this option will slow down VM creation. Example:  0.03 .  spot_ondemand_fallback  [Boolean, optional]: Set to  true  to use an on demand instance if a spot instance is not available during VM creation. Defaults to  false . Available in v36.  elbs  [Array, optional]: Array of ELB names that should be attached to created VMs. Example:  [prod-elb] . Default is  [] .  lb_target_groups  [Array, optional]: Array of Load Balancer Target Groups to which created VMs should be attached. Example:  [prod-group1, prod-group2] . Default is  [] . Available in v63 or newer.  iam_instance_profile  [String, optional]: Name of an  IAM instance profile . Example:  director .  placement_group  [String, optional]: Name of a  placement group . Example:  my-group .  tenancy  [String, optional]: VM  tenancy  configuration. Example:  dedicated . Default is  default .  auto_assign_public_ip  [Boolean, optional]: Assigns a public IP address to the created VM. This IP is ephemeral and may change; use an  Elastic IP  instead for a persistent address. Defaults to  false . Available in v55+.  advertised_routes  [Array, optional]: Creates routes in an  AWS Route Table  with the created BOSH VM as the target. Requires IAM action  ec2 : CreateRoute ,  ec2 : DescribeRouteTables ,  ec2 : ReplaceRoute .  table_id  [String, required]: ID of the route table in which to create the route (e.g.  rt-abcdef123 ).  destination  [String, required]: Destination CIDR for the route. All traffic with a destination within this CIDR will be routed through the created BOSH VM.  raw_instance_storage  [Boolean, optional]: Exposes all available  instance storage via labeled disks . Defaults to  false .  source_dest_check  [Boolean, optional]: Specifies whether the instance must be the source or destination of any traffic it sends or receives. If set to  false , the instance does  not  need to be the source or destination. Used for network address translation (NAT) boxes, frequently to communicate between VPCs. Defaults to  true . Requires IAM action  ec2 : ModifyInstanceAttribute . Available in v59+.  ephemeral_disk  [Hash, optional]: EBS backed ephemeral disk of custom size. Default disk size is either the size of first instance storage disk, if the instance_type offers it, or 10GB. Before v53: Used EBS only if instance storage is not large enough or not available for selected instance type.  size  [Integer, required]: Specifies the disk size in megabytes.  type  [String, optional]: Type of the  disk :  standard ,  gp2 . Defaults to  gp2 .  standard  stands for EBS magnetic drives  gp2  stands for EBS general purpose drives (SSD)  io1  stands for EBS provisioned IOPS drives (SSD)    iops  [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.  Only valid for  io1  type drive.  Required when  io1  type drive is specified.    encrypted  [Boolean, optional] Enables encryption for the EBS backed ephemeral disk. An error is raised, if the  instance_type  does not support it. Since v53. Defaults to  false . Overrides the global  encrypted  property.  kms_key_arn  [String, optional] The ARN of an Amazon KMS key to use when encrypting the disk.  use_instance_storage  [Boolean, optional] Forces the usage of instance storage as ephemeral disk backing. Will raise an error, if the used  instance_type  does not have instance storage. Cannot be combined with any other option under  ephemeral_disk  or with  raw_instance_storage . Since v53. Defaults to  false .    root_disk  [Hash, optional]: EBS backed root disk of custom size.  size  [Integer, required]: Specifies the disk size in megabytes.  type  [String, optional]: Type of the  disk :  standard ,  gp2 . Defaults to  gp2 .  standard  stands for EBS magnetic drives  gp2  stands for EBS general purpose drives (SSD)    iops  [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.  Only valid for  io1  type drive.  Required when  io1  type drive is specified.       Example of an  m3.medium  instance:  resource_pools :  -   name :   default \n   network :   default \n   stemcell : \n     name :   bosh-aws-xen-hvm-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     instance_type :   m3.medium \n     availability_zone :   us-east-1a",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/aws-cpi/#disk-pools-disk-types",
            "text": "Schema for  cloud_properties  section:   type  [String, optional]: Type of the  disk :  standard ,  gp2 . Defaults to  gp2 .  standard  stands for EBS magnetic drives  gp2  stands for EBS general purpose drives (SSD)  io1  stands for EBS provisioned IOPS drives (SSD)  iops  [Integer, optional]: Specifies the number of I/O operations per second to provision for the drive.  Only valid for  io1  type drive.  Required when  io1  type drive is specified.  encrypted  [Boolean, optional]: Turns on  EBS volume encryption  for this persistent disk. VM root and ephemeral disk are not encrypted. Defaults to  false . Overrides the global  encrypted  property.  kms_key_arn  [String, optional]: Encrypts the disk using an encryption key stored in the  AWS Key Management Service (KMS) . The format of the ID is  XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX . Be sure to use the Key ID, not the Alias. If omitted the disk will be encrypted using the global  kms_key_arn  property. If, no global  kms_key_arn  is set will use your account's default  aws/ebs  encryption key.   EBS volumes are created in the availability zone of an instance that volume will be attached.  Example of 10GB disk:  disk_pools :  -   name :   default \n   disk_size :   10_240 \n   cloud_properties : \n     type :   gp2",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/aws-cpi/#global-configuration",
            "text": "The CPI can only talk to a single AWS region.  Schema:   credentials_source  [String, optional]: Selects credentials source between credentials provided in this configuration, or from an  IAM instance profile . Default:  static .  access_key_id  [String, optional]: Accesss Key ID. Example:  AKI... .  secret_access_key  [String, optional]: Secret Access Key. Example:  0kwh... .  default_key_name  [String, required]: Name of the  Key Pair  that will be applied to all created VMs. Example:  bosh  default_security_groups  [Array, required]: See description under  networks .  default_iam_instance_profile  [String, optional]: Name of the  IAM instance profile  that will be applied to all created VMs. Example:  director .  region  [String, required]: AWS region name. Example:  us-east-1  max_retries  [Integer, optional]: The maximum number of times AWS service errors (500) and throttling errors ( AWS::EC2::Errors::RequestLimitExceeded ) should be retried. There is an exponential backoff in between retries, so the more retries the longer it can take to fail. Defaults to 2.  encrypted  [Boolean, optional]: Turns on  EBS volume encryption  for all VM's root (system), ephemeral and persistent disks. Defaults to  false . Available in v67+.  kms_key_arn  [String, optional]: Encrypts the disks using an encryption key stored in the  AWS Key Management Service (KMS) . The format of the ID is  XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX . Be sure to use the Key ID, not the Alias. If this property is omitted and  encrypted  is  true , the disks will be encrypted using your account's default  aws/ebs  encryption key. Available in v67+.   See  all configuration options .  Example with hard-coded credentials:  properties : \n   aws : \n     access_key_id :   ACCESS-KEY-ID \n     secret_access_key :   SECRET-ACCESS-KEY \n     default_key_name :   bosh \n     default_security_groups :   [ bosh ] \n     region :   us-east-1   Example when  IAM instance profiles  are used:  properties : \n   aws : \n     credentials_source :   env_or_profile \n     default_key_name :   bosh \n     default_security_groups :   [ bosh ] \n     default_iam_instance_profile :   deployed-vm \n     region :   us-east-1",
            "title": "Global Configuration "
        },
        {
            "location": "/aws-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1 \n   cloud_properties :   { availability_zone :   us-east-1a }  -   name :   z2 \n   cloud_properties :   { availability_zone :   us-east-1b }  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   t2.micro \n     ephemeral_disk :   { size :   3000 ,   type :   gp2 }  -   name :   large \n   cloud_properties : \n     instance_type :   m3.large \n     ephemeral_disk :   { size :   30000 ,   type :   gp2 }  disk_types :  -   name :   default \n   disk_size :   3000 \n   cloud_properties :   { type :   gp2 }  -   name :   large \n   disk_size :   50_000 \n   cloud_properties :   { type :   gp2 }  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     az :   z1 \n     static :   [ 10.10.0.62 ] \n     dns :   [ 10.10.0.2 ] \n     cloud_properties :   { subnet :   subnet-f2744a86 } \n   -   range :   10.10.64.0/24 \n     gateway :   10.10.64.1 \n     az :   z2 \n     static :   [ 10.10.64.121 ,   10.10.64.122 ] \n     dns :   [ 10.10.0.2 ] \n     cloud_properties :   { subnet :   subnet-eb8bd3ad }  -   name :   vip \n   type :   vip  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   large \n   network :   default",
            "title": "Example Cloud Config "
        },
        {
            "location": "/aws-cpi/#errors",
            "text": "Stemcell does not contain an AMI for this region (us-west-2c)  Make sure that  region  specified in global CPI configuration is one of the  official AWS regions . AWS regions typically end with a number, so in the example above region is erroneously specified (it's set to an AZ since each region is divided into multiple AZ which end with a letter.)  Network vpc-a09e18c5 is not attached to any internet gateway  You need to create and attach an internet gateway to your VPC so that VMs can connect to the Internet.  The subnet ID 'subnet-c3051fad' does not exist  Make sure that the  region  specified in the global CPI configuration matches the region where your specified subnet resides.  Signature expired: 20141106T010406Z is now earlier than 20141106T011252Z (20141106T011752Z - 5 min.)  This error is usually caused by out-of-sync system time. Use  ntpdate  to sync the clock on the machine where BOSH CLI is run:  sudo ntpdate pool.ntp.org . Alternatively make sure that  ntpd  is correctly configured and running.  resource eipalloc-6a45950f is already associated with associate-id eipassoc-427beb26  This error indicates that elastic IP specified in the manifest to be associated to the VM is in use by another VM. Check AWS console and decide whether other VM should be deleted to make elastic IP available for use.  Specifying an IP address is only valid for VPC instances and thus requires a subnet in which to launch  Make sure that each manual network subnet has  cloud_properties  key and its contents include  subnet  key with the AWS Subnet ID. (You may have accidentally specified  cloud_properties  on the network itself.)  Arguments are not correct  This error may be raised when:\n-  instance_type  is missing from the compilation or one of the resource pools'  cloud_properties  section\n- the deployment job instance is not assigned a static IP  Address 10.10.16.251 is in use.  This error indicates that unknown VM took up the IP that the Director is trying to assign to a new VM. Either let the Director know to not use this IP by including it in the reserved section of a subnet in your manual network, or make that IP available by terminating the unknown VM.  When specifying a security group you must specify a group id for each item.  Make sure all security groups in the CPI configuration and networks'  cloud_properties  sections are specified in the same format, as IDs (e.g.  sg-384fher ) or names (e.g.  cf-public ).  You are not authorized to perform this operation. Encoded authorization failure message: vHU-KncL6Yo4pG5J9p...  See  IAM instance profiles errors .  Non-Windows instances with a virtualization type of 'hvm' are currently not supported for this instance type.  You cannot use HVM stemcells with certain instance types. Review which instance type is specified in a referenced resource pool.  AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded.  AWS API is throttling the number of request in your account. You can reduce the number of threads running in BOSH, or increase the value of  aws.max_retries  to let the AWS client library perform retries in a exponential backoff. Note that the more retries, the longer will take to fail.   Next:  Using IAM instance profiles",
            "title": "Errors "
        },
        {
            "location": "/aws-iam-users/",
            "text": "Creating new IAM user \n\u00b6\n\n\n\n\n\n\nLog into the AWS console: \nhttps://console.aws.amazon.com/console/home\n.\n\n\n\n\n\n\n\n\nClick your account name and select \nSecurity Credentials\n.\n\n\n\n\n\n\n\n\nIf the AWS IAM confirmation box is presented, click \nGet Started with IAM Users\n to go to IAM Users management page. Alternatively go directly to \nusers list\n.\n\n\n\n\n\n\n\n\nClick \nCreate New Users\n button.\n\n\n\n\n\n\n\n\nEnter a descriptive name for a new user, make sure that access keys will be generated for each user and click \nCreate\n button.\n\n\n\n\n\n\n\n\nRecord \nAccess Key ID\n and \nSecret Access Key\n for later use. Click \nClose\n link to get back to the list of users.\n\n\n\n\n\n\n\n\nClick on a new user from the list of users.\n\n\n\n\n\n\nClick on \nInline Policies\n panel and choose to create a new inline policy.\n\n\n\n\n\n\n\n\nChoose \nCustom Policy\n.\n\n\n\n\n\n\nAdd a policy configuration for the chosen user and click \nApply Policy\n.\n\n\n\n\nThe CPI provides a \nsample policy\n which supports all functionality provided by the CPI. Please review the policy to further limit unnecessary access for your environment.",
            "title": "IAM Users"
        },
        {
            "location": "/aws-iam-users/#creating-new-iam-user",
            "text": "Log into the AWS console:  https://console.aws.amazon.com/console/home .     Click your account name and select  Security Credentials .     If the AWS IAM confirmation box is presented, click  Get Started with IAM Users  to go to IAM Users management page. Alternatively go directly to  users list .     Click  Create New Users  button.     Enter a descriptive name for a new user, make sure that access keys will be generated for each user and click  Create  button.     Record  Access Key ID  and  Secret Access Key  for later use. Click  Close  link to get back to the list of users.     Click on a new user from the list of users.    Click on  Inline Policies  panel and choose to create a new inline policy.     Choose  Custom Policy .    Add a policy configuration for the chosen user and click  Apply Policy .   The CPI provides a  sample policy  which supports all functionality provided by the CPI. Please review the policy to further limit unnecessary access for your environment.",
            "title": "Creating new IAM user "
        },
        {
            "location": "/aws-iam-instance-profiles/",
            "text": "Note: This feature is available with bosh-release v208+ (1.3087.0) colocated with bosh-aws-cpi v31+.\n\n\n\nThis topic describes how to configure BOSH to use \nAWS IAM instance profiles\n to avoid hard coding specific AWS credentials.\n\n\nYou may have to create one or more IAM instance profiles to limit access to AWS resources depending on how BOSH is configured and what software you are planning to deploy. Below are a few example configurations.\n\n\nNote: Each IAM role when created has an associated IAM instance profile with the same name. There is no need to create instance profiles explicitly.\n\n\n\nExample A: AWS CPI and Director configured with default blobstore \n\u00b6\n\n\n\n\n\n\nCreate \ndirector\n IAM role using the same policy as your existing user (see \nCreating IAM Users\n).\n\n\n\n\n\n\nChange deployment manifest for the Director to configure AWS CPI to use \ndirector\n IAM profile:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n \n{\n \n...\n \n}\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm3.xlarge\n\n    \niam_instance_profile\n:\n \ndirector\n\n\n\n\n\n\n\n\n\nInstead of providing \naccess_key_id\n and \nsecret_access_key\n, configure AWS CPI in the deployment manifest to use IAM instance profile as a credentials source:\n\n\nproperties\n:\n\n  \naws\n:\n \n&aws\n\n    \ncredentials_source\n:\n \nenv_or_profile\n\n    \n# access_key_id and secret_access_key are not provided\n\n    \n# ...\n\n\n\n\n\nNote: To use IAM instance profile as a credentials source when using \nbosh create-env\n command, you have to run the command from a \njumpbox\n, an existing AWS instance with IAM instance profile (you can reuse \ndirector\n IAM role). Alternatively if you are deploying the Director VM from outside of the AWS, you can use hard coded credentials with \nbosh create-env\n command and have the AWS CPI on the Director VM use IAM instance profile as a credentials source.\n\n\nNote: Even though value specified is \nenv_or_profile\n, \nbosh create-env\n command or the Director do not currently take advantage of the environment variables, only instance the profile, hence to take advantage of this feature you have to run on an AWS instance.\n\n\n\n\n\n\n\n\nExample B: AWS CPI and Director configured with an S3 blobstore \n\u00b6\n\n\nThis configuration is similar to the previous one except that it's used when the Director and the Agents use S3 as their \nblobstore\n instead of an internal blobstore provided by the bosh release.\n\n\n\n\n\n\nCreate \ndeployed-vm\n IAM role which allows \ns3:*\n actions for a chosen S3 bucket. This IAM role will be used by default for all VMs created by the Director.\n\n\n{\n\n  \n\"Version\"\n:\n \n\"2012-10-17\"\n,\n\n  \n\"Statement\"\n:\n \n[{\n\n    \n\"Effect\"\n:\n \n\"Allow\"\n,\n\n    \n\"Action\"\n:\n \n[\n \n\"s3:*\"\n \n],\n\n    \n\"Resource\"\n:\n \n[\n\n      \n\"arn:aws:s3:::<bosh_bucket_name>\"\n,\n\n      \n\"arn:aws:s3:::<bosh_bucket_name>/*\"\n,\n\n    \n]\n\n  \n}]\n\n\n}\n\n\n\n\n\n\n\n\n\nCreate \ndirector\n IAM role using the same policy as your existing user (see \nCreating IAM Users\n) with the following additional policy.\n\n\n{\n\n  \n\"Version\"\n:\n \n\"2012-10-17\"\n,\n\n  \n\"Statement\"\n:\n \n[{\n\n    \n\"Effect\"\n:\n\"Allow\"\n,\n\n    \n\"Action\"\n:\n\"iam:PassRole\"\n,\n\n    \n\"Resource\"\n:\n\"arn:aws:iam::<accound_id>:role/deployed-vm\"\n\n  \n},{\n\n    \n\"Effect\"\n:\n \n\"Allow\"\n,\n\n    \n\"Action\"\n:\n \n[\n \n\"s3:*\"\n \n],\n\n    \n\"Resource\"\n:\n \n[\n\n      \n\"arn:aws:s3:::<bosh_bucket_name>\"\n,\n\n      \n\"arn:aws:s3:::<bosh_bucket_name>/*\"\n,\n\n    \n]\n\n  \n}]\n\n\n}\n\n\n\n\n\n\n\n\n\nIn addition to configuring AWS CPI in the deployment manifest to use IAM instance profile as a credentials source, also set default IAM instance profile for all future deployed VMs:\n\n\nproperties\n:\n\n  \naws\n:\n \n&aws\n\n    \ncredentials_source\n:\n \nenv_or_profile\n\n    \ndefault_iam_instance_profile\n:\n \ndeployed-vm\n\n    \n# ...\n\n\n\n\n\nNote: \niam_instance_profile\n key in resource pool's cloud_properties takes precedence over the default IAM instance profile, so that specific VMs can have greater access to the AWS resources.\n\n\n\n\n\n\n\n\nErrors \n\u00b6\n\n\nYou are not authorized to perform this operation. Encoded authorization failure message: vHU-KncL6Yo4pG5J9p...\n\n\n\n\nUse \naws sts decode-authorization-message\n command to decode message included in the error. For example:\n\n\n$ aws sts decode-authorization-message --encoded-message vHU-KncL6Yo4pG5J9p... \n|\n jq.DecodedMessage\n\n{\n\n  \n\"allowed\"\n: false,\n  \n\"explicitDeny\"\n: false,\n  \n\"matchedStatements\"\n: \n{\n\n    \n\"items\"\n: \n[]\n\n  \n}\n,\n  \n\"failures\"\n: \n{\n\n    \n\"items\"\n: \n[]\n\n  \n}\n,\n  \n\"context\"\n: \n{\n\n    \n\"principal\"\n: \n{\n\n      \n\"id\"\n: \n\"AROxxx:i-56a18483\"\n,\n      \n\"arn\"\n: \n\"arn:aws:sts::xxx:assumed-role/director/i-56a18483\"\n\n    \n}\n,\n    \n\"action\"\n: \n\"iam:PassRole\"\n,\n    \n\"resource\"\n: \n\"arn:aws:iam::xxx:role/deployed-vm\"\n,\n    \n\"conditions\"\n: \n{\n\n      \n\"items\"\n: \n[]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nDecoded message above indicates that \niam\n:\nPassRole\n action needs to be added to the \ndirector\n IAM role so that the AWS CPI can create VMs with \ndeployed-vm\n IAM role.\n\n\n\n\nNext: \nUsing instance storage\n\n\nPrevious: \nAWS",
            "title": "IAM Profiles"
        },
        {
            "location": "/aws-iam-instance-profiles/#example-a-aws-cpi-and-director-configured-with-default-blobstore",
            "text": "Create  director  IAM role using the same policy as your existing user (see  Creating IAM Users ).    Change deployment manifest for the Director to configure AWS CPI to use  director  IAM profile:  resource_pools :  -   name :   default \n   network :   default \n   stemcell :   {   ...   } \n   cloud_properties : \n     instance_type :   m3.xlarge \n     iam_instance_profile :   director     Instead of providing  access_key_id  and  secret_access_key , configure AWS CPI in the deployment manifest to use IAM instance profile as a credentials source:  properties : \n   aws :   &aws \n     credentials_source :   env_or_profile \n     # access_key_id and secret_access_key are not provided \n     # ...   Note: To use IAM instance profile as a credentials source when using  bosh create-env  command, you have to run the command from a  jumpbox , an existing AWS instance with IAM instance profile (you can reuse  director  IAM role). Alternatively if you are deploying the Director VM from outside of the AWS, you can use hard coded credentials with  bosh create-env  command and have the AWS CPI on the Director VM use IAM instance profile as a credentials source.  Note: Even though value specified is  env_or_profile ,  bosh create-env  command or the Director do not currently take advantage of the environment variables, only instance the profile, hence to take advantage of this feature you have to run on an AWS instance.",
            "title": "Example A: AWS CPI and Director configured with default blobstore "
        },
        {
            "location": "/aws-iam-instance-profiles/#example-b-aws-cpi-and-director-configured-with-an-s3-blobstore",
            "text": "This configuration is similar to the previous one except that it's used when the Director and the Agents use S3 as their  blobstore  instead of an internal blobstore provided by the bosh release.    Create  deployed-vm  IAM role which allows  s3:*  actions for a chosen S3 bucket. This IAM role will be used by default for all VMs created by the Director.  { \n   \"Version\" :   \"2012-10-17\" , \n   \"Statement\" :   [{ \n     \"Effect\" :   \"Allow\" , \n     \"Action\" :   [   \"s3:*\"   ], \n     \"Resource\" :   [ \n       \"arn:aws:s3:::<bosh_bucket_name>\" , \n       \"arn:aws:s3:::<bosh_bucket_name>/*\" , \n     ] \n   }]  }     Create  director  IAM role using the same policy as your existing user (see  Creating IAM Users ) with the following additional policy.  { \n   \"Version\" :   \"2012-10-17\" , \n   \"Statement\" :   [{ \n     \"Effect\" : \"Allow\" , \n     \"Action\" : \"iam:PassRole\" , \n     \"Resource\" : \"arn:aws:iam::<accound_id>:role/deployed-vm\" \n   },{ \n     \"Effect\" :   \"Allow\" , \n     \"Action\" :   [   \"s3:*\"   ], \n     \"Resource\" :   [ \n       \"arn:aws:s3:::<bosh_bucket_name>\" , \n       \"arn:aws:s3:::<bosh_bucket_name>/*\" , \n     ] \n   }]  }     In addition to configuring AWS CPI in the deployment manifest to use IAM instance profile as a credentials source, also set default IAM instance profile for all future deployed VMs:  properties : \n   aws :   &aws \n     credentials_source :   env_or_profile \n     default_iam_instance_profile :   deployed-vm \n     # ...   Note:  iam_instance_profile  key in resource pool's cloud_properties takes precedence over the default IAM instance profile, so that specific VMs can have greater access to the AWS resources.",
            "title": "Example B: AWS CPI and Director configured with an S3 blobstore "
        },
        {
            "location": "/aws-iam-instance-profiles/#errors",
            "text": "You are not authorized to perform this operation. Encoded authorization failure message: vHU-KncL6Yo4pG5J9p...  Use  aws sts decode-authorization-message  command to decode message included in the error. For example:  $ aws sts decode-authorization-message --encoded-message vHU-KncL6Yo4pG5J9p...  |  jq.DecodedMessage { \n   \"allowed\" : false,\n   \"explicitDeny\" : false,\n   \"matchedStatements\" :  { \n     \"items\" :  [] \n   } ,\n   \"failures\" :  { \n     \"items\" :  [] \n   } ,\n   \"context\" :  { \n     \"principal\" :  { \n       \"id\" :  \"AROxxx:i-56a18483\" ,\n       \"arn\" :  \"arn:aws:sts::xxx:assumed-role/director/i-56a18483\" \n     } ,\n     \"action\" :  \"iam:PassRole\" ,\n     \"resource\" :  \"arn:aws:iam::xxx:role/deployed-vm\" ,\n     \"conditions\" :  { \n       \"items\" :  [] \n     } \n   }  }   Decoded message above indicates that  iam : PassRole  action needs to be added to the  director  IAM role so that the AWS CPI can create VMs with  deployed-vm  IAM role.   Next:  Using instance storage  Previous:  AWS",
            "title": "Errors "
        },
        {
            "location": "/aws-instance-storage/",
            "text": "Note: This feature is available with bosh-aws-cpi v32+ and only for releases deployed with ? stemcells.\n\n\n\nCertain \ninstance types\n have access to instance storage. All BOSH managed VMs have to store some ephemeral data such as release jobs, packages, logs and other scratch data. First instance storage disk is used if possible; otherwise, a separate EBS volume is created as an ephemeral disk.\n\n\nApplications may need access to all instance storage disks. In that case separate EBS volume will always be created to store ephemeral data. To enable access to all instance storage disks add \nraw_instance_storage\n:\n \ntrue\n:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-aws-xen-hvm-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nd2.2xlarge\n\n    \nraw_instance_storage\n:\n \ntrue\n\n\n\n\n\nWith multiple disks attached, the Agent partitions and labels instance storage disks with label \nraw-ephemeral-*\n so that release jobs can easily find and use them:\n\n\n$ ls -la /dev/disk/by-partlabel/raw-ephemeral-*\nlrwxrwxrwx \n1\n root root \n12\n Oct  \n5\n \n03\n:09 /dev/disk/by-partlabel/raw-ephemeral-0 -> ../../xvdba1\nlrwxrwxrwx \n1\n root root \n12\n Oct  \n5\n \n03\n:09 /dev/disk/by-partlabel/raw-ephemeral-1 -> ../../xvdbb1\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nUsing IAM instance profiles",
            "title": "Using Instance Storage"
        },
        {
            "location": "/google-cpi/",
            "text": "This topic describes cloud properties for different resources created by the Google CPI.\n\n\nAZs \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nzone\n [String, required]: \nZone\n to use for creating VMs. Example: \nus-central1-f\n.\n\n\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \nzone\n:\n \nus-central1-f\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by dynamic network or manual network subnet:\n\n\n\n\nnetwork_name\n (String, optional) - The name of the \nGoogle Compute Engine Network\n the CPI will use when creating the instance (if not set, by default it will use the \ndefault\n network). Example: \ncf\n.\n\n\nxpn_host_project_id\n (String, optional) - The \nproject id\n that owns the network resource to support \nShared VPC Networks (XPN)\n (if not set, it will default to the project hosting the compute resources). Example: \nmy-other-project\n.\n\n\nsubnetwork_name\n (String, optional) - The name of the \nGoogle Compute Engine Subnet Network\n the CPI will use when creating the instance. If the network is in legacy mode, do not provide this property. If the network is in auto subnet mode, providing the subnetwork is optional. If the network is in custom subnet mode, then this field is required. Example: \ncf-east\n.\n\n\nephemeral_external_ip\n (Boolean, optional) - If instances must have an \nephemeral external IP\n (\nfalse\n by default). Can be overridden in resource_pools. Example: \nfalse\n.\n\n\nip_forwarding\n (Boolean, optional) - If instances must have \nIP forwarding\n enabled (\nfalse\n by default). Can be overridden in resource_pools. Example: \nfalse\n.\n\n\ntags\n (Array<String>, optional) - A list of \ntags\n to apply to the instances, useful if you want to apply firewall or routes rules based on tags. Will be merged with tags in resource_pools. Example: \n[\"foo\",\"bar\"]\n.\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ndns\n:\n \n[\n8.8.8.8\n,\n \n8.8.4.4\n]\n\n    \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n    \ncloud_properties\n:\n\n      \nnetwork_name\n:\n \ncf\n\n      \nephemeral_external_ip\n:\n \ntrue\n\n      \ntags\n:\n \n[\ninternal\n,\n \nconcourse\n]\n\n\n\n\n\nExample of dynamic network spanning two zones:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \nsubnets\n:\n\n  \n-\n \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n    \ndns\n:\n \n[\n8.8.8.8\n,\n \n8.8.4.4\n]\n\n    \ncloud_properties\n:\n\n      \nnetwork_name\n:\n \ncf\n\n      \nephemeral_external_ip\n:\n \ntrue\n\n      \ntags\n:\n \n[\ninternal\n,\n \nconcourse\n]\n\n\n\n\n\nExample of vip network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nvip\n\n\n\n\n\n\n\nVM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nmachine_type\n (String, required) - The name of the \nGoogle Compute Engine Machine Type\n the CPI will use when creating the instance (required if not using \ncpu\n and \nram\n). Example: \nn1-standard-1\n.\n\n\ncpu\n (Integer, required) - Number of vCPUs (\nGoogle Compute Engine Custom Machine Types\n) the CPI will use when creating the instance (required if not using \nmachine_type\n). Example: \n2\n.\n\n\nram\n (Integer, required) - Amount of memory (\nGoogle Compute Engine Custom Machine Types\n) the CPI will use when creating the instance (required if not using \nmachine_type\n). Example: \n2048\n.\n\n\nzone\n (String, optional) - The name of the \nGoogle Compute Engine Zone\n where the instance must be created. Example: \nus-west1-a\n.\n\n\nroot_disk_size_gb\n (Integer, optional) - The size (in Gb) of the instance root disk (default is \n10Gb\n). Example: \n10\n.\n\n\nroot_disk_type\n (String, optional) - The name of the \nGoogle Compute Engine Disk Type\n the CPI will use when creating the instance root disk. Example: \npd-standard\n.\n\n\nautomatic_restart\n (Boolean, optional) - If the instances should be \nrestarted automatically\n if they are terminated for non-user-initiated reasons (\nfalse\n by default). Example: \nfalse\n.\n\n\non_host_maintenance\n (String, optional) - \nInstance behavior\n on infrastructure maintenance that may temporarily impact instance performance (supported values are \nMIGRATE\n (default) or \nTERMINATE\n). Example: \nMIGRATE\n.\n\n\npreemptible\n (Boolean, optional) - If the instances should be \npreemptible\n (\nfalse\n by default). Example: \nfalse\n.\n\n\nservice_account\n (String, optional) - The full service account address of the service account to launch the VM with. If a value is provided, \nservice_scopes\n will default to \nhttps://www.googleapis.com/auth/cloud-platform\n unless it is explicitly set. See \nservice account permissions\n for more details. To use the default service account, leave this field empty and specify \nservice_scopes\n. Example: \nservice-account-name@project-name.iam.gserviceaccount.com\n.\n\n\nservice_scopes\n (Array<String>, optional) - If this value is specified and \nservice_account\n is empty, \ndefault\n will be used for \nservice_account\n. This value supports both short (e.g., \ncloud-platform\n) and fully-qualified (e.g., \nhttps://www.googleapis.com/auth/cloud-platform\n formats. See \nAuthorization scope names\n for more details. Example: \ncloud-platform\n.\n\n\ntarget_pool\n (String, optional) - The name of the \nGoogle Compute Engine Target Pool\n the instances should be added to. Example: \ncf-router\n.\n\n\nbackend_service\n (String OR Map<String,String>, optional) - The name of the \nGoogle Compute Engine Backend Service\n the instances should be added to. The backend service must already be configured with an \nInstance Group\n in the same zone as this instance. To set up \nInternal Load Balancing\n use a map and set \nscheme\n to \nINTERNAL\n and \nname\n to the name of the backend service. Example: \ncf-router\n (external), \n{name: \"cf-internal\", scheme: \"INTERNAL\"} (internal)\n.\n\n\nephemeral_external_ip\n (Boolean, optional) - Overrides the equivalent option in the networks section. Example: \nfalse\n.\n\n\nip_forwarding\n (Boolean, optional) - Overrides the equivalent option in the networks section. Example: \nfalse\n.\n\n\ntags\n (Array<String>, optional) - Merged with tags from the networks section. Example: \n[\"foo\",\"bar\"]\n.\n\n\nlabels\n (Map<String,String>, optional) - A dictionary of (key,value) labels applied to the VM. Example: \n{\"foo\":\"bar\"}\n.\n\n\n\n\nExample of an \nn1-standard-2\n VM:\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nn1-standard-2\n\n    \nroot_disk_size_gb\n:\n \n20\n\n    \nroot_disk_type\n:\n \npd-ssd\n\n    \nservice_scopes\n:\n\n    \n-\n \ncompute.readonly\n\n    \n-\n \ndevstorage.read_write\n\n\n\n\n\nExample of an \nINTERNAL\n backend service:\n\n\nvm_extensions\n:\n\n\n-\n \nname\n:\n \nbackend-pool\n\n  \ncloud_properties\n:\n\n    \nephemeral_external_ip\n:\n \ntrue\n\n    \nbackend_service\n:\n\n      \nname\n:\n \nname-of-backend-service\n\n      \nscheme\n:\n \nINTERNAL\n\n\n\n\n\nExample of an \nEXTERNAL\n backend service:\n\n\nvm_extensions\n:\n\n\n-\n \nname\n:\n \nbackend-pool\n\n  \ncloud_properties\n:\n\n    \nbackend_service\n:\n\n      \nname\n:\n \nname-of-backend-service\n\n\n\nThe above backend-service cloud configuration examples are referenced within the deployment manifest as such:\n\n\ninstance_groups\n:\n\n\n-\n \nname\n:\n \nproxy\n\n  \ninstances\n:\n \n2\n\n  \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n  \nnetworks\n:\n \n[{\nname\n:\n \ndefault\n}]\n\n  \nvm_type\n:\n \ndefault\n\n  \nstemcell\n:\n \ndefault\n\n  \njobs\n:\n\n  \n-\n \nname\n:\n \nproxy\n\n    \nrelease\n:\n \ndefault\n\n  \nvm_extensions\n:\n \n[\nbackend-pool\n]\n\n\n\n\n\n\n\nDisk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ntype\n [String, optional]: Type of the \ndisk\n: \npd-standard\n, \npd-ssd\n. Defaults to \npd-standard\n.\n\n\n\n\nPersistent disks are created in the zone of a VM that disk will be attached.\n\n\nExample of 10GB disk:\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nThe CPI can only talk to a single Google Compute Engine region.\n\n\nSee \nall configuration options\n.\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n \n{\nzone\n:\n \nus-central1-f\n}\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n \n{\nzone\n:\n \nus-central1-a\n}\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \nmachine_type\n:\n \nn1-standard-4\n\n    \nroot_disk_size_gb\n:\n \n20\n\n    \nroot_disk_type\n:\n \npd-ssd\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n3000\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n   \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ndns\n:\n     \n[\n8.8.8.8\n,\n \n8.8.4.4\n]\n\n    \nazs\n:\n     \n[\nz1\n,\n \nz2\n]\n\n    \ncloud_properties\n:\n\n      \nnetwork_name\n:\n \ncf\n\n      \nephemeral_external_ip\n:\n \ntrue\n\n      \ntags\n:\n \n[\ninternal\n,\n \nconcourse\n]\n\n\n-\n \nname\n:\n \nvip\n\n  \ntype\n:\n \nvip\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n3\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/google-cpi/#azs",
            "text": "Schema for  cloud_properties  section:   zone  [String, required]:  Zone  to use for creating VMs. Example:  us-central1-f .   Example:  azs :  -   name :   z1 \n   cloud_properties : \n     zone :   us-central1-f",
            "title": "AZs "
        },
        {
            "location": "/google-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by dynamic network or manual network subnet:   network_name  (String, optional) - The name of the  Google Compute Engine Network  the CPI will use when creating the instance (if not set, by default it will use the  default  network). Example:  cf .  xpn_host_project_id  (String, optional) - The  project id  that owns the network resource to support  Shared VPC Networks (XPN)  (if not set, it will default to the project hosting the compute resources). Example:  my-other-project .  subnetwork_name  (String, optional) - The name of the  Google Compute Engine Subnet Network  the CPI will use when creating the instance. If the network is in legacy mode, do not provide this property. If the network is in auto subnet mode, providing the subnetwork is optional. If the network is in custom subnet mode, then this field is required. Example:  cf-east .  ephemeral_external_ip  (Boolean, optional) - If instances must have an  ephemeral external IP  ( false  by default). Can be overridden in resource_pools. Example:  false .  ip_forwarding  (Boolean, optional) - If instances must have  IP forwarding  enabled ( false  by default). Can be overridden in resource_pools. Example:  false .  tags  (Array<String>, optional) - A list of  tags  to apply to the instances, useful if you want to apply firewall or routes rules based on tags. Will be merged with tags in resource_pools. Example:  [\"foo\",\"bar\"] .   Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     dns :   [ 8.8.8.8 ,   8.8.4.4 ] \n     azs :   [ z1 ,   z2 ] \n     cloud_properties : \n       network_name :   cf \n       ephemeral_external_ip :   true \n       tags :   [ internal ,   concourse ]   Example of dynamic network spanning two zones:  networks :  -   name :   default \n   type :   dynamic \n   subnets : \n   -   azs :   [ z1 ,   z2 ] \n     dns :   [ 8.8.8.8 ,   8.8.4.4 ] \n     cloud_properties : \n       network_name :   cf \n       ephemeral_external_ip :   true \n       tags :   [ internal ,   concourse ]   Example of vip network:  networks :  -   name :   default \n   type :   vip",
            "title": "Networks "
        },
        {
            "location": "/google-cpi/#vm-types",
            "text": "Schema for  cloud_properties  section:   machine_type  (String, required) - The name of the  Google Compute Engine Machine Type  the CPI will use when creating the instance (required if not using  cpu  and  ram ). Example:  n1-standard-1 .  cpu  (Integer, required) - Number of vCPUs ( Google Compute Engine Custom Machine Types ) the CPI will use when creating the instance (required if not using  machine_type ). Example:  2 .  ram  (Integer, required) - Amount of memory ( Google Compute Engine Custom Machine Types ) the CPI will use when creating the instance (required if not using  machine_type ). Example:  2048 .  zone  (String, optional) - The name of the  Google Compute Engine Zone  where the instance must be created. Example:  us-west1-a .  root_disk_size_gb  (Integer, optional) - The size (in Gb) of the instance root disk (default is  10Gb ). Example:  10 .  root_disk_type  (String, optional) - The name of the  Google Compute Engine Disk Type  the CPI will use when creating the instance root disk. Example:  pd-standard .  automatic_restart  (Boolean, optional) - If the instances should be  restarted automatically  if they are terminated for non-user-initiated reasons ( false  by default). Example:  false .  on_host_maintenance  (String, optional) -  Instance behavior  on infrastructure maintenance that may temporarily impact instance performance (supported values are  MIGRATE  (default) or  TERMINATE ). Example:  MIGRATE .  preemptible  (Boolean, optional) - If the instances should be  preemptible  ( false  by default). Example:  false .  service_account  (String, optional) - The full service account address of the service account to launch the VM with. If a value is provided,  service_scopes  will default to  https://www.googleapis.com/auth/cloud-platform  unless it is explicitly set. See  service account permissions  for more details. To use the default service account, leave this field empty and specify  service_scopes . Example:  service-account-name@project-name.iam.gserviceaccount.com .  service_scopes  (Array<String>, optional) - If this value is specified and  service_account  is empty,  default  will be used for  service_account . This value supports both short (e.g.,  cloud-platform ) and fully-qualified (e.g.,  https://www.googleapis.com/auth/cloud-platform  formats. See  Authorization scope names  for more details. Example:  cloud-platform .  target_pool  (String, optional) - The name of the  Google Compute Engine Target Pool  the instances should be added to. Example:  cf-router .  backend_service  (String OR Map<String,String>, optional) - The name of the  Google Compute Engine Backend Service  the instances should be added to. The backend service must already be configured with an  Instance Group  in the same zone as this instance. To set up  Internal Load Balancing  use a map and set  scheme  to  INTERNAL  and  name  to the name of the backend service. Example:  cf-router  (external),  {name: \"cf-internal\", scheme: \"INTERNAL\"} (internal) .  ephemeral_external_ip  (Boolean, optional) - Overrides the equivalent option in the networks section. Example:  false .  ip_forwarding  (Boolean, optional) - Overrides the equivalent option in the networks section. Example:  false .  tags  (Array<String>, optional) - Merged with tags from the networks section. Example:  [\"foo\",\"bar\"] .  labels  (Map<String,String>, optional) - A dictionary of (key,value) labels applied to the VM. Example:  {\"foo\":\"bar\"} .   Example of an  n1-standard-2  VM:  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   n1-standard-2 \n     root_disk_size_gb :   20 \n     root_disk_type :   pd-ssd \n     service_scopes : \n     -   compute.readonly \n     -   devstorage.read_write   Example of an  INTERNAL  backend service:  vm_extensions :  -   name :   backend-pool \n   cloud_properties : \n     ephemeral_external_ip :   true \n     backend_service : \n       name :   name-of-backend-service \n       scheme :   INTERNAL   Example of an  EXTERNAL  backend service:  vm_extensions :  -   name :   backend-pool \n   cloud_properties : \n     backend_service : \n       name :   name-of-backend-service  \nThe above backend-service cloud configuration examples are referenced within the deployment manifest as such:  instance_groups :  -   name :   proxy \n   instances :   2 \n   azs :   [ z1 ,   z2 ] \n   networks :   [{ name :   default }] \n   vm_type :   default \n   stemcell :   default \n   jobs : \n   -   name :   proxy \n     release :   default \n   vm_extensions :   [ backend-pool ]",
            "title": "VM Types "
        },
        {
            "location": "/google-cpi/#disk-types",
            "text": "Schema for  cloud_properties  section:   type  [String, optional]: Type of the  disk :  pd-standard ,  pd-ssd . Defaults to  pd-standard .   Persistent disks are created in the zone of a VM that disk will be attached.  Example of 10GB disk:  disk_types :  -   name :   default \n   disk_size :   10_240",
            "title": "Disk Types "
        },
        {
            "location": "/google-cpi/#global-configuration",
            "text": "The CPI can only talk to a single Google Compute Engine region.  See  all configuration options .",
            "title": "Global Configuration "
        },
        {
            "location": "/google-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1 \n   cloud_properties :   { zone :   us-central1-f }  -   name :   z2 \n   cloud_properties :   { zone :   us-central1-a }  vm_types :  -   name :   default \n   cloud_properties : \n     machine_type :   n1-standard-4 \n     root_disk_size_gb :   20 \n     root_disk_type :   pd-ssd  disk_types :  -   name :   default \n   disk_size :   3000  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :     10.10.0.0/24 \n     gateway :   10.10.0.1 \n     dns :       [ 8.8.8.8 ,   8.8.4.4 ] \n     azs :       [ z1 ,   z2 ] \n     cloud_properties : \n       network_name :   cf \n       ephemeral_external_ip :   true \n       tags :   [ internal ,   concourse ]  -   name :   vip \n   type :   vip  compilation : \n   workers :   3 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   default \n   network :   default    Back to Table of Contents",
            "title": "Example Cloud Config "
        },
        {
            "location": "/google-required-permissions/",
            "text": "This topic describes how to configure BOSH with the minimum set of permissions on Google Cloud Engine.\n\n\nGoogle CPI and Director \n\u00b6\n\n\nThis section will guide you through how to configure the Google Cloud CPI and Director with the default blobstore.\n\n\nNote: If you wish to configure BOSH to use GCS blobstore also follow [these steps](google-required-permissions.md#director-with-gcs-blobstore)\n\n\n\n\n\n\n\nDownload and install the \nGoogle Cloud CLI\n and \nBOSH CLI v2\n.\n\n\n\n\n\n\nCreate a file called \nbosh-director-role.yml\n.\n\n\ntitle\n:\n \nBOSH Director\n\n\nstage\n:\n \nBETA\n\n\ndescription\n:\n \nAllows BOSH Google CPI to perform all BOSH Director actions\n\n\nname\n:\n \nprojects/((project_id))/roles/bosh.director\n\n\nincluded_permissions\n:\n\n\n# addresses\n\n\n-\n \ncompute.addresses.get\n\n\n-\n \ncompute.addresses.list\n\n\n\n# backend services\n\n\n-\n \ncompute.backendServices.get\n\n\n-\n \ncompute.backendServices.list\n\n\n\n# disk types\n\n\n-\n \ncompute.diskTypes.get\n\n\n\n# disks\n\n\n-\n \ncompute.disks.delete\n\n\n-\n \ncompute.disks.list\n\n\n-\n \ncompute.disks.get\n\n\n-\n \ncompute.disks.createSnapshot\n\n\n-\n \ncompute.snapshots.create\n\n\n-\n \ncompute.disks.create\n\n\n-\n \ncompute.images.useReadOnly\n\n\n\n# global operations\n\n\n-\n \ncompute.globalOperations.get\n\n\n# images\n\n\n-\n \ncompute.images.delete\n\n\n-\n \ncompute.images.get\n\n\n-\n \ncompute.images.create\n\n\n\n# instance groups\n\n\n-\n \ncompute.instanceGroups.get\n\n\n-\n \ncompute.instanceGroups.list\n\n\n-\n \ncompute.instanceGroups.update\n\n\n\n# instances\n\n\n-\n \ncompute.instances.setMetadata\n\n\n-\n \ncompute.instances.setLabels\n\n\n-\n \ncompute.instances.setTags\n\n\n-\n \ncompute.instances.reset\n\n\n-\n \ncompute.instances.start\n\n\n-\n \ncompute.instances.list\n\n\n-\n \ncompute.instances.get\n\n\n-\n \ncompute.instances.delete\n\n\n-\n \ncompute.instances.create\n\n\n-\n \ncompute.subnetworks.use\n\n\n-\n \ncompute.subnetworks.useExternalIp\n\n\n-\n \ncompute.instances.detachDisk\n\n\n-\n \ncompute.instances.attachDisk\n\n\n-\n \ncompute.disks.use\n\n\n-\n \ncompute.instances.deleteAccessConfig\n\n\n-\n \ncompute.instances.addAccessConfig\n\n\n-\n \ncompute.addresses.use\n\n\n\n# machine type\n\n\n-\n \ncompute.machineTypes.get\n\n\n\n# region operations\n\n\n-\n \ncompute.regionOperations.get\n\n\n\n# zone operations\n\n\n-\n \ncompute.zoneOperations.get\n\n\n\n# networks\n\n\n-\n \ncompute.networks.get\n\n\n\n# subnetworks\n\n\n-\n \ncompute.subnetworks.get\n\n\n\n# snapshots\n\n\n-\n \ncompute.snapshots.delete\n\n\n-\n \ncompute.snapshots.get\n\n\n\n# target pool\n\n\n-\n \ncompute.targetPools.list\n\n\n-\n \ncompute.targetPools.get\n\n\n-\n \ncompute.targetPools.addInstance\n\n\n-\n \ncompute.targetPools.removeInstance\n\n\n-\n \ncompute.instances.use\n\n\n\n# Storage services - used when uploading heavy stemcells\n\n\n-\n \nstorage.buckets.create\n\n\n-\n \nstorage.objects.create\n\n\n\n\n\n\n\n\n\nCreate the BOSH Director role:\n\n\ngcloud beta iam roles --project <project-id> create bosh.director \n\\\n\n  --file <\n(\n bosh int -v \nproject_id\n=\n<project-id> bosh-director-role.yml \n)\n\n\n\n\n\n\n\n\n\nOn Google Cloud navigate go to \nIAM & admin\n > \nService accounts\n and click on \nCREATE SERVICE ACCOUNT\n.\n   Give your service account a name, check \nFurnish a new private key\n and select the aforementioned role.\n\n\n\n\n\n\n\n\nMinimum permissions for GCS blobstore\n\u00b6\n\n\nIn addition to configuring the Google CPI and Director with the previous permissions, in order to allow them to use GCS blobstore you need to grant additional permissions.\n\n\nNote: We highly recommend creating separate service accounts.\n\n\n\nThis configuration is similar to the previous one except that it's used when the Director and the Agents use GCS as their \nblobstore\n instead of an internal blobstore provided by the bosh release.\n\n\n\n\n\n\nDownload and install the \nGoogle Cloud CLI\n and \nBOSH CLI v2\n.\n\n\n\n\n\n\nCreate a file called \ndirector-blobstore-role.yml\n.\n\n\ntitle\n:\n \nBOSH Director GCS Blobstore\n\n\nstage\n:\n \nBETA\n\n\ndescription\n:\n \nAllows BOSH Director GCS Blobstore client to perform necessary operations to the blobstore\n\n\nname\n:\n \nprojects/((project_id))/roles/blobstore.director\n\n\nincluded_permissions\n:\n\n\n-\n \nstorage.buckets.get\n\n\n-\n \nstorage.objects.get\n\n\n-\n \nstorage.objects.create\n\n\n-\n \nstorage.objects.delete\n\n\n\n\n\nand \nagent-blobstore-role.yml\n.\n\n\ntitle\n:\n \nBOSH Agent GCS Blobstore\n\n\nstage\n:\n \nBETA\n\n\ndescription\n:\n \nAllows BOSH Agent GCS Blobstore client to perform necessary operations to the blobstore\n\n\nname\n:\n \nprojects/((project_id))/roles/blobstore.agent\n\n\nincluded_permissions\n:\n\n\n-\n \nstorage.buckets.get\n\n\n-\n \nstorage.objects.get\n\n\n-\n \nstorage.objects.create\n\n\n\n\n\nNote: The agent does not need to delete files from the blobstore\n\n\n\n\n\n\nConfigure roles.\n\n\ngcloud beta iam roles --project <project-id> create blobstore.director \n\\\n\n  --file <\n(\n bosh int -v \nproject_id\n=\n<project-id> director-blobstore-role.yml \n)\n\n\ngcloud beta iam roles --project <project-id> create blobstore.agent \n\\\n\n  --file <\n(\n bosh int -v \nproject_id\n=\n<project-id> agent-blobstore-role.yml \n)\n\n\n\n\n\n\n\n\n\nOn Google Cloud navigate go to \nIAM & admin\n > \nService accounts\n and click on \nCREATE SERVICE ACCOUNT\n.\n   Give your service account a name, check \nFurnish a new private key\n and select the aforementioned roles.\n\n\n\n\n\n\n\n\n\n\nConfigure \nGCS Blobstore\n\n\n\n\n\n\nNext: \nRackHD\n\n\nPrevious: \nGoogle Compute Engine",
            "title": "Required Permissions"
        },
        {
            "location": "/google-required-permissions/#google-cpi-and-director",
            "text": "This section will guide you through how to configure the Google Cloud CPI and Director with the default blobstore.  Note: If you wish to configure BOSH to use GCS blobstore also follow [these steps](google-required-permissions.md#director-with-gcs-blobstore)    Download and install the  Google Cloud CLI  and  BOSH CLI v2 .    Create a file called  bosh-director-role.yml .  title :   BOSH Director  stage :   BETA  description :   Allows BOSH Google CPI to perform all BOSH Director actions  name :   projects/((project_id))/roles/bosh.director  included_permissions :  # addresses  -   compute.addresses.get  -   compute.addresses.list  # backend services  -   compute.backendServices.get  -   compute.backendServices.list  # disk types  -   compute.diskTypes.get  # disks  -   compute.disks.delete  -   compute.disks.list  -   compute.disks.get  -   compute.disks.createSnapshot  -   compute.snapshots.create  -   compute.disks.create  -   compute.images.useReadOnly  # global operations  -   compute.globalOperations.get  # images  -   compute.images.delete  -   compute.images.get  -   compute.images.create  # instance groups  -   compute.instanceGroups.get  -   compute.instanceGroups.list  -   compute.instanceGroups.update  # instances  -   compute.instances.setMetadata  -   compute.instances.setLabels  -   compute.instances.setTags  -   compute.instances.reset  -   compute.instances.start  -   compute.instances.list  -   compute.instances.get  -   compute.instances.delete  -   compute.instances.create  -   compute.subnetworks.use  -   compute.subnetworks.useExternalIp  -   compute.instances.detachDisk  -   compute.instances.attachDisk  -   compute.disks.use  -   compute.instances.deleteAccessConfig  -   compute.instances.addAccessConfig  -   compute.addresses.use  # machine type  -   compute.machineTypes.get  # region operations  -   compute.regionOperations.get  # zone operations  -   compute.zoneOperations.get  # networks  -   compute.networks.get  # subnetworks  -   compute.subnetworks.get  # snapshots  -   compute.snapshots.delete  -   compute.snapshots.get  # target pool  -   compute.targetPools.list  -   compute.targetPools.get  -   compute.targetPools.addInstance  -   compute.targetPools.removeInstance  -   compute.instances.use  # Storage services - used when uploading heavy stemcells  -   storage.buckets.create  -   storage.objects.create     Create the BOSH Director role:  gcloud beta iam roles --project <project-id> create bosh.director  \\ \n  --file < (  bosh int -v  project_id = <project-id> bosh-director-role.yml  )     On Google Cloud navigate go to  IAM & admin  >  Service accounts  and click on  CREATE SERVICE ACCOUNT .\n   Give your service account a name, check  Furnish a new private key  and select the aforementioned role.",
            "title": "Google CPI and Director "
        },
        {
            "location": "/azure-cpi/",
            "text": "This topic describes cloud properties for different resources created by the Azure CPI.\n\n\nAZs \n\u00b6\n\n\n\n\navailability_zone\n [String, optional]: Availability zone to use for creating instances (available in v33+). Possible values: \n'1'\n, \n'2'\n, \n'3'\n. Read this \ndocument\n to get regions and VM sizes on Azure that support availability zones. \nMore details about availability zone\n.\n\n\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \n'1'\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nDynamic Network or Manual Network\n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nresource_group_name\n [String, optional]: Name of a resource group. If it is set, Azure CPI will search the virtual network and security group in this resource group. Otherwise, Azure CPI will search the virtual network and security group in \nresource_group_name\n in the global CPI settings.\n\n\nvirtual_network_name\n [String, required]: Name of a virtual network. Example: \nboshnet\n.\n\n\nsubnet_name\n [String, required]: Name of a subnet within virtual network.\n\n\nsecurity_group\n [String, optional]: The \nsecurity group\n to apply to network interfaces of all VMs placed in this network. The security group of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority); if it is not specified, the default security group (specified by \ndefault_security_group\n in the global CPI settings) will be used.\n\n\napplication_security_groups\n [Array, optional]: The \napplication security group\n to apply to network interfaces of all VMs placed in this network. The application security groups of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority).\n\n\nThis property is supported in v31+.\n\n\nYou must reference the \ndocument\n to register your subscription with this new feature.\n\n\n\n\n\n\n\n\nSee \nhow to create a virtual network and subnets\n.\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ncloud_properties\n:\n\n      \nresource_group_name\n:\n \nmy-resource-group-name\n\n      \nvirtual_network_name\n:\n \nmy-vnet-name\n\n      \nsubnet_name\n:\n \nmy-subnet-name\n\n      \nsecurity_group\n:\n \nmy-security-group-name\n\n      \napplication_security_groups\n:\n \n[\n\"my-application-security-group-name-1\"\n,\n \n\"my-application-security-group-name-2\"\n]\n\n\n\n\n\nVip Network\n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nresource_group_name\n [String, optional]: Name of a resource group. If it is set, Azure CPI will search the public IP in this resource group. Otherwise, Azure CPI will search the public IP in \nresource_group_name\n in the global CPI settings.\n\n\n\n\nSee \nhow to create public IP\n to use with vip networks.\n\n\nExample of vip network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \npublic\n\n  \ntype\n:\n \nvip\n\n  \ncloud_properties\n:\n\n    \nresource_group_name\n:\n \nmy-resource-group\n\n\n\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ninstance_type\n [String, required]: Type of the \ninstance\n. Example: \nStandard_A2\n. \nBasic Tier Virtual Machines\n should not be used if you need to bind the instance to Azure Load Balancer (ALB), because Basic Tier VM doesn't support ALB.\n\n\nroot_disk\n [Hash, optional]: OS disk of custom size.\n\n\nsize\n [Integer, optional]: Specifies the disk size in MiB.\n\n\nThe size must be greater than 3 * 1024 and less than the max disk size for \nunmanaged\n or \nmanaged\n disk. Please always use \nN * 1024\n as the size because Azure always uses GiB but not MiB.\n\n\nIt has a default value \n30 * 1024\n only when ephemeral_disk.use_root_disk is set to true.\n\n\n\n\n\n\ncaching\n [String, optional]: Type of the disk caching of the VMs' OS disks. It can be either \nNone\n, \nReadOnly\n or \nReadWrite\n. Default is \nReadWrite\n.\n\n\n\n\nephemeral_disk\n [Hash, optional]: Ephemeral disk to apply for all VMs that are in this resource pool. By default a data disk with the default size as below will be created as the ephemeral disk.\n\n\n\n\nuse_root_disk\n [Boolean, optional]: Enable to use OS disk to store the ephemeral data. The default value is false. When it is true, ephemeral_disk.size will not be used.\n\n\nsize\n [Integer, optional]: Specifies the disk size in MiB. If this is not set, the default size as below will be used. The size of the ephemeral disk for the BOSH VM should be larger than or equal to \n30*1024\n MiB. Please always use \nN * 1024\n as the size because Azure always uses GiB not MiB.\n\n\nIf the Azure temporary disk size for the instance type is less than \n30*1024\n MiB, the default size is \n30*1024\n MiB because the space may not be enough.\n\n\nIf the Azure temporary disk size for the instance type is larger than \n1000*1024\n MiB, the default size is \n1000*1024\n MiB because it is not expected to use such a large ephemeral disk in CF currently.\n\n\nOtherwise, the Azure temporary disk size will be used as the default size. See more information about \nAzure temporary disk size\n.\n\n\n\n\n\n\n\n\nload_balancer\n [String, optional]: Name of a \nload balancer\n the VMs should belong to. You need to create the load balancer manually before configuring it. Notes:\n\n\n\n\nBasic Tier Virtual Machines\n (Example: \nBasic_A1\n) doesn't support Azure Load Balancer.\n\n\nIf \navailability_zone\n is specified for the VM, \nstandard sku load balancer\n must be used, as \nbasic sku load balancer\n does not work for zone.\n\n\n\n\n\n\napplication_gateway\n [String, optional]: Name of the \napplication gateway\n which the VMs should be associated to.\n\n\nThis property is supported in CPI v28+.\n\n\nYou need to create the application gateway manually before configuring it. Please refer to \nthe guidance\n.\n\n\n\n\n\n\nsecurity_group\n [String, optional]: The \nsecurity group\n to apply to network interfaces of all VMs placed in this resource pool. The security group of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority); if it is not specified, the default security group (specified by \ndefault_security_group\n in the global CPI settings) will be used.\n\n\napplication_security_groups\n [Array, optional]: The \napplication security group\n to apply to network interfaces of all VMs placed in this resource group. The application security groups of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority).\n\n\nThis property is supported in v31+.\n\n\nYou must reference the \ndocument\n to register your subscription with this new feature.\n\n\n\n\n\n\n\n\nassign_dynamic_public_ip\n [Boolean, optional]: Enable to create and assign dynamic public IP to the VM automatically (to solve the \nazure SNAT issue\n). Default value is \nfalse\n. Only the VM without vip will be assigned a dynamic public IP when this value is set to true, and the dynamic public IP will be deleted when the VM is deleted.\n\n\n\n\n\n\navailability_zone\n [String, optional]: Availability zone to use for creating instances (available in v33+). Possible values: \n'1'\n, \n'2'\n, \n'3'\n. Read this \ndocument\n to get regions and VM sizes on Azure that support availability zones. \nMore details about availability zone\n.\n\n\n\n\navailability_set\n [String, optional]: Name of an \navailability set\n to use for VMs. \nMore details\n.\n\n\nIf available set does not exist, it will be automatically created.\n\n\nIf \navailability_set\n is not specified, Azure CPI will search \nenv.bosh.group\n as the name of availability set.\n\n\nbosh release\n v258+ will generate a value for \nenv.bosh.group\n automatically.\n\n\nOn Azure the length of the availability set name must be between 1 and 80 characters. The name got from  \nenv.bosh.group\n may be too long. CPI will truncate the name to the following format \naz-MD5-[LAST-40-CHARACTERS-OF-GROUP]\n if the length is greater than 80.\n\n\nCPI v27+ will delete the empty availability set.\n\n\nOnly one of \navailability_zone\n and \navailability_set\n is allowed to be configured for a VM. If \navailability_zone\n is specified, the VM will be in a zone and not in any availability set.\n\n\n\n\n\n\nplatform_update_domain_count\n [Integer, optional]: The count of \nupdate domain\n in the availability set.\n\n\nFor Azure, the default value is \n5\n.\n\n\nFor Azure Stack, the default value is \n1\n.\n\n\n\n\n\n\n\n\nplatform_fault_domain_count\n [Integer, optional]: The count of \nfault domain\n in the availability set.\n\n\n\n\nFor Azure, the default value of an unmanaged availability set is \n3\n. The default value of a managed availability set is \n2\n, because \nsome regions don't support 3 fault domains for now\n\n\nFor Azure Stack, the default value is \n1\n. Before 1802 update, only \n1\n is allowed. After \n1802 update\n, you can configure up to 3 fault domains.\n\n\n\n\n\n\n\n\nstorage_account_name\n [String, optional]: Storage account for VMs. Valid only when \nuse_managed_disks\n is \nfalse\n. If this is not set, the VMs will be created in the default storage account. See \nthis document\n for more details on why this option exists.\n\n\n\n\nIf you use \nDS-series\n or \nGS-series\n as \ninstance_type\n, you should set this to a premium storage account. See more information about \nAzure premium storage\n. See \navaliable regions\n where you can create premium storage accounts.\n\n\nIf you use a different storage account which must be in the same resource group, please make sure:\n\n\nThe permissions for the container \nstemcell\n in the default storage account is set to \nPublic read access for blobs only\n.\n\n\nA table \nstemcells\n is created in the default storage account.\n\n\nTwo containers \nbosh\n and \nstemcell\n are created in the new storage account.\n\n\nIf this storage account does not exist, it can be created automatically by Azure CPI. But you must specify storage_account_type and make sure:\n\n\nThe name must be \nunique within Azure\n.\n\n\nThe name must be between 3 and 24 characters in length and use numbers and lower-case letters only\n.\n\n\nIf you use a pattern \n*keyword*\n. CPI will filter all storage accounts under the default resource group by the pattern and pick one available storage account to create the VM.\n\n\nThe pattern must start with \n*\n and end with \n*\n.\n\n\nThe keyword must only contain numbers and lower-case letters because of the naming rule of storage account name.\n\n\nThe rule to select an available storage account is to check the number of disks under the container \nbosh\n does not exceed the limitation.\n\n\nThe default number of disks limitation is 30 but you can specify it in \nstorage_account_max_disk_number\n.\n\n\n\n\n\n\nstorage_account_max_disk_number\n [Integer, optional]: Number of disks limitation in a storage account. Valid only when \nuse_managed_disks\n is \nfalse\n. Default value is 30. This will be used only when \nstorage_account_name\n is a pattern.\n\n\nEvery storage account has a limitation to host disks. You may hit the performance issue if you create too many disks in one storage account.\n\n\nThe maximum number of disks of a standard storage account is 40 because the maximum IOPS of a standard storage account is 20,000 and the maximum IOPS of a standard disk is 500.\n\n\nIf you are using premium storage account, Azure maps the disk size (rounded up) to the nearest Premium Storage Disk option (P10, P20 and P30). For example, a disk of size 100 GiB is classified as a P10 option.\n\n\nThe maximum number of disks of a premium storage account is 280 if you are using P10 (128 GiB) as your disk type.\n\n\nThe maximum number of disks of a premium storage account is 70 if you are using P20 (512 GiB) as your disk type.\n\n\nThe maximum number of disks of a premium storage account is 35 if you are using P30 (1024 GiB) as your disk type.\n\n\nstorage_account_max_disk_number\n should be less than the maximum number. Suggest you to use (MAX - 10) as the value because CPI always creates VMs in parallel.\n\n\nPlease see more information about \nazure-subscription-service-limits\n.\n\n\n\n\n\n\nstorage_account_type\n [String, optional]: Storage account type. Valid only when \nuse_managed_disks\n is \nfalse\n. It is required if the storage account does not exist. It can be either \nStandard_LRS\n, \nStandard_ZRS\n, \nStandard_GRS\n, \nStandard_RAGRS\n or \nPremium_LRS\n. You can click \nHERE\n to learn more about the type of Azure storage account.\n\n\n\n\nstorage_account_location\n [String, optional]: Location of the storage account. This configuration is deprecated in CPI v25+: if you specify a storage account which does not exist, CPI (v25+) will create it automatically in the same location as VMs' VNET.\n\n\n\n\n\n\nresource_group_name\n [String, optional]: Name of a resource group (Available in v26+). If it is set, related resources will be created in this resource group; otherwise, they will be created in \nresource_group_name\n specified in the global CPI settings. The resources affected by this property are:\n\n\n\n\nVirtual Machine\n\n\nNetwork Interface Card\n\n\nManaged Disks (including OS disk, ephemeral disk, persistent disk and snapshot)\n\n\nDynamic Public IP for the VM\n\n\nAvailability Set\n\n\n\n\n\n\n\n\nExample of a \nStandard_A2\n instance:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-azure-hyperv-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nStandard_A2\n\n    \navailability_set\n:\n \n<availability-set-name>\n\n    \nroot_disk\n:\n\n      \nsize\n:\n \n30_720\n\n    \nephemeral_disk\n:\n\n      \nuse_root_disk\n:\n \nfalse\n\n      \nsize\n:\n \n30_720\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ncaching\n [String, optional]: Type of the disk caching. It can be either \nNone\n, \nReadOnly\n or \nReadWrite\n. Default is \nNone\n.\n\n\nstorage_account_type\n [String, optional]: Storage account type. Valid only when \nuse_managed_disks\n is \ntrue\n. It can be either \nStandard_LRS\n or \nPremium_LRS\n. You can click \nHERE\n to learn more about the type of Azure storage account.\n\n\n\n\nExample of 10GB disk:\n\n\n\n\ndisk_size\n [Integer, required]: Size of the disk in MiB. On Azure the disk size must be greater than 1 * 1024 and less than the max disk size for \nunmanaged\n or \nmanaged\n disk. Please always use \nN * 1024\n as the size because Azure always uses GiB not MiB.\n\n\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nSchema:\n\n\n\n\nenvironment\n [String, required]: Azure environment name. Possible values are: \nAzureCloud\n, \nAzureChinaCloud\n, \nAzureUSGovernment\n (available in v19+), \nAzureGermanCloud\n (available in v22+) or \nAzureStack\n.\n\n\nlocation\n [String, optional]: Azure region name. Only required when \nvm_resources\n is specified in the deployment manifest. Available in v33+.\n\n\nsubscription_id\n [String, required]: Subscription ID.\n\n\ntenant_id\n [String, required]: Tenant ID of the service principal.\n\n\nclient_id\n [String, required]: Client ID of the service principal.\n\n\nclient_secret\n [String, optional]: Client secret of the service principal.\n\n\ncertificate\n [String, optional]: The certificate for your service principal. Azure CPI v35.0.0+ supports the \nservice principal with a certificate\n. Only one of \nclient_secret\n and \ncertificate\n can be specified.\n\n\nresource_group_name\n [String, required]: Resource group name.\n\n\nstorage_account_name\n [String, optional]: Storage account name. It will be used as a default storage account for VM disks and stemcells. If \nuse_managed_disks\n is \nfalse\n, \nstorage_account_name\n is required. Otherwise, \nstorage_account_name\n is optional.\n\n\nssh_user\n [String, required]: SSH username. Default: \nvcap\n.\n\n\nssh_public_key\n [String, required]: SSH public key.\n\n\ndefault_security_group\n [String, optional]: Name of the default \nsecurity group\n that will be applied to all created VMs. This property is required before v35.0.0, and optional in v35.0.0+.\n\n\nazure_stack\n [Hash, optional]: \nConfigration for AzureStack\n. Available in v23+.\n\n\ndomain\n [String, optional]: The domain for your AzureStack deployment. Default is \nlocal.azurestack.external\n. You can use the default value for \nAzure Stack development kit\n. To get this value for Azure Stack integrated systems, contact your service provider.\n\n\nauthentication\n [String, optional]: The authentication type for your AzureStack deployment. Possible values are: \nAzureAD\n and \nADFS\n. You need to specify \ncertificate\n if you select \nADFS\n, because Azure Stack with ADFS authentication only supports the service principal with a certificate.\n\n\nresource\n [String, optional]: Active Directory Service Endpoint Resource ID, where you can get the token for your AzureStack deployment.\n\n\nendpoint_prefix\n [String, optional]: The endpoint prefix for your AzureStack deployment. Default is \nmanagement\n.\n\n\nskip_ssl_validation\n [Boolean, optional]: Toggles verification of the Azure Resource Manager REST API SSL certificate. Default is \nfalse\n. Deprecated in v35.0.0+.\n\n\nuse_http_to_access_storage_account\n [Boolean, optional]: Flag for using HTTP to access storage account rather than the default HTTPS. Default is \nfalse\n. Deprecated in v35.0.0+.\n\n\nca_cert\n [String, required]: All required custom CA certificates for AzureStack. You can \nexport the Azure Stack CA root certificate\n. Available in v27+.\n\n\nThe property is required for v35.0.0+.\n\n\nFor the versions from v27 to v34, if \nca_cert\n is not provided, the \nskip_ssl_validation\n and \nuse_http_to_access_storage_account\n must be set to \ntrue\n.\n\n\n\n\n\n\n\n\n\n\nparallel_upload_thread_num\n [Integer, optional]: The number of threads to upload stemcells in parallel. The default value is 16.\n\n\ndebug_mode\n [Boolean, optional]: Enable debug mode. The default value is \nfalse\n. When \ndebug_mode\n is \ntrue\n:\n\n\nCPI will log all raw HTTP requests/responses.\n\n\nThe new created VMs (only for VMs in same region with \nstorage_account_name\n specified in \nGlobal Configuration\n) will have \nboot diagnostics\n enabled. Available in v26+.\n\n\n\n\n\n\nuse_managed_disks\n [Boolean, optional]: Enable managed disks. The default value is \nfalse\n. For \nAzureCloud\n, the option is supported in v21+. For \nAzureChinaCloud\n, \nAzureUSGovernment\n, and \nAzureGermanCloud\n, the option is supported in v26+. For \nAzureStack\n, the option is not yet supported.\n\n\npip_idle_timeout_in_minutes\n [Integer, optional]: Set idle timeouts in minutes for dynamic public IPs. It must be in the range [4, 30]. The default value is 4. It is only used when \nassign_dynamic_public_ip\n is set to \ntrue\n in \nresouce_pool\n. Available in V24+.\n\n\nkeep_failed_vms\n [Boolean, optional]: A flag to keep the failed VM. If it's set to \ntrue\n and CPI fails to \nprovision\n the VM, CPI will keep the VM for troubleshooting. The default value is \nfalse\n. Available in v32+. Please note that the option is different from \nkeep_unreachable_vms\n of the \ndirector configuration\n. The latter is to keep the VM whose BOSH agent is unresponsive.\n\n\n\n\nSee \nall configuration options\n.\n\n\nSee \nCreating Azure resources\n page for more details on how to create and configure above resources.\n\n\nExample with hard-coded credentials:\n\n\nproperties\n:\n\n  \nazure\n:\n \n&azure\n\n    \nenvironment\n:\n \nAzureCloud\n\n    \nsubscription_id\n:\n \n3c39a033-c306-4615-a4cb-260418d63879\n\n    \ntenant_id\n:\n \n0412d4fa-43d2-414b-b392-25d5ca46561da\n\n    \nclient_id\n:\n \n33e56099-0bde-8z93-a005-89c0f6df7465\n\n    \nclient_secret\n:\n \nclient-secret\n\n    \nresource_group_name\n:\n \nbosh-res-group\n\n    \nstorage_account_name\n:\n \nboshstore\n\n    \nssh_user\n:\n \nvcap\n\n    \nssh_public_key\n:\n \n\"ssh-rsa\n \nAAAAB3N...6HySEF6IkbJ\"\n\n    \ndefault_security_group\n:\n \nnsg-azure\n\n\n\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n-\n \nname\n:\n \nz2\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nStandard_A2\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ndns\n:\n \n[\n168.63.129.16\n]\n\n    \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n    \ncloud_properties\n:\n\n      \nvirtual_network_name\n:\n \nboshnet\n\n      \nsubnet_name\n:\n \nboshsub\n\n\n-\n \nname\n:\n \nvip\n\n  \ntype\n:\n \nvip\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nErrors \n\u00b6\n\n\n\n\nInvalid service principal\n\n\n\n\nhttp_get_response - get_token - http error: 400\n\n\n\n\nService principal is most likely invalid. Verify that client ID, client secret and tenant ID successfully work:\n\n\n$ azure login --username client-id --password client-secret --service-principal --tenant tenant-id\n\n\n\n\nIf your service principal worked and you get the above error suddenly, it may be caused by that your service principal expired. You need to go to Azure Portal to update client secret. By default, the service principal will expire in one year.\n\n\n\n\n\n\nGo to \nAzure Portal\n, select \nactive directory\n -- > ORGANIZATION-NAME -- > \nApplications\n -- > search your service principal name.\n\n\n\n\n\n\nThen choose your service principal, select \nConfigure\n -- > \nkeys\n -- > add a new key.\n\n\n\n\n\n\nExceeding quota limits of Core\n\n\n\n\n\n\nhttp_put - error: 409 message: {\n  \"error\": {\n    \"code\": \"OperationNotAllowed\",\n    \"message\": \"Operation results in exceeding quota limits of Core. Maximum allowed: 4, Current in use: 4, Additional requested: 1.\"\n  }\n}\n\n\n\n\nEither upgrade your trial account, or file a support ticket in the Azure portal to raise account quotas.\n\n\n\n\nNicInUse\n\n\n\n\nhttp_delete - error: 400 message: {\n    \"error\": {\n        \"code\": \"NicInUse\",\n        \"message\": \"Network Interface /.../networkInterfaces/dc0d3a9a-0b00-40d8-830d-41e6f4ac9809 is used by existing VM /.../virtualMachines/dc0d3a9a-0b00-40d8-830d-41e6f4ac9809.\",\n        \"details\": []\n    }\n}\n\n\n\n\nThis error indicates that unknown VM (to the Director) took up the IP that the Director is trying to assign to a new VM. Either let the Director know to not use this IP by including it in the reserved section of a subnet in your manual network, or make that IP available by terminating the unknown VM.\n\n\n\n\nLimits of Premium Storage blob snapshots\n\n\n\n\nError 100: Unknown CPI error 'Unknown' with message 'SnaphotOperationRateExceeded (409): The rate of snapshot blob calls is exceeded.\n\n\n\n\nThe BOSH snapshot operation may be throttled if you do all of the following:\n\n\n\n\n\n\nUse Premium Storage for the Cloud Foundry VMs.\n\n\n\n\n\n\nEnable snapshot in \nbosh.yml\n. For more information on BOSH Snapshots, please go to \nhttps://bosh.io/docs/snapshots.html\n.\n\n\ndirector\n:\n\n  \nenable_snapshots\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nThe time between consecutive snapshots by BOSH is less than \n10 minutes\n. The limits are documented in \nSnapshots and Copy Blob for Premium Storage\n.\n\n\n\n\n\n\nThe workaround is:\n\n\n\n\n\n\nDisable snapshot temporarily.\n\n\ndirector\n:\n\n  \nenable_snapshots\n:\n \nfalse\n\n\n\n\n\n\n\n\n\nAdjust the snapshot interval to more than 10 minutes.\n\n\n\n\n\n\nVersion mismatch between CPI and Stemcell\n\n\n\n\n\n\nFor CPI v11 or later, the compatible stemcell version is v3181 or later. If the stemcell version is older than v3181, you may hit the following failure when deploying BOSH.\n\n\nPerforming\n \nPOST\n \nrequest\n:\n\n  \nPost\n \nhttps\n:\n//mbus-user:<redacted>@10.0.0.4:6868/agent: dial tcp 10.0.0.4:6868: getsockopt: connection refused\n\n\n\n\n\nIt is recommended to use the latest version. For example, Stemcell v3232.5 or later, and CPI v12 or later. You may hit the issue \n#135\n if you still use an older stemcell than v3232.5.\n\n\n\n\nOut of memory\n\n\n\n\nIf you hit \nOut of memory\n or \nVirtual memory exhausted\n, please check whether you use Standard_A0 as instance_type. You should change instance_type to a VM size with more memory.\n  Please reference the issue \n#230\n.\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/azure-cpi/#azs",
            "text": "availability_zone  [String, optional]: Availability zone to use for creating instances (available in v33+). Possible values:  '1' ,  '2' ,  '3' . Read this  document  to get regions and VM sizes on Azure that support availability zones.  More details about availability zone .   Example:  azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   '1'",
            "title": "AZs "
        },
        {
            "location": "/azure-cpi/#networks",
            "text": "",
            "title": "Networks "
        },
        {
            "location": "/azure-cpi/#dynamic-network-or-manual-network",
            "text": "Schema for  cloud_properties  section:   resource_group_name  [String, optional]: Name of a resource group. If it is set, Azure CPI will search the virtual network and security group in this resource group. Otherwise, Azure CPI will search the virtual network and security group in  resource_group_name  in the global CPI settings.  virtual_network_name  [String, required]: Name of a virtual network. Example:  boshnet .  subnet_name  [String, required]: Name of a subnet within virtual network.  security_group  [String, optional]: The  security group  to apply to network interfaces of all VMs placed in this network. The security group of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority); if it is not specified, the default security group (specified by  default_security_group  in the global CPI settings) will be used.  application_security_groups  [Array, optional]: The  application security group  to apply to network interfaces of all VMs placed in this network. The application security groups of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority).  This property is supported in v31+.  You must reference the  document  to register your subscription with this new feature.     See  how to create a virtual network and subnets .  Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     cloud_properties : \n       resource_group_name :   my-resource-group-name \n       virtual_network_name :   my-vnet-name \n       subnet_name :   my-subnet-name \n       security_group :   my-security-group-name \n       application_security_groups :   [ \"my-application-security-group-name-1\" ,   \"my-application-security-group-name-2\" ]",
            "title": "Dynamic Network or Manual Network"
        },
        {
            "location": "/azure-cpi/#vip-network",
            "text": "Schema for  cloud_properties  section:   resource_group_name  [String, optional]: Name of a resource group. If it is set, Azure CPI will search the public IP in this resource group. Otherwise, Azure CPI will search the public IP in  resource_group_name  in the global CPI settings.   See  how to create public IP  to use with vip networks.  Example of vip network:  networks :  -   name :   public \n   type :   vip \n   cloud_properties : \n     resource_group_name :   my-resource-group",
            "title": "Vip Network"
        },
        {
            "location": "/azure-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   instance_type  [String, required]: Type of the  instance . Example:  Standard_A2 .  Basic Tier Virtual Machines  should not be used if you need to bind the instance to Azure Load Balancer (ALB), because Basic Tier VM doesn't support ALB.  root_disk  [Hash, optional]: OS disk of custom size.  size  [Integer, optional]: Specifies the disk size in MiB.  The size must be greater than 3 * 1024 and less than the max disk size for  unmanaged  or  managed  disk. Please always use  N * 1024  as the size because Azure always uses GiB but not MiB.  It has a default value  30 * 1024  only when ephemeral_disk.use_root_disk is set to true.    caching  [String, optional]: Type of the disk caching of the VMs' OS disks. It can be either  None ,  ReadOnly  or  ReadWrite . Default is  ReadWrite .   ephemeral_disk  [Hash, optional]: Ephemeral disk to apply for all VMs that are in this resource pool. By default a data disk with the default size as below will be created as the ephemeral disk.   use_root_disk  [Boolean, optional]: Enable to use OS disk to store the ephemeral data. The default value is false. When it is true, ephemeral_disk.size will not be used.  size  [Integer, optional]: Specifies the disk size in MiB. If this is not set, the default size as below will be used. The size of the ephemeral disk for the BOSH VM should be larger than or equal to  30*1024  MiB. Please always use  N * 1024  as the size because Azure always uses GiB not MiB.  If the Azure temporary disk size for the instance type is less than  30*1024  MiB, the default size is  30*1024  MiB because the space may not be enough.  If the Azure temporary disk size for the instance type is larger than  1000*1024  MiB, the default size is  1000*1024  MiB because it is not expected to use such a large ephemeral disk in CF currently.  Otherwise, the Azure temporary disk size will be used as the default size. See more information about  Azure temporary disk size .     load_balancer  [String, optional]: Name of a  load balancer  the VMs should belong to. You need to create the load balancer manually before configuring it. Notes:   Basic Tier Virtual Machines  (Example:  Basic_A1 ) doesn't support Azure Load Balancer.  If  availability_zone  is specified for the VM,  standard sku load balancer  must be used, as  basic sku load balancer  does not work for zone.    application_gateway  [String, optional]: Name of the  application gateway  which the VMs should be associated to.  This property is supported in CPI v28+.  You need to create the application gateway manually before configuring it. Please refer to  the guidance .    security_group  [String, optional]: The  security group  to apply to network interfaces of all VMs placed in this resource pool. The security group of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority); if it is not specified, the default security group (specified by  default_security_group  in the global CPI settings) will be used.  application_security_groups  [Array, optional]: The  application security group  to apply to network interfaces of all VMs placed in this resource group. The application security groups of a network interface can be specified either in a resource pool(higher priority) or a network configuration(lower priority).  This property is supported in v31+.  You must reference the  document  to register your subscription with this new feature.     assign_dynamic_public_ip  [Boolean, optional]: Enable to create and assign dynamic public IP to the VM automatically (to solve the  azure SNAT issue ). Default value is  false . Only the VM without vip will be assigned a dynamic public IP when this value is set to true, and the dynamic public IP will be deleted when the VM is deleted.    availability_zone  [String, optional]: Availability zone to use for creating instances (available in v33+). Possible values:  '1' ,  '2' ,  '3' . Read this  document  to get regions and VM sizes on Azure that support availability zones.  More details about availability zone .   availability_set  [String, optional]: Name of an  availability set  to use for VMs.  More details .  If available set does not exist, it will be automatically created.  If  availability_set  is not specified, Azure CPI will search  env.bosh.group  as the name of availability set.  bosh release  v258+ will generate a value for  env.bosh.group  automatically.  On Azure the length of the availability set name must be between 1 and 80 characters. The name got from   env.bosh.group  may be too long. CPI will truncate the name to the following format  az-MD5-[LAST-40-CHARACTERS-OF-GROUP]  if the length is greater than 80.  CPI v27+ will delete the empty availability set.  Only one of  availability_zone  and  availability_set  is allowed to be configured for a VM. If  availability_zone  is specified, the VM will be in a zone and not in any availability set.    platform_update_domain_count  [Integer, optional]: The count of  update domain  in the availability set.  For Azure, the default value is  5 .  For Azure Stack, the default value is  1 .     platform_fault_domain_count  [Integer, optional]: The count of  fault domain  in the availability set.   For Azure, the default value of an unmanaged availability set is  3 . The default value of a managed availability set is  2 , because  some regions don't support 3 fault domains for now  For Azure Stack, the default value is  1 . Before 1802 update, only  1  is allowed. After  1802 update , you can configure up to 3 fault domains.     storage_account_name  [String, optional]: Storage account for VMs. Valid only when  use_managed_disks  is  false . If this is not set, the VMs will be created in the default storage account. See  this document  for more details on why this option exists.   If you use  DS-series  or  GS-series  as  instance_type , you should set this to a premium storage account. See more information about  Azure premium storage . See  avaliable regions  where you can create premium storage accounts.  If you use a different storage account which must be in the same resource group, please make sure:  The permissions for the container  stemcell  in the default storage account is set to  Public read access for blobs only .  A table  stemcells  is created in the default storage account.  Two containers  bosh  and  stemcell  are created in the new storage account.  If this storage account does not exist, it can be created automatically by Azure CPI. But you must specify storage_account_type and make sure:  The name must be  unique within Azure .  The name must be between 3 and 24 characters in length and use numbers and lower-case letters only .  If you use a pattern  *keyword* . CPI will filter all storage accounts under the default resource group by the pattern and pick one available storage account to create the VM.  The pattern must start with  *  and end with  * .  The keyword must only contain numbers and lower-case letters because of the naming rule of storage account name.  The rule to select an available storage account is to check the number of disks under the container  bosh  does not exceed the limitation.  The default number of disks limitation is 30 but you can specify it in  storage_account_max_disk_number .    storage_account_max_disk_number  [Integer, optional]: Number of disks limitation in a storage account. Valid only when  use_managed_disks  is  false . Default value is 30. This will be used only when  storage_account_name  is a pattern.  Every storage account has a limitation to host disks. You may hit the performance issue if you create too many disks in one storage account.  The maximum number of disks of a standard storage account is 40 because the maximum IOPS of a standard storage account is 20,000 and the maximum IOPS of a standard disk is 500.  If you are using premium storage account, Azure maps the disk size (rounded up) to the nearest Premium Storage Disk option (P10, P20 and P30). For example, a disk of size 100 GiB is classified as a P10 option.  The maximum number of disks of a premium storage account is 280 if you are using P10 (128 GiB) as your disk type.  The maximum number of disks of a premium storage account is 70 if you are using P20 (512 GiB) as your disk type.  The maximum number of disks of a premium storage account is 35 if you are using P30 (1024 GiB) as your disk type.  storage_account_max_disk_number  should be less than the maximum number. Suggest you to use (MAX - 10) as the value because CPI always creates VMs in parallel.  Please see more information about  azure-subscription-service-limits .    storage_account_type  [String, optional]: Storage account type. Valid only when  use_managed_disks  is  false . It is required if the storage account does not exist. It can be either  Standard_LRS ,  Standard_ZRS ,  Standard_GRS ,  Standard_RAGRS  or  Premium_LRS . You can click  HERE  to learn more about the type of Azure storage account.   storage_account_location  [String, optional]: Location of the storage account. This configuration is deprecated in CPI v25+: if you specify a storage account which does not exist, CPI (v25+) will create it automatically in the same location as VMs' VNET.    resource_group_name  [String, optional]: Name of a resource group (Available in v26+). If it is set, related resources will be created in this resource group; otherwise, they will be created in  resource_group_name  specified in the global CPI settings. The resources affected by this property are:   Virtual Machine  Network Interface Card  Managed Disks (including OS disk, ephemeral disk, persistent disk and snapshot)  Dynamic Public IP for the VM  Availability Set     Example of a  Standard_A2  instance:  resource_pools :  -   name :   default \n   network :   default \n   stemcell : \n     name :   bosh-azure-hyperv-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     instance_type :   Standard_A2 \n     availability_set :   <availability-set-name> \n     root_disk : \n       size :   30_720 \n     ephemeral_disk : \n       use_root_disk :   false \n       size :   30_720",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/azure-cpi/#disk-pools-disk-types",
            "text": "Schema for  cloud_properties  section:   caching  [String, optional]: Type of the disk caching. It can be either  None ,  ReadOnly  or  ReadWrite . Default is  None .  storage_account_type  [String, optional]: Storage account type. Valid only when  use_managed_disks  is  true . It can be either  Standard_LRS  or  Premium_LRS . You can click  HERE  to learn more about the type of Azure storage account.   Example of 10GB disk:   disk_size  [Integer, required]: Size of the disk in MiB. On Azure the disk size must be greater than 1 * 1024 and less than the max disk size for  unmanaged  or  managed  disk. Please always use  N * 1024  as the size because Azure always uses GiB not MiB.   disk_pools :  -   name :   default \n   disk_size :   10_240",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/azure-cpi/#global-configuration",
            "text": "Schema:   environment  [String, required]: Azure environment name. Possible values are:  AzureCloud ,  AzureChinaCloud ,  AzureUSGovernment  (available in v19+),  AzureGermanCloud  (available in v22+) or  AzureStack .  location  [String, optional]: Azure region name. Only required when  vm_resources  is specified in the deployment manifest. Available in v33+.  subscription_id  [String, required]: Subscription ID.  tenant_id  [String, required]: Tenant ID of the service principal.  client_id  [String, required]: Client ID of the service principal.  client_secret  [String, optional]: Client secret of the service principal.  certificate  [String, optional]: The certificate for your service principal. Azure CPI v35.0.0+ supports the  service principal with a certificate . Only one of  client_secret  and  certificate  can be specified.  resource_group_name  [String, required]: Resource group name.  storage_account_name  [String, optional]: Storage account name. It will be used as a default storage account for VM disks and stemcells. If  use_managed_disks  is  false ,  storage_account_name  is required. Otherwise,  storage_account_name  is optional.  ssh_user  [String, required]: SSH username. Default:  vcap .  ssh_public_key  [String, required]: SSH public key.  default_security_group  [String, optional]: Name of the default  security group  that will be applied to all created VMs. This property is required before v35.0.0, and optional in v35.0.0+.  azure_stack  [Hash, optional]:  Configration for AzureStack . Available in v23+.  domain  [String, optional]: The domain for your AzureStack deployment. Default is  local.azurestack.external . You can use the default value for  Azure Stack development kit . To get this value for Azure Stack integrated systems, contact your service provider.  authentication  [String, optional]: The authentication type for your AzureStack deployment. Possible values are:  AzureAD  and  ADFS . You need to specify  certificate  if you select  ADFS , because Azure Stack with ADFS authentication only supports the service principal with a certificate.  resource  [String, optional]: Active Directory Service Endpoint Resource ID, where you can get the token for your AzureStack deployment.  endpoint_prefix  [String, optional]: The endpoint prefix for your AzureStack deployment. Default is  management .  skip_ssl_validation  [Boolean, optional]: Toggles verification of the Azure Resource Manager REST API SSL certificate. Default is  false . Deprecated in v35.0.0+.  use_http_to_access_storage_account  [Boolean, optional]: Flag for using HTTP to access storage account rather than the default HTTPS. Default is  false . Deprecated in v35.0.0+.  ca_cert  [String, required]: All required custom CA certificates for AzureStack. You can  export the Azure Stack CA root certificate . Available in v27+.  The property is required for v35.0.0+.  For the versions from v27 to v34, if  ca_cert  is not provided, the  skip_ssl_validation  and  use_http_to_access_storage_account  must be set to  true .      parallel_upload_thread_num  [Integer, optional]: The number of threads to upload stemcells in parallel. The default value is 16.  debug_mode  [Boolean, optional]: Enable debug mode. The default value is  false . When  debug_mode  is  true :  CPI will log all raw HTTP requests/responses.  The new created VMs (only for VMs in same region with  storage_account_name  specified in  Global Configuration ) will have  boot diagnostics  enabled. Available in v26+.    use_managed_disks  [Boolean, optional]: Enable managed disks. The default value is  false . For  AzureCloud , the option is supported in v21+. For  AzureChinaCloud ,  AzureUSGovernment , and  AzureGermanCloud , the option is supported in v26+. For  AzureStack , the option is not yet supported.  pip_idle_timeout_in_minutes  [Integer, optional]: Set idle timeouts in minutes for dynamic public IPs. It must be in the range [4, 30]. The default value is 4. It is only used when  assign_dynamic_public_ip  is set to  true  in  resouce_pool . Available in V24+.  keep_failed_vms  [Boolean, optional]: A flag to keep the failed VM. If it's set to  true  and CPI fails to  provision  the VM, CPI will keep the VM for troubleshooting. The default value is  false . Available in v32+. Please note that the option is different from  keep_unreachable_vms  of the  director configuration . The latter is to keep the VM whose BOSH agent is unresponsive.   See  all configuration options .  See  Creating Azure resources  page for more details on how to create and configure above resources.  Example with hard-coded credentials:  properties : \n   azure :   &azure \n     environment :   AzureCloud \n     subscription_id :   3c39a033-c306-4615-a4cb-260418d63879 \n     tenant_id :   0412d4fa-43d2-414b-b392-25d5ca46561da \n     client_id :   33e56099-0bde-8z93-a005-89c0f6df7465 \n     client_secret :   client-secret \n     resource_group_name :   bosh-res-group \n     storage_account_name :   boshstore \n     ssh_user :   vcap \n     ssh_public_key :   \"ssh-rsa   AAAAB3N...6HySEF6IkbJ\" \n     default_security_group :   nsg-azure",
            "title": "Global Configuration "
        },
        {
            "location": "/azure-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1  -   name :   z2  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   Standard_A2  disk_types :  -   name :   default \n   disk_size :   10_240  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     dns :   [ 168.63.129.16 ] \n     azs :   [ z1 ,   z2 ] \n     cloud_properties : \n       virtual_network_name :   boshnet \n       subnet_name :   boshsub  -   name :   vip \n   type :   vip  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   default \n   network :   default",
            "title": "Example Cloud Config "
        },
        {
            "location": "/azure-cpi/#errors",
            "text": "Invalid service principal   http_get_response - get_token - http error: 400  Service principal is most likely invalid. Verify that client ID, client secret and tenant ID successfully work:  $ azure login --username client-id --password client-secret --service-principal --tenant tenant-id  If your service principal worked and you get the above error suddenly, it may be caused by that your service principal expired. You need to go to Azure Portal to update client secret. By default, the service principal will expire in one year.    Go to  Azure Portal , select  active directory  -- > ORGANIZATION-NAME -- >  Applications  -- > search your service principal name.    Then choose your service principal, select  Configure  -- >  keys  -- > add a new key.    Exceeding quota limits of Core    http_put - error: 409 message: {\n  \"error\": {\n    \"code\": \"OperationNotAllowed\",\n    \"message\": \"Operation results in exceeding quota limits of Core. Maximum allowed: 4, Current in use: 4, Additional requested: 1.\"\n  }\n}  Either upgrade your trial account, or file a support ticket in the Azure portal to raise account quotas.   NicInUse   http_delete - error: 400 message: {\n    \"error\": {\n        \"code\": \"NicInUse\",\n        \"message\": \"Network Interface /.../networkInterfaces/dc0d3a9a-0b00-40d8-830d-41e6f4ac9809 is used by existing VM /.../virtualMachines/dc0d3a9a-0b00-40d8-830d-41e6f4ac9809.\",\n        \"details\": []\n    }\n}  This error indicates that unknown VM (to the Director) took up the IP that the Director is trying to assign to a new VM. Either let the Director know to not use this IP by including it in the reserved section of a subnet in your manual network, or make that IP available by terminating the unknown VM.   Limits of Premium Storage blob snapshots   Error 100: Unknown CPI error 'Unknown' with message 'SnaphotOperationRateExceeded (409): The rate of snapshot blob calls is exceeded.  The BOSH snapshot operation may be throttled if you do all of the following:    Use Premium Storage for the Cloud Foundry VMs.    Enable snapshot in  bosh.yml . For more information on BOSH Snapshots, please go to  https://bosh.io/docs/snapshots.html .  director : \n   enable_snapshots :   true     The time between consecutive snapshots by BOSH is less than  10 minutes . The limits are documented in  Snapshots and Copy Blob for Premium Storage .    The workaround is:    Disable snapshot temporarily.  director : \n   enable_snapshots :   false     Adjust the snapshot interval to more than 10 minutes.    Version mismatch between CPI and Stemcell    For CPI v11 or later, the compatible stemcell version is v3181 or later. If the stemcell version is older than v3181, you may hit the following failure when deploying BOSH.  Performing   POST   request : \n   Post   https : //mbus-user:<redacted>@10.0.0.4:6868/agent: dial tcp 10.0.0.4:6868: getsockopt: connection refused   It is recommended to use the latest version. For example, Stemcell v3232.5 or later, and CPI v12 or later. You may hit the issue  #135  if you still use an older stemcell than v3232.5.   Out of memory   If you hit  Out of memory  or  Virtual memory exhausted , please check whether you use Standard_A0 as instance_type. You should change instance_type to a VM size with more memory.\n  Please reference the issue  #230 .   Back to Table of Contents",
            "title": "Errors "
        },
        {
            "location": "/azure-resources/",
            "text": "Subscription \n\u00b6\n\n\nTo find out subscription and tenant ID use following commands:\n\n\nNote: All azure commands were tested with the azure-cli v[2.0.21] on Ubuntu 16.04. The azure commands may vary based on your version and OS.\n\n\n\n$ az cloud \nset\n --name AzureCloud\n\n$ az login\n\n$ az account list --output json\n\n[\n\n  \n{\n\n    \n\"cloudName\"\n: \n\"AzureCloud\"\n,\n    \n\"id\"\n: \n\"my-subscription-id\"\n,\n    \n\"isDefault\"\n: true,\n    \n\"name\"\n: \n\"my-subscription-name\"\n,\n    \n\"state\"\n: \n\"Enabled\"\n,\n    \n\"tenantId\"\n: \n\"my-tenant-id\"\n,\n    \n\"user\"\n: \n{\n\n      \n\"name\"\n: \n\"my-user-name\"\n,\n      \n\"type\"\n: \n\"user\"\n\n    \n}\n\n  \n}\n\n\n]\n\n\n\n\n\n\nNote:\nIf `tenantId` is not present, you may be using a personal account to log in to your Azure subscription. Switch to using work or school account.\nIf you are using Azure cloud in China, you should switch the cloud from `AzureCloud` to `AzureChinaCloud`.\nIf you are using Azure cloud in Azure Government, you should switch the cloud from `AzureCloud` to `AzureUSGovernment`.\nIf you are using Azure cloud in German Cloud, you should switch the cloud from `AzureCloud` to `AzureGermanCloud`.\n\n\n\n\nOnce you've determined your subscription ID, switch to using that account:\n\n\n$ az account \nset\n --subscription my-subscription-id\n\n\n\n\nRegister the required providers:\n\n\n$ az provider register --namespace Microsoft.Network\n$ az provider register --namespace Microsoft.Storage\n$ az provider register --namespace Microsoft.Compute\n\n\n\n\n\n\nClient \n\u00b6\n\n\nAzure CPI needs client ID and secret to make authenticated requests.\n\n\n$ az ad app create --display-name \n\"mycpi\"\n --password client-secret --identifier-uris \n\"http://mycpi\"\n --homepage \n\"http://mycpi\"\n\n\n{\n\n  \n\"appId\"\n: \n\"my-app-id\"\n,\n  \n\"appPermissions\"\n: null,\n  \n\"availableToOtherTenants\"\n: false,\n  \n\"displayName\"\n: \n\"mycpi\"\n,\n  \n\"homepage\"\n: \n\"http://mycpi\"\n,\n  \n\"identifierUris\"\n: \n[\n\n    \n\"http://mycpi\"\n\n  \n]\n,\n  \n\"objectId\"\n: \n\"my-object-id\"\n,\n  \n\"objectType\"\n: \n\"Application\"\n,\n  \n\"replyUrls\"\n: \n[]\n\n\n}\n\n\n\n\n\nApplication ID (\nmy-app-id\n in the above output) is the client ID and specified password (\nclient-secret\n in above example) is the client secret.\n\n\nFinally create service principal to enable authenticated access:\n\n\n$ az ad sp create --id my-app-id\n$ az role assignment create --role \n\"Contributor\"\n --assignee \n\"http://mycpi\"\n --scope /subscriptions/my-subscription-id\n\n\n\n\n\n\nResource Group \n\u00b6\n\n\nCreate a resource group in one of the supported \nAzure locations\n:\n\n\n$ az group create --name bosh-res-group --location \n\"Central US\"\n\n\n$ az group show --name bosh-res-group\n\n{\n\n  \n\"id\"\n: \n\"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group\"\n,\n  \n\"location\"\n: \n\"centralus\"\n,\n  \n\"managedBy\"\n: null,\n  \n\"name\"\n: \n\"bosh-res-group\"\n,\n  \n\"properties\"\n: \n{\n\n    \n\"provisioningState\"\n: \n\"Succeeded\"\n\n  \n}\n,\n  \n\"tags\"\n: null\n\n}\n\n\n\n\n\nMake sure to wait for 'Provisioning State' to become \nSucceeded\n.\n\n\n\n\nVirtual Network & Subnet \n\u00b6\n\n\nCreate a virtual network:\n\n\n$ az network vnet create --name boshnet --address-prefixes \n10\n.0.0.0/8 --resource-group bosh-res-group --location \n\"Central US\"\n --dns-server \n168\n.64.129.16\n$ az network vnet subnet create --name bosh --address-prefix \n10\n.0.0.0/24 --vnet-name boshnet --resource-group bosh-res-group\n\n$ az network vnet show --name boshnet --resource-group bosh-res-group\n\n{\n\n  \n\"addressSpace\"\n: \n{\n\n    \n\"addressPrefixes\"\n: \n[\n\n      \n\"10.0.0.0/8\"\n\n    \n]\n\n  \n}\n,\n  \n\"dhcpOptions\"\n: \n{\n\n    \n\"dnsServers\"\n: \n[\n\n      \n\"168.64.129.16\"\n\n    \n]\n\n  \n}\n,\n  \n\"enableDdosProtection\"\n: false,\n  \n\"enableVmProtection\"\n: false,\n  \n\"etag\"\n: \n\"W/\\\"e62167ab-0e8c-4e78-9d78-18d1a963da2e\\\"\"\n,\n  \n\"id\"\n: \n\"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/virtualNetworks/boshnet\"\n,\n  \n\"location\"\n: \n\"centralus\"\n,\n  \n\"name\"\n: \n\"boshnet\"\n,\n  \n\"provisioningState\"\n: \n\"Succeeded\"\n,\n  \n\"resourceGroup\"\n: \n\"bosh-res-group\"\n,\n  \n\"resourceGuid\"\n: \n\"9fa33ff9-2a3f-4139-8e03-af0cc3af67a0\"\n,\n  \n\"subnets\"\n: \n[\n\n    \n{\n\n      \n\"addressPrefix\"\n: \n\"10.0.0.0/24\"\n,\n      \n\"etag\"\n: \n\"W/\\\"e62167ab-0e8c-4e78-9d78-18d1a963da2e\\\"\"\n,\n      \n\"id\"\n: \n\"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/virtualNetworks/boshnet/subnets/bosh\"\n,\n      \n\"ipConfigurations\"\n: null,\n      \n\"name\"\n: \n\"bosh\"\n,\n      \n\"networkSecurityGroup\"\n: null,\n      \n\"provisioningState\"\n: \n\"Succeeded\"\n,\n      \n\"resourceGroup\"\n: \n\"bosh-res-group\"\n,\n      \n\"resourceNavigationLinks\"\n: null,\n      \n\"routeTable\"\n: null,\n      \n\"serviceEndpoints\"\n: null\n    \n}\n\n  \n]\n,\n  \n\"tags\"\n: \n{}\n,\n  \n\"type\"\n: \n\"Microsoft.Network/virtualNetworks\"\n,\n  \n\"virtualNetworkPeerings\"\n: \n[]\n\n\n}\n\n\n\n\n\n\n\nNetwork Security Group \n\u00b6\n\n\nCreate two network security groups:\n\n\n$ az network nsg create --resource-group bosh-res-group --location \n\"Central US\"\n --name nsg-bosh\n$ az network nsg create --resource-group bosh-res-group --location \n\"Central US\"\n --name nsg-cf\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority \n200\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'ssh'\n --destination-port-range \n22\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority \n201\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'bosh-agent'\n --destination-port-range \n6868\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority \n202\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'bosh-director'\n --destination-port-range \n25555\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol \n'*'\n --direction Inbound --priority \n203\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'dns'\n --destination-port-range \n53\n\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-cf --access Allow --protocol Tcp --direction Inbound --priority \n201\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'cf-https'\n --destination-port-range \n443\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-cf --access Allow --protocol Tcp --direction Inbound --priority \n202\n --source-address-prefix Internet --source-port-range \n'*'\n --destination-address-prefix \n'*'\n --name \n'cf-log'\n --destination-port-range \n4443\n\n\n\n\n\n\n\nPublic IPs \n\u00b6\n\n\nTo make certain VMs publicly accessible, you will need to create a Public IP. If Azure Availability Zones is used in \nAZs\n, the Public IP should be created with type \nStandard SKU\n; otherwise, you can use the default \nBasic SKU\n.\n\n\n$ az network public-ip create --name my-public-ip --allocation-method Static --resource-group bosh-res-group --location \n\"Central US\"\n --sku Basic \n# sku should be `Standard' when using Azure Availability Zones\n\n\n$ az network public-ip show --name my-public-ip --resource-group bosh-res-group\n\n{\n\n  \n\"dnsSettings\"\n: null,\n  \n\"etag\"\n: \n\"W/\\\"b3686484-21fe-470a-a059-32d02b4f9589\\\"\"\n,\n  \n\"id\"\n: \n\"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/publicIPAddresses/my-public-ip\"\n,\n  \n\"idleTimeoutInMinutes\"\n: \n4\n,\n  \n\"ipAddress\"\n: \n\"13.89.236.107\"\n,\n  \n\"ipConfiguration\"\n: null,\n  \n\"location\"\n: \n\"centralus\"\n,\n  \n\"name\"\n: \n\"my-public-ip\"\n,\n  \n\"provisioningState\"\n: \n\"Succeeded\"\n,\n  \n\"publicIpAddressVersion\"\n: \n\"IPv4\"\n,\n  \n\"publicIpAllocationMethod\"\n: \n\"Static\"\n,\n  \n\"resourceGroup\"\n: \n\"bosh-res-group\"\n,\n  \n\"resourceGuid\"\n: \n\"a25d2f1e-d8f7-4258-9c83-7c30e4a2c270\"\n,\n  \n\"sku\"\n: \n{\n\n    \n\"name\"\n: \n\"Basic\"\n\n  \n}\n,\n  \n\"tags\"\n: null,\n  \n\"type\"\n: \n\"Microsoft.Network/publicIPAddresses\"\n,\n  \n\"zones\"\n: null\n\n}\n\n\n\n\n\n\nNote:\nYou can skip below section if you are using managed disks with Azure CPI v21+\n\n\n\n\n\n\nStorage Account \n\u00b6\n\n\nCreate a default storage account to hold root disks, persistent disks, stemcells, etc.\nIf unsure of desired SKU Name, choose \nLRS\n, desired Kind, choose \nStorage\n:\n\n\n$ az storage account create --name myboshstore --resource-group bosh-res-group --location \n\"Central US\"\n\n\n$ az storage account show --name myboshstore --resource-group bosh-res-group\n\n{\n\n  \n\"accessTier\"\n: null,\n  \n\"creationTime\"\n: \n\"2017-11-21T03:36:36.568159+00:00\"\n,\n  \n\"customDomain\"\n: null,\n  \n\"enableHttpsTrafficOnly\"\n: false,\n  \n\"encryption\"\n: \n{\n\n    \n\"keySource\"\n: \n\"Microsoft.Storage\"\n,\n    \n\"keyVaultProperties\"\n: null,\n    \n\"services\"\n: \n{\n\n      \n\"blob\"\n: \n{\n\n        \n\"enabled\"\n: true,\n        \n\"lastEnabledTime\"\n: \n\"2017-11-21T03:36:36.571160+00:00\"\n\n      \n}\n,\n      \n\"file\"\n: \n{\n\n        \n\"enabled\"\n: true,\n        \n\"lastEnabledTime\"\n: \n\"2017-11-21T03:36:36.571160+00:00\"\n\n      \n}\n,\n      \n\"queue\"\n: null,\n      \n\"table\"\n: null\n    \n}\n\n  \n}\n,\n  \n\"id\"\n: \n\"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Storage/storageAccounts/myboshstore\"\n,\n  \n\"identity\"\n: null,\n  \n\"kind\"\n: \n\"Storage\"\n,\n  \n\"lastGeoFailoverTime\"\n: null,\n  \n\"location\"\n: \n\"centralus\"\n,\n  \n\"name\"\n: \n\"myboshstore\"\n,\n  \n\"networkRuleSet\"\n: \n{\n\n    \n\"bypass\"\n: \n\"AzureServices\"\n,\n    \n\"defaultAction\"\n: \n\"Allow\"\n,\n    \n\"ipRules\"\n: \n[]\n,\n    \n\"virtualNetworkRules\"\n: \n[]\n\n  \n}\n,\n  \n\"primaryEndpoints\"\n: \n{\n\n    \n\"blob\"\n: \n\"https://myboshstore.blob.core.windows.net/\"\n,\n    \n\"file\"\n: \n\"https://myboshstore.file.core.windows.net/\"\n,\n    \n\"queue\"\n: \n\"https://myboshstore.queue.core.windows.net/\"\n,\n    \n\"table\"\n: \n\"https://myboshstore.table.core.windows.net/\"\n\n  \n}\n,\n  \n\"primaryLocation\"\n: \n\"centralus\"\n,\n  \n\"provisioningState\"\n: \n\"Succeeded\"\n,\n  \n\"resourceGroup\"\n: \n\"bosh-res-group\"\n,\n  \n\"secondaryEndpoints\"\n: \n{\n\n    \n\"blob\"\n: \n\"https://myboshstore-secondary.blob.core.windows.net/\"\n,\n    \n\"file\"\n: null,\n    \n\"queue\"\n: \n\"https://myboshstore-secondary.queue.core.windows.net/\"\n,\n    \n\"table\"\n: \n\"https://myboshstore-secondary.table.core.windows.net/\"\n\n  \n}\n,\n  \n\"secondaryLocation\"\n: \n\"eastus2\"\n,\n  \n\"sku\"\n: \n{\n\n    \n\"capabilities\"\n: null,\n    \n\"kind\"\n: null,\n    \n\"locations\"\n: null,\n    \n\"name\"\n: \n\"Standard_RAGRS\"\n,\n    \n\"resourceType\"\n: null,\n    \n\"restrictions\"\n: null,\n    \n\"tier\"\n: \n\"Standard\"\n\n  \n}\n,\n  \n\"statusOfPrimary\"\n: \n\"available\"\n,\n  \n\"statusOfSecondary\"\n: \n\"available\"\n,\n  \n\"tags\"\n: \n{}\n,\n  \n\"type\"\n: \n\"Microsoft.Storage/storageAccounts\"\n\n\n}\n\n\n\n\n\nNote: Even if create command returns an error, check whether the storage account is created successfully via `storage account show` command.\n\n\nOnce storage account is created you can retrieve primary storage access key:\n\n\u0002wzxhzdk:10\u0003\n\n---\n### Storage Account Containers \n\n\nCPI expects to find `bosh` and `stemcell` containers within a default storage account:\n\n- `bosh` container is used for storing root and persistent disks.\n- `stemcell` container is used for storing uploaded stemcells.\n\n\nNote: If you are planning to use multiple storage accounts, make sure to set stemcell container permissions to \"Public read access for blobs only\".\n\n\n\u0002wzxhzdk:11\u0003\n\n---\n### Storage Account Tables \n\n\nTo support multiple storage accounts, you need to create the following tables in the default storage account:\n\n- `stemcells` is used to store metadata of stemcells in multiple storage accounts\n\n\u0002wzxhzdk:12\u0003",
            "title": "Creating Resources"
        },
        {
            "location": "/azure-resources/#subscription",
            "text": "To find out subscription and tenant ID use following commands:  Note: All azure commands were tested with the azure-cli v[2.0.21] on Ubuntu 16.04. The azure commands may vary based on your version and OS.  $ az cloud  set  --name AzureCloud\n\n$ az login\n\n$ az account list --output json [ \n   { \n     \"cloudName\" :  \"AzureCloud\" ,\n     \"id\" :  \"my-subscription-id\" ,\n     \"isDefault\" : true,\n     \"name\" :  \"my-subscription-name\" ,\n     \"state\" :  \"Enabled\" ,\n     \"tenantId\" :  \"my-tenant-id\" ,\n     \"user\" :  { \n       \"name\" :  \"my-user-name\" ,\n       \"type\" :  \"user\" \n     } \n   }  ]   \nNote:\nIf `tenantId` is not present, you may be using a personal account to log in to your Azure subscription. Switch to using work or school account.\nIf you are using Azure cloud in China, you should switch the cloud from `AzureCloud` to `AzureChinaCloud`.\nIf you are using Azure cloud in Azure Government, you should switch the cloud from `AzureCloud` to `AzureUSGovernment`.\nIf you are using Azure cloud in German Cloud, you should switch the cloud from `AzureCloud` to `AzureGermanCloud`.  Once you've determined your subscription ID, switch to using that account:  $ az account  set  --subscription my-subscription-id  Register the required providers:  $ az provider register --namespace Microsoft.Network\n$ az provider register --namespace Microsoft.Storage\n$ az provider register --namespace Microsoft.Compute",
            "title": "Subscription "
        },
        {
            "location": "/azure-resources/#client",
            "text": "Azure CPI needs client ID and secret to make authenticated requests.  $ az ad app create --display-name  \"mycpi\"  --password client-secret --identifier-uris  \"http://mycpi\"  --homepage  \"http://mycpi\"  { \n   \"appId\" :  \"my-app-id\" ,\n   \"appPermissions\" : null,\n   \"availableToOtherTenants\" : false,\n   \"displayName\" :  \"mycpi\" ,\n   \"homepage\" :  \"http://mycpi\" ,\n   \"identifierUris\" :  [ \n     \"http://mycpi\" \n   ] ,\n   \"objectId\" :  \"my-object-id\" ,\n   \"objectType\" :  \"Application\" ,\n   \"replyUrls\" :  []  }   Application ID ( my-app-id  in the above output) is the client ID and specified password ( client-secret  in above example) is the client secret.  Finally create service principal to enable authenticated access:  $ az ad sp create --id my-app-id\n$ az role assignment create --role  \"Contributor\"  --assignee  \"http://mycpi\"  --scope /subscriptions/my-subscription-id",
            "title": "Client "
        },
        {
            "location": "/azure-resources/#resource-group",
            "text": "Create a resource group in one of the supported  Azure locations :  $ az group create --name bosh-res-group --location  \"Central US\" \n\n$ az group show --name bosh-res-group { \n   \"id\" :  \"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group\" ,\n   \"location\" :  \"centralus\" ,\n   \"managedBy\" : null,\n   \"name\" :  \"bosh-res-group\" ,\n   \"properties\" :  { \n     \"provisioningState\" :  \"Succeeded\" \n   } ,\n   \"tags\" : null }   Make sure to wait for 'Provisioning State' to become  Succeeded .",
            "title": "Resource Group "
        },
        {
            "location": "/azure-resources/#virtual-network-subnet",
            "text": "Create a virtual network:  $ az network vnet create --name boshnet --address-prefixes  10 .0.0.0/8 --resource-group bosh-res-group --location  \"Central US\"  --dns-server  168 .64.129.16\n$ az network vnet subnet create --name bosh --address-prefix  10 .0.0.0/24 --vnet-name boshnet --resource-group bosh-res-group\n\n$ az network vnet show --name boshnet --resource-group bosh-res-group { \n   \"addressSpace\" :  { \n     \"addressPrefixes\" :  [ \n       \"10.0.0.0/8\" \n     ] \n   } ,\n   \"dhcpOptions\" :  { \n     \"dnsServers\" :  [ \n       \"168.64.129.16\" \n     ] \n   } ,\n   \"enableDdosProtection\" : false,\n   \"enableVmProtection\" : false,\n   \"etag\" :  \"W/\\\"e62167ab-0e8c-4e78-9d78-18d1a963da2e\\\"\" ,\n   \"id\" :  \"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/virtualNetworks/boshnet\" ,\n   \"location\" :  \"centralus\" ,\n   \"name\" :  \"boshnet\" ,\n   \"provisioningState\" :  \"Succeeded\" ,\n   \"resourceGroup\" :  \"bosh-res-group\" ,\n   \"resourceGuid\" :  \"9fa33ff9-2a3f-4139-8e03-af0cc3af67a0\" ,\n   \"subnets\" :  [ \n     { \n       \"addressPrefix\" :  \"10.0.0.0/24\" ,\n       \"etag\" :  \"W/\\\"e62167ab-0e8c-4e78-9d78-18d1a963da2e\\\"\" ,\n       \"id\" :  \"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/virtualNetworks/boshnet/subnets/bosh\" ,\n       \"ipConfigurations\" : null,\n       \"name\" :  \"bosh\" ,\n       \"networkSecurityGroup\" : null,\n       \"provisioningState\" :  \"Succeeded\" ,\n       \"resourceGroup\" :  \"bosh-res-group\" ,\n       \"resourceNavigationLinks\" : null,\n       \"routeTable\" : null,\n       \"serviceEndpoints\" : null\n     } \n   ] ,\n   \"tags\" :  {} ,\n   \"type\" :  \"Microsoft.Network/virtualNetworks\" ,\n   \"virtualNetworkPeerings\" :  []  }",
            "title": "Virtual Network &amp; Subnet "
        },
        {
            "location": "/azure-resources/#network-security-group",
            "text": "Create two network security groups:  $ az network nsg create --resource-group bosh-res-group --location  \"Central US\"  --name nsg-bosh\n$ az network nsg create --resource-group bosh-res-group --location  \"Central US\"  --name nsg-cf\n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority  200  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'ssh'  --destination-port-range  22 \n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority  201  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'bosh-agent'  --destination-port-range  6868 \n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol Tcp --direction Inbound --priority  202  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'bosh-director'  --destination-port-range  25555 \n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-bosh --access Allow --protocol  '*'  --direction Inbound --priority  203  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'dns'  --destination-port-range  53 \n\n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-cf --access Allow --protocol Tcp --direction Inbound --priority  201  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'cf-https'  --destination-port-range  443 \n$ az network nsg rule create --resource-group bosh-res-group --nsg-name nsg-cf --access Allow --protocol Tcp --direction Inbound --priority  202  --source-address-prefix Internet --source-port-range  '*'  --destination-address-prefix  '*'  --name  'cf-log'  --destination-port-range  4443",
            "title": "Network Security Group "
        },
        {
            "location": "/azure-resources/#public-ips",
            "text": "To make certain VMs publicly accessible, you will need to create a Public IP. If Azure Availability Zones is used in  AZs , the Public IP should be created with type  Standard SKU ; otherwise, you can use the default  Basic SKU .  $ az network public-ip create --name my-public-ip --allocation-method Static --resource-group bosh-res-group --location  \"Central US\"  --sku Basic  # sku should be `Standard' when using Azure Availability Zones \n\n$ az network public-ip show --name my-public-ip --resource-group bosh-res-group { \n   \"dnsSettings\" : null,\n   \"etag\" :  \"W/\\\"b3686484-21fe-470a-a059-32d02b4f9589\\\"\" ,\n   \"id\" :  \"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Network/publicIPAddresses/my-public-ip\" ,\n   \"idleTimeoutInMinutes\" :  4 ,\n   \"ipAddress\" :  \"13.89.236.107\" ,\n   \"ipConfiguration\" : null,\n   \"location\" :  \"centralus\" ,\n   \"name\" :  \"my-public-ip\" ,\n   \"provisioningState\" :  \"Succeeded\" ,\n   \"publicIpAddressVersion\" :  \"IPv4\" ,\n   \"publicIpAllocationMethod\" :  \"Static\" ,\n   \"resourceGroup\" :  \"bosh-res-group\" ,\n   \"resourceGuid\" :  \"a25d2f1e-d8f7-4258-9c83-7c30e4a2c270\" ,\n   \"sku\" :  { \n     \"name\" :  \"Basic\" \n   } ,\n   \"tags\" : null,\n   \"type\" :  \"Microsoft.Network/publicIPAddresses\" ,\n   \"zones\" : null }   \nNote:\nYou can skip below section if you are using managed disks with Azure CPI v21+",
            "title": "Public IPs "
        },
        {
            "location": "/azure-resources/#storage-account",
            "text": "Create a default storage account to hold root disks, persistent disks, stemcells, etc.\nIf unsure of desired SKU Name, choose  LRS , desired Kind, choose  Storage :  $ az storage account create --name myboshstore --resource-group bosh-res-group --location  \"Central US\" \n\n$ az storage account show --name myboshstore --resource-group bosh-res-group { \n   \"accessTier\" : null,\n   \"creationTime\" :  \"2017-11-21T03:36:36.568159+00:00\" ,\n   \"customDomain\" : null,\n   \"enableHttpsTrafficOnly\" : false,\n   \"encryption\" :  { \n     \"keySource\" :  \"Microsoft.Storage\" ,\n     \"keyVaultProperties\" : null,\n     \"services\" :  { \n       \"blob\" :  { \n         \"enabled\" : true,\n         \"lastEnabledTime\" :  \"2017-11-21T03:36:36.571160+00:00\" \n       } ,\n       \"file\" :  { \n         \"enabled\" : true,\n         \"lastEnabledTime\" :  \"2017-11-21T03:36:36.571160+00:00\" \n       } ,\n       \"queue\" : null,\n       \"table\" : null\n     } \n   } ,\n   \"id\" :  \"/subscriptions/my-subscription-id/resourceGroups/bosh-res-group/providers/Microsoft.Storage/storageAccounts/myboshstore\" ,\n   \"identity\" : null,\n   \"kind\" :  \"Storage\" ,\n   \"lastGeoFailoverTime\" : null,\n   \"location\" :  \"centralus\" ,\n   \"name\" :  \"myboshstore\" ,\n   \"networkRuleSet\" :  { \n     \"bypass\" :  \"AzureServices\" ,\n     \"defaultAction\" :  \"Allow\" ,\n     \"ipRules\" :  [] ,\n     \"virtualNetworkRules\" :  [] \n   } ,\n   \"primaryEndpoints\" :  { \n     \"blob\" :  \"https://myboshstore.blob.core.windows.net/\" ,\n     \"file\" :  \"https://myboshstore.file.core.windows.net/\" ,\n     \"queue\" :  \"https://myboshstore.queue.core.windows.net/\" ,\n     \"table\" :  \"https://myboshstore.table.core.windows.net/\" \n   } ,\n   \"primaryLocation\" :  \"centralus\" ,\n   \"provisioningState\" :  \"Succeeded\" ,\n   \"resourceGroup\" :  \"bosh-res-group\" ,\n   \"secondaryEndpoints\" :  { \n     \"blob\" :  \"https://myboshstore-secondary.blob.core.windows.net/\" ,\n     \"file\" : null,\n     \"queue\" :  \"https://myboshstore-secondary.queue.core.windows.net/\" ,\n     \"table\" :  \"https://myboshstore-secondary.table.core.windows.net/\" \n   } ,\n   \"secondaryLocation\" :  \"eastus2\" ,\n   \"sku\" :  { \n     \"capabilities\" : null,\n     \"kind\" : null,\n     \"locations\" : null,\n     \"name\" :  \"Standard_RAGRS\" ,\n     \"resourceType\" : null,\n     \"restrictions\" : null,\n     \"tier\" :  \"Standard\" \n   } ,\n   \"statusOfPrimary\" :  \"available\" ,\n   \"statusOfSecondary\" :  \"available\" ,\n   \"tags\" :  {} ,\n   \"type\" :  \"Microsoft.Storage/storageAccounts\"  }   Note: Even if create command returns an error, check whether the storage account is created successfully via `storage account show` command. \n\nOnce storage account is created you can retrieve primary storage access key:\n\n\u0002wzxhzdk:10\u0003\n\n---\n### Storage Account Containers  \n\nCPI expects to find `bosh` and `stemcell` containers within a default storage account:\n\n- `bosh` container is used for storing root and persistent disks.\n- `stemcell` container is used for storing uploaded stemcells. Note: If you are planning to use multiple storage accounts, make sure to set stemcell container permissions to \"Public read access for blobs only\". \n\n\u0002wzxhzdk:11\u0003\n\n---\n### Storage Account Tables  \n\nTo support multiple storage accounts, you need to create the following tables in the default storage account:\n\n- `stemcells` is used to store metadata of stemcells in multiple storage accounts\n\n\u0002wzxhzdk:12\u0003",
            "title": "Storage Account "
        },
        {
            "location": "/openstack-cpi/",
            "text": "This topic describes cloud properties for different resources created by the OpenStack CPI.\n\n\nAZs \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\navailability_zone\n [String, required]: Availability zone to use for creating instances. Example: \neast\n.\n\n\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \neast\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by dynamic network or manual network subnet:\n\n\n\n\nnet_id\n [String, required]: Network ID containing the subnet in which the instance will be created. Example: \nnet-b98ab66e-6fae-4c6a-81af-566e630d21d1\n.\n\n\nsecurity_groups\n [Array, optional]: Array of security groups to apply for all VMs that are placed on this network. Defaults to security groups specified by \ndefault_security_groups\n in the global CPI settings unless security groups are specified on a resource pool/vm type for a VM. If security groups are specified on a resource pool and a network, the resource pool security groups takes precedence since CPI v34+. In older CPI versions prior v34, security groups can either be specified for a network or a resource pool.\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ncloud_properties\n:\n\n      \nnet_id\n:\n \nnet-b98ab66e-6fae-4c6a-81af-566e630d21d1\n\n      \nsecurity_groups\n:\n \n[\nmy-sec-group\n]\n\n\n\n\n\nExample of dynamic network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \ncloud_properties\n:\n\n    \nnet_id\n:\n \nnet-b98ab66e-6fae-4c6a-81af-566e630d21d1\n\n\n\n\n\nExample of vip network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nvip\n\n\n\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ninstance_type\n [String, required]: Type of the instance. Example: \nm1.small\n.\n\n\navailability_zone\n [String, required]: Availability zone to use for creating instances. Example: \neast\n.\n\n\nsecurity_groups\n [Array, optional]: Array of security groups to apply for all VMs that are in this resource pool. Defaults to security groups specified by \ndefault_security_groups\n in the global CPI settings unless security groups are specified on one of the VM networks. If security groups are specified on a resource pool and a network, the resource pool security groups takes precedence since CPI v34+. In older CPI versions prior v34, security groups can either be specified for a network or a resource pool.\n\n\nkey_name\n [String, optional]: Key pair name. Defaults to key pair name specified by \ndefault_key_name\n in the global CPI settings. Example: \nbosh\n.\n\n\nscheduler_hints\n [Hash, optional]: Data passed to the OpenStack Filter scheduler to influence its decision where new VMs can be placed. See \nVM Anti-Affinity\n for a detailed example. Example: \n{ group: af09abf2-2283... }\n\n\nroot_disk\n [Hash, optional]: Custom root disk properties. Requires \nboot_from_volume\n:\n \ntrue\n either \nglobally\n or locally in this VM Type to enable cinder-backed boot volumes. Available in v25+.\n\n\nsize\n [Integer, required]: Specifies the disk size in gigabytes.\n\n\n\n\n\n\nloadbalancer_pools\n [Array, optional]:  Array of Hashes defining LBaaSv2 pools to attach this instance to. Requires neutron LBaaSv2 extension and OpenStack Mitaka or newer. Available in v32+.\n\n\nname\n [String, required]: The name of the LBaaSv2 loadbalancer pool\n\n\nport\n [Integer, required]: The port exposed on the instance\n\n\n\n\n\n\nboot_from_volume\n [Boolean, optional]: Override global \nboot_from_volume\n to enable cinder-backed boot volumes for this VM Type. Available in v34+.\n\n\n\n\nExample of an \nm1.small\n instance:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-openstack-kvm-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n    \navailability_zone\n:\n \neast\n\n\n\n\n\nExample of an \nm1.small\n instance, attached to LBaaSv2 pool named 'my-lb-pool'. Instance exposes port 8080 and is locked down by specific security groups:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nweb-workers\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-openstack-kvm-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n    \navailability_zone\n:\n \neast\n\n    \nsecurity_groups\n:\n \n[\nbosh-vms\n,\n \nlb-accessible\n]\n\n    \nloadbalancer_pools\n:\n\n      \n-\n \nname\n:\n \nmy-lb-pool\n\n        \nport\n:\n \n8080\n\n\n\n\n\nExample of an \nm1.small\n instance with custom root disk size of 50GB:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-openstack-kvm-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n    \navailability_zone\n:\n \neast\n\n    \nroot_disk\n:\n\n      \nsize\n:\n \n50\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ntype\n [String, optional]: Volume type as configured in your OpenStack installation. Example: \nSSD\n\n\n\n\nCinder volumes are created in the availability zone of an instance that volume will be attached.\n\n\nExample of 10GB SSD disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n  \ncloud_properties\n:\n\n    \ntype\n:\n \nSSD\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nSee \nCPI job configuration\n for details.\n\n\nSchema:\n\n\n\n\ndefault_volume_type\n [String, optional]: sets volume type for persistent disks unless overridden in resource pool/VM Type. \ncinder type-list\n will return the available volume types. Example: \nSSD\n.\n\n\n\n\nExample with Keystone V3:\n\n\nproperties\n:\n\n  \nopenstack\n:\n\n    \nauth_url\n:\n \nhttp://pistoncloud.com:5000/v3\n\n    \nusername\n:\n \nchristopher\n\n    \napi_key\n:\n \nQRoqsenPsNGX6\n\n    \nproject\n:\n \nBosh\n\n    \ndomain\n:\n \nsample-domain\n\n    \nregion\n:\n \nRegionOne\n\n    \ndefault_key_name\n:\n \nbosh\n\n    \ndefault_security_groups\n:\n \n[\nbosh\n]\n\n\n\n\n\nExample with Keystone V2 and default volume type \nceph\n:\n\n\nproperties\n:\n\n  \nopenstack\n:\n\n    \nauth_url\n:\n \nhttp://pistoncloud.com:5000/v2.0\n\n    \nusername\n:\n \nchristopher\n\n    \napi_key\n:\n \nQRoqsenPsNGX6\n\n    \ntenant\n:\n \nBosh\n\n    \nregion\n:\n \nRegionOne\n\n    \ndefault_key_name\n:\n \nbosh\n\n    \ndefault_security_groups\n:\n \n[\nbosh\n]\n\n    \ndefault_volume_type\n:\n \nceph\n\n\n\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \neast1\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n\n    \navailability_zone\n:\n \neast2\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nsmall\n\n\n-\n \nname\n:\n \nlarge\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nlarge\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n3000\n\n\n-\n \nname\n:\n \nlarge\n\n  \ndisk_size\n:\n \n50_000\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \naz\n:\n \nz1\n\n    \ndns\n:\n \n[\n8.8.8.8\n]\n\n    \ncloud_properties\n:\n\n      \nnet_id\n:\n \nnet-b98ab66e-6fae-4c6a-81af-566e630d21d1\n\n  \n-\n \nrange\n:\n \n10.10.1.0/24\n\n    \ngateway\n:\n \n10.10.1.1\n\n    \naz\n:\n \nz2\n\n    \ndns\n:\n \n[\n8.8.8.8\n]\n\n    \ncloud_properties\n:\n\n      \nnet_id\n:\n \nnet-85940t48-8ffe-3c3a-81af-27d499ff9842\n\n\n-\n \nname\n:\n \nvip\n\n  \ntype\n:\n \nvip\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \nlarge\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nErrors \n\u00b6\n\n\nCommand 'deploy' failed:\n  Deploying:\n    Creating instance 'bosh/0':\n      Waiting until instance is ready:\n        Starting SSH tunnel:\n          Parsing private key file './bosh.pem':\n            asn1: structure error: superfluous leading zeros in length\n\n\n\n\nIf you're using OpenStack Liberty or Mitaka, you cannot use SSH keys generated by nova with BOSH CLI \ndue to an OpenStack bug\n. OpenStack versions before Liberty and after Mitaka are not affected. As a workaround, \ngenerate your ssh key manually\n and import it to nova.\n\n\nBosh::Clouds::VMCreationFailed\n\n\n\n\nThis error is raised if OpenStack is unable to create a VM. That may happen because:\n\n\n\n\nnot enough resources (vCPUs, RAM, disk) to run the VM. For example if you have selected \nm1.xlarge\n flavor that uses 10 vCPUs and you have 4 hypervisors and each one of them only has 3 vCPUs available, OpenStack is unable to start the VM anywhere even though, total vCPUs across all hypervisors is more than enough.\n\n\n\n\nImage `4c1d6840-6ac7-4b42-bf29-c95fef6d986e' not found\n\n\n\n\nIt's possible that image was deleted from OpenStack directly and BOSH is not aware of it. You can recover with \nbosh upload stemcell X --fix\n to reupload the stemcell.\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nUsing Keystone v2 API",
            "title": "Usage"
        },
        {
            "location": "/openstack-cpi/#azs",
            "text": "Schema for  cloud_properties  section:   availability_zone  [String, required]: Availability zone to use for creating instances. Example:  east .   Example:  azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   east",
            "title": "AZs "
        },
        {
            "location": "/openstack-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by dynamic network or manual network subnet:   net_id  [String, required]: Network ID containing the subnet in which the instance will be created. Example:  net-b98ab66e-6fae-4c6a-81af-566e630d21d1 .  security_groups  [Array, optional]: Array of security groups to apply for all VMs that are placed on this network. Defaults to security groups specified by  default_security_groups  in the global CPI settings unless security groups are specified on a resource pool/vm type for a VM. If security groups are specified on a resource pool and a network, the resource pool security groups takes precedence since CPI v34+. In older CPI versions prior v34, security groups can either be specified for a network or a resource pool.   Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     cloud_properties : \n       net_id :   net-b98ab66e-6fae-4c6a-81af-566e630d21d1 \n       security_groups :   [ my-sec-group ]   Example of dynamic network:  networks :  -   name :   default \n   type :   dynamic \n   cloud_properties : \n     net_id :   net-b98ab66e-6fae-4c6a-81af-566e630d21d1   Example of vip network:  networks :  -   name :   default \n   type :   vip",
            "title": "Networks "
        },
        {
            "location": "/openstack-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   instance_type  [String, required]: Type of the instance. Example:  m1.small .  availability_zone  [String, required]: Availability zone to use for creating instances. Example:  east .  security_groups  [Array, optional]: Array of security groups to apply for all VMs that are in this resource pool. Defaults to security groups specified by  default_security_groups  in the global CPI settings unless security groups are specified on one of the VM networks. If security groups are specified on a resource pool and a network, the resource pool security groups takes precedence since CPI v34+. In older CPI versions prior v34, security groups can either be specified for a network or a resource pool.  key_name  [String, optional]: Key pair name. Defaults to key pair name specified by  default_key_name  in the global CPI settings. Example:  bosh .  scheduler_hints  [Hash, optional]: Data passed to the OpenStack Filter scheduler to influence its decision where new VMs can be placed. See  VM Anti-Affinity  for a detailed example. Example:  { group: af09abf2-2283... }  root_disk  [Hash, optional]: Custom root disk properties. Requires  boot_from_volume :   true  either  globally  or locally in this VM Type to enable cinder-backed boot volumes. Available in v25+.  size  [Integer, required]: Specifies the disk size in gigabytes.    loadbalancer_pools  [Array, optional]:  Array of Hashes defining LBaaSv2 pools to attach this instance to. Requires neutron LBaaSv2 extension and OpenStack Mitaka or newer. Available in v32+.  name  [String, required]: The name of the LBaaSv2 loadbalancer pool  port  [Integer, required]: The port exposed on the instance    boot_from_volume  [Boolean, optional]: Override global  boot_from_volume  to enable cinder-backed boot volumes for this VM Type. Available in v34+.   Example of an  m1.small  instance:  resource_pools :  -   name :   default \n   network :   default \n   stemcell : \n     name :   bosh-openstack-kvm-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     instance_type :   m1.small \n     availability_zone :   east   Example of an  m1.small  instance, attached to LBaaSv2 pool named 'my-lb-pool'. Instance exposes port 8080 and is locked down by specific security groups:  resource_pools :  -   name :   web-workers \n   network :   default \n   stemcell : \n     name :   bosh-openstack-kvm-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     instance_type :   m1.small \n     availability_zone :   east \n     security_groups :   [ bosh-vms ,   lb-accessible ] \n     loadbalancer_pools : \n       -   name :   my-lb-pool \n         port :   8080   Example of an  m1.small  instance with custom root disk size of 50GB:  resource_pools :  -   name :   default \n   network :   default \n   stemcell : \n     name :   bosh-openstack-kvm-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     instance_type :   m1.small \n     availability_zone :   east \n     root_disk : \n       size :   50",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/openstack-cpi/#disk-pools-disk-types",
            "text": "Schema for  cloud_properties  section:   type  [String, optional]: Volume type as configured in your OpenStack installation. Example:  SSD   Cinder volumes are created in the availability zone of an instance that volume will be attached.  Example of 10GB SSD disk:  disk_pools :  -   name :   default \n   disk_size :   10_240 \n   cloud_properties : \n     type :   SSD",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/openstack-cpi/#global-configuration",
            "text": "See  CPI job configuration  for details.  Schema:   default_volume_type  [String, optional]: sets volume type for persistent disks unless overridden in resource pool/VM Type.  cinder type-list  will return the available volume types. Example:  SSD .   Example with Keystone V3:  properties : \n   openstack : \n     auth_url :   http://pistoncloud.com:5000/v3 \n     username :   christopher \n     api_key :   QRoqsenPsNGX6 \n     project :   Bosh \n     domain :   sample-domain \n     region :   RegionOne \n     default_key_name :   bosh \n     default_security_groups :   [ bosh ]   Example with Keystone V2 and default volume type  ceph :  properties : \n   openstack : \n     auth_url :   http://pistoncloud.com:5000/v2.0 \n     username :   christopher \n     api_key :   QRoqsenPsNGX6 \n     tenant :   Bosh \n     region :   RegionOne \n     default_key_name :   bosh \n     default_security_groups :   [ bosh ] \n     default_volume_type :   ceph",
            "title": "Global Configuration "
        },
        {
            "location": "/openstack-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1 \n   cloud_properties : \n     availability_zone :   east1  -   name :   z2 \n   cloud_properties : \n     availability_zone :   east2  vm_types :  -   name :   default \n   cloud_properties : \n     instance_type :   small  -   name :   large \n   cloud_properties : \n     instance_type :   large  disk_types :  -   name :   default \n   disk_size :   3000  -   name :   large \n   disk_size :   50_000  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     az :   z1 \n     dns :   [ 8.8.8.8 ] \n     cloud_properties : \n       net_id :   net-b98ab66e-6fae-4c6a-81af-566e630d21d1 \n   -   range :   10.10.1.0/24 \n     gateway :   10.10.1.1 \n     az :   z2 \n     dns :   [ 8.8.8.8 ] \n     cloud_properties : \n       net_id :   net-85940t48-8ffe-3c3a-81af-27d499ff9842  -   name :   vip \n   type :   vip  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   large \n   network :   default",
            "title": "Example Cloud Config "
        },
        {
            "location": "/openstack-cpi/#errors",
            "text": "Command 'deploy' failed:\n  Deploying:\n    Creating instance 'bosh/0':\n      Waiting until instance is ready:\n        Starting SSH tunnel:\n          Parsing private key file './bosh.pem':\n            asn1: structure error: superfluous leading zeros in length  If you're using OpenStack Liberty or Mitaka, you cannot use SSH keys generated by nova with BOSH CLI  due to an OpenStack bug . OpenStack versions before Liberty and after Mitaka are not affected. As a workaround,  generate your ssh key manually  and import it to nova.  Bosh::Clouds::VMCreationFailed  This error is raised if OpenStack is unable to create a VM. That may happen because:   not enough resources (vCPUs, RAM, disk) to run the VM. For example if you have selected  m1.xlarge  flavor that uses 10 vCPUs and you have 4 hypervisors and each one of them only has 3 vCPUs available, OpenStack is unable to start the VM anywhere even though, total vCPUs across all hypervisors is more than enough.   Image `4c1d6840-6ac7-4b42-bf29-c95fef6d986e' not found  It's possible that image was deleted from OpenStack directly and BOSH is not aware of it. You can recover with  bosh upload stemcell X --fix  to reupload the stemcell.   Back to Table of Contents  Next:  Using Keystone v2 API",
            "title": "Errors "
        },
        {
            "location": "/openstack-auto-anti-affinity/",
            "text": "Note: This feature is available with bosh-openstack-cpi v36+.\n\n\n\nNote: This feature is available with OpenStack Mitaka and higher.\n\n\n\nIn OpenStack, you can use server groups with different policies to influence how VMs are placed on the available hypervisors. In OpenStack Mitaka the policy \nsoft-anti-affinity\n was added, allowing for a best-effort approach to place VMs within a server group on different hypervisors. This means the VM creation does not fail, even when the VM needs to be placed on a hypervisor that does already contain a VM from the same server group.\n\n\nThe OpenStack CPI can automatically create a server group for each instance group in your deployment manifest. Therefore, all instances within an instance group will be placed on different hypervisors in your OpenStack, if hypervisor capacity allows. The server groups are created with the following naming schema \n<Director UUID>-<Deployment name>-<Instance group name>\n.\n\n\nWith bosh-deployment, you can enable auto-anti-affinity with a \nseparate ops-file\n. Alternatively, you can set the necessary property manually:\n\n\n\n\n\n\nEnable auto-anti-affinity in your Director manifest\n\n\nproperties\n:\n\n  \n(...)\n\n  \nopenstack\n:\n \n&openstack\n\n    \nenable_auto_anti_affinity\n:\n \ntrue\n\n\n\n1. Deploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nUsing Keystone v2\n\n\nPrevious: \nOpenStack",
            "title": "Using Auto-anti-affinity"
        },
        {
            "location": "/openstack-human-readable-vm-names/",
            "text": "Note: This feature is available with bosh-openstack-cpi v23+.\n\n\n\nYou can enable human-readable VM names in your Director manifest to get VMs with names such as \nrunner_z1/0\n instead of UUIDs such as \nvm-3151dbb0-7cea-475b-9ff8-7faa94a8188e\n.\n\n\n\n\n\n\nEnable the human-readable-vm-names feature\n\n\nproperties\n:\n\n  \nopenstack\n:\n\n    \nhuman_readable_vm_names\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nSet the \nregistry.endpoint\n configuration to \ninclude basic auth credentials\n\n\n\n\n\n\nDeploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nValidating self-signed OpenStack endpoints\n\n\nPrevious: \nExtended Registry configuration",
            "title": "Using Human-readable VM names"
        },
        {
            "location": "/openstack-keystonev2/",
            "text": "Default configuration typically uses Keystone v3 API. This document describes how to use Keystone v2 if your OpenStack installation enforces this.\n\n\n\n\n\n\nConfigure OpenStack CPI\n\n\nIn \nproperties.openstack\n:\n- switch property \nauth_url\n to use v2 endpoint.\n    \nNote: path is \nv2.0\n including the minor revision!\n\n- add property \ntenant\n\n- remove properties \ndomain\n and \nproject\n\n\nproperties\n:\n\n  \nopenstack\n:\n \n&openstack\n\n    \nauth_url\n:\n \nhttps://keystone.my-openstack.com:5000/v2.0\n \n# <--- Replace with Keystone URL\n\n    \ntenant\n:\n \nOPENSTACK-TENANT\n \n# <--- Replace with OpenStack tenant name\n\n    \nusername\n:\n \nOPENSTACK-USERNAME\n \n# <--- Replace with OpenStack username\n\n    \napi_key\n:\n \nOPENSTACK-PASSWORD\n \n# <--- Replace with OpenStack password\n\n    \ndefault_key_name\n:\n \nbosh\n\n    \ndefault_security_groups\n:\n \n[\nbosh\n]\n\n\n\n\n\n\n\n\n\nDeploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nUsing nova-networking\n\n\nPrevious: \nUsing Auto-anti-affinity",
            "title": "Using Keystone API v2"
        },
        {
            "location": "/openstack-light-stemcells/",
            "text": "Note: This feature is available with bosh-openstack-cpi v28+.\n\n\n\nNote: This feature is available with bosh-cli v2.0.40+.\n\n\n\nYou can create your own OpenStack light stemcells to re-use stemcell images already uploaded to your OpenStack image store. \nNote:\n Future deployments will fail if the stemcell image referenced by a light stemcell is removed from your OpenStack image store.\n\n\n\n\nDownload the heavy stemcell for which you want to create a light stemcell\n\n\nUpload the stemcell to your OpenStack with \nbosh upload-stemcell\n\n\nRetrieve the UUID and version of the uploaded stemcell image with \nbosh stemcells\n\n\n\n\nUse \nbosh repack-stemcell\n to create a light stemcell archive from a heavy stemcell\n\n\n$ bosh repack-stemcell --version \n\"<Stemcell version>\"\n \n\\\n--empty-image \n\\\n--format openstack-light \n\\\n--cloud-properties\n=\n\"{\\\"image_id\\\": \\\"<Stemcell UUID>\\\"}\"\n \n\\\nheavy-stemcell.tgz ./light-bosh-stemcell-<Stemcell version>-openstack-kvm-ubuntu-trusty-go_agent.tgz\n\n\n\n\n\n\nYou can use the light stemcell archive like a regular stemcell archive in BOSH deployment manifests and with \nbosh create-env\n command.\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nMulti-homed VMs",
            "title": "Using Light Stemcells"
        },
        {
            "location": "/openstack-nova-networking/",
            "text": "Note: This feature is available with bosh-openstack-cpi v28+.\n\n\n\nThe OpenStack CPI v28+ uses neutron networking by default. This document describes how to enable nova-networking instead if your OpenStack installation doesn't provide neutron. \nNote:\n nova-networking is deprecated as of the OpenStack Newton release and will be removed in the future.\n\n\n\n\n\n\nConfigure OpenStack CPI\n\n\nIn \nproperties.openstack\n:\n- add property \nuse_nova_networking\n:\n \ntrue\n\n\nproperties\n:\n\n  \nopenstack\n:\n \n&openstack\n\n    \n(...)\n\n    \nuse_nova_networking\n:\n \ntrue\n\n\n\n\n\n\n\n\n\nDeploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nExtended Registry configuration\n\n\nPrevious: \nUsing Keystone API v2",
            "title": "Using nova-networking"
        },
        {
            "location": "/openstack-multiple-networks/",
            "text": "Note: This feature is available with bosh-openstack-cpi v24+.\n\n\n\nNote: This feature requires OpenStack Neutron.\n\n\n\nLimitation: This feature requires DHCP to be disabled\n\u00b6\n\n\nDisabling DHCP means that the network devices on your VMs will not get configuration\nsuch as default gateway, DNS, and MTU. If you require specific values for these settings,\nyou will need to set them by other means.\n\n\n\n\nIn your Director deployment manifest, set [\nproperties.openstack.use_dhcp: false\n]\n   (\nhttps://bosh.io/jobs/openstack_cpi?source=github.com/cloudfoundry-incubator/bosh-openstack-cpi-release#p=openstack.use_dhcp\n).\n   This means the BOSH agent will configure the network devices without DHCP. This is a Director-wide setting\n   and switches off DHCP for all VMs deployed with this Director.\n\n\nIn your Director deployment manifest, set [\nproperties.openstack.config_drive: cdrom\n]\n   (\nhttps://bosh.io/jobs/openstack_cpi?source=github.com/cloudfoundry-incubator/bosh-openstack-cpi-release#p=openstack.config_drive\n).\n   This means OpenStack will mount a cdrom drive to distribute meta-data and user-data instead of using an HTTP metadata service.\n\n\nIn your \nBOSH network configuration\n, set \ngateway\n and \ndns\n to allow outbound communication.\n\n\nIf you're not using VLAN, but a tunnel mechanism for Neutron networking, you also need to set the MTU for your network devices on \nall\n VMs:\n\n\nGRE Tunnels incur an overhead of 42 bytes, therefore set your MTU to \n1458\n\n\nVXLAN Tunnels incur an overhead of 50 bytes, therefore set your MTU to \n1450\n\n   \nNote: The above numbers assume that you're using an MTU of 1500 for the physical network. If your physical network is setup differently, adapt the MTU values accordingly.\n\n\n\n\nSetting the MTU for network devices is currently not possible in the deployment manifest's \nnetworks\n section and thus requires manual user interaction. We recommend to co-locate the \nnetworking-release\n's \nset_mtu\n job using \naddons\n.\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nUsing Light Stemcells\n\n\nPrevious: \nValidating self-signed OpenStack endpoints",
            "title": "Multi-homed VMs"
        },
        {
            "location": "/openstack-multiple-networks/#limitation-this-feature-requires-dhcp-to-be-disabled",
            "text": "Disabling DHCP means that the network devices on your VMs will not get configuration\nsuch as default gateway, DNS, and MTU. If you require specific values for these settings,\nyou will need to set them by other means.   In your Director deployment manifest, set [ properties.openstack.use_dhcp: false ]\n   ( https://bosh.io/jobs/openstack_cpi?source=github.com/cloudfoundry-incubator/bosh-openstack-cpi-release#p=openstack.use_dhcp ).\n   This means the BOSH agent will configure the network devices without DHCP. This is a Director-wide setting\n   and switches off DHCP for all VMs deployed with this Director.  In your Director deployment manifest, set [ properties.openstack.config_drive: cdrom ]\n   ( https://bosh.io/jobs/openstack_cpi?source=github.com/cloudfoundry-incubator/bosh-openstack-cpi-release#p=openstack.config_drive ).\n   This means OpenStack will mount a cdrom drive to distribute meta-data and user-data instead of using an HTTP metadata service.  In your  BOSH network configuration , set  gateway  and  dns  to allow outbound communication.  If you're not using VLAN, but a tunnel mechanism for Neutron networking, you also need to set the MTU for your network devices on  all  VMs:  GRE Tunnels incur an overhead of 42 bytes, therefore set your MTU to  1458  VXLAN Tunnels incur an overhead of 50 bytes, therefore set your MTU to  1450 \n    Note: The above numbers assume that you're using an MTU of 1500 for the physical network. If your physical network is setup differently, adapt the MTU values accordingly.   Setting the MTU for network devices is currently not possible in the deployment manifest's  networks  section and thus requires manual user interaction. We recommend to co-locate the  networking-release 's  set_mtu  job using  addons .   Back to Table of Contents  Next:  Using Light Stemcells  Previous:  Validating self-signed OpenStack endpoints",
            "title": "Limitation: This feature requires DHCP to be disabled"
        },
        {
            "location": "/openstack-registry/",
            "text": "Note: We are actively pursuing to remove the Registry to simplify BOSH architecture.\n\n\n\nDefault configuration typically uses Registry and defaults it to IP source authentication. Due to certain networking configurations (NAT) IP source authentication may not work correctly, hence switching to basic authentication is necessary.\n\n\n\n\n\n\nConfigure the Registry and the OpenStack CPI to use basic authentication by setting \nregistry.endpoint\n\n\nproperties\n:\n\n  \nregistry\n:\n\n    \naddress\n:\n \nPRIVATE-IP\n\n    \nhost\n:\n \nPRIVATE-IP\n\n    \ndb\n:\n \n*db\n\n    \nhttp\n:\n \n{\nuser\n:\n \nadmin\n,\n \npassword\n:\n \nadmin-password\n,\n \nport\n:\n \n25777\n}\n\n    \nusername\n:\n \nadmin\n\n    \npassword\n:\n \nadmin-password\n\n    \nport\n:\n \n25777\n\n    \nendpoint\n:\n \nhttp://admin:admin-password@PRIVATE-IP:25777\n \n# <---\n\n\n\n\n\n\n\n\n\nDeploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nUsing human-readable VM names\n\n\nPrevious: \nUsing nova-networking",
            "title": "Extended Registry configuration"
        },
        {
            "location": "/openstack-self-signed-endpoints/",
            "text": "Note: This feature is available with bosh-openstack-cpi v23+.\n\n\n\nWhen your OpenStack is using a self-signed certificate, you want to enable the OpenStack CPI to validate it. You can configure the OpenStack CPI with the public certificate of the RootCA that signed the OpenStack endpoint certificate.\n\n\n\n\n\n\nConfigure \nproperties.openstack.connection_options\n to include the property \nca_cert\n. It can contain one or more certificates.\n\n\nproperties\n:\n\n  \nopenstack\n:\n\n    \nconnection_options\n:\n\n      \nca_cert\n:\n \n|+\n\n        \n-----BEGIN CERTIFICATE-----\n\n        \nMII...\n\n        \n-----END CERTIFICATE-----\n\n\n\n\n\n\n\n\n\nSet the \nregistry.endpoint\n configuration to \ninclude basic auth credentials\n\n\n\n\n\n\nDeploy the Director\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nMulti-homed VMs\n\n\nPrevious: \nUsing human-readable VM names",
            "title": "Self-signed Endpoints"
        },
        {
            "location": "/rackhd-cpi/",
            "text": "RackHD CPI\n works with \nOpenStack raw stemcells\n.\n\n\nSee more details on Github: [\nhttps://github.com/cloudfoundry-incubator/bosh-rackhd-cpi-release\n].\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/softlayer-cpi/",
            "text": "This topic describes cloud properties for different resources created by the SoftLayer CPI.\n\n\nAZs \n\u00b6\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \nDatacenter\n:\n \n{\n \nName\n:\n \nlon02\n \n}\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nExample of dynamic network (both public and private networks are available):\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \nsubnets\n:\n\n  \n-\n \naz\n:\n \nz1\n\n    \ndns\n:\n \n[\n10.1.2.3\n,\n \n10.0.80.11\n,\n \n10.0.80.12\n]\n\n    \ncloud_properties\n:\n\n      \nPrimaryNetworkComponent\n:\n\n         \nNetworkVlan\n:\n\n            \nId\n:\n \n524956\n\n      \nPrimaryBackendNetworkComponent\n:\n\n         \nNetworkVlan\n:\n\n            \nId\n:\n \n524954\n\n\n\n\n\nExample of dynamic network (only private network is available):\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \nsubnets\n:\n\n  \n-\n \naz\n:\n \nz1\n\n    \ndns\n:\n \n[\n10.1.2.3\n,\n \n10.0.80.11\n,\n \n10.0.80.12\n]\n\n    \ncloud_properties\n:\n\n      \nPrivateNetworkOnlyFlag\n:\n \ntrue\n\n      \nPrimaryBackendNetworkComponent\n:\n\n         \nNetworkVlan\n:\n\n             \nId\n:\n \n524954\n\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nmanual_network\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.112.166.128/26\n\n    \ngateway\n:\n \n10.112.166.129\n\n    \ndns\n:\n\n    \n-\n \n10.1.2.3\n\n    \n-\n \n10.0.80.11\n\n    \n-\n \n10.0.80.12\n\n    \nreserved\n:\n\n    \n-\n \n10.112.166.128\n\n    \n-\n \n10.112.166.129\n\n    \n-\n \n10.112.166.130\n\n    \n-\n \n10.112.166.131\n\n    \nstatic\n:\n\n    \n-\n \n10.112.166.132 - 10.112.166.162\n\n\n\n\n\nCurrently SoftLayer CPI does not support vip network.\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nDomain\n [String, required]: Name of the domain. Example: \nsoftlayer.com\n.\n\n\nVmNamePrefix\n [String, required]: Prefix of the vm name. Example: \nbosh-softlayer\n. Please note that, for bosh director, this property is the full hostname, and for other VMs in deployments, a timestamp will be appended to the property value to make the hostname.\n\n\nEphemeralDiskSize\n [Integer, required]: Ephemeral disk size in gigabyte. Example: \n100\n.\n\n\nStartCpus\n [Integer, required]: Number of CPUs. Example: \n4\n.\n\n\nMaxMemory\n [Integer, required]: Memory in megabytes. Example: \n8192\n.\n\n\nDatacenter\n :\n\n\nName\n [String, required]: Name of the datacenter. Example: \nlon02\n.\n\n\n\n\n\n\nHourlyBillingFlag\n [Boolean, optional]: If the vm is hourly billing. Default is \nfalse\n. \n\n\n\n\nExample:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nvms\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nurl\n:\n \nlight-bosh-stemcell-3169-softlayer-esxi-ubuntu-trusty-go_agent\n\n  \ncloud_properties\n:\n\n    \nDomain\n:\n \nsoftlayer.com\n\n    \nVmNamePrefix\n:\n \nbosh-softlayer\n\n    \nEphemeralDiskSize\n:\n \n100\n\n    \nStartCpus\n:\n \n4\n\n    \nMaxMemory\n:\n \n8192\n\n    \nDatacenter\n:\n\n       \nName\n:\n \nlon02\n\n    \nHourlyBillingFlag\n:\n \ntrue\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nIops\n [Integer, optional]: input/output operations per second (IOPS) value. Example: \n1000\n. If it's not set, a medium IOPS value of the specified disk size will be chosen.\n\n\nUseHourlyPricing\n [Boolean, optional]: If the disk is hourly pricing. Default is \nfalse\n.\n\n\n\n\nExample of 100GB disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndisks\n\n  \ndisk_size\n:\n \n100_000\n\n  \ncloud_properties\n:\n\n    \nIops\n:\n \n1000\n\n    \nUseHourlyPricing\n:\n \ntrue\n\n\n\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \nDatacenter\n:\n \n{\n \nName\n:\n \nlon02\n  \n}\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ncompilation\n\n  \ncloud_properties\n:\n\n    \nBosh_ip\n:\n \n10.1.2.3\n\n    \nStartCpus\n:\n  \n4\n\n    \nMaxMemory\n:\n  \n8192\n\n    \nEphemeralDiskSize\n:\n \n100\n\n    \nHourlyBillingFlag\n:\n \ntrue\n\n    \nVmNamePrefix\n:\n \nsl-compilation-worker-\n\n\n-\n \nname\n:\n \nsl-server\n\n  \ncloud_properties\n:\n\n    \nBosh_ip\n:\n \n10.1.2.3\n\n    \nStartCpus\n:\n  \n4\n\n    \nMaxMemory\n:\n  \n8192\n\n    \nEphemeralDiskSize\n:\n \n100\n\n    \nHourlyBillingFlag\n:\n \ntrue\n\n    \nVmNamePrefix\n:\n \nsl-\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n50_000\n\n\n-\n \nname\n:\n \nlarge\n\n  \ndisk_size\n:\n \n500_000\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n  \nsubnets\n:\n\n  \n-\n \naz\n:\n \nz1\n\n  \n-\n \ndns\n:\n \n[\n10.1.2.3\n,\n \n10.0.80.11\n,\n \n10.0.80.12\n]\n\n  \ncloud_properties\n:\n\n    \nPrimaryNetworkComponent\n:\n\n       \nNetworkVlan\n:\n\n          \nId\n:\n \n524956\n\n    \nPrimaryBackendNetworkComponent\n:\n\n       \nNetworkVlan\n:\n\n          \nId\n:\n \n524954\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \ncompilation\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\nThe \nBosh_ip\n property specified under \ncloud_properties\n is used for SoftLayer CPI to differentiate the director and common vms. The one with cloud_property \nBosh_ip\n is a common vm. The one without \nBosh_ip\n is the director.\n\n\nPlease notice that when the VM hostname length is exactly 64, the deployment is failing due to ssh problem. This is SoftLayer\u2019s limitation which can\u2019t be fixed in a short term. We have a work around in the CPI that when the hostname with 64 characters is identified, a padding \"-1\" is appended to make it longer than 64.\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/softlayer-cpi/#azs",
            "text": "Example:  azs :  -   name :   z1 \n   cloud_properties : \n     Datacenter :   {   Name :   lon02   }",
            "title": "AZs "
        },
        {
            "location": "/softlayer-cpi/#networks",
            "text": "Example of dynamic network (both public and private networks are available):  networks :  -   name :   default \n   type :   dynamic \n   subnets : \n   -   az :   z1 \n     dns :   [ 10.1.2.3 ,   10.0.80.11 ,   10.0.80.12 ] \n     cloud_properties : \n       PrimaryNetworkComponent : \n          NetworkVlan : \n             Id :   524956 \n       PrimaryBackendNetworkComponent : \n          NetworkVlan : \n             Id :   524954   Example of dynamic network (only private network is available):  networks :  -   name :   default \n   type :   dynamic \n   subnets : \n   -   az :   z1 \n     dns :   [ 10.1.2.3 ,   10.0.80.11 ,   10.0.80.12 ] \n     cloud_properties : \n       PrivateNetworkOnlyFlag :   true \n       PrimaryBackendNetworkComponent : \n          NetworkVlan : \n              Id :   524954   Example of manual network:  networks :  -   name :   manual_network \n   type :   manual \n   subnets : \n   -   range :   10.112.166.128/26 \n     gateway :   10.112.166.129 \n     dns : \n     -   10.1.2.3 \n     -   10.0.80.11 \n     -   10.0.80.12 \n     reserved : \n     -   10.112.166.128 \n     -   10.112.166.129 \n     -   10.112.166.130 \n     -   10.112.166.131 \n     static : \n     -   10.112.166.132 - 10.112.166.162   Currently SoftLayer CPI does not support vip network.",
            "title": "Networks "
        },
        {
            "location": "/softlayer-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   Domain  [String, required]: Name of the domain. Example:  softlayer.com .  VmNamePrefix  [String, required]: Prefix of the vm name. Example:  bosh-softlayer . Please note that, for bosh director, this property is the full hostname, and for other VMs in deployments, a timestamp will be appended to the property value to make the hostname.  EphemeralDiskSize  [Integer, required]: Ephemeral disk size in gigabyte. Example:  100 .  StartCpus  [Integer, required]: Number of CPUs. Example:  4 .  MaxMemory  [Integer, required]: Memory in megabytes. Example:  8192 .  Datacenter  :  Name  [String, required]: Name of the datacenter. Example:  lon02 .    HourlyBillingFlag  [Boolean, optional]: If the vm is hourly billing. Default is  false .    Example:  resource_pools :  -   name :   vms \n   network :   default \n   stemcell : \n     url :   light-bosh-stemcell-3169-softlayer-esxi-ubuntu-trusty-go_agent \n   cloud_properties : \n     Domain :   softlayer.com \n     VmNamePrefix :   bosh-softlayer \n     EphemeralDiskSize :   100 \n     StartCpus :   4 \n     MaxMemory :   8192 \n     Datacenter : \n        Name :   lon02 \n     HourlyBillingFlag :   true",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/softlayer-cpi/#disk-pools-disk-types",
            "text": "Schema for  cloud_properties  section:   Iops  [Integer, optional]: input/output operations per second (IOPS) value. Example:  1000 . If it's not set, a medium IOPS value of the specified disk size will be chosen.  UseHourlyPricing  [Boolean, optional]: If the disk is hourly pricing. Default is  false .   Example of 100GB disk:  disk_pools :  -   name :   disks \n   disk_size :   100_000 \n   cloud_properties : \n     Iops :   1000 \n     UseHourlyPricing :   true",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/softlayer-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1 \n   cloud_properties : \n     Datacenter :   {   Name :   lon02    }  vm_types :  -   name :   compilation \n   cloud_properties : \n     Bosh_ip :   10.1.2.3 \n     StartCpus :    4 \n     MaxMemory :    8192 \n     EphemeralDiskSize :   100 \n     HourlyBillingFlag :   true \n     VmNamePrefix :   sl-compilation-worker-  -   name :   sl-server \n   cloud_properties : \n     Bosh_ip :   10.1.2.3 \n     StartCpus :    4 \n     MaxMemory :    8192 \n     EphemeralDiskSize :   100 \n     HourlyBillingFlag :   true \n     VmNamePrefix :   sl-  disk_types :  -   name :   default \n   disk_size :   50_000  -   name :   large \n   disk_size :   500_000  networks :  -   name :   default \n   type :   dynamic \n   subnets : \n   -   az :   z1 \n   -   dns :   [ 10.1.2.3 ,   10.0.80.11 ,   10.0.80.12 ] \n   cloud_properties : \n     PrimaryNetworkComponent : \n        NetworkVlan : \n           Id :   524956 \n     PrimaryBackendNetworkComponent : \n        NetworkVlan : \n           Id :   524954  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   compilation \n   network :   default   The  Bosh_ip  property specified under  cloud_properties  is used for SoftLayer CPI to differentiate the director and common vms. The one with cloud_property  Bosh_ip  is a common vm. The one without  Bosh_ip  is the director.  Please notice that when the VM hostname length is exactly 64, the deployment is failing due to ssh problem. This is SoftLayer\u2019s limitation which can\u2019t be fixed in a short term. We have a work around in the CPI that when the hostname with 64 characters is identified, a padding \"-1\" is appended to make it longer than 64.   Back to Table of Contents",
            "title": "Example Cloud Config "
        },
        {
            "location": "/virtualbox-cpi/",
            "text": "This topic describes cloud properties for different resources created by the \nVirtualBox CPI\n. VirtualBox CPI works with \nvSphere ESXI stemcells\n.\n\n\nAZs \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for AZs.\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by network subnet:\n\n\n\n\nname\n [String, required]: Name of the network. Example: \nvboxnet0\n. Default: \nvboxnet0\n.\n\n\ntype\n [String, optional]: Type of the network. See \nVBoxManage modifyvm\n networking settings\n for valid values. Example: \nhostonly\n. Default: \nhostonly\n.\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \nprivate\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n   \n192.168.50.0/24\n\n    \ngateway\n:\n \n192.168.50.1\n\n    \ndns\n:\n     \n[\n192.168.50.1\n]\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nvboxnet0\n\n\n\n\n\n\n\nVM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ncpus\n [Integer, optional]: Number of CPUs. Example: \n1\n. Default: \n1\n.\n\n\nmemory\n [Integer, optional]: RAM in megabytes. Example: \n1024\n. Default: \n512\n.\n\n\nephemeral_disk\n [Integer, optional]: Ephemeral disk size in megabytes. Example: \n10240\n. Default: \n5000\n.\n\n\nparavirtprovider\n [String, optional]: Paravirtual provider type. See \nVBoxManage modifyvm\n general settings\n for valid values. Default: \nminimal\n.\n\n\n\n\nExample of a VM type:\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ncpus\n:\n \n2\n\n    \nmemory\n:\n \n2_048\n\n    \nephemeral_disk\n:\n \n4_096\n\n    \nparavirtprovider\n:\n \nminimal\n\n\n\n\n\n\n\nDisk Types \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for disks.\n\n\nExample of 10GB disk:\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nThe CPI uses individual VirtualBox VMs and disks. Since the CPI can only talk to a single VirtualBox server it can only manage resources on a single machine.\n\n\nExample of a CPI configuration:\n\n\nproperties\n:\n\n  \nhost\n:\n \n192.168.50.1\n\n  \nusername\n:\n \nubuntu\n\n  \nprivate_key\n:\n \n|\n\n    \n-----BEGIN RSA PRIVATE KEY-----\n\n    \nMIIEowIBAAKCAQEAr/c6pUbrq/U+s0dSU+Z6dxrHC7LOGDijv8LYN5cc7alYg+TV\n\n    \n...\n\n    \nfe5h79YLG+gJDqVQyKJm0nDRCVz0IkM7Nhz8j07PNJzWjee/kcvv\n\n    \n-----END RSA PRIVATE KEY-----\n\n\n  \nagent\n:\n \n{\nmbus\n:\n \n\"https://mbus:mbus-password@0.0.0.0:6868\"\n}\n\n\n  \nntp\n:\n\n  \n-\n \n0.pool.ntp.org\n\n  \n-\n \n1.pool.ntp.org\n\n\n  \nblobstore\n:\n\n    \nprovider\n:\n \nlocal\n\n    \npath\n:\n \n/var/vcap/micro_bosh/data/cache\n\n\n\n\n\nSee \nvirtualbox_cpi job\n for more details.\n\n\n\n\nExample Cloud Config\n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n-\n \nname\n:\n \nz2\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n3000\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n192.168.50.0/24\n\n    \ngateway\n:\n \n192.168.50.1\n\n    \nazs\n:\n \n[\nz1\n,\n \nz2\n]\n\n    \nreserved\n:\n \n[\n192.168.50.6\n]\n\n    \ndns\n:\n \n[\n192.168.50.1\n]\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nvboxnet0\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n2\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/virtualbox-cpi/#azs",
            "text": "Currently the CPI does not support any cloud properties for AZs.  Example:  azs :  -   name :   z1",
            "title": "AZs "
        },
        {
            "location": "/virtualbox-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by network subnet:   name  [String, required]: Name of the network. Example:  vboxnet0 . Default:  vboxnet0 .  type  [String, optional]: Type of the network. See  VBoxManage modifyvm  networking settings  for valid values. Example:  hostonly . Default:  hostonly .   Example of manual network:  networks :  -   name :   private \n   type :   manual \n   subnets : \n   -   range :     192.168.50.0/24 \n     gateway :   192.168.50.1 \n     dns :       [ 192.168.50.1 ] \n     cloud_properties : \n       name :   vboxnet0",
            "title": "Networks "
        },
        {
            "location": "/virtualbox-cpi/#vm-types",
            "text": "Schema for  cloud_properties  section:   cpus  [Integer, optional]: Number of CPUs. Example:  1 . Default:  1 .  memory  [Integer, optional]: RAM in megabytes. Example:  1024 . Default:  512 .  ephemeral_disk  [Integer, optional]: Ephemeral disk size in megabytes. Example:  10240 . Default:  5000 .  paravirtprovider  [String, optional]: Paravirtual provider type. See  VBoxManage modifyvm  general settings  for valid values. Default:  minimal .   Example of a VM type:  vm_types :  -   name :   default \n   cloud_properties : \n     cpus :   2 \n     memory :   2_048 \n     ephemeral_disk :   4_096 \n     paravirtprovider :   minimal",
            "title": "VM Types "
        },
        {
            "location": "/virtualbox-cpi/#disk-types",
            "text": "Currently the CPI does not support any cloud properties for disks.  Example of 10GB disk:  disk_types :  -   name :   default \n   disk_size :   10_240",
            "title": "Disk Types "
        },
        {
            "location": "/virtualbox-cpi/#global-configuration",
            "text": "The CPI uses individual VirtualBox VMs and disks. Since the CPI can only talk to a single VirtualBox server it can only manage resources on a single machine.  Example of a CPI configuration:  properties : \n   host :   192.168.50.1 \n   username :   ubuntu \n   private_key :   | \n     -----BEGIN RSA PRIVATE KEY----- \n     MIIEowIBAAKCAQEAr/c6pUbrq/U+s0dSU+Z6dxrHC7LOGDijv8LYN5cc7alYg+TV \n     ... \n     fe5h79YLG+gJDqVQyKJm0nDRCVz0IkM7Nhz8j07PNJzWjee/kcvv \n     -----END RSA PRIVATE KEY----- \n\n   agent :   { mbus :   \"https://mbus:mbus-password@0.0.0.0:6868\" } \n\n   ntp : \n   -   0.pool.ntp.org \n   -   1.pool.ntp.org \n\n   blobstore : \n     provider :   local \n     path :   /var/vcap/micro_bosh/data/cache   See  virtualbox_cpi job  for more details.",
            "title": "Global Configuration "
        },
        {
            "location": "/virtualbox-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1  -   name :   z2  vm_types :  -   name :   default  disk_types :  -   name :   default \n   disk_size :   3000  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   192.168.50.0/24 \n     gateway :   192.168.50.1 \n     azs :   [ z1 ,   z2 ] \n     reserved :   [ 192.168.50.6 ] \n     dns :   [ 192.168.50.1 ] \n     cloud_properties : \n       name :   vboxnet0  compilation : \n   workers :   2 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   default \n   network :   default    Back to Table of Contents",
            "title": "Example Cloud Config"
        },
        {
            "location": "/vcloud-cpi/",
            "text": "This topic describes cloud properties for different resources created by the vCloud CPI.\n\n\nAZs \n\u00b6\n\n\nCurrently CPI does not support any cloud properties for AZs.\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by manual network subnet:\n\n\n\n\nname\n [String, required]: Name of vApp network in which instance will be created. Example: \nVPC_BOSH\n.\n\n\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nVPC_BOSH\n\n\n\n\n\nvCloud CPI does not support dynamic or vip networks.\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ncpu\n [Integer, required]: Number of CPUs. Example: \n1\n.\n\n\nram\n [Integer, required]: RAM in megabytes. Example: \n1024\n.\n\n\ndisk\n [Integer, required]: Ephemeral disk size in megabytes. Example: \n10240\n\n\n\n\nExample of a VM with 1 CPU, 1GB RAM, and 10GB ephemeral disk:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-vcloud-esxi-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ncpu\n:\n \n1\n\n    \nram\n:\n \n1_024\n\n    \ndisk\n:\n \n10_240\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for disks.\n\n\nExample of 10GB disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/vcloud-cpi/#azs",
            "text": "Currently CPI does not support any cloud properties for AZs.  Example:  azs :  -   name :   z1",
            "title": "AZs "
        },
        {
            "location": "/vcloud-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by manual network subnet:   name  [String, required]: Name of vApp network in which instance will be created. Example:  VPC_BOSH .   Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     cloud_properties : \n       name :   VPC_BOSH   vCloud CPI does not support dynamic or vip networks.",
            "title": "Networks "
        },
        {
            "location": "/vcloud-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   cpu  [Integer, required]: Number of CPUs. Example:  1 .  ram  [Integer, required]: RAM in megabytes. Example:  1024 .  disk  [Integer, required]: Ephemeral disk size in megabytes. Example:  10240   Example of a VM with 1 CPU, 1GB RAM, and 10GB ephemeral disk:  resource_pools :  -   name :   default \n   network :   default \n   stemcell : \n     name :   bosh-vcloud-esxi-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     cpu :   1 \n     ram :   1_024 \n     disk :   10_240",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/vcloud-cpi/#disk-pools-disk-types",
            "text": "Currently the CPI does not support any cloud properties for disks.  Example of 10GB disk:  disk_pools :  -   name :   default \n   disk_size :   10_240    Back to Table of Contents",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/vsphere-cpi/",
            "text": "This topic describes cloud properties for different resources created by the vSphere CPI.\n\n\nAZs \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ndatacenters\n [Array, optional]: Array of datacenters to use for VM placement. Must have only one and it must match datacenter configured in global CPI options.\n\n\nname\n [String, required]: Datacenter name.\n\n\nclusters\n [Array, required]: Array of clusters to use for VM placement.\n\n\n<cluster name>\n [String, required]: Cluster name.\n\n\nresource_pool\n [String, optional]: Name of vSphere Resource Pool to use for VM placement.\n\n\ndrs_rules\n [Array, optional]: Array of DRS rules applied to \nconstrain VM placement\n. Must have only one.\n\n\nname\n [String, required]: Name of a DRS rule that the Director will create.\n\n\ntype\n [String, required]: Type of a DRS rule. Currently only \nseparate_vms\n is supported.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \ndatacenters\n:\n\n    \n-\n \nname\n:\n \nmy-dc\n\n      \nclusters\n:\n\n      \n-\n \n{\nmy-vsphere-cluster\n:\n \n{\nresource_pool\n:\n \nmy-vsphere-res-pool\n}}\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nSchema for \ncloud_properties\n section used by manual network subnet:\n\n\n\n\nname\n [String, required]: Name of the vSphere network. Example: \nVM Network\n.\n\n\n\n\nNote:\n To assign a distributed virtual portgroup when\nthere exists a standard virtual portgroup with the same name,\nprepend the distributed virtual switch's name followed by a slash to the\ndistributed virtual portgroup, e.g. \ndvs/distributed-portgroup-1\n. This may\nbe required when working with VxRack. Available in v28+.\n\n\nNote:\n The name may also be an NSX opaque network. Available in v40+.\n\n\nExample of manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nVM Network\n\n\n\n\n\nvSphere CPI does not support dynamic or vip networks.\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\ncpu\n [Integer, required]: Number of CPUs. Example: \n1\n.\n\n\nram\n [Integer, required]: RAM in megabytes. Example: \n1024\n.\n\n\ndisk\n [Integer, required]: Ephemeral disk size in megabytes. Example: \n10240\n.\n\n\ncpu_hot_add_enabled\n [Boolean, optional]: Allows operator to add additional CPU resources while the VM is on. Default: \nfalse\n. Available in v21+.\n\n\nmemory_hot_add_enabled\n [Boolean, optional]: Allows operator to add additional memory resources while the VM is on. Default: \nfalse\n. Available in v21+.\n\n\nnested_hardware_virtualization\n [Boolean, optional]: Exposes hardware assisted virtualization to the VM. Default: \nfalse\n.\n\n\ndatastores\n [Array, optional]: Allows operator to specify a list of ephemeral datastores, datastore clusters for the VM. Datastore names are exact datastore names and not regex patterns. At least one of these datastores must be accessible from clusters provided in \nresource_pools.cloud_properties\n/\nazs.cloud_properties\n or in the global CPI configuration. Available in v23+. Datastore Clusters can be specified as an array of datastore cluster names. Available in v47+\n\n\ndatacenters\n [Array, optional]: Used to override the VM placement specified under \nazs.cloud_properties\n. The format is the same as under \nAZs\n.\n\n\nnsx\n [Dictionary, optional]: \nVMware NSX\n additions section. Available in CPI v30+ and NSX v6.1+.\n\n\nsecurity_groups\n [Array, optional]: A collection of \nsecurity group\n names that the instances should belong to. The CPI will create the security groups if they do not exist.\nBOSH will also automatically create security groups based on metadata such as deployment name and instance group name. The full list of groups can be seen under \ncreate_vm's environment groups\n.\n\n\nlbs\n [Array, optional]: A collection of \nNSX Edge Load Balancers\n (LBs) to which instances should be attached. The LB and \nServer Pool\n must exist prior to the deployment.\n\n\nedge_name\n [String, required]: Name of the NSX Edge.\n\n\npool_name\n [String, required]: Name of the Edge's Server Pool.\n\n\nsecurity_group\n [String, required]: Name of the Pool's target Security Group. The CPI will add the VM to the specified security group (creating the security group if needed), then add the security group to the specified Server Pool.\n\n\nport\n [Integer, required]: The port that the VM's service is listening on (e.g. 80 for HTTP).\n\n\nmonitor_port\n [Integer, optional]: The healthcheck port that the VM is listening on. Defaults to the value of \nport\n.\n\n\n\n\n\n\n\n\n\n\nvmx_options\n [Dictionary, optional]: Allows operator to specify \nVM advanced configuration options\n. All values are subject to YAML's type interpretation, and given that for certain configuration options vSphere will accept only a specific value type please take note of the difference between values with similar appearances such as: \ntrue\n vs \n\"true\"\n and \n\"1234\"\n vs \n1234\n. Refer to the vSphere documentation for more information about what configuration options are accepted. Available in v42+.\n\n\nnsxt\n [Dictionary, optional]: \nVMware NSX\n additions section. Available in CPI v45+.\n\n\nns_groups\n [Array, optional]: A collection of \nNS Groups\n names that the instances should belong to. Available in NSX-T v1.1+.\n\n\nvif_type\n [String, optional]: Supported types: \nPARENT\n, \nnull\n. Overrides the global \ndefault_vif_type\n. Available in NSX-T v2.0+.\n\n\n\n\n\n\n\n\nExample of a VM asked to be placed into a specific vSphere resource pool with NSX-V and NSX-T integration:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nnsx\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-vsphere-esxi-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \nlatest\n\n  \ncloud_properties\n:\n\n    \ncpu\n:\n \n1\n\n    \nram\n:\n \n1_024\n\n    \ndisk\n:\n \n10_240\n\n    \ndatastores\n:\n \n[\nprod-ds-1\n,\n \nprod-ds-2\n,\n \n{\nclusters\n:\n \n[\nvcpi-sp1\n:\n \n{}\n  \n,\n \nvcpi-sp2\n:\n \n{}]}]\n\n    \ndatacenters\n:\n\n    \n-\n \nname\n:\n \nmy-dc\n\n      \nclusters\n:\n\n      \n-\n \nmy-vsphere-cluster\n:\n \n{\nresource_pool\n:\n \nother-vsphere-res-pool\n}\n\n    \nnsx\n:\n \n# NSX-V configuration\n\n      \nsecurity_groups\n:\n \n[\npublic\n,\n \ndmz\n]\n\n      \nlbs\n:\n\n      \n-\n \nedge_name\n:\n \nmy-lb\n\n        \npool_name\n:\n \nhttps-pool\n\n        \nsecurity_group\n:\n \nhttps-sg\n\n        \nport\n:\n \n443\n\n        \nmonitor_port\n:\n \n4443\n \n# optional, defaults to `port` value\n\n    \nvmx_options\n:\n\n      \nsched.mem.maxmemctl\n:\n \n\"1330\"\n\n    \nnsxt\n:\n \n# NSX-T configuration\n\n      \nns_groups\n:\n \n[\npublic\n,\n \ndmz\n]\n\n      \nvif_type\n:\n \nPARENT\n\n\n\n\n\n\n\nDisk Pools / Disk Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\n\n\ntype\n [String, optional]: Type of the\n  \ndisk\n:\n  \nthick\n, \nthin\n, \npreallocated\n, \neagerZeroedThick\n. Defaults to\n  \npreallocated\n. Available in v12. Overrides the global \ndefault_disk_type\n.\n\n\n\n\n\n\ndatastores\n [Array, optional]: List of datastore names, datastore clusters for storing persistent disks. Overrides the global \npersistent_datastore_pattern\n. These names are exact datastore names and not regex patterns. Available in v29+. Datastore Clusters can be specified as an array of datastore cluster names. Available in v47+\n\n\n\n\n\n\nExample of 10GB disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\nExample of disk with type eagerZeroedThick:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n  \ncloud_properties\n:\n\n    \ntype\n:\n \neagerZeroedThick\n\n\n\n\n\nExample of disk stored in specific datastores:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n  \ncloud_properties\n:\n\n    \ndatastores\n:\n \n[\n'prod-ds-1'\n,\n \n'prod-ds-2'\n,\n \n{\nclusters\n:\n \n[\nvcpi-sp1\n:\n \n{}\n  \n,\n \nvcpi-sp2\n:\n \n{}]}]\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nThe CPI can only talk to a single vCenter installation and manage VMs within a single vSphere datacenter.\n\n\nSchema:\n\n\n\n\nhost\n [String, required]: IP address of the vCenter. Example: \n172.16.68.3\n.\n\n\nuser\n [String, required]: Username for the API access. Example: \nroot\n.\n\n\npassword\n [String, required]: Password for the API access. Example: \nvmware\n\n\nhttp_logging\n [Boolean, optional]: Enables logging all HTTP requests and responses to vSphere API. Default: \nfalse\n. Available in v37+.\n\n\n\n\ndefault_disk_type\n [String, optional]: Sets the default\n  \ndisk type\n.\n  Can be either \nthin\n or \npreallocated\n, defaults to \npreallocated\n. \npreallocated\n\n  sets \"all space allocated at [VM] creation time and the space is zeroed on demand as the space is used\",\n  and \nthin\n, \"virtual disk is allocated and zeroed on demand as the space is used.\"\n  Applies to both ephemeral and persistent disks.\n\n\n\n\n\n\ndatacenters\n [Array, optional]: Array of datacenters to use for VM placement. Must have only one.\n\n\n\n\nname\n [String, required]: Datacenter name.\n\n\nvm_folder\n [String, required]: Path to a folder (relative to the datacenter) for storing created VMs. Folder will be automatically created if not found.\n\n\ntemplate_folder\n [String, required]: Path to a folder (relative to the datacenter) for storing uploaded stemcells. Folder will be automatically created if not found.\n\n\ndisk_path\n [String, required]: Path to a \ndisk\n folder for storing persistent disks. Folder will be automatically created in the datastore if not found.\n\n\ndatastore_pattern\n [String, required]: Pattern for selecting datastores for storing ephemeral disks and replicated stemcells.\n\n\npersistent_datastore_pattern\n [String, required]: Pattern for selecting datastores for storing persistent disks.\n\n\nclusters\n [Array, required]: Array of clusters to use for VM placement.\n\n\n<cluster name>\n [String, required]: Cluster name.\n\n\nresource_pool\n [String, optional]: Specific vSphere resource pool to use within the cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnsx\n [Dictionary, optional]: NSX-V configuration options.  This is required if the other NSX features are used below (e.g. 'security_groups' for \nresource_pools\n).\n\n\naddress\n [String, required]: The NSX server's address. Can be a hostname (e.g. \nnsx-server.example.com\n) or an IP address.\n\n\nuser\n [String, required]: The login username for the NSX server.\n\n\npassword\n [String, required]: The login password for the NSX server.\n\n\nca_cert\n [String, optional]: A CA certificate that can authenticate the NSX server certificate. \nRequired\n if the NSX Manager has a self-signed SSL certificate. Must be in PEM format.\n\n\n\n\n\n\nenable_auto_anti_affinity_drs_rules\n [Boolean, optional]: Creates DRS rule to place VMs on separate hosts. DRS Automation Level must be set to \"Fully Automated\"; does not work when DRS is set to \"Partially Automated\" or \"Manual\". May cause VMs to fail to power on if there are more VMs than hosts after initial deployment. Default: \nfalse\n. Available in v33+.\n\n\nnsxt\n [Dictionary, optional]: NSX-T configuration options. Available in v45+.\n\n\nhost\n [String, required]: The NSX-T server's address. Can be a hostname (e.g. \nnsx-server.example.com\n) or an IP address.\n\n\nusername\n [String, required]: The login username for the NSX-T server.\n\n\npassword\n [String, required]: The login password for the NSX-T server.\n\n\nca_cert\n [String, optional]: A CA certificate that can authenticate the NSX-T server certificate. \nRequired\n if the NSX-T Manager has a self-signed SSL certificate. Must be in PEM format.\n\n\ndefault_vif_type\n [String, optional]: Supported Types: \nPARENT\n. Default VIF type attached to logical port. Available in NSX-T v2.0+.\n\n\n\n\n\n\n\n\nNote: If the NSX-V or NSX-T Manager has a self-signed certificate, the certificate must be set in the `ca_cert` property.\n\n\n\nExample of a CPI configuration that will place VMs into \nBOSH_CL\n cluster within \nBOSH_DC\n:\n\n\nproperties\n:\n\n  \nvcenter\n:\n\n    \naddress\n:\n \n172.16.68.3\n\n    \nuser\n:\n \nroot\n\n    \npassword\n:\n \nvmware\n\n    \ndatacenters\n:\n\n    \n-\n \nname\n:\n \nBOSH_DC\n\n      \nvm_folder\n:\n \nprod-vms\n\n      \ntemplate_folder\n:\n \nprod-templates\n\n      \ndisk_path\n:\n \nprod-disks\n\n      \ndatastore_pattern\n:\n \n'^prod-ds$'\n\n      \npersistent_datastore_pattern\n:\n \n'^prod-ds$'\n\n      \nclusters\n:\n \n[\nBOSH_CL\n]\n\n\n\n\n\nExample that places VMs by default into \nBOSH_RP\n vSphere resource pool with NSX integration and enables VM anti-affinity DRS rule:\n\n\nproperties\n:\n\n  \nvcenter\n:\n\n    \naddress\n:\n \n172.16.68.3\n\n    \nuser\n:\n \nroot\n\n    \npassword\n:\n \nvmware\n\n    \ndefault_disk_type\n:\n \nthin\n\n    \nenable_auto_anti_affinity_drs_rules\n:\n \ntrue\n\n    \ndatacenters\n:\n\n    \n-\n \nname\n:\n \nBOSH_DC\n\n      \nvm_folder\n:\n \nprod-vms\n\n      \ntemplate_folder\n:\n \nprod-templates\n\n      \ndisk_path\n:\n \nprod-disks\n\n      \ndatastore_pattern\n:\n \n'\\Aprod-ds\\z'\n\n      \npersistent_datastore_pattern\n:\n \n'\\Aprod-ds\\z'\n\n      \nclusters\n:\n\n      \n-\n \nBOSH_CL\n:\n \n{\nresource_pool\n:\n \nBOSH_RP\n}\n\n    \nnsx\n:\n\n      \naddress\n:\n \n172.16.68.4\n\n      \nuser\n:\n \nadministrator@vsphere.local\n\n      \npassword\n:\n \nvmware\n\n\n\n\n\n\n\nExample Cloud Config \n\u00b6\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n  \ncloud_properties\n:\n\n    \ndatacenters\n:\n\n    \n-\n \nclusters\n:\n \n[\nz1\n:\n \n{}]\n\n\n-\n \nname\n:\n \nz2\n\n  \ncloud_properties\n:\n\n    \ndatacenters\n:\n\n    \n-\n \nclusters\n:\n \n[\nz2\n:\n \n{}]\n\n\n\nvm_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ncloud_properties\n:\n\n    \ncpu\n:\n \n2\n\n    \nram\n:\n \n1024\n\n    \ndisk\n:\n \n3240\n\n\n-\n \nname\n:\n \nlarge\n\n  \ncloud_properties\n:\n\n    \ncpu\n:\n \n2\n\n    \nram\n:\n \n4096\n\n    \ndisk\n:\n \n30_240\n\n\n\ndisk_types\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n3000\n\n\n-\n \nname\n:\n \nlarge\n\n  \ndisk_size\n:\n \n50_000\n\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.10.0.0/24\n\n    \ngateway\n:\n \n10.10.0.1\n\n    \naz\n:\n \nz1\n\n    \ndns\n:\n \n[\n8.8.8.8\n]\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nvm-net1\n\n  \n-\n \nrange\n:\n \n10.10.1.0/24\n\n    \ngateway\n:\n \n10.10.1.1\n\n    \naz\n:\n \nz1\n\n    \ndns\n:\n \n[\n8.8.8.8\n]\n\n    \ncloud_properties\n:\n\n      \nname\n:\n \nvm-net2\n\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n5\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \naz\n:\n \nz1\n\n  \nvm_type\n:\n \nlarge\n\n  \nnetwork\n:\n \ndefault\n\n\n\n\n\n\n\nNotes \n\u00b6\n\n\n\n\n\n\nAssigned VM names (e.g. \nvm-8dg349-s7cn74-...\n) should not be manually changed since the CPI uses them to find created VMs. You can use \nbosh vms --details\n to find which VM is assigned which job. VMs are also tagged with their assigned job, index and deployment.\n\n\n\n\n\n\nStorage DRS and vMotion can be used with vSphere CPI version v18 and above. For additional details see \nStorage DRS and vMotion Support\n.\n\n\n\n\n\n\nallow_mixed_datastores\n configuration has been deprecated in favor of setting same datastore pattern for \ndatastore_pattern\n and \npersistent_datastore_pattern\n keys.\n\n\n\n\n\n\nThe vSphere CPI requires access to port 80/443 for all the ESXi hosts in your\nvSphere resource pool(s).  In order to upload stemcells to vSphere, the\nvSphere CPI makes use of an API call that returns a URL that the CPI should\nmake a \nPOST\n request to in order to upload the stemcell. This URL could have\na hostname that resolves to any one of the ESXi hosts that are associated\nwith your vSphere resource pool(s).\n\n\n\n\n\n\nSetting \nenable_auto_anti_affinity_drs_rules\n to true may cause \nbosh deploy\n to fail after the initial deployment if there are more VMs than hosts. A workaround is to set \nenable_auto_anti_affinity_drs_rules\n to false to perform subsequent deployments.\n\n\n\n\n\n\nSupport for specifying Datastore Clusters for ephemeral and persistent disks is available with vSphere CPI version v47 and above. For additional detais see \nRelease Notes for v47\n\n\n\n\n\n\nVMs\n\u00b6\n\n\nVMs have randomly generated cloud identifiers, in the format \n\"vm-#{SecureRandom.uuid}\"\n. They are stored on a datacenter as follows:\n\n\ndatacenter.name/vm/vm_folder.name/vm_cid\n\n\ne.g.\n\n\nTEST_DATACENTER/vm/58bc710b-aec7-41f6-bb78-7d65f8033e51/vm-f050dbdb-ddcf-4524-b6d8-fad1135c6f7e\n\n\nYour datacenter will be queried for the path above to try and find a matching VM.\n\n\nThere is nothing in the CPI that prevents using an id generated by the cloud for the cloud id. Indeed, the AWS CPI uses the instance ids generated on AWS\nfor cloud ids. The create_vm() CPI call returns the cid of the created VM, so it's up to the CPI implementor to decide what to use for cloud id.\n\n\nAlthough it's technically possible to use the instanceUuid on vSphere (much like how we use AWS instance ids), it's worth noting that this breaks backwards compatibility and would require a fairly hefty migration. This would open up the possibility of allowing an operator to move a VM out of its containing folder on a datacenter, as it would be possible to identify a VM independent of its inventory location.\n\n\nNetworks\n\u00b6\n\n\nNetworks are uniquely identified by datacenter and network name (which must be unique within the datacenter).\n\n\nDatastores\n\u00b6\n\n\nDatastores are identified by their name and are matched by a regular expression that matches against that name. For example, consider the following datastores in folders:\n\n\n\n\n/foo/bar/datastore1\n\n\n/foo/bar/anothername\n\n\n/foo/baz/datastore1\n\n\n\n\nThe name of datastore is the last part of each line above, e.g. 'datastore1'. An operator may choose to select both \n/foo/bar/datastore1\n and \n/foo/baz/datastore1\n with the regular expression 'datastore' or even 'datastore1'. They cannot, however, select both \n/foo/bar/datastore1\n and \n/foo/bar/anothername\n by using a regular expression that matches against directory structure, e.g. \n/foo/bar\n.\n\n\nEphemeral and persistent datastores are consumed before shared datastores.\n\n\nDatastore Paths\n\u00b6\n\n\nPersistent disks are stored on datastores in the following paths:\n\n\n/<datacenter disk path from manifest>/disk-<random disk uuid>.vmdk\n\n\nLinked Clones\n\u00b6\n\n\nThe vSphere CPI uses linked clones by default. Linked clones require the clone to be on the same datastore as the source so stemcells are automatically copied to each datastore that their clones will be on. These stemcells look like \nsc-<uuid> / <datastore managed object id>\n in the inventory. In the datastore browser the \"/\" will be quoted to \"%2f\".\n\n\nClusters\n\u00b6\n\n\nEach datacenter can have multiple clusters in the cloud properties.\n\n\nA cluster is identified by its name and its datacenter. Its location within folders in each datacenter does not matter.\n\n\nIn vSphere, cluster names do not need to be unique per datacenter, only their paths needs to be unique. The current vSphere CPI code does not handle this and would only see one cluster if two had the same name.\n\n\nClusters do not have any unique ID like a VM's instanceUuid.\n\n\nVM Placement\n\u00b6\n\n\nWhen placing a VM, a cluster is chosen (along with a datastore) based on the VM's memory, ephemeral disk, and persistent disk requirements.\n\n\nVMs are placed on clusters and datastores based on a weighted random algorithm. The weights are calculated by how many times the requested memory, ephemeral and persistent disk could fit on the cluster.\n\n\nDuring VM placement local datastores and shared datastores are not treated differently. All datastores registered on a cluster are treated the same.\n\n\nLocality\n\u00b6\n\n\nWhen recreating an existing VM, the CPI tries to create it in a cluster and datastore that are near the largest of its existing persistent disks.\n\n\nDatacenters\n\u00b6\n\n\nThe vSphere CPI only supports a single datacenter and errors if more than one is defined in the manifest. It is identified by name.\n\n\nThe current code will not work with a datacenter inside a folder.\n\n\n\n\nErrors \n\u00b6\n\n\nField object_set is not optional\n\n\n\n\nThis error may occur when global CPI configuration references:\n\n\n\n\ncluster (via \nclusters\n key) that cannot be found in the datacenter\n\n\nstemcell that cannot be found in the templates folder\n\n\n\n\n...should have the following properties: [\"info.progress\", \"info.state\", \"info.result\", \"info.error\"] (VSphereCloud::CloudSearcher::MissingPropertiesException), but they were missing these: #<Set: {\"info.state\"}>\n\n\n\n\nAdd \nSystem.View\n on vCenter server level so that persistent disks can be moved between the datastores.\n\n\nField counter_id is not optional\n...lib/cloud/vsphere/client.rb:270:in `fetch_perf_metric_names'\n\n\n\n\nThe CPI requires access to performance metrics from ESXi hosts. This error may be returned if one of the hosts in the cluster is not returning these metrics (e.g. \nmemory.usage.average\n). Possible solution is to \nrestart management agents\n on the hosts.\n\n\nFailed to add disk scsi0:2.\n\n\n\n\nThis error typically occurs when persistent disk is being attached to a second VM while it is attached to another VM. That may happen when first VM was not properly deleted and BOSH is no longer aware of its existence.\n\n\nCould not acquire HTTP NFC lease, message is: 'A specified parameter was not correct.' fault cause is: '', fault message is: [], dynamic type is '', dynamic property is []'\n\n\n\n\nThe \nvCenter docs\n show that the value should be \npreset\n rather than \ndefault\n inside the OVF file. Switching \npowerOpInfo.*\n properties resolved the problem.\n\n\n\n\nNext: \nvSphere HA",
            "title": "Usage"
        },
        {
            "location": "/vsphere-cpi/#azs",
            "text": "Schema for  cloud_properties  section:   datacenters  [Array, optional]: Array of datacenters to use for VM placement. Must have only one and it must match datacenter configured in global CPI options.  name  [String, required]: Datacenter name.  clusters  [Array, required]: Array of clusters to use for VM placement.  <cluster name>  [String, required]: Cluster name.  resource_pool  [String, optional]: Name of vSphere Resource Pool to use for VM placement.  drs_rules  [Array, optional]: Array of DRS rules applied to  constrain VM placement . Must have only one.  name  [String, required]: Name of a DRS rule that the Director will create.  type  [String, required]: Type of a DRS rule. Currently only  separate_vms  is supported.           Example:  azs :  -   name :   z1 \n   cloud_properties : \n     datacenters : \n     -   name :   my-dc \n       clusters : \n       -   { my-vsphere-cluster :   { resource_pool :   my-vsphere-res-pool }}",
            "title": "AZs "
        },
        {
            "location": "/vsphere-cpi/#networks",
            "text": "Schema for  cloud_properties  section used by manual network subnet:   name  [String, required]: Name of the vSphere network. Example:  VM Network .   Note:  To assign a distributed virtual portgroup when\nthere exists a standard virtual portgroup with the same name,\nprepend the distributed virtual switch's name followed by a slash to the\ndistributed virtual portgroup, e.g.  dvs/distributed-portgroup-1 . This may\nbe required when working with VxRack. Available in v28+.  Note:  The name may also be an NSX opaque network. Available in v40+.  Example of manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     cloud_properties : \n       name :   VM Network   vSphere CPI does not support dynamic or vip networks.",
            "title": "Networks "
        },
        {
            "location": "/vsphere-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   cpu  [Integer, required]: Number of CPUs. Example:  1 .  ram  [Integer, required]: RAM in megabytes. Example:  1024 .  disk  [Integer, required]: Ephemeral disk size in megabytes. Example:  10240 .  cpu_hot_add_enabled  [Boolean, optional]: Allows operator to add additional CPU resources while the VM is on. Default:  false . Available in v21+.  memory_hot_add_enabled  [Boolean, optional]: Allows operator to add additional memory resources while the VM is on. Default:  false . Available in v21+.  nested_hardware_virtualization  [Boolean, optional]: Exposes hardware assisted virtualization to the VM. Default:  false .  datastores  [Array, optional]: Allows operator to specify a list of ephemeral datastores, datastore clusters for the VM. Datastore names are exact datastore names and not regex patterns. At least one of these datastores must be accessible from clusters provided in  resource_pools.cloud_properties / azs.cloud_properties  or in the global CPI configuration. Available in v23+. Datastore Clusters can be specified as an array of datastore cluster names. Available in v47+  datacenters  [Array, optional]: Used to override the VM placement specified under  azs.cloud_properties . The format is the same as under  AZs .  nsx  [Dictionary, optional]:  VMware NSX  additions section. Available in CPI v30+ and NSX v6.1+.  security_groups  [Array, optional]: A collection of  security group  names that the instances should belong to. The CPI will create the security groups if they do not exist.\nBOSH will also automatically create security groups based on metadata such as deployment name and instance group name. The full list of groups can be seen under  create_vm's environment groups .  lbs  [Array, optional]: A collection of  NSX Edge Load Balancers  (LBs) to which instances should be attached. The LB and  Server Pool  must exist prior to the deployment.  edge_name  [String, required]: Name of the NSX Edge.  pool_name  [String, required]: Name of the Edge's Server Pool.  security_group  [String, required]: Name of the Pool's target Security Group. The CPI will add the VM to the specified security group (creating the security group if needed), then add the security group to the specified Server Pool.  port  [Integer, required]: The port that the VM's service is listening on (e.g. 80 for HTTP).  monitor_port  [Integer, optional]: The healthcheck port that the VM is listening on. Defaults to the value of  port .      vmx_options  [Dictionary, optional]: Allows operator to specify  VM advanced configuration options . All values are subject to YAML's type interpretation, and given that for certain configuration options vSphere will accept only a specific value type please take note of the difference between values with similar appearances such as:  true  vs  \"true\"  and  \"1234\"  vs  1234 . Refer to the vSphere documentation for more information about what configuration options are accepted. Available in v42+.  nsxt  [Dictionary, optional]:  VMware NSX  additions section. Available in CPI v45+.  ns_groups  [Array, optional]: A collection of  NS Groups  names that the instances should belong to. Available in NSX-T v1.1+.  vif_type  [String, optional]: Supported types:  PARENT ,  null . Overrides the global  default_vif_type . Available in NSX-T v2.0+.     Example of a VM asked to be placed into a specific vSphere resource pool with NSX-V and NSX-T integration:  resource_pools :  -   name :   nsx \n   network :   default \n   stemcell : \n     name :   bosh-vsphere-esxi-ubuntu-trusty-go_agent \n     version :   latest \n   cloud_properties : \n     cpu :   1 \n     ram :   1_024 \n     disk :   10_240 \n     datastores :   [ prod-ds-1 ,   prod-ds-2 ,   { clusters :   [ vcpi-sp1 :   {}    ,   vcpi-sp2 :   {}]}] \n     datacenters : \n     -   name :   my-dc \n       clusters : \n       -   my-vsphere-cluster :   { resource_pool :   other-vsphere-res-pool } \n     nsx :   # NSX-V configuration \n       security_groups :   [ public ,   dmz ] \n       lbs : \n       -   edge_name :   my-lb \n         pool_name :   https-pool \n         security_group :   https-sg \n         port :   443 \n         monitor_port :   4443   # optional, defaults to `port` value \n     vmx_options : \n       sched.mem.maxmemctl :   \"1330\" \n     nsxt :   # NSX-T configuration \n       ns_groups :   [ public ,   dmz ] \n       vif_type :   PARENT",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/vsphere-cpi/#disk-pools-disk-types",
            "text": "Schema for  cloud_properties  section:    type  [String, optional]: Type of the\n   disk :\n   thick ,  thin ,  preallocated ,  eagerZeroedThick . Defaults to\n   preallocated . Available in v12. Overrides the global  default_disk_type .    datastores  [Array, optional]: List of datastore names, datastore clusters for storing persistent disks. Overrides the global  persistent_datastore_pattern . These names are exact datastore names and not regex patterns. Available in v29+. Datastore Clusters can be specified as an array of datastore cluster names. Available in v47+    Example of 10GB disk:  disk_pools :  -   name :   default \n   disk_size :   10_240   Example of disk with type eagerZeroedThick:  disk_pools :  -   name :   default \n   disk_size :   10_240 \n   cloud_properties : \n     type :   eagerZeroedThick   Example of disk stored in specific datastores:  disk_pools :  -   name :   default \n   disk_size :   10_240 \n   cloud_properties : \n     datastores :   [ 'prod-ds-1' ,   'prod-ds-2' ,   { clusters :   [ vcpi-sp1 :   {}    ,   vcpi-sp2 :   {}]}]",
            "title": "Disk Pools / Disk Types "
        },
        {
            "location": "/vsphere-cpi/#global-configuration",
            "text": "The CPI can only talk to a single vCenter installation and manage VMs within a single vSphere datacenter.  Schema:   host  [String, required]: IP address of the vCenter. Example:  172.16.68.3 .  user  [String, required]: Username for the API access. Example:  root .  password  [String, required]: Password for the API access. Example:  vmware  http_logging  [Boolean, optional]: Enables logging all HTTP requests and responses to vSphere API. Default:  false . Available in v37+.   default_disk_type  [String, optional]: Sets the default\n   disk type .\n  Can be either  thin  or  preallocated , defaults to  preallocated .  preallocated \n  sets \"all space allocated at [VM] creation time and the space is zeroed on demand as the space is used\",\n  and  thin , \"virtual disk is allocated and zeroed on demand as the space is used.\"\n  Applies to both ephemeral and persistent disks.    datacenters  [Array, optional]: Array of datacenters to use for VM placement. Must have only one.   name  [String, required]: Datacenter name.  vm_folder  [String, required]: Path to a folder (relative to the datacenter) for storing created VMs. Folder will be automatically created if not found.  template_folder  [String, required]: Path to a folder (relative to the datacenter) for storing uploaded stemcells. Folder will be automatically created if not found.  disk_path  [String, required]: Path to a  disk  folder for storing persistent disks. Folder will be automatically created in the datastore if not found.  datastore_pattern  [String, required]: Pattern for selecting datastores for storing ephemeral disks and replicated stemcells.  persistent_datastore_pattern  [String, required]: Pattern for selecting datastores for storing persistent disks.  clusters  [Array, required]: Array of clusters to use for VM placement.  <cluster name>  [String, required]: Cluster name.  resource_pool  [String, optional]: Specific vSphere resource pool to use within the cluster.        nsx  [Dictionary, optional]: NSX-V configuration options.  This is required if the other NSX features are used below (e.g. 'security_groups' for  resource_pools ).  address  [String, required]: The NSX server's address. Can be a hostname (e.g.  nsx-server.example.com ) or an IP address.  user  [String, required]: The login username for the NSX server.  password  [String, required]: The login password for the NSX server.  ca_cert  [String, optional]: A CA certificate that can authenticate the NSX server certificate.  Required  if the NSX Manager has a self-signed SSL certificate. Must be in PEM format.    enable_auto_anti_affinity_drs_rules  [Boolean, optional]: Creates DRS rule to place VMs on separate hosts. DRS Automation Level must be set to \"Fully Automated\"; does not work when DRS is set to \"Partially Automated\" or \"Manual\". May cause VMs to fail to power on if there are more VMs than hosts after initial deployment. Default:  false . Available in v33+.  nsxt  [Dictionary, optional]: NSX-T configuration options. Available in v45+.  host  [String, required]: The NSX-T server's address. Can be a hostname (e.g.  nsx-server.example.com ) or an IP address.  username  [String, required]: The login username for the NSX-T server.  password  [String, required]: The login password for the NSX-T server.  ca_cert  [String, optional]: A CA certificate that can authenticate the NSX-T server certificate.  Required  if the NSX-T Manager has a self-signed SSL certificate. Must be in PEM format.  default_vif_type  [String, optional]: Supported Types:  PARENT . Default VIF type attached to logical port. Available in NSX-T v2.0+.     Note: If the NSX-V or NSX-T Manager has a self-signed certificate, the certificate must be set in the `ca_cert` property.  Example of a CPI configuration that will place VMs into  BOSH_CL  cluster within  BOSH_DC :  properties : \n   vcenter : \n     address :   172.16.68.3 \n     user :   root \n     password :   vmware \n     datacenters : \n     -   name :   BOSH_DC \n       vm_folder :   prod-vms \n       template_folder :   prod-templates \n       disk_path :   prod-disks \n       datastore_pattern :   '^prod-ds$' \n       persistent_datastore_pattern :   '^prod-ds$' \n       clusters :   [ BOSH_CL ]   Example that places VMs by default into  BOSH_RP  vSphere resource pool with NSX integration and enables VM anti-affinity DRS rule:  properties : \n   vcenter : \n     address :   172.16.68.3 \n     user :   root \n     password :   vmware \n     default_disk_type :   thin \n     enable_auto_anti_affinity_drs_rules :   true \n     datacenters : \n     -   name :   BOSH_DC \n       vm_folder :   prod-vms \n       template_folder :   prod-templates \n       disk_path :   prod-disks \n       datastore_pattern :   '\\Aprod-ds\\z' \n       persistent_datastore_pattern :   '\\Aprod-ds\\z' \n       clusters : \n       -   BOSH_CL :   { resource_pool :   BOSH_RP } \n     nsx : \n       address :   172.16.68.4 \n       user :   administrator@vsphere.local \n       password :   vmware",
            "title": "Global Configuration "
        },
        {
            "location": "/vsphere-cpi/#example-cloud-config",
            "text": "azs :  -   name :   z1 \n   cloud_properties : \n     datacenters : \n     -   clusters :   [ z1 :   {}]  -   name :   z2 \n   cloud_properties : \n     datacenters : \n     -   clusters :   [ z2 :   {}]  vm_types :  -   name :   default \n   cloud_properties : \n     cpu :   2 \n     ram :   1024 \n     disk :   3240  -   name :   large \n   cloud_properties : \n     cpu :   2 \n     ram :   4096 \n     disk :   30_240  disk_types :  -   name :   default \n   disk_size :   3000  -   name :   large \n   disk_size :   50_000  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.10.0.0/24 \n     gateway :   10.10.0.1 \n     az :   z1 \n     dns :   [ 8.8.8.8 ] \n     cloud_properties : \n       name :   vm-net1 \n   -   range :   10.10.1.0/24 \n     gateway :   10.10.1.1 \n     az :   z1 \n     dns :   [ 8.8.8.8 ] \n     cloud_properties : \n       name :   vm-net2  compilation : \n   workers :   5 \n   reuse_compilation_vms :   true \n   az :   z1 \n   vm_type :   large \n   network :   default",
            "title": "Example Cloud Config "
        },
        {
            "location": "/vsphere-cpi/#notes",
            "text": "Assigned VM names (e.g.  vm-8dg349-s7cn74-... ) should not be manually changed since the CPI uses them to find created VMs. You can use  bosh vms --details  to find which VM is assigned which job. VMs are also tagged with their assigned job, index and deployment.    Storage DRS and vMotion can be used with vSphere CPI version v18 and above. For additional details see  Storage DRS and vMotion Support .    allow_mixed_datastores  configuration has been deprecated in favor of setting same datastore pattern for  datastore_pattern  and  persistent_datastore_pattern  keys.    The vSphere CPI requires access to port 80/443 for all the ESXi hosts in your\nvSphere resource pool(s).  In order to upload stemcells to vSphere, the\nvSphere CPI makes use of an API call that returns a URL that the CPI should\nmake a  POST  request to in order to upload the stemcell. This URL could have\na hostname that resolves to any one of the ESXi hosts that are associated\nwith your vSphere resource pool(s).    Setting  enable_auto_anti_affinity_drs_rules  to true may cause  bosh deploy  to fail after the initial deployment if there are more VMs than hosts. A workaround is to set  enable_auto_anti_affinity_drs_rules  to false to perform subsequent deployments.    Support for specifying Datastore Clusters for ephemeral and persistent disks is available with vSphere CPI version v47 and above. For additional detais see  Release Notes for v47",
            "title": "Notes "
        },
        {
            "location": "/vsphere-cpi/#errors",
            "text": "Field object_set is not optional  This error may occur when global CPI configuration references:   cluster (via  clusters  key) that cannot be found in the datacenter  stemcell that cannot be found in the templates folder   ...should have the following properties: [\"info.progress\", \"info.state\", \"info.result\", \"info.error\"] (VSphereCloud::CloudSearcher::MissingPropertiesException), but they were missing these: #<Set: {\"info.state\"}>  Add  System.View  on vCenter server level so that persistent disks can be moved between the datastores.  Field counter_id is not optional\n...lib/cloud/vsphere/client.rb:270:in `fetch_perf_metric_names'  The CPI requires access to performance metrics from ESXi hosts. This error may be returned if one of the hosts in the cluster is not returning these metrics (e.g.  memory.usage.average ). Possible solution is to  restart management agents  on the hosts.  Failed to add disk scsi0:2.  This error typically occurs when persistent disk is being attached to a second VM while it is attached to another VM. That may happen when first VM was not properly deleted and BOSH is no longer aware of its existence.  Could not acquire HTTP NFC lease, message is: 'A specified parameter was not correct.' fault cause is: '', fault message is: [], dynamic type is '', dynamic property is []'  The  vCenter docs  show that the value should be  preset  rather than  default  inside the OVF file. Switching  powerOpInfo.*  properties resolved the problem.   Next:  vSphere HA",
            "title": "Errors "
        },
        {
            "location": "/vsphere-esxi-host-failure/",
            "text": "Note: Do not follow this procedure if vSphere HA is enabled and bosh-vsphere-cpi is v30+; vSphere HA will automatically move all VMs from the failed host to other good hosts.\n\n\n\nThis topic describes how to recreate VMs in the event of an ESXi host failure.\nThe BOSH Resurrector is unable to recreate a VM on a failed ESXi host without\nmanual intervention.\nIt can not recreate a VM until the VM has\nbeen successfully deleted, and it can not delete the VM because\nthe ESXi host is unavailable.\nThe following steps will allow the Resurrector to recreate these VMs on a healthy host.\n\n\n\n\nManually remove the failed Host from its cluster to force removal of all VMs\n\n\nselect the ESXi host from the cluster: \nvCenter \u2192 Hosts and Clusters\n\u2192 \ndatacenter\n \u2192 \ncluster\n\n\nright-click the failed ESXi host\n\n\nselect \nRemove from Inventory\n\n\nRe-upload all stemcells currently in use by the director\n\n\n\n\nbosh stemcells\n\n\n+------------------------------------------+---------------+---------+-----------------------------------------+\n| Name                                     | OS            | Version | CID                                     |\n+------------------------------------------+---------------+---------+-----------------------------------------+\n| bosh-vsphere-esxi-hvm-centos-7-go_agent  | centos-7      | 3184.1  | sc-bc3d762c-71a1-4e76-ae6d-7d2d4366821b |\n| bosh-vsphere-esxi-ubuntu-trusty-go_agent | ubuntu-trusty | 3192    | sc-46509b02-a164-4306-89de-99abdaffe8a8 |\n| bosh-vsphere-esxi-ubuntu-trusty-go_agent | ubuntu-trusty | 3202    | sc-86d76a55-5bcb-4c12-9fa7-460edd8f94cf |\n| bosh-vsphere-esxi-ubuntu-trusty-go_agent | ubuntu-trusty | 3262.4* | sc-97e9ba2d-6ae0-41d1-beea-082b6635e7cb |\n+------------------------------------------+---------------+---------+-----------------------------------------+\n\n\n   - re-upload the in-use stemcells (the ones with asterisks ('*') next to their version) with the \n--fix\n flag, e.g.:\n\n\nbosh upload stemcell https://bosh.io/d/stemcells/bosh-vsphere-esxi-ubuntu-trusty-go_agent?v=3262.4 --fix\n\n\n6. Wait for the resurrector to recreate the VMs. Alternatively, force a recreate using \nbosh cck\n\n   and choose the \nRecreate\n option for each missing VM\n9. Clean-up: after the ESXi host has been recovered and added back to the cluster,\n   preferably while it's in maintenance mode, delete stemcells and powered-off, stale VMs:\n   * \nvCenter \u2192 Hosts and Clusters\n \u2192 \ndatacenter\n \u2192 \ncluster\n\n   * select the recovered ESXi host\n   * \nRelated Objects \u2192 Virtual Machines\n\n   * delete stale VMs (VMs whose name match this pattern: \nvm-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n)\n   * delete stale stemcells (VMs whose name match this pattern: \nsc-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n)\n   * VMs and stemcells can be deleted by right-clicking on them, selecting \nAll vCenter Actions \u2192 Delete from Disk\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nvSphere HA\n\n\nNext: \nRecovery from a vSphere Network Partitioning Fault",
            "title": "Recovery from an ESXi Host Failure"
        },
        {
            "location": "/vsphere-ha/",
            "text": "vSphere High Availability (HA) is a VMware product that detects ESXi host failure, for example host power off or network partition, and automatically restarts virtual machines on other hosts in the cluster.\nIt can interoperate effectively with the \nBOSH Resurrector\n, which recreates VMs if the Director loses contact with a VM's BOSH Agent.\n\n\nNote: This feature is available with bosh-vsphere-cpi v30+.\n\n\n\nvCenter Configuration\n\u00b6\n\n\nConfigure vSphere HA as follows:\n\n\n\n\n\n\nCheck \nCluster\n \u2192 Manage \u2192 Settings \u2192 vSphere HA \u2192\nEdit... \u2192 Turn on vSphere HA\n\n\n\n\n\n\nCheck \nHost Monitoring\n\n\n\n\n\n\nEnsure the Response for \nFailure conditions and VM response \u2192 Host Isolation\n is set to \nShut down and restart VMs\n\n\n\n\n\n\nBOSH Director Configuration\n\u00b6\n\n\nIncrease the timeout values of the \nBOSH Health Monitor\n on the BOSH Director to allow for smooth interoperation between BOSH and vCenter.\nWe recommend increasing the \nagent_timeout\n from the default 60s to 180s in the BOSH Director's manifest to allow vCenter time to detect the failed host:\n\n\njobs\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \nproperties\n:\n\n    \n...\n\n    \nhm\n:\n\n      \nresurrector_enabled\n:\n \ntrue\n\n      \nintervals\n:\n\n        \nagent_timeout\n:\n \n180\n\n\n\n\n\n Warning: If vSphere HA is not enabled on the cluster and a host failure occurs, the BOSH Resurrector will be unable to recreate the VMs without manual intervention.\nFollow the manual procedure as appropriate: \nHost Failure\n or \nNetwork Partition\n.\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nMigrating from one datastore to another",
            "title": "vSphere High Availability"
        },
        {
            "location": "/vsphere-ha/#vcenter-configuration",
            "text": "Configure vSphere HA as follows:    Check  Cluster  \u2192 Manage \u2192 Settings \u2192 vSphere HA \u2192\nEdit... \u2192 Turn on vSphere HA    Check  Host Monitoring    Ensure the Response for  Failure conditions and VM response \u2192 Host Isolation  is set to  Shut down and restart VMs",
            "title": "vCenter Configuration"
        },
        {
            "location": "/vsphere-ha/#bosh-director-configuration",
            "text": "Increase the timeout values of the  BOSH Health Monitor  on the BOSH Director to allow for smooth interoperation between BOSH and vCenter.\nWe recommend increasing the  agent_timeout  from the default 60s to 180s in the BOSH Director's manifest to allow vCenter time to detect the failed host:  jobs :  -   name :   bosh \n   properties : \n     ... \n     hm : \n       resurrector_enabled :   true \n       intervals : \n         agent_timeout :   180    Warning: If vSphere HA is not enabled on the cluster and a host failure occurs, the BOSH Resurrector will be unable to recreate the VMs without manual intervention.\nFollow the manual procedure as appropriate:  Host Failure  or  Network Partition .   Back to Table of Contents  Next:  Migrating from one datastore to another",
            "title": "BOSH Director Configuration"
        },
        {
            "location": "/vsphere-migrate-datastores/",
            "text": "Note: This feature is available with bosh-vsphere-cpi v9+.\n\n\n\nThis topic describes how to migrate VMs and persistent disks from one datastore to another without downtime.\n\n\n\n\n\n\nAttach new datastore(s) to the hosts where the VMs are running while keeping the old datastore(s) attached to the same hosts.\n\n\n\n\n\n\nChange deployment manifest for the Director to configure vSphere CPI to reference new datastore(s).\n\n\nproperties:\n\n  \nvsphere:\n\n    \nhost:\n \n172.16\n.\n68.3\n\n    \nuser:\n \nroot\n\n    \npassword:\n \nvmware\n\n    \ndatacenters:\n\n    \n-\n \nname:\n \nBOSH_DC\n\n      \nvm_folder:\n \nprod-vms\n\n      \ntemplate_folder:\n \nprod-templates\n\n      \ndisk_path:\n \nprod-disks\n\n      \ndatastore_pattern:\n \n'\\Anew-prod-ds\\z'\n \n#\n \n<---\n\n      \npersistent_datastore_pattern:\n \n'\\Anew-prod-ds\\z'\n \n#\n \n<---\n\n      \nclusters:\n \n[\nBOSH_CL\n]\n\n\n\n\n\n\n\n\n\nRedeploy the Director\n\n\n\n\n\n\nVerify that the Director VM's root, ephemeral and persistent disks are now on the new datastore(s).\n\n\n\n\n\n\nFor each one of the deployments managed by the Director (visible via \nbosh deployments\n), run \nbosh deploy --recreate\n so that VMs are recreated and persistent disks are reattached.\n\n\n\n\n\n\nVerify that the persistent disks and VMs were moved to new datastore(s) and there are no remaining disks in the old datastore(s).\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nNext: \nStorage DRS and vMotion Support\n\n\nPrevious: \nvSphere HA",
            "title": "Migrating Datastores"
        },
        {
            "location": "/vsphere-network-partition/",
            "text": "Note: Do not follow this procedure if vSphere HA is enabled and bosh-vsphere-cpi is v30+;\nvSphere HA will automatically recreate VMs that were on the partitioned host.\n\n\n\nThis topic describes how to recreate VMs in the event of a network partition\nthat disrupts the following:\n\n\n\n\nthe vCenter's ability to communicate with an ESXi host\n\n\nthe BOSH Director's ability to communicate with the VMs on that host.\n\n\n\n\nThere are two options.\n\n\n\n\n\n\nPower down the ESXi host. Follow the instructions to\n\nrecover from an ESXi host failure\n to recover\nyour BOSH deployment.\n\n\n\n\n\n\nIf you cannot power down your ESXi host, then you must shut down the VMs\nrunning on the partitioned ESXi host:\n\n\n\n\n\n\nDetermine which VMs are affected by using the \nbosh vms --details\n;\n     the output should resemble the following:\n\n\n+------------------------------------------------+--------------------+----+---------+-------------+-----------------------------------------+--------------------------------------+--------------+--------+\n| VM                                             | State              | AZ | VM Type | IPs         | CID                                     | Agent ID                             | Resurrection | Ignore |\n+------------------------------------------------+--------------------+----+---------+-------------+-----------------------------------------+--------------------------------------+--------------+--------+\n| dummy/0 (4f9b0722-d004-43a6-b258-adf5e2cc5c70) | running            | z1 | default | 10.85.57.7  | vm-073648a9-57da-4122-953b-5ccf5b74c563 | 98ee24dd-c7e5-4f4b-8e6f-4f3dfa4cb5b1 | active       | false  |\n| dummy/1 (df4732aa-9f4b-4635-aedb-54278b3fac31) | running            | z1 | default | 10.85.57.11 | vm-debbd710-8829-4484-9098-78a4410ed3cc | 4f3491bd-3ab8-4fa7-9930-cf0ec0a56fec | active       | false  |\n| dummy/2 (56957582-ca58-418d-a7e6-ea0151010302) | unresponsive agent | z1 | default |             | vm-c2d2a8ac-7afb-4875-9cf3-d69978c9e8c3 | d38569a5-389a-4de6-95a8-0790e8e5ede4 | active       | false  |\n| dummy/3 (60e0b351-6524-4f45-af12-953a47af5a29) | running            | z1 | default | 10.85.57.10 | vm-bf3bbeaf-3506-4fe1-9e7e-76e2c26ce5d8 | f98c9763-6518-4305-8f16-b451a36d1b91 | active       | false  |\n| dummy/4 (473a2bf2-7147-41d5-805a-532f27c6f833) | unresponsive agent | z1 | default |             | vm-2c520edb-9202-499f-a079-b3468633bd37 | 43ff0019-2af1-4c87-944b-76aa06f97b83 | active       | false  |\n+------------------------------------------------+--------------------+----+---------+-------------+-----------------------------------------+--------------------------------------+--------------+--------+\n\n\n  - Connect to the partitioned ESXi host, and using the \nCID\n from the\n  previous command find the Vmids of the VMs using the \nCID\n from the previous command,\ne.g.\n\n\nesxcli vm process list | grep -A 1 ^vm-c2d2a8ac-7afb-4875-9cf3-d69978c9e8c3\nesxcli vm process list | grep -A 1 ^vm-2c520edb-9202-499f-a079-b3468633bd37\n# We see that the WorldNumbers (World IDs) are 199401 &amp; 199751, respectively\nesxcli vm process kill --type=force --world-id=199401\nesxcli vm process kill --type=force --world-id=199751\n\n\n  - Follow the instructions \nRecover from an ESXi host failure\n.\n\n\n\n\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nRecovery from an ESXi host failure",
            "title": "Recovery from a vSphere Network Partitioning Fault"
        },
        {
            "location": "/vsphere-vmotion-support/",
            "text": "Note: Storage DRS and vMotion can be used with bosh-vsphere-cpi v18+.\n\n\n\nWarning: If a VM was accidentally deleted after a disk was migrated via DRS or vMotion, BOSH may be unable to locate the disk.\n\n\n\nTypically Storage DRS and vMotion moves attached persistent disks with the VM.\nWhen doing so it renames attached disks and places them into moved VM folder (typically called \nvm-<uuid>\n).\nPrior to bosh-vsphere-cpi v18, Storage DRS and vMotion were not supported since the CPI was unable to locate renamed disks.\nLater versions of the CPI are able to locate disks migrated by vSphere as long as the disks are attached to the VMs.\n\n\nAs VMs are recreated, the CPI will move persistent disks out of VM folders so that they are not deleted with the VMs.\nThis procedure will happen automatically when VMs are deleted (in \ndelete_vm\n CPI call) and when disks are detached (in \ndetach_disk\n CPI call).\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nMigrating from one datastore to another",
            "title": "Storage DRS and vMotion Support"
        },
        {
            "location": "/init-vsphere-rp/",
            "text": "If the BOSH director is required to be deployed within a vSphere Resource Pool, utilize the following additional CLI arguments when creating the BOSH env:\n\n\n$ bosh create-env bosh-deployment/bosh.yml \n\\\n\n    -o ... \n\\\n\n    -o bosh-deployment/vsphere/resource-pool.yml \n\\\n\n    -v ... \n\\\n\n    -v \nvcenter_rp\n=\nmy-bosh-rp\n\n\n\n\nNote that the vSphere resource pool must already be created before running the `bosh create-env` command.",
            "title": "Bootstrapping with Resource Pools"
        },
        {
            "location": "/warden-cpi/",
            "text": "Note: Updated for bosh-warden-cpi v28+.\n\n\n\nThis topic describes cloud properties for different resources created by the Warden/Garden CPI.\n\n\nAZs \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for AZs.\n\n\nExample:\n\n\nazs\n:\n\n\n-\n \nname\n:\n \nz1\n\n\n\n\n\n\n\nNetworks \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for networks.\n\n\nExample of a manual network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \nmanual\n\n  \nsubnets\n:\n\n  \n-\n \nrange\n:\n \n10.244.1.0/24\n\n    \ngateway\n:\n \n10.244.1.0\n\n    \nstatic\n:\n \n[\n10.244.1.34\n]\n\n\n\n\n\nNote: bosh-warden-cpi v24+ makes it possible to use subnets bigger than /30 as exemplified above. bosh-lite v9000.48.0 uses that newer bosh-warden-cpi.\n\n\n\nExample of a dynamic network:\n\n\nnetworks\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ntype\n:\n \ndynamic\n\n\n\n\n\nThe CPI does not support vip networks.\n\n\n\n\nResource Pools / VM Types \n\u00b6\n\n\nSchema for \ncloud_properties\n section:\n\n\n\n\nports\n [Array, optional]: Allows to define port mapping between host and associated containers. Available in v30+.\n\n\nhost\n [String, required]: Port or range of ports. Example: \n80\n.\n\n\ncontainer\n [String, optional]: Port or range of ports. Defaults to \nhost\n defined port or range. Example: \n80\n.\n\n\nprotocol\n [String, optional]: Connection protocol. Defaults to \ntcp\n. Example: \nudp\n.\n\n\n\n\nWe may add simple load balancing via iptables for testing if ports is forwarded to multiple containers.\n\n\nExample:\n\n\nvm_extensions\n:\n\n\n-\n \nname\n:\n \nexternal-access\n\n  \ncloud_properties\n:\n\n    \nports\n:\n\n    \n# Forward 80 to 80 tcp\n\n    \n-\n \nhost\n:\n \n80\n\n    \n# Forward 443 to 8443 tcp\n\n    \n-\n \nhost\n:\n \n443\n\n      \ncontainer\n:\n \n8443\n\n    \n# Forward 53 to 53 udp\n\n    \n-\n \nhost\n:\n \n53\n\n      \nprotocol\n:\n \nudp\n\n    \n# Forward 1000-2000 to 1000-2000 tcp\n\n    \n-\n \nhost\n:\n \n1000-2000\n\n\n\n\n\n\n\nDisk Pools \n\u00b6\n\n\nCurrently the CPI does not support any cloud properties for disks.\n\n\nExample of 10GB disk:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n10_240\n\n\n\n\n\n\n\nGlobal Configuration \n\u00b6\n\n\nThe CPI uses containers to represent VMs and loopback devices to represent disks. Since the CPI can only talk to a single Garden server it can only manage resources on a single machine.\n\n\nExample of a CPI configuration:\n\n\nproperties\n:\n\n  \nwarden_cpi\n:\n\n    \nloopback_range\n:\n \n[\n100\n,\n \n130\n]\n\n    \nwarden\n:\n\n      \nconnect_network\n:\n \ntcp\n\n      \nconnect_address\n:\n \n127.0.0.1:7777\n\n    \nactions\n:\n\n      \nstemcells_dir\n:\n \n\"/var/vcap/data/cpi/stemcells\"\n\n      \ndisks_dir\n:\n \n\"/var/vcap/store/cpi/disks\"\n\n      \nhost_ephemeral_bind_mounts_dir\n:\n \n\"/var/vcap/data/cpi/ephemeral_bind_mounts_dir\"\n\n      \nhost_persistent_bind_mounts_dir\n:\n \n\"/var/vcap/data/cpi/persistent_bind_mounts_dir\"\n\n    \nagent\n:\n\n      \nmbus\n:\n \nnats://nats:((nats_password))@10.244.8.2:4222\n\n      \nblobstore\n:\n\n        \nprovider\n:\n \ndav\n\n        \noptions\n:\n\n          \nendpoint\n:\n \nhttp://10.244.8.2:25251\n\n          \nuser\n:\n \nagent\n\n          \npassword\n:\n \n((blobstore_agent_password))\n\n\n\n\n\n\n\nExample Cloud Config\n\u00b6\n\n\nSee \nbosh-deployment\n.\n\n\n\n\nNotes \n\u00b6\n\n\n\n\nGarden server does not have a UI; however, you can use \ngaol CLI\n to interact with it directly.\n\n\n\n\n\n\nBack to Table of Contents",
            "title": "Usage"
        },
        {
            "location": "/warden-cpi/#azs",
            "text": "Currently the CPI does not support any cloud properties for AZs.  Example:  azs :  -   name :   z1",
            "title": "AZs "
        },
        {
            "location": "/warden-cpi/#networks",
            "text": "Currently the CPI does not support any cloud properties for networks.  Example of a manual network:  networks :  -   name :   default \n   type :   manual \n   subnets : \n   -   range :   10.244.1.0/24 \n     gateway :   10.244.1.0 \n     static :   [ 10.244.1.34 ]   Note: bosh-warden-cpi v24+ makes it possible to use subnets bigger than /30 as exemplified above. bosh-lite v9000.48.0 uses that newer bosh-warden-cpi.  Example of a dynamic network:  networks :  -   name :   default \n   type :   dynamic   The CPI does not support vip networks.",
            "title": "Networks "
        },
        {
            "location": "/warden-cpi/#resource-pools-vm-types",
            "text": "Schema for  cloud_properties  section:   ports  [Array, optional]: Allows to define port mapping between host and associated containers. Available in v30+.  host  [String, required]: Port or range of ports. Example:  80 .  container  [String, optional]: Port or range of ports. Defaults to  host  defined port or range. Example:  80 .  protocol  [String, optional]: Connection protocol. Defaults to  tcp . Example:  udp .   We may add simple load balancing via iptables for testing if ports is forwarded to multiple containers.  Example:  vm_extensions :  -   name :   external-access \n   cloud_properties : \n     ports : \n     # Forward 80 to 80 tcp \n     -   host :   80 \n     # Forward 443 to 8443 tcp \n     -   host :   443 \n       container :   8443 \n     # Forward 53 to 53 udp \n     -   host :   53 \n       protocol :   udp \n     # Forward 1000-2000 to 1000-2000 tcp \n     -   host :   1000-2000",
            "title": "Resource Pools / VM Types "
        },
        {
            "location": "/warden-cpi/#disk-pools",
            "text": "Currently the CPI does not support any cloud properties for disks.  Example of 10GB disk:  disk_pools :  -   name :   default \n   disk_size :   10_240",
            "title": "Disk Pools "
        },
        {
            "location": "/warden-cpi/#global-configuration",
            "text": "The CPI uses containers to represent VMs and loopback devices to represent disks. Since the CPI can only talk to a single Garden server it can only manage resources on a single machine.  Example of a CPI configuration:  properties : \n   warden_cpi : \n     loopback_range :   [ 100 ,   130 ] \n     warden : \n       connect_network :   tcp \n       connect_address :   127.0.0.1:7777 \n     actions : \n       stemcells_dir :   \"/var/vcap/data/cpi/stemcells\" \n       disks_dir :   \"/var/vcap/store/cpi/disks\" \n       host_ephemeral_bind_mounts_dir :   \"/var/vcap/data/cpi/ephemeral_bind_mounts_dir\" \n       host_persistent_bind_mounts_dir :   \"/var/vcap/data/cpi/persistent_bind_mounts_dir\" \n     agent : \n       mbus :   nats://nats:((nats_password))@10.244.8.2:4222 \n       blobstore : \n         provider :   dav \n         options : \n           endpoint :   http://10.244.8.2:25251 \n           user :   agent \n           password :   ((blobstore_agent_password))",
            "title": "Global Configuration "
        },
        {
            "location": "/warden-cpi/#example-cloud-config",
            "text": "See  bosh-deployment .",
            "title": "Example Cloud Config"
        },
        {
            "location": "/warden-cpi/#notes",
            "text": "Garden server does not have a UI; however, you can use  gaol CLI  to interact with it directly.    Back to Table of Contents",
            "title": "Notes "
        },
        {
            "location": "/deployment/",
            "text": "A deployment is a collection of VMs, built from a \nstemcell\n, that has been populated with specific \nreleases\n and disks that keep persistent data. These resources are created in the IaaS based on a deployment manifest and managed by the \nDirector\n, a centralized management server.\n\n\nThe deployment process begins with deciding which Operating System images (stemcells) need to be used and which software (releases) need to be deployed, how to track persistent data while a cluster is updated and transformed, and how to automate steps of deploying images to an IaaS; this includes configuring machines to use said image, and placing correct software versions onto provisioned machines. BOSH builds upon previously introduced primitives (stemcells and releases) by providing a way to state an explicit combination of stemcells, releases, and operator-specified properties in a human readable file. This file is called a deployment manifest.\n\n\nWhen a deployment manifest is uploaded to the Director, requested resources are allocated and stored. These resources form a deployment. The deployment keeps track of associated VMs and persistent disks that are attached to the VMs. Over time, as the deployment manifest changes, VMs are replaced and updated. However, persistent disks are retained and are re-attached to the newer VMs.\n\n\nA user can manage a deployment via its deployment manifest. A deployment manifest contains all needed information for tracking, managing, and updating software on the deployment's VMs. It describes the deployment in an IaaS-agnostic way. For example, to update a Zookeeper cluster (deployment is named 'zookeeper') to a later version of a Zookeeper release, one would update a few lines in the deployment manifest.\n\n\n\n\nBack to Table of Contents\n\n\nPrevious: \nWhat is a Release?",
            "title": "What is a Deployment?"
        },
        {
            "location": "/stemcell/",
            "text": "A stemcell is a versioned Operating System image wrapped with IaaS specific packaging.\n\n\nA typical stemcell contains a bare minimum OS skeleton with a few common utilities pre-installed, a BOSH Agent, and a few configuration files to securely configure the OS by default. For example: with vSphere, the official stemcell for Ubuntu Trusty is an approximately 500MB VMDK file. With AWS, official stemcells are published as AMIs that can be used in your AWS account.\n\n\nStemcells do not contain any specific information about any software that will be installed once that stemcell becomes a specialized machine in the cluster; nor do they contain any sensitive information which would make them unable to be shared with other BOSH users. This clear separation between base Operating System and later-installed software is what makes stemcells a powerful concept.\n\n\nIn addition to being generic, stemcells for one OS (e.g. all Ubuntu Trusty stemcells) are exactly the same for all infrastructures. This property of stemcells allows BOSH users to quickly and reliably switch between different infrastructures without worrying about the differences between OS images.\n\n\nThe Cloud Foundry BOSH team is responsible for producing and maintaining an official set of stemcells. Cloud Foundry currently supports Ubuntu Trusty and CentOS 6.5 on vSphere, AWS, OpenStack, and vCloud infrastructures.\n\n\nStemcells are distributed as tarballs.\n\n\nBy introducing the concept of a stemcell, the following problems have been solved:\n\n\n\n\nCapturing a base Operating System image\n\n\nVersioning changes to the Operating System image\n\n\nReusing base Operating System images across VMs of different types\n\n\nReusing base Operating System images across different IaaS\n\n\n\n\n\n\nNext: \nWhat is a Release?\n\n\nPrevious: \nWhat problems does BOSH solve?",
            "title": "What is a Stemcell?"
        },
        {
            "location": "/basic-workflow/",
            "text": "(Follow \nCreate an environment?\n to create the Director.)\n\n\nThe Director can manage multiple \ndeployments\n.\n\n\nCreation of a deployment consists of the following steps:\n\n\n\n\nCreate a \ncloud config\n\n\nCreate a \ndeployment manifest\n\n\nUpload stemcells and releases from the deployment manifest\n\n\nKick off the deploy to make a deployment on the Director\n\n\n\n\nUpdating an existing deployment is the same procedure:\n\n\n\n\nUpdate cloud config if anything changed\n\n\nUpdate deployment manifest if anything changed\n\n\nUpload new stemcells and/or releases if necessary\n\n\nKick off the deploy to apply changes to the deployment\n\n\n\n\nIn the next several steps we are going to deploy simple \nZookeeper\n deployment to the Director.\n\n\n\n\nNext: \nUpdate Cloud Config",
            "title": "Basic Workflow"
        },
        {
            "location": "/deployment-manifest/",
            "text": "Note: Once you opt into using cloud config all deployments must be converted to use manifest v2 format that disallows IaaS specific configuration. See \nmanifest v2 schema\n for allowed configurations. v257+ supports deploying both v1 and v2 manifests to the same director.\n\n\n\nThe deployment manifest is a YAML file that defines the components and properties of the deployment. When an operator initiates a new deployment using the CLI, the Director receives a version of the deployment manifest and creates a new deployment using this manifest.\n\n\nContents of a deployment manifest:\n\n\n\n\nDeployment Identification\n: A name for the deployment and the UUID of the Director managing the deployment\n\n\nReleases Block\n: Name and version of each release in a deployment\n\n\nNetworks Block\n: Network configuration information\n\n\nResource Pools Block\n: Properties of VMs that BOSH creates and manages\n\n\nDisk Pools Block\n: Properties of disk pools that BOSH creates and manages\n\n\nCompilation Block\n: Properties of compilation VMs\n\n\nUpdate Block\n: Defines how BOSH updates job instances during deployment\n\n\nJobs Block\n: Configuration and resource information for jobs\n\n\nProperties Block\n: Describes global properties and generalized configuration information\n\n\n\n\nThe examples below originate from a \nsample deployment manifest\n.\n\n\n\n\nDeployment Identification\n\u00b6\n\n\nname\n [String, required]: The name of the deployment. A single Director can manage multiple deployments and distinguishes them by name.\n\n\ndirector_uuid\n [String, required]: This string must match the UUID of the currently targeted Director for the CLI to allow any operations on the deployment. Use \nbosh status\n to display the UUID of the currently targeted Director.\n\n\nExample:\n\n\nname\n:\n \nmy-redis-deployment\n\n\n\ndirector_uuid\n:\n \ncf8dc1fc-9c42-4ffc-96f1-fbad983a6ce6\n\n\n\n\n\n\n\nReleases Block\n\u00b6\n\n\nreleases\n [Array, required]: The name and version of each release in the deployment.\n\n\n\n\nname\n [String, required]: Name of a release used in the deployment.\n\n\nversion\n [String, required]: The version of the release to use. Version can be \nlatest\n.\n\n\n\n\nExample:\n\n\nreleases\n:\n\n\n-\n \n{\nname\n:\n \nredis\n,\n \nversion\n:\n \n12\n}\n\n\n\n\n\nReleases Block using URLs\n\u00b6\n\n\nreleases\n [Array, required]: The name, url and possibly SHA1 of each release in the deployment.\n\n\n\n\nname\n [String, required]: Name of a release used in the deployment.\n\n\nurl\n [String, required]: URL of the release to use. URL may use the file protocol (\nfile://\n) or HTTP(s) (\nhttp(s)://\n). File URLs can be absolute or relative to the current directory of \nbosh-init\n execution.\n\n\nsha1\n [String, required]: The SHA1 of the release tarball. SHA1 is only required when using HTTP(s) URLs.\n\n\n\n\nExample:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nbosh\n\n  \nurl\n:\n \nhttps://bosh.io/d/github.com/cloudfoundry/bosh?v=158\n\n  \nsha1\n:\n \na97811864b96bee096477961b5b4dadd449224b4\n\n\n-\n \nname\n:\n \nbosh-aws-cpi\n\n  \nurl\n:\n \nfile://bosh-aws-cpi-release-158.tgz\n\n\n\n\n\nReleases Block using local release directory\n\u00b6\n\n\nreleases\n [Array, required]: The name and local release directory of a release in the deployment.\n\n\n\n\nname\n [String, required]: Name of a release used in the deployment.\n\n\nurl\n [String, required]: Path to release directory on local filesystem (relative to working directory).\n\n\nversion\n [String, required]: Must be \ncreate\n\n\n\n\nExample:\n\n\nreleases\n:\n\n\n-\n \nname\n:\n \nbosh-aws-cpi\n\n  \nurl\n:\n \n/Users/cloudfoundry/workspace/bosh-aws-cpi-release\n\n  \nversion\n:\n \ncreate\n\n\n\n\n\n\n\nNetworks Block\n\u00b6\n\n\nnetworks\n [Array, required]: Each sub-block listed in the Networks block specifies a network configuration that jobs can reference. There are three different network types: \nmanual\n, \ndynamic\n, and \nvip\n.\n\n\nSee \nnetworks\n for more details.\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI network cloud properties\n\n\nSee Azure CPI network cloud properties\n\n\nSee OpenStack CPI network cloud properties\n\n\nSee SoftLayer CPI network cloud properties\n\n\nSee Google Cloud Platform CPI network cloud properties\n\n\nSee vSphere CPI network cloud properties\n\n\nSee vCloud CPI network cloud properties\n\n\n\n\n\n\nResource Pools Block\n\u00b6\n\n\nresource_pools\n [Array, required]: Specifies the \nresource pools\n a deployment uses. A deployment manifest can describe multiple resource pools and uses unique names to identify and reference them.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the resource pool\n\n\nnetwork\n [String, required]: References a valid network name defined in the Networks block. Newly created resource pool VMs use the described configuration.\n\n\nsize\n [Integer, optional]: The number of VMs in the resource pool. If you omit this value, BOSH calculates the resource pool size based on the total number of job instances that belong to this resource pool. If you specify this value, BOSH requires that the size be at least as large as the total number of job instances using it.\n\n\nstemcell\n [Hash, required]: The stemcell used to create resource pool VMs.\n\n\nname\n [String, required]: The stemcell name\n\n\nversion\n [String, required]: The stemcell version\n\n\n\n\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create VMs. Most IaaSes require this. See \nCPI Specific \ncloud_properties\n below. Examples: \ninstance_type\n, \navailability_zone\n. Default is \n{}\n (empty Hash).\n\n\nenv\n [Hash, optional]: Describes the VM environment and provides a specific VM environment to the CPI \ncreate_stemcell\n call. \nenv\n data is available to BOSH Agents as VM settings. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nredis-servers\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nname\n:\n \nbosh-aws-xen-ubuntu-trusty-go_agent\n\n    \nversion\n:\n \n2708\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n    \navailability_zone\n:\n \nus-east-1c\n\n\n\n\n\nCustom bosh-init Stemcell Key Schema \n\u00b6\n\n\nstemcell\n [Hash, required]: The stemcell used to create resource pool VMs.\n\n\n\n\nurl\n [String, required]: URL of the stemcell tarball. URL may use the file protocol (\nfile://\n) or HTTP(s) (\nhttp(s)://\n). File URLs can be absolute or relative to the current directory of \nbosh-init\n execution.\n\n\nsha1\n [String, required]: The SHA1 of the stemcell tarball. SHA1 is only required when using HTTP(s) URLs.\n\n\n\n\nExample:\n\n\nresource_pools\n:\n\n\n-\n \nname\n:\n \nredis-servers\n\n  \nnetwork\n:\n \ndefault\n\n  \nstemcell\n:\n\n    \nurl\n:\n \nhttps://bosh.io/d/stemcells/bosh-aws-xen-ubuntu-trusty-go_agent?v=2950\n\n    \nsha1\n:\n \n6489bff150d1ecbd12fda30267896f041dd279f0\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nm1.small\n\n    \navailability_zone\n:\n \nus-east-1c\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI resource pool cloud properties\n\n\nSee Azure CPI resource pool cloud properties\n\n\nSee OpenStack CPI resource pool cloud properties\n\n\nSee SoftLayer CPI resource pool cloud properties\n\n\nSee Google Cloud Platform CPI resource pool cloud properties\n\n\nSee vSphere CPI resource pool cloud properties\n\n\nSee vCloud CPI resource pool cloud properties\n\n\n\n\n\n\nDisk Pools Block\n\u00b6\n\n\ndisk_pools\n [Array, required]: Specifies the \ndisk pools\n a deployment uses. A deployment manifest can describe multiple disk pools and uses unique names to identify and reference them.\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference the disk pool\n\n\ndisk_size\n [Integer, required]: Specifies the disk size. \ndisk_size\n must be a positive integer. BOSH creates a \npersistent disk\n of that size in megabytes and attaches it to each job instance VM.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create disks. Examples: \ntype\n, \niops\n. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\ndisk_pools\n:\n\n\n-\n \nname\n:\n \ndefault\n\n  \ndisk_size\n:\n \n2\n\n  \ncloud_properties\n:\n\n    \ntype\n:\n \ngp2\n\n\n\n\n\nCPI Specific \ncloud_properties\n \n\u00b6\n\n\n\n\nSee AWS CPI disk pool cloud properties\n\n\nSee Azure CPI disk pool cloud properties\n\n\nSee OpenStack CPI disk pool cloud properties\n\n\nSee SoftLayer CPI disk pool cloud properties\n\n\nSee Google Cloud Platform CPI disk pool cloud properties\n\n\nSee vSphere CPI disk pool cloud properties\n\n\nSee vCloud CPI disk pool cloud properties\n\n\n\n\n\n\nCompilation Block\n\u00b6\n\n\ncompilation\n [Hash, required]: Properties of compilation VMs.\n\n\n\n\nworkers\n [Integer, required]: The maximum number of compilation VMs.\n\n\nnetwork\n [String, required]: References a valid network name defined in the Networks block. BOSH assigns network properties to compilation VMs according to the type and properties of the specified network.\n\n\nreuse_compilation_vms\n [Boolean, optional]: If \nfalse\n, BOSH creates a new compilation VM for each new package compilation and destroys the VM when compilation is complete. If \ntrue\n, compilation VMs are re-used when compiling packages. Defaults to \nfalse\n.\n\n\ncloud_properties\n [Hash, optional]: Describes any IaaS-specific properties needed to create compilation VMs; for most IaaSes, some data here is actually required. For the Compilation Block, the required \ncloud_properties\n are the same as for Resource Pools; see the \nCPI Specific \ncloud_properties\n for Resource Pools. Examples: \ninstance_type\n, \navailability_zone\n. Default is \n{}\n (empty Hash).\n\n\n\n\nExample:\n\n\ncompilation\n:\n\n  \nworkers\n:\n \n2\n\n  \nnetwork\n:\n \ndefault\n\n  \nreuse_compilation_vms\n:\n \ntrue\n\n  \ncloud_properties\n:\n\n    \ninstance_type\n:\n \nc1.medium\n\n    \navailability_zone\n:\n \nus-east-1c\n\n\n\n\n\n\n\nUpdate Block\n\u00b6\n\n\nupdate\n [Hash, required]: This specifies instance update properties. These properties control how BOSH updates job instances during the deployment.\n\n\n\n\ncanaries\n [Integer, required]: The number of \ncanary\n instances.\n\n\nmax_in_flight\n [Integer or Percentage, required]: The maximum number of non-canary instances to update in parallel.\n\n\ncanary_watch_time\n [Integer or Range, required]\n\n\nIf the \ncanary_watch_time\n is an integer, the Director sleeps for that many milliseconds, then checks whether the canary instances are healthy.\n\n\nIf the \ncanary_watch_time\n is a range (low-high), the Director:\n\n\nWaits for \nlow\n milliseconds\n\n\nWaits until instances are healthy or \nhigh\n milliseconds have passed since instances started updating\n\n\n\n\n\n\n\n\n\n\nupdate_watch_time\n [Integer or Range, required]\n\n\nIf the \nupdate_watch_time\n is an integer, the Director sleeps for that many milliseconds, then checks whether the instances are healthy.\n\n\nIf the \nupdate_watch_time\n is a range (low-high), the Director:\n\n\nWaits for \nlow\n milliseconds\n\n\nWaits until instances are healthy or \nhigh\n milliseconds have passed since instances started updating\n\n\n\n\n\n\n\n\n\n\nserial\n [Boolean, optional]: If disabled (set to \nfalse\n), deployment jobs will be deployed in parallel, otherwise - sequentially. Instances within a deployment job will still follow \ncanary\n and \nmax_in_flight\n configuration. Defaults to \ntrue\n.\n\n\n\n\nExample:\n\n\nupdate\n:\n\n  \ncanaries\n:\n \n1\n\n  \nmax_in_flight\n:\n \n10\n\n  \ncanary_watch_time\n:\n \n1000-30000\n\n  \nupdate_watch_time\n:\n \n1000-30000\n\n\n\n\n\n\n\nJobs Block\n\u00b6\n\n\njobs\n [Array, required]: Specifies the mapping between BOSH release \njobs\n and cloud instances. Jobs are defined in the BOSH release. The Jobs block defines how BOSH associates jobs with the VMs started by the IaaS. The most commonly used job properties are:\n\n\n\n\nname\n [String, required]: A unique name used to identify and reference this  association between a BOSH release job and a VM.\n\n\ntemplates\n [Array, required]: Specifies the name and release of a job template.\n\n\nname\n [String, required]: The job template name\n\n\nrelease\n [String, required]: The release where the job template exists\n\n\nlifecycle\n [String, optional]: Specifies the kind of task the job represents. Valid values are \nservice\n and \nerrand\n; defaults to \nservice\n. A \nservice\n runs indefinitely and restarts if it fails. An \nerrand\n starts with a manual trigger and does not restart if it fails.\n\n\npersistent_disk\n [Integer, optional]: Specifies the persistent disk size; defaults to 0 (no persistent disk). If \npersistent_disk\n is a positive integer, BOSH creates a persistent disk of that size in megabytes and attaches it to each job instance VM. \nRead more about persistent disks\n\n\nproperties\n [Hash, optional]: Specifies job properties. Properties allow BOSH to configure jobs to a specific environment. \nproperties\n defined in a Job block are accessible only to that job, and override any identically named global properties.\n\n\nresource_pool\n [String, required]: A valid resource pool name from the Resource Pools block. BOSH runs instances of this job in a VM from the named resource pool.\n\n\nupdate\n [Hash, optional]: Specific update settings for this job. Use this to override \nglobal job update settings\n on a per-job basis.\n\n\ninstances\n [Integer, required]: The number of job instances. Each instance is a VM running this particular job.\n\n\nnetworks\n [Array, required]: Specifies the networks this job requires. Each network can have the following properties specified:\n\n\nname\n [String, required]: A valid network name from the Networks block\n\n\nstatic_ips\n [Array, optional]: Array of IP addresses reserved for the job on the network\n\n\ndefault\n [Array, optional]: Specifies which network components (DNS, Gateway) BOSH populates by default from this network. BOSH references this property if the Networks block defines multiple networks.\n\n\n\n\nExample:\n\n\njobs\n:\n\n  \n-\n \nname\n:\n \nredis-master\n\n    \ninstances\n:\n \n1\n\n    \ntemplates\n:\n\n    \n-\n \n{\nname\n:\n \nredis-server\n,\n \nrelease\n:\n \nredis\n}\n\n    \npersistent_disk\n:\n \n10_240\n\n    \nresource_pool\n:\n \nredis-servers\n\n    \nnetworks\n:\n\n    \n-\n \nname\n:\n \ndefault\n\n\n  \n-\n \nname\n:\n \nredis-slave\n\n    \ninstances\n:\n \n2\n\n    \ntemplates\n:\n\n    \n-\n \n{\nname\n:\n \nredis-server\n,\n \nrelease\n:\n \nredis\n}\n\n    \npersistent_disk\n:\n \n10_240\n\n    \nresource_pool\n:\n \nredis-servers\n\n    \nnetworks\n:\n\n    \n-\n \nname\n:\n \ndefault\n\n\n\n\n\n\n\nProperties Block\n\u00b6\n\n\nproperties\n [Hash, optional]: Describes global properties and general configuration information.\n\n\nGlobal properties allow BOSH to configure jobs to a specific environment. \nproperties\n defined in the \nProperties\n block are accessible to all jobs. Any identically named \nproperties\n in the \nJobs\n block will override the global property for that job.\n\n\nBy default, general configuration information resides in the template spec file of a job in a BOSH release. If you move this information from the release into the manifest, you can reconfigure a deployment by changing the manifest instead of the release. To do this, add the general configuration information to the manifest in a sub-block of the Properties block. The name of the job template identifies the sub-block.\n\n\nTypical general configuration information includes but is not limited to:\n\n\n\n\nPasswords\n\n\nAccount names\n\n\nShared secrets\n\n\nHost names\n\n\nIP addresses\n\n\nPort numbers\n\n\n\n\nExample:\n\n\nproperties\n:\n\n  \nredis\n:\n\n    \nmax_connections\n:\n \n10\n\n\n\n\n\nJob Property Precedence\n\u00b6\n\n\n\n\nBOSH applies the properties in the template spec file to the job.\n\n\nIf an identically named property exists in the \nProperties\n block of the deployment manifest, the value of this property overrides the previous value.\n\n\nIf an identically named property exists in the Properties sub-block of the \nJobs\n block of the deployment manifest, the value of this property overrides all previous values.\n\n\n\n\nNote\n: If you declare specific properties in a job template spec, BOSH ignores all other properties. If you do not declare any specific properties in a job template spec, BOSH applies all properties from the deployment manifest to the job. \n\n\n\n\n\nCloud Provider Block \n\u00b6\n\n\ncloud_provider\n [Hash, required]: Specifies CPI configuration for the \nbosh-init\n to create VMs, etc. Regular deployment manifests cannot specify this block.\n\n\n\n\ntemplate\n [Hash, required]: Specifies the name of the CPI job and release where the CPI job exists. It will be used by the \nbosh-init\n to create VMs, persistent disks, etc.\n\n\nname\n [String, required]: The CPI job name.\n\n\nrelease\n [String, required]: The CPI release name.\n\n\nmbus\n [String, required]: HTTPs URL used by the \nbosh-init\n to contact the Agent on a created VM. The URL includes basic auth credentials that should be customized for each deployment. Example: \n\"https://mbus:mbus-password@10.0.0.6:6868\"\n.\n\n\nproperties\n [Hash, required]: Properties required by the CPI job.\n\n\n\n\nExample from \nInitializing BOSH environment on vSphere\n:\n\n\ncloud_provider\n:\n\n  \ntemplate\n:\n \n{\nname\n:\n \ncpi\n,\n \nrelease\n:\n \nbosh-vsphere-cpi\n}\n\n\n  \nmbus\n:\n \n\"https://mbus:mbus-password@10.0.0.6:6868\"\n\n\n  \nproperties\n:\n\n    \nvcenter\n:\n \n{\n \n...\n \n}\n\n\n    \nagent\n:\n \n{\nmbus\n:\n \n\"https://mbus:mbus-password@0.0.0.0:6868\"\n}\n\n\n    \nblobstore\n:\n\n      \nprovider\n:\n \nlocal\n\n      \npath\n:\n \n/var/vcap/micro_bosh/data/cache\n\n\n    \nntp\n:\n\n    \n-\n \n0.pool.ntp.org\n\n    \n-\n \n1.pool.ntp.org",
            "title": "Deployment Manifest v1"
        },
        {
            "location": "/deployment-manifest/#cpi-specific-cloud_properties",
            "text": "See AWS CPI network cloud properties  See Azure CPI network cloud properties  See OpenStack CPI network cloud properties  See SoftLayer CPI network cloud properties  See Google Cloud Platform CPI network cloud properties  See vSphere CPI network cloud properties  See vCloud CPI network cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/deployment-manifest/#custom-bosh-init-stemcell-key-schema",
            "text": "stemcell  [Hash, required]: The stemcell used to create resource pool VMs.   url  [String, required]: URL of the stemcell tarball. URL may use the file protocol ( file:// ) or HTTP(s) ( http(s):// ). File URLs can be absolute or relative to the current directory of  bosh-init  execution.  sha1  [String, required]: The SHA1 of the stemcell tarball. SHA1 is only required when using HTTP(s) URLs.   Example:  resource_pools :  -   name :   redis-servers \n   network :   default \n   stemcell : \n     url :   https://bosh.io/d/stemcells/bosh-aws-xen-ubuntu-trusty-go_agent?v=2950 \n     sha1 :   6489bff150d1ecbd12fda30267896f041dd279f0 \n   cloud_properties : \n     instance_type :   m1.small \n     availability_zone :   us-east-1c",
            "title": "Custom bosh-init Stemcell Key Schema "
        },
        {
            "location": "/deployment-manifest/#cpi-specific-cloud_properties_1",
            "text": "See AWS CPI resource pool cloud properties  See Azure CPI resource pool cloud properties  See OpenStack CPI resource pool cloud properties  See SoftLayer CPI resource pool cloud properties  See Google Cloud Platform CPI resource pool cloud properties  See vSphere CPI resource pool cloud properties  See vCloud CPI resource pool cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/deployment-manifest/#cpi-specific-cloud_properties_2",
            "text": "See AWS CPI disk pool cloud properties  See Azure CPI disk pool cloud properties  See OpenStack CPI disk pool cloud properties  See SoftLayer CPI disk pool cloud properties  See Google Cloud Platform CPI disk pool cloud properties  See vSphere CPI disk pool cloud properties  See vCloud CPI disk pool cloud properties",
            "title": "CPI Specific cloud_properties "
        },
        {
            "location": "/deployment-manifest/#cloud-provider-block",
            "text": "cloud_provider  [Hash, required]: Specifies CPI configuration for the  bosh-init  to create VMs, etc. Regular deployment manifests cannot specify this block.   template  [Hash, required]: Specifies the name of the CPI job and release where the CPI job exists. It will be used by the  bosh-init  to create VMs, persistent disks, etc.  name  [String, required]: The CPI job name.  release  [String, required]: The CPI release name.  mbus  [String, required]: HTTPs URL used by the  bosh-init  to contact the Agent on a created VM. The URL includes basic auth credentials that should be customized for each deployment. Example:  \"https://mbus:mbus-password@10.0.0.6:6868\" .  properties  [Hash, required]: Properties required by the CPI job.   Example from  Initializing BOSH environment on vSphere :  cloud_provider : \n   template :   { name :   cpi ,   release :   bosh-vsphere-cpi } \n\n   mbus :   \"https://mbus:mbus-password@10.0.0.6:6868\" \n\n   properties : \n     vcenter :   {   ...   } \n\n     agent :   { mbus :   \"https://mbus:mbus-password@0.0.0.0:6868\" } \n\n     blobstore : \n       provider :   local \n       path :   /var/vcap/micro_bosh/data/cache \n\n     ntp : \n     -   0.pool.ntp.org \n     -   1.pool.ntp.org",
            "title": "Cloud Provider Block "
        },
        {
            "location": "/bosh-cli/",
            "text": "BOSH Command Line Interface (CLI) is used to interact with the Director. The CLI is written in Ruby and is distributed via \nbosh_cli\n gem.\n\n\n$ gem install bosh_cli --no-ri --no-rdoc\n\n\n\n\nNote: BOSH CLI requires Ruby 2+\n\n\n\nIf gem installation does not succeed, make sure pre-requisites for your OS are met.\n\n\nPrerequisites on Ubuntu Trusty\n\u00b6\n\n\nMake sure following packages are installed:\n\n\n$ sudo apt-get install build-essential ruby ruby-dev libxml2-dev libsqlite3-dev libxslt1-dev libpq-dev libmysqlclient-dev zlib1g-dev\n\n\n\n\nMake sure \nruby\n and \ngem\n binaries are on your path before continuing.\n\n\nPrerequisites on CentOS\n\u00b6\n\n\nMake sure following packages are installed:\n\n\n$ sudo yum install gcc ruby ruby-devel mysql-devel postgresql-devel postgresql-libs sqlite-devel libxslt-devel libxml2-devel yajl-ruby\n\n\n\n\nPrerequisites on Mac OS X\n\u00b6\n\n\nYou may see an error like this:\n\n\nERROR\n:\n  \nWhile\n \nexecuting\n \ngem\n \n...\n \n(\nGem\n::\nFilePermissionError\n)\n\n\nYou\n \ndon\n'\nt\n \nhave\n \nwrite\n \npermissions\n \nfor\n \nthe\n \n/Library/Ruby/Gems/\n2.0\n.\n0\n \ndirectory\n.\n\n\n\n\n\n\nInstead of using the system Ruby, install a separate Ruby for your own use, and switch to that one using a package like RVM, rbenv, or chruby.\n\n\nYou may see an error like this:\n\n\nThe compiler failed to generate an executable file. (RuntimeError). You have to install development tools first.\n\n\n\n\n\nMake sure you have installed Xcode and the command-line developer tools, and agreed to the license.\n\n\n$ xcode-select --install\nxcode-select: note: install requested \nfor\n \ncommand\n line developer tools\n\n\n\n\nA window will pop up saying:\n\n\nThe \"xcode-select\" command requires the command line developer tools. Would you like to install the tools now?\n\n\n\n\n\nChoose Install to continue. Choose Get Xcode to install Xcode and the command line developer tools from the App Store. If you have already installed Xcode from the App Store, you can choose Install and it will install the cli tools.\n\n\nIf you have successfully installed them, you will see this:\n\n\n$ xcode-select --install\nxcode-select: error: \ncommand\n line tools are already installed, use \n\"Software Update\"\n to install updates\n\n\n\n\nTo agree to the license:\n\n\n$ sudo xcodebuild -license",
            "title": "CLI v1 Installation"
        },
        {
            "location": "/bosh-cli/#prerequisites-on-ubuntu-trusty",
            "text": "Make sure following packages are installed:  $ sudo apt-get install build-essential ruby ruby-dev libxml2-dev libsqlite3-dev libxslt1-dev libpq-dev libmysqlclient-dev zlib1g-dev  Make sure  ruby  and  gem  binaries are on your path before continuing.",
            "title": "Prerequisites on Ubuntu Trusty"
        },
        {
            "location": "/bosh-cli/#prerequisites-on-centos",
            "text": "Make sure following packages are installed:  $ sudo yum install gcc ruby ruby-devel mysql-devel postgresql-devel postgresql-libs sqlite-devel libxslt-devel libxml2-devel yajl-ruby",
            "title": "Prerequisites on CentOS"
        },
        {
            "location": "/bosh-cli/#prerequisites-on-mac-os-x",
            "text": "You may see an error like this:  ERROR :    While   executing   gem   ...   ( Gem :: FilePermissionError )  You   don ' t   have   write   permissions   for   the   /Library/Ruby/Gems/ 2.0 . 0   directory .   Instead of using the system Ruby, install a separate Ruby for your own use, and switch to that one using a package like RVM, rbenv, or chruby.  You may see an error like this:  The compiler failed to generate an executable file. (RuntimeError). You have to install development tools first.  Make sure you have installed Xcode and the command-line developer tools, and agreed to the license.  $ xcode-select --install\nxcode-select: note: install requested  for   command  line developer tools  A window will pop up saying:  The \"xcode-select\" command requires the command line developer tools. Would you like to install the tools now?  Choose Install to continue. Choose Get Xcode to install Xcode and the command line developer tools from the App Store. If you have already installed Xcode from the App Store, you can choose Install and it will install the cli tools.  If you have successfully installed them, you will see this:  $ xcode-select --install\nxcode-select: error:  command  line tools are already installed, use  \"Software Update\"  to install updates  To agree to the license:  $ sudo xcodebuild -license",
            "title": "Prerequisites on Mac OS X"
        },
        {
            "location": "/sysadmin-commands/",
            "text": "Note: See [CLI v2](cli-v2.md) for an updated CLI.\n\n\n\nThis document lists the all CLI commands you use to perform system administration tasks.\n\n\n\n\nFor Cluster Operators \n\u00b6\n\n\nUse these commands against the Director to manage deployments and associated assets.\n\n\nDirector Location \n\u00b6\n\n\n$ bosh target \n[\nDIRECTOR_URL\n]\n \n[\nALIAS\n]\n\n\n\n\n\nConnects to the Director URL/IP that you specify for ongoing communication. You can provide \nALIAS\n to set an alias for the Director. Displays the currently targeted Director if you do not provide a URL.\n\n\n$ bosh login \n[\nUSERNAME\n]\n \n[\nPASSWORD\n]\n\n\n\n\n\nAuthenticates a user with the Director when you provide a username and password. Prompts for this information if you omit it.\n\n\n$ bosh status \n[\n--uuid\n]\n\n\n\n\n\nDisplays the configuration file and deployment manifest in use, and information about the BOSH Director such as name, URL, version, current username, UUID, and CPI.\n\n\n\n\nUsers \n\u00b6\n\n\nUse these commands to create and delete users on the Director.\n\n\n$ bosh create user \n[\nUSERNAME\n]\n \n[\nPASSWORD\n]\n\n\n\n\n\nCreates a user with the Director. Prompts you for a \nUSERNAME\n and \nPASSWORD\n if you omit this information.\n\n\n$ bosh delete user \n[\nUSERNAME\n]\n\n\n\n\n\nDeletes a specific user from the BOSH Director. Prompts you for a \nUSERNAME\n if you omit this information.\n\n\n\n\nReleases \n\u00b6\n\n\n$ bosh releases \n[\n--jobs\n]\n\n\n\n\n\nDisplays the list of available releases.\n\n\n$ bosh upload release \n[\nRELEASE_FILE\n]\n \n[\n--rebase\n]\n \n[\n--skip-if-exists\n]\n\n$ bosh upload release RELEASE_URL\n\n\n\n\nUploads the file that you specify as a release. If you do not provide a \nRELEASE_FILE\n, you must run this command from a valid release directory.\n\n\n$ bosh delete release NAME \n[\nVERSION\n]\n \n[\n--force\n]\n\n\n\n\n\nDeletes a release and associated jobs, packages, compiled packages, and all package metadata. Fails if a deployment references this release.\n\n\n$ bosh inspect release NAME/VERSION\n\n\n\n\nPrints release jobs and packages and their associated blobstore records as known by the Director.\n\n\n$ bosh \nexport\n release NAME/VERSION OS/VERSION\n\n\n\n\nExports given release as a tarball, including compiled packages for stemcell that matches OS and version.\n\n\n\n\nStemcells \n\u00b6\n\n\n$ bosh stemcells\n\n\n\n\nDisplays the name, version, and CID of available stemcells.\n\n\n$ bosh upload stemcell STEMCELL_PATH \n[\n--skip-if-exists\n]\n\n$ bosh upload stemcell STEMCELL_URL\n\n\n\n\nUploads specified stemcell. See \nstemcells section of bosh.io\n for all available stemcells.\n\n\n$ bosh delete stemcell NAME VERSION \n[\n--force\n]\n\n\n\n\n\nDeletes a stemcell and all associated compiled packages. Fails if any deployment references this stemcell.\n\n\n\n\nCloud config \n\u00b6\n\n\n$ bosh cloud-config\n\n\n\n\nDisplays current cloud config saved in the Director.\n\n\n$ bosh update cloud-config FILE_PATH\n\n\n\n\nUpdates currently saved cloud config in the Director. See \ncloud config description\n.\n\n\n\n\nRuntime config \n\u00b6\n\n\n$ bosh runtime-config\n\n\n\n\nDisplays current runtime config saved in the Director.\n\n\n$ bosh update runtime-config FILE_PATH\n\n\n\n\nUpdates currently saved runtime config in the Director. See \nruntime config description\n.\n\n\n\n\nDeployment \n\u00b6\n\n\n$ bosh deployments\n\n\n\n\nDisplays the list of created deployments on the Director. Shows stemcells and releases used by each deployment.\n\n\n$ bosh deployment \n[\nFILE_PATH\n]\n\n\n\n\n\nSwitches the CLI to operate on the deployment specified by the deployment manifest \nFILE_PATH\n. Displays the current deployment if you omit \nFILENAME\n.\n\n\n$ bosh deploy \n[\n--recreate\n]\n\n\n\n\n\nCreates/updates or recreates a deployment.\n\n\n$ bosh download manifest DEPLOYMENT_NAME \n[\nFILE_PATH\n]\n\n\n\n\n\nDownloads and saves the deployment manifest of the deployment \nDEPLOYMENT_NAME\n to \nFILE_PATH\n.\n\n\n$ bosh delete deployment DEPLOYMENT_NAME \n[\n--force\n]\n\n\n\n\n\nDeletes job instances, VMs, disks, snapshots, templates associated with the deployment \nDEPLOYMENT_NAME\n.\n\n\n\n\nJob/VM Health \n\u00b6\n\n\n$ bosh instances \n[\n--ps\n]\n \n[\n--details\n]\n \n[\n--dns\n]\n \n[\n--vitals\n]\n\n\n\n\n\nDisplays a table that provides an overview of the instances in a current deployment. You can specify the following options:\n\n\n\n\nps\n: includes process information\n\n\ndetails\n: includes VM cloud ID, agent ID\n\n\ndns\n: includes the DNS A record\n\n\nvitals\n: includes load, CPU, memory, swap, system disk, ephemeral disk, and persistent disk usage\n\n\n\n\n$ bosh vms \n[\nDEPLOYMENT_NAME\n]\n \n[\n--details\n]\n \n[\n--dns\n]\n \n[\n--vitals\n]\n\n\n\n\n\nDisplays a table that provides an overview of the VMs in \nDEPLOYMENT_NAME\n. You can specify the following options:\n\n\n\n\ndetails\n: includes VM cloud ID, agent ID\n\n\ndns\n: includes the DNS A record\n\n\nvitals\n: includes load, CPU, memory, swap, system disk, ephemeral disk, and persistent disk usage\n\n\n\n\n$ bosh recreate JOB \n[\nINDEX\n]\n \n[\n--force\n]\n\n\n\n\n\nStops the job process, recreates the VM, creates the job instance on the new VM, then starts the job process. \n--force\n\nexecutes the command even if the local manifest doesn't match the version the BOSH director has.\n\n\n$ bosh stop JOB \n[\nINDEX\n]\n \n[\n--hard\n]\n \n[\n--force\n]\n\n\n\n\n\nStops the job processes, and if \n--hard\n is specified deletes a VM keeping persistent disks.\n\n\n$ bosh start JOB \n[\nINDEX\n]\n \n[\n--force\n]\n\n\n\n\n\nCreates a VM and reattaches active persistent disk if VM does not exist, then starts the job processes.\n\n\n\n\n$ bosh vm resurrection \n[\nSTATE\n]\n\n$ bosh vm resurrection JOB INDEX \n[\nSTATE\n]\n\n\n\n\n\nSets resurrection state (\non\n or \noff\n) for all VMs managed by the Director or for a single deployment job instance.\n\n\n$ bosh cck \n[\nDEPLOYMENT_NAME\n]\n \n[\n--auto\n]\n \n[\n--report\n]\n\n\n\n\n\nScans for differences between the VM state database that the Director maintains and the actual state of the VMs. For each difference the scan detects, \nbosh cck\n offers possible repair options.\n\n\n\n\nErrands \n\u00b6\n\n\n$ bosh errands\n\n\n\n\nDisplays a table that lists all available errands for the set deployment.\n\n\n$ bosh run errand ERRAND_NAME \n[\n--download-logs\n]\n \n[\n--logs-dir DESTINATION_DIRECTORY\n]\n\n\n\n\n\nInstructs the BOSH Director to run the named errand on a job instance on a VM.\n\n\n\n\nSSH \n\u00b6\n\n\n$ bosh ssh \n[\n--gateway_host HOST\n]\n \n[\n--gateway_user USER\n]\n \n[\n--gateway_identity_file FILE\n]\n \n[\n--default_password PASSWORD\n]\n\n\n\n\n\nExecutes a command or starts an interactive shell via SSH. You can tunnel the SSH connection over a \ngateway\n by specifying additional options.\n\n\n$ bosh ssh JOB \n[\nINDEX\n]\n \n[\nCOMMANDS\n]\n\n\n\n\n\nWhen you provide arguments without an option flag, the Director executes the arguments as commands on the job VM. For example, \nbosh ssh redis 0 \"ls -R\"\n runs the \nls -R\n command on the redis/0 job VM.\n\n\n\n\nDirector Tasks \n\u00b6\n\n\n$ bosh tasks \n[\n--no-filter\n]\n\n\n\n\n\nDisplays a table that lists the following for all \ncurrently running\n tasks: ID number, state, timestamp, user, description, and result.\n\n\n$ bosh tasks recent \n[\nCOUNT\n]\n \n[\n--no-filter\n]\n\n\n\n\n\nDisplays a table that lists the following for the last \nCOUNT\n tasks: ID number, state, timestamp, user, description, and result. \nCOUNT\n defaults to 30.\n\n\n$ bosh task \n[\nTASK_ID\n]\n \n[\n--event\n]\n \n[\n--cpi\n]\n \n[\n--debug\n]\n \n[\n--result\n]\n \n[\n--raw\n]\n\n\n\n\n\nDisplays the status of a task that you specify and tracks its output. You can track only one of the following log types at a time: event, CPI, debug, or result. Defaults to event.\n\n\n\n\nLogs \n\u00b6\n\n\n$ bosh logs JOB \n[\nINDEX\n]\n \n[\n--agent\n]\n \n[\n--job\n]\n \n[\n--only filter1,filter2,...\n]\n \n[\n--dir DESTINATION_DIRECTORY\n]\n\n\n\n\n\nFetches a job or agent log from a VM. Supports custom filtering only for job logs.\n\n\n\n\nEvents \n\u00b6\n\n\n$ bosh events \n[\n--before-id ID\n]\n \n[\n--deployment NAME\n]\n \n[\n--task ID\n]\n \n[\n--instance NAME/ID\n]\n\n\n\n\n\nDisplays table that lists events based on filters specified. See \nevents details\n.\n\n\n\n\nDisks \n\u00b6\n\n\n$ bosh disks --orphaned\n\n\n\n\nDisplays disk CID, previous deployment and instance, and orphaned date of all orphaned persistent disks.\n\n\n$ bosh attach disk DISK_ID INSTANCE_NAME/ID\n\n\n\n\nAttaches persistent disk to given instance. Instance must be in the stopped state.\n\n\n\n\nSnapshots \n\u00b6\n\n\n$ bosh snapshots\n\n\n\n\nDisplays the job, CID, and created date of all snapshots.\n\n\n$ bosh take snapshot \n[\nJOB\n]\n \n[\nINDEX\n]\n\n\n\n\n\nTakes a snapshot of the job VM that you specify. If you do not specify a \nJOB\n, takes a snapshot of every VM in your deployment.\n\n\n$ bosh delete snapshots\n\n\n\n\nDeletes all snapshots.\n\n\n$ bosh delete snapshot SNAPSHOT_CID\n\n\n\n\nDeletes a snapshot.\n\n\n\n\nFor Release Maintainers \n\u00b6\n\n\nUse these commands against the Director to create and update releases.\n\n\nRelease Creation \n\u00b6\n\n\n$ bosh create release \n[\nMANIFEST_FILE\n]\n \n[\n--name NAME\n]\n \n[\n--version VERSION\n]\n \n[\n--with-tarball\n]\n \n[\n--force\n]\n\n\n\n\n\nCreates a development release.\n\n\n$ bosh create release \n[\nMANIFEST_FILE\n]\n \n[\n--name NAME\n]\n \n[\n--version VERSION\n]\n \n[\n--with-tarball\n]\n \n[\n--force\n]\n --final\n\n\n\n\nCreates a final release.\n\n\n$ bosh finalize release RELEASE_PATH \n[\n--name NAME\n]\n \n[\n--version VERSION\n]\n\n\n\n\n\nCreate a final release from a development release tarball (assumes current directory to be a release directory).\n\n\nBlobs \n\u00b6\n\n\n$ bosh blobs\n\n\n\n\nLists blobs pending an upload.\n\n\n$ bosh upload blobs\n\n\n\n\nUploads new and updated blobs to the blobstore.",
            "title": "CLI v1 Commands"
        },
        {
            "location": "/sysadmin-commands/#for-cluster-operators",
            "text": "Use these commands against the Director to manage deployments and associated assets.",
            "title": "For Cluster Operators "
        },
        {
            "location": "/sysadmin-commands/#director-location",
            "text": "$ bosh target  [ DIRECTOR_URL ]   [ ALIAS ]   Connects to the Director URL/IP that you specify for ongoing communication. You can provide  ALIAS  to set an alias for the Director. Displays the currently targeted Director if you do not provide a URL.  $ bosh login  [ USERNAME ]   [ PASSWORD ]   Authenticates a user with the Director when you provide a username and password. Prompts for this information if you omit it.  $ bosh status  [ --uuid ]   Displays the configuration file and deployment manifest in use, and information about the BOSH Director such as name, URL, version, current username, UUID, and CPI.",
            "title": "Director Location "
        },
        {
            "location": "/sysadmin-commands/#users",
            "text": "Use these commands to create and delete users on the Director.  $ bosh create user  [ USERNAME ]   [ PASSWORD ]   Creates a user with the Director. Prompts you for a  USERNAME  and  PASSWORD  if you omit this information.  $ bosh delete user  [ USERNAME ]   Deletes a specific user from the BOSH Director. Prompts you for a  USERNAME  if you omit this information.",
            "title": "Users "
        },
        {
            "location": "/sysadmin-commands/#releases",
            "text": "$ bosh releases  [ --jobs ]   Displays the list of available releases.  $ bosh upload release  [ RELEASE_FILE ]   [ --rebase ]   [ --skip-if-exists ] \n$ bosh upload release RELEASE_URL  Uploads the file that you specify as a release. If you do not provide a  RELEASE_FILE , you must run this command from a valid release directory.  $ bosh delete release NAME  [ VERSION ]   [ --force ]   Deletes a release and associated jobs, packages, compiled packages, and all package metadata. Fails if a deployment references this release.  $ bosh inspect release NAME/VERSION  Prints release jobs and packages and their associated blobstore records as known by the Director.  $ bosh  export  release NAME/VERSION OS/VERSION  Exports given release as a tarball, including compiled packages for stemcell that matches OS and version.",
            "title": "Releases "
        },
        {
            "location": "/sysadmin-commands/#stemcells",
            "text": "$ bosh stemcells  Displays the name, version, and CID of available stemcells.  $ bosh upload stemcell STEMCELL_PATH  [ --skip-if-exists ] \n$ bosh upload stemcell STEMCELL_URL  Uploads specified stemcell. See  stemcells section of bosh.io  for all available stemcells.  $ bosh delete stemcell NAME VERSION  [ --force ]   Deletes a stemcell and all associated compiled packages. Fails if any deployment references this stemcell.",
            "title": "Stemcells "
        },
        {
            "location": "/sysadmin-commands/#cloud-config",
            "text": "$ bosh cloud-config  Displays current cloud config saved in the Director.  $ bosh update cloud-config FILE_PATH  Updates currently saved cloud config in the Director. See  cloud config description .",
            "title": "Cloud config "
        },
        {
            "location": "/sysadmin-commands/#runtime-config",
            "text": "$ bosh runtime-config  Displays current runtime config saved in the Director.  $ bosh update runtime-config FILE_PATH  Updates currently saved runtime config in the Director. See  runtime config description .",
            "title": "Runtime config "
        },
        {
            "location": "/sysadmin-commands/#deployment",
            "text": "$ bosh deployments  Displays the list of created deployments on the Director. Shows stemcells and releases used by each deployment.  $ bosh deployment  [ FILE_PATH ]   Switches the CLI to operate on the deployment specified by the deployment manifest  FILE_PATH . Displays the current deployment if you omit  FILENAME .  $ bosh deploy  [ --recreate ]   Creates/updates or recreates a deployment.  $ bosh download manifest DEPLOYMENT_NAME  [ FILE_PATH ]   Downloads and saves the deployment manifest of the deployment  DEPLOYMENT_NAME  to  FILE_PATH .  $ bosh delete deployment DEPLOYMENT_NAME  [ --force ]   Deletes job instances, VMs, disks, snapshots, templates associated with the deployment  DEPLOYMENT_NAME .",
            "title": "Deployment "
        },
        {
            "location": "/sysadmin-commands/#jobvm-health",
            "text": "$ bosh instances  [ --ps ]   [ --details ]   [ --dns ]   [ --vitals ]   Displays a table that provides an overview of the instances in a current deployment. You can specify the following options:   ps : includes process information  details : includes VM cloud ID, agent ID  dns : includes the DNS A record  vitals : includes load, CPU, memory, swap, system disk, ephemeral disk, and persistent disk usage   $ bosh vms  [ DEPLOYMENT_NAME ]   [ --details ]   [ --dns ]   [ --vitals ]   Displays a table that provides an overview of the VMs in  DEPLOYMENT_NAME . You can specify the following options:   details : includes VM cloud ID, agent ID  dns : includes the DNS A record  vitals : includes load, CPU, memory, swap, system disk, ephemeral disk, and persistent disk usage   $ bosh recreate JOB  [ INDEX ]   [ --force ]   Stops the job process, recreates the VM, creates the job instance on the new VM, then starts the job process.  --force \nexecutes the command even if the local manifest doesn't match the version the BOSH director has.  $ bosh stop JOB  [ INDEX ]   [ --hard ]   [ --force ]   Stops the job processes, and if  --hard  is specified deletes a VM keeping persistent disks.  $ bosh start JOB  [ INDEX ]   [ --force ]   Creates a VM and reattaches active persistent disk if VM does not exist, then starts the job processes.   $ bosh vm resurrection  [ STATE ] \n$ bosh vm resurrection JOB INDEX  [ STATE ]   Sets resurrection state ( on  or  off ) for all VMs managed by the Director or for a single deployment job instance.  $ bosh cck  [ DEPLOYMENT_NAME ]   [ --auto ]   [ --report ]   Scans for differences between the VM state database that the Director maintains and the actual state of the VMs. For each difference the scan detects,  bosh cck  offers possible repair options.",
            "title": "Job/VM Health "
        },
        {
            "location": "/sysadmin-commands/#errands",
            "text": "$ bosh errands  Displays a table that lists all available errands for the set deployment.  $ bosh run errand ERRAND_NAME  [ --download-logs ]   [ --logs-dir DESTINATION_DIRECTORY ]   Instructs the BOSH Director to run the named errand on a job instance on a VM.",
            "title": "Errands "
        },
        {
            "location": "/sysadmin-commands/#ssh",
            "text": "$ bosh ssh  [ --gateway_host HOST ]   [ --gateway_user USER ]   [ --gateway_identity_file FILE ]   [ --default_password PASSWORD ]   Executes a command or starts an interactive shell via SSH. You can tunnel the SSH connection over a  gateway  by specifying additional options.  $ bosh ssh JOB  [ INDEX ]   [ COMMANDS ]   When you provide arguments without an option flag, the Director executes the arguments as commands on the job VM. For example,  bosh ssh redis 0 \"ls -R\"  runs the  ls -R  command on the redis/0 job VM.",
            "title": "SSH "
        },
        {
            "location": "/sysadmin-commands/#director-tasks",
            "text": "$ bosh tasks  [ --no-filter ]   Displays a table that lists the following for all  currently running  tasks: ID number, state, timestamp, user, description, and result.  $ bosh tasks recent  [ COUNT ]   [ --no-filter ]   Displays a table that lists the following for the last  COUNT  tasks: ID number, state, timestamp, user, description, and result.  COUNT  defaults to 30.  $ bosh task  [ TASK_ID ]   [ --event ]   [ --cpi ]   [ --debug ]   [ --result ]   [ --raw ]   Displays the status of a task that you specify and tracks its output. You can track only one of the following log types at a time: event, CPI, debug, or result. Defaults to event.",
            "title": "Director Tasks "
        },
        {
            "location": "/sysadmin-commands/#logs",
            "text": "$ bosh logs JOB  [ INDEX ]   [ --agent ]   [ --job ]   [ --only filter1,filter2,... ]   [ --dir DESTINATION_DIRECTORY ]   Fetches a job or agent log from a VM. Supports custom filtering only for job logs.",
            "title": "Logs "
        },
        {
            "location": "/sysadmin-commands/#events",
            "text": "$ bosh events  [ --before-id ID ]   [ --deployment NAME ]   [ --task ID ]   [ --instance NAME/ID ]   Displays table that lists events based on filters specified. See  events details .",
            "title": "Events "
        },
        {
            "location": "/sysadmin-commands/#disks",
            "text": "$ bosh disks --orphaned  Displays disk CID, previous deployment and instance, and orphaned date of all orphaned persistent disks.  $ bosh attach disk DISK_ID INSTANCE_NAME/ID  Attaches persistent disk to given instance. Instance must be in the stopped state.",
            "title": "Disks "
        },
        {
            "location": "/sysadmin-commands/#snapshots",
            "text": "$ bosh snapshots  Displays the job, CID, and created date of all snapshots.  $ bosh take snapshot  [ JOB ]   [ INDEX ]   Takes a snapshot of the job VM that you specify. If you do not specify a  JOB , takes a snapshot of every VM in your deployment.  $ bosh delete snapshots  Deletes all snapshots.  $ bosh delete snapshot SNAPSHOT_CID  Deletes a snapshot.",
            "title": "Snapshots "
        },
        {
            "location": "/sysadmin-commands/#for-release-maintainers",
            "text": "Use these commands against the Director to create and update releases.",
            "title": "For Release Maintainers "
        },
        {
            "location": "/sysadmin-commands/#release-creation",
            "text": "$ bosh create release  [ MANIFEST_FILE ]   [ --name NAME ]   [ --version VERSION ]   [ --with-tarball ]   [ --force ]   Creates a development release.  $ bosh create release  [ MANIFEST_FILE ]   [ --name NAME ]   [ --version VERSION ]   [ --with-tarball ]   [ --force ]  --final  Creates a final release.  $ bosh finalize release RELEASE_PATH  [ --name NAME ]   [ --version VERSION ]   Create a final release from a development release tarball (assumes current directory to be a release directory).",
            "title": "Release Creation "
        },
        {
            "location": "/sysadmin-commands/#blobs",
            "text": "$ bosh blobs  Lists blobs pending an upload.  $ bosh upload blobs  Uploads new and updated blobs to the blobstore.",
            "title": "Blobs "
        },
        {
            "location": "/director-backup/",
            "text": "Why performing bosh director backup and restores ?\n\u00b6\n\n\nIf using bosh-init to deploy your bosh-director, it is useful to backup the deployment state file containing associated Iaas information (IP, floating IP, persistent disk volume id). This would enable recovery of a lost bosh director VM from the persistent disk still present in the Iaas. See \nrecovering state\n.\n\n\nPerforming regular backup of the bosh director is essential to be able to operate your bosh deployments (CF, services ...) up despite a loss the the bosh director persistent disk, or an operator error such as the deletion by mistake of the director deployment. See related testimony of such \nnot fun experience\n\n\nBosh provides built-in commands to export the content of the director database and restore it on a fresh empty director deployment. The back up however does not include the data than can be recovered from artifact repositories, namely bosh stemcells and releases. The latter need to be re uploaded manually.\n\n\nBackup your bosh director\n\u00b6\n\n\nTarget the bosh director deployment that you need to backup:\n\n\nbosh deployment your_bosh_director_manifest\n\n\nMake a backup of your bosh instance :\n\n\nbosh backup\n\n\nThis command will generate a .tar.gz file that contains a dump of director's database\n\n\nRestore the backup following an outage ## \n\u00b6\n\n\nWe assume that your bosh director deployment was deleted so you need to restore it.\n\n\nThe first step is to deploy a new fresh empty bosh director deployment.\n\n\nThe second step is to restore the content of the bosh director db\n\n\n\n\nConnect to your bosh instance (using \nbosh target https://your_bosh_ip\n).\n\n\nUse the bosh restore command : \nbosh restore yourBackupFile.tar.gz\n\n\n\n\nThe third step is to manually re-upload the releases and stemcells binaries.\n\n\n\n\nList the expected stemcells and releases\n\n\n\n\nbosh stemcells\n\n \nbosh releases\n\n\nThen upload stemcells/releases from your repositories such as bosh.io, using the \n--fix\n option so bosh will fix them in the db (fixing the missing blobs). Without the \n--fix\n option, bosh would complain about duplicate stemcells/releases with same name.\n\n\nbosh upload stemcell https://stemcells_url --fix\n\n \nbosh upload releases https://release_url --fix\n\n\nFollowing these restoration steps, the bosh director is now able to manage your previous deployments.\n\n\nbosh cloudcheck\n\n\nbosh instances --ps\n\n\nYou can now safely update your deployments using the usual deploy command.\n\n\nbosh deploy",
            "title": "CLI v1 Backup / Restore"
        },
        {
            "location": "/director-backup/#restore-the-backup-following-an-outage",
            "text": "We assume that your bosh director deployment was deleted so you need to restore it.  The first step is to deploy a new fresh empty bosh director deployment.  The second step is to restore the content of the bosh director db   Connect to your bosh instance (using  bosh target https://your_bosh_ip ).  Use the bosh restore command :  bosh restore yourBackupFile.tar.gz   The third step is to manually re-upload the releases and stemcells binaries.   List the expected stemcells and releases   bosh stemcells \n  bosh releases  Then upload stemcells/releases from your repositories such as bosh.io, using the  --fix  option so bosh will fix them in the db (fixing the missing blobs). Without the  --fix  option, bosh would complain about duplicate stemcells/releases with same name.  bosh upload stemcell https://stemcells_url --fix \n  bosh upload releases https://release_url --fix  Following these restoration steps, the bosh director is now able to manage your previous deployments.  bosh cloudcheck  bosh instances --ps  You can now safely update your deployments using the usual deploy command.  bosh deploy",
            "title": "Restore the backup following an outage ## "
        },
        {
            "location": "/stemcells/",
            "text": "",
            "title": "Stemcells"
        },
        {
            "location": "/releases/",
            "text": "",
            "title": "Releases"
        }
    ]
}